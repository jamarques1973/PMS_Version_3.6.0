{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxHRSP0urTVWTp1eM1WRf6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7fc0afe58bdf453b9fc9a8d253d9b15e": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2876e80d1e6443f294166860dc38ac7f",
            "msg_id": "",
            "outputs": []
          }
        },
        "2876e80d1e6443f294166860dc38ac7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5842eef7f7f44ac2a75bbdb06d2d7587": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edd237a2675c4e02b880078db1cf977a": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_5842eef7f7f44ac2a75bbdb06d2d7587",
            "msg_id": "",
            "outputs": []
          }
        },
        "a7a278021a7c487c8439839997017fd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2e2646a3891471b8807094aac79a861": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a7a278021a7c487c8439839997017fd1",
            "msg_id": "",
            "outputs": []
          }
        },
        "c8a60fd4d33740afbffc2aa693697f85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f430a6c173b4747a76e9507b7de9304": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c8a60fd4d33740afbffc2aa693697f85",
            "msg_id": "",
            "outputs": []
          }
        },
        "8c83ecff325b4735bf4fc5ff385b4e81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "670dc6aeae984c3b86a690f03651ad34": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_8c83ecff325b4735bf4fc5ff385b4e81",
            "msg_id": "",
            "outputs": []
          }
        },
        "f829dc46033642c9adfc93e550750301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4276874e205f4773a23b128ecf1d0251": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f829dc46033642c9adfc93e550750301",
            "msg_id": "",
            "outputs": []
          }
        },
        "919e3eff833541f9ae00a2c385b32a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3eb0a55f8044831aa45e116d4a6730e": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_919e3eff833541f9ae00a2c385b32a0e",
            "msg_id": "",
            "outputs": []
          }
        },
        "0f5817d01b0f40318504805491728c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98241a67dd0e4d9592eff31baf72ef30": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_0f5817d01b0f40318504805491728c5e",
            "msg_id": "",
            "outputs": []
          }
        },
        "3cbc1394f08e4127ab3393b7165417fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f7f7b60ab3406a8455a1ca982c88c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fee1b1861c8442ebb2243e5de847f2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "ba296edc19694bd195af5396c36787ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33c1a4964ec045218f32535f0ce9968b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c61e73ba88ba426cbbed8cfed13269be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62694866e04b47928a2722c9b29bc07e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "40%"
          }
        },
        "2c8478edfaf94d7f912022b6d1185ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7cd40e9f8284f459d592efde209ec53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "593a5f8c069e4061995b8a0fbeddc33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "092e670f9b834b239aa0bf5ef521cb53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3222d4fb5c1e4138a594cf9edbaa0748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "d409974836ee4c2a965e721ad347c460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de1a11fa427646d3becc2e8c99ab2812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "847b518cb96948009cb97e0ae3681e4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d6a5470f567411289ce5def9c347abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "51dbdea738db4b688df8d4d2971babfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e928878511c742c2989a9e888e99797f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "b6fceaefb91545068c9be746981e9cf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6c793e725154a2298631adf6d807ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "a7332fe1349248c0bb14222040716e88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2afeee701b564b6eaec77ebe5c06e34d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "24d2633945b14ff0932130d6dc6b5171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bed4efd79e543049a04c1df3c72a50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "green",
            "description_width": ""
          }
        },
        "6e91c4f5807241a09441a1f7c1b1bf84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bac08d2ef8849fbbba1549429f404a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494affada79849c4a974652fe7a070c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9584861d1474450da2aac186b1a4481c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SelectMultipleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SelectMultipleModel",
            "_options_labels": [
              "Pearson",
              "Spearman",
              "Mutualinfo",
              "Boruta",
              "UMAP",
              "Todos"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "SelectMultipleView",
            "description": "Mtodos Seleccin Variables X:",
            "description_tooltip": null,
            "disabled": false,
            "index": [],
            "layout": "IPY_MODEL_0fee1b1861c8442ebb2243e5de847f2b",
            "rows": 5,
            "style": "IPY_MODEL_ba296edc19694bd195af5396c36787ae"
          }
        },
        "04784ca9825a4257a788a76835bd7493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SelectMultipleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SelectMultipleModel",
            "_options_labels": [
              "RandomSearch",
              "BayesianOptimization",
              "Hyperband",
              "Optuna",
              "Todos"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "SelectMultipleView",
            "description": "Motores:",
            "description_tooltip": null,
            "disabled": false,
            "index": [],
            "layout": "IPY_MODEL_33c1a4964ec045218f32535f0ce9968b",
            "rows": 5,
            "style": "IPY_MODEL_c61e73ba88ba426cbbed8cfed13269be"
          }
        },
        "32d0722e22284d2999579e244303881e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4465fd38a444c68b0f67fe7ab67457a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "pocas:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_092e670f9b834b239aa0bf5ef521cb53",
            "max": 200,
            "min": 1,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 10,
            "style": "IPY_MODEL_3222d4fb5c1e4138a594cf9edbaa0748",
            "value": 200
          }
        },
        "dc3686da512447a6ac66663c11e39abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "Capas:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_d409974836ee4c2a965e721ad347c460",
            "max": 6,
            "min": 1,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_de1a11fa427646d3becc2e8c99ab2812",
            "value": 3
          }
        },
        "2786b5a4367e4213a765e3963b277f51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "097e842afc9e4bc8bab29ca22ba6c07b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "Neuronas/capa:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_847b518cb96948009cb97e0ae3681e4b",
            "max": 512,
            "min": 256,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 8,
            "style": "IPY_MODEL_6d6a5470f567411289ce5def9c347abd",
            "value": 256
          }
        },
        "4ef6c8c5c1c64d25b3252f9f509bf7a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "Dropout:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_51dbdea738db4b688df8d4d2971babfc",
            "max": 0.7,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 0.05,
            "style": "IPY_MODEL_e928878511c742c2989a9e888e99797f",
            "value": 0.2
          }
        },
        "f006cb096af84ce5b75adbd83043108f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "L2 Reg:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b6fceaefb91545068c9be746981e9cf7",
            "max": 0.01,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 0.0005,
            "style": "IPY_MODEL_f6c793e725154a2298631adf6d807ce7",
            "value": 0.001
          }
        },
        "5ba19ca33a174db68883aa8712d71578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d22b5d395913418aa608ea277ceafa3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bac08d2ef8849fbbba1549429f404a0",
            "placeholder": "",
            "style": "IPY_MODEL_494affada79849c4a974652fe7a070c6",
            "value": "<h3> Configuracin Optimizacin NN</h3>"
          }
        },
        "9e4ab1aaac1e44e290d27f59d9be0bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cbc1394f08e4127ab3393b7165417fd",
            "placeholder": "",
            "style": "IPY_MODEL_92f7f7b60ab3406a8455a1ca982c88c1",
            "value": "\n<h4> Explicacin de Parmetros</h4>\n<ul>\n  <li><b>Mtodos X:</b> Mtodos de seleccin de variables predictoras. Usan correlaciones estadsticas o algoritmos de reduccin de dimensin. <br>\n      <i>Pearson</i> y <i>Spearman</i>: correlaciones lineales y montonas.<br>\n      <i>MutualInfo</i>: mide dependencia informacional. <br>\n      <i>Boruta</i>: seleccin envolvente basada en rboles. <br>\n      <i>UMAP</i>: reduccin no lineal de dimensiones. <br>\n      <b>Todos</b> ejecuta cada uno secuencialmente.</li>\n  <li><b>Motores:</b> Algoritmos de optimizacin de hiperparmetros. <br>\n      <i>RandomSearch</i>: bsqueda aleatoria. <br>\n      <i>BayesianOptimization</i>: estima regiones ptimas. <br>\n      <i>Hyperband</i>: eficiente para grandes espacios de bsqueda. <br>\n      <i>Optuna</i>: flexible y potente. <br>\n      <b>Todos</b> ejecuta todos los motores.</li>\n  <li><b>Funcin objetivo:</b> Mtrica a maximizar o minimizar: <br>\n      <i>R2</i>: se desea maximizar. <i>MAE</i> y <i>MSE</i>: se minimizan.</li>\n  <li><b>Trials:</b> Nmero de combinaciones a evaluar en la bsqueda.</li>\n  <li><b>pocas:</b> Iteraciones completas sobre el dataset de entrenamiento (100 a 2000 recomendado).</li>\n  <li><b>Capas:</b> Cantidad de capas ocultas en la red (1 a 20 habitual, mximo 100 para pruebas avanzadas).</li>\n  <li><b>Neuronas/capa:</b> Nmero de neuronas por capa (32 a 512 recomendado).</li>\n  <li><b>Dropout:</b> Fraccin de neuronas descartadas en entrenamiento (0.1 a 0.4 recomendado).</li>\n  <li><b>L2 Reg:</b> Regularizacin L2 para evitar sobreajuste (0.001 a 0.01 habitual).</li>\n</ul>\n"
          }
        },
        "b87174a312154bb0b21f1fa9b4a10ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9584861d1474450da2aac186b1a4481c",
              "IPY_MODEL_04784ca9825a4257a788a76835bd7493"
            ],
            "layout": "IPY_MODEL_32d0722e22284d2999579e244303881e"
          }
        },
        "0b83efb382274762948ec1b1c2b21c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "R2",
              "MAE",
              "MSE"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Funcin objetivo:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_62694866e04b47928a2722c9b29bc07e",
            "style": "IPY_MODEL_2c8478edfaf94d7f912022b6d1185ad8"
          }
        },
        "2ea64f2fbd8644dea97dd6ee08fe22bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "Trials:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b7cd40e9f8284f459d592efde209ec53",
            "max": 50,
            "min": 1,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_593a5f8c069e4061995b8a0fbeddc33a",
            "value": 10
          }
        },
        "1157881e56804c99a5f1f8ef1b4d1850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4465fd38a444c68b0f67fe7ab67457a",
              "IPY_MODEL_dc3686da512447a6ac66663c11e39abf"
            ],
            "layout": "IPY_MODEL_2786b5a4367e4213a765e3963b277f51"
          }
        },
        "d26b4c62ddb04a9bb824b9ce2ce306ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_097e842afc9e4bc8bab29ca22ba6c07b",
              "IPY_MODEL_4ef6c8c5c1c64d25b3252f9f509bf7a3",
              "IPY_MODEL_f006cb096af84ce5b75adbd83043108f"
            ],
            "layout": "IPY_MODEL_5ba19ca33a174db68883aa8712d71578"
          }
        },
        "71d421482b4a4860b56eb229f708eb33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": " Ejecutar Optimizacin NN",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_a7332fe1349248c0bb14222040716e88",
            "style": "IPY_MODEL_2afeee701b564b6eaec77ebe5c06e34d",
            "tooltip": ""
          }
        },
        "e5f7f55cd2414b5883aed40094410e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "Progreso:",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24d2633945b14ff0932130d6dc6b5171",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bed4efd79e543049a04c1df3c72a50f",
            "value": 0
          }
        },
        "adcf3930810b4875ac661551e21bc852": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_6e91c4f5807241a09441a1f7c1b1bf84",
            "msg_id": "",
            "outputs": []
          }
        },
        "e5106a7f20a845479749080ee3f6ffa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c15b5a7938b345e683e0c62506fa87b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce88efe7cbd04ce28e42da378097baf5": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c15b5a7938b345e683e0c62506fa87b5",
            "msg_id": "",
            "outputs": []
          }
        },
        "e82419e2f78c4765a900e8f7a2ec581f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c35f63e727cf43ce9472b8dde6be478d": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e82419e2f78c4765a900e8f7a2ec581f",
            "msg_id": "",
            "outputs": []
          }
        },
        "afd56917323c48ddbd8659d5a5153fd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d3da4f4034427f82d2a7c1a14c6fdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "21f23b86578b43029711ab0c738bc1da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "190px"
          }
        },
        "86c337421d204c7dbd1e7b5d3ea6b6ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f1fcb04c6cd643b3b68973c983929a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "140px"
          }
        },
        "70e1c0808ecd4e418817eba1b2ffc1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "98915d7e2be14836a5e833ee30db1402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid #ccc",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": "600px",
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": "hidden",
            "overflow_y": "auto",
            "padding": "12px",
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "292cdd96db474a32b72c0d7d19865225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": "700px",
            "min_height": null,
            "min_width": "400px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "1051c382444545a499c6c163113d973e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "160px"
          }
        },
        "6b6a1b2ed63f417a8712d79c54ba09e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36102fdd4844493b8c06ce22f319fac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "946d76c552b448279f2c0f2a2f372387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "Seleccionar Siguiente",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_21f23b86578b43029711ab0c738bc1da",
            "style": "IPY_MODEL_86c337421d204c7dbd1e7b5d3ea6b6ea",
            "tooltip": ""
          }
        },
        "584346a65568423da688e3063e5fb840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Ejecutar",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f1fcb04c6cd643b3b68973c983929a4e",
            "style": "IPY_MODEL_70e1c0808ecd4e418817eba1b2ffc1dd",
            "tooltip": ""
          }
        },
        "cdfca94dd0254e92a87f2a0ba02b706b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f03d3c0e8f3a47c5a836b9ecd1f16bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b6a1b2ed63f417a8712d79c54ba09e0",
            "placeholder": "",
            "style": "IPY_MODEL_36102fdd4844493b8c06ce22f319fac2",
            "value": "<h3 style='font-size:1.3rem;margin-bottom:5px;'> Men Principal</h3>"
          }
        },
        "45661577f69949c0a69dac54bd7383cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9c0b3a5334f40508989952c82e744fb",
              "IPY_MODEL_d16186e75a0c4b4fb69fa013c0f42eb9"
            ],
            "layout": "IPY_MODEL_d4d3da4f4034427f82d2a7c1a14c6fdd"
          }
        },
        "b87506a2d19943709a4a31d60f13feda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_946d76c552b448279f2c0f2a2f372387",
              "IPY_MODEL_584346a65568423da688e3063e5fb840"
            ],
            "layout": "IPY_MODEL_cdfca94dd0254e92a87f2a0ba02b706b"
          }
        },
        "81f71060f06a467f938037fdd945a03b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_98915d7e2be14836a5e833ee30db1402",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "VBox(children=(HTML(value=\"<h2 style='color:#1E90FF;'> AYUDA GLOBAL  Gua Completa de la Aplicacin</h2>\"), ",
                  "application/vnd.jupyter.widget-view+json": {
                    "version_major": 2,
                    "version_minor": 0,
                    "model_id": "9cd63676a2f84d8a883b0261c17ada56"
                  }
                },
                "metadata": {
                  "application/vnd.jupyter.widget-view+json": {
                    "colab": {
                      "custom_widget_manager": {
                        "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                      }
                    }
                  }
                }
              }
            ]
          }
        },
        "0c513d02f1914314a848683355ef1c6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cabff7d374b4ad7b4467c8baa6b23c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f03d3c0e8f3a47c5a836b9ecd1f16bd1",
              "IPY_MODEL_45661577f69949c0a69dac54bd7383cf",
              "IPY_MODEL_b87506a2d19943709a4a31d60f13feda",
              "IPY_MODEL_81f71060f06a467f938037fdd945a03b"
            ],
            "layout": "IPY_MODEL_0c513d02f1914314a848683355ef1c6d"
          }
        },
        "d9c0b3a5334f40508989952c82e744fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Bienvenida",
              "Ayuda General",
              "Bloque 1  Carga y segmentacin de datos y Seleccin Variables",
              "Bloque 2  Entrenamiento de modelos IA y Prediccin de Salidas",
              "Bloque 3  Optimizacin de Modelos IA",
              "Bloque 4  Inteligencia Artificial Explicativa xIA",
              "Generar Informe Final"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Nivel-1:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_292cdd96db474a32b72c0d7d19865225",
            "style": "IPY_MODEL_1051c382444545a499c6c163113d973e"
          }
        },
        "5db0ca6dc6d64f2b9461c2fa7ac88c36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "150px"
          }
        },
        "7ffe7c666e02400f82a48375ecc0109c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1a141e8ccd144bf7bbd16a39863e4064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "warning",
            "description": " Limpiar pantalla",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5db0ca6dc6d64f2b9461c2fa7ac88c36",
            "style": "IPY_MODEL_7ffe7c666e02400f82a48375ecc0109c",
            "tooltip": ""
          }
        },
        "ccbaf9630780425ea9e222053bd354c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": "700px",
            "min_height": null,
            "min_width": "400px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "385259253b7f4fd0aa376c07a5c7969a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "160px"
          }
        },
        "d16186e75a0c4b4fb69fa013c0f42eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "2. Ayuda Global"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Nivel-2:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_ccbaf9630780425ea9e222053bd354c6",
            "style": "IPY_MODEL_385259253b7f4fd0aa376c07a5c7969a"
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamarques1973/PMS_Version_3.6.0/blob/main/PMS_3_6_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "BBBS6j7M2b6b",
        "outputId": "f01a8a7e-eeac-49f1-b527-b25a4cbd7436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=4dec1d320ec659800d563746a1606dfbe70ebcff702a0ab2d1341b9cdf4d5ce4\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n",
            "Collecting interpret\n",
            "  Downloading interpret-0.7.1-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting interpret-core==0.7.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading interpret_core-0.7.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.5.1)\n",
            "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.9.5)\n",
            "Requirement already satisfied: ipykernel>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.17.1)\n",
            "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (7.34.0)\n",
            "Requirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.24.1)\n",
            "Collecting SALib>=1.3.3 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading salib-1.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: shap>=0.28.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.48.0)\n",
            "Requirement already satisfied: dill>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.3.8)\n",
            "Collecting aplr>=10.6.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading aplr-10.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting dash<3.0.0,>=2.0.0 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting dash-cytoscape>=0.1.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading dash_cytoscape-1.0.2.tar.gz (4.0 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gevent>=1.3.6 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading gevent-25.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.32.3)\n",
            "Collecting Flask<3.1,>=1.0.4 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting Werkzeug<3.1 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting dash-html-components==2.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.14.1)\n",
            "Collecting retrying (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading retrying-1.4.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (75.2.0)\n",
            "Requirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Collecting zope.event (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading zope_event-5.1.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting zope.interface (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.1.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (25.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.7.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=3.8.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.7.14)\n",
            "Requirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.70.16)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18.1->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.1)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.2.1)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.8.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.23.0)\n",
            "Collecting setuptools (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.3.8)\n",
            "Downloading interpret-0.7.1-py3-none-any.whl (1.4 kB)\n",
            "Downloading interpret_core-0.7.1-py3-none-any.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aplr-10.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading gevent-25.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading salib-1.5.1-py3-none-any.whl (778 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m778.9/778.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.1-py3-none-any.whl (12 kB)\n",
            "Downloading zope_event-5.1.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: dash-cytoscape\n",
            "  Building wheel for dash-cytoscape (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-cytoscape: filename=dash_cytoscape-1.0.2-py3-none-any.whl size=4010717 sha256=97e38ca810edc348bb62dcd429dc7c30404a5b90420bb55a4210eccd4dd965ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/b1/ab/6c999ab288b4849d372e23c0a8f6ece7edb7ffeb8c97959ab0\n",
            "Successfully built dash-cytoscape\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, Werkzeug, setuptools, retrying, aplr, zope.interface, zope.event, Flask, SALib, interpret-core, gevent, dash, dash-cytoscape, interpret\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 3.1.1\n",
            "    Uninstalling Flask-3.1.1:\n",
            "      Successfully uninstalled Flask-3.1.1\n",
            "Successfully installed Flask-3.0.3 SALib-1.5.1 Werkzeug-3.0.6 aplr-10.9.0 dash-2.18.2 dash-core-components-2.0.0 dash-cytoscape-1.0.2 dash-html-components-2.0.0 dash-table-5.0.0 gevent-25.5.1 interpret-0.7.1 interpret-core-0.7.1 retrying-1.4.1 setuptools-80.9.0 zope.event-5.1.1 zope.interface-7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "34003bbd2f48468887caa423bbb19295"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boruta\n",
            "  Downloading Boruta-0.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from boruta) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (3.6.0)\n",
            "Downloading Boruta-0.4.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: boruta\n",
            "Successfully installed boruta-0.4.3\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.9.post2)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn>=1.6 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6->umap-learn) (3.6.0)\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "Paths used for configuration of notebook: \n",
            "    \t\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (80.9.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "Paths used for configuration of notebook: \n",
            "    \t\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\n",
            "Collecting optuna-integration[sklearn]\n",
            "  Downloading optuna_integration-4.4.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.4)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from optuna-integration[sklearn]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from optuna-integration[sklearn]) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from optuna-integration[sklearn]) (1.16.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->optuna-integration[sklearn]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->optuna-integration[sklearn]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->optuna-integration[sklearn]) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->optuna-integration[sklearn]) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->optuna-integration[sklearn]) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->optuna-integration[sklearn]) (1.17.0)\n",
            "Downloading optuna_integration-4.4.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: optuna-integration\n",
            "Successfully installed optuna-integration-4.4.0\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (25.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.14.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n",
            "Requirement already satisfied: interpret in /usr/local/lib/python3.11/dist-packages (0.7.1)\n",
            "Requirement already satisfied: interpret-core==0.7.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.5.1)\n",
            "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.9.5)\n",
            "Requirement already satisfied: ipykernel>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.17.1)\n",
            "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (7.34.0)\n",
            "Requirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.24.1)\n",
            "Requirement already satisfied: SALib>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.5.1)\n",
            "Requirement already satisfied: shap>=0.28.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.48.0)\n",
            "Requirement already satisfied: dill>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.3.8)\n",
            "Requirement already satisfied: aplr>=10.6.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (10.9.0)\n",
            "Requirement already satisfied: dash<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.18.2)\n",
            "Requirement already satisfied: dash-cytoscape>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.0.2)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (25.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.32.3)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.3)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.6)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.14.1)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.4.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (80.9.0)\n",
            "Requirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.1.1)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (7.2)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.1.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (25.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.7.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=3.8.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.7.14)\n",
            "Requirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.70.16)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18.1->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.1)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.2.1)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.8.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.23.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.3.8)\n",
            "Found existing installation: interpret 0.7.1\n",
            "Uninstalling interpret-0.7.1:\n",
            "  Successfully uninstalled interpret-0.7.1\n",
            "Found existing installation: interpret-core 0.7.1\n",
            "Uninstalling interpret-core-0.7.1:\n",
            "  Successfully uninstalled interpret-core-0.7.1\n",
            "\u001b[33mWARNING: Skipping interpret as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting interpret\n",
            "  Downloading interpret-0.7.1-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting interpret-core==0.7.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading interpret_core-0.7.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.5.1)\n",
            "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.9.5)\n",
            "Requirement already satisfied: ipykernel>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.17.1)\n",
            "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (7.34.0)\n",
            "Requirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.24.1)\n",
            "Requirement already satisfied: SALib>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.5.1)\n",
            "Requirement already satisfied: shap>=0.28.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.48.0)\n",
            "Requirement already satisfied: dill>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.3.8)\n",
            "Requirement already satisfied: aplr>=10.6.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (10.9.0)\n",
            "Requirement already satisfied: dash<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.18.2)\n",
            "Requirement already satisfied: dash-cytoscape>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.0.2)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (25.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.32.3)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.3)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.6)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.14.1)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.4.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (80.9.0)\n",
            "Requirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.1.1)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (7.2)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.1.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (25.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.7.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=3.8.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.7.14)\n",
            "Requirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.70.16)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18.1->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.1)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.2.1)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.8.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.23.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.3.8)\n",
            "Downloading interpret-0.7.1-py3-none-any.whl (1.4 kB)\n",
            "Downloading interpret_core-0.7.1-py3-none-any.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m201.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: interpret-core, interpret\n",
            "Successfully installed interpret-0.7.1 interpret-core-0.7.1\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40758 sha256=e3ca32b95ef75330a52225b36d8bd39da526fab0eeb31e139675a7990229d7ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "#CELDA DE INSTALACIONES - EJECUTAR ANTES QUE LA CELDA DEL PROGRAMA PMS\n",
        "!pip install ipywidgets\n",
        "!pip install keras-tuner scikit-learn openpyxl -q\n",
        "!pip install keras-tuner -q\n",
        "!pip install shap\n",
        "!pip install lime\n",
        "!pip install interpret\n",
        "!pip install shap lime xgboost -q\n",
        "!pip install boruta\n",
        "!pip install umap-learn\n",
        "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
        "!pip install ipywidgets openpyxl\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "!pip install --quiet ipywidgets openpyxl\n",
        "!pip install -q ipywidgets openpyxl\n",
        "!pip install optuna scikit-optimize --quiet\n",
        "!pip install optuna optuna-integration[sklearn]\n",
        "!pip install --upgrade scikeras\n",
        "!pip install -U interpret\n",
        "!pip uninstall -y interpret interpret-core\n",
        "!pip uninstall -y interpret\n",
        "!pip install --no-cache-dir interpret\n",
        "!pip install PyPDF2\n",
        "!pip install python-docx\n",
        "!pip install fpdf\n",
        "!pip install pypandoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4lPa1MnT-fiB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239,
          "referenced_widgets": [
            "7fc0afe58bdf453b9fc9a8d253d9b15e",
            "2876e80d1e6443f294166860dc38ac7f",
            "5842eef7f7f44ac2a75bbdb06d2d7587",
            "edd237a2675c4e02b880078db1cf977a",
            "a7a278021a7c487c8439839997017fd1",
            "c2e2646a3891471b8807094aac79a861",
            "c8a60fd4d33740afbffc2aa693697f85",
            "8f430a6c173b4747a76e9507b7de9304",
            "8c83ecff325b4735bf4fc5ff385b4e81",
            "670dc6aeae984c3b86a690f03651ad34",
            "f829dc46033642c9adfc93e550750301",
            "4276874e205f4773a23b128ecf1d0251",
            "919e3eff833541f9ae00a2c385b32a0e",
            "f3eb0a55f8044831aa45e116d4a6730e",
            "0f5817d01b0f40318504805491728c5e",
            "98241a67dd0e4d9592eff31baf72ef30",
            "3cbc1394f08e4127ab3393b7165417fd",
            "92f7f7b60ab3406a8455a1ca982c88c1",
            "0fee1b1861c8442ebb2243e5de847f2b",
            "ba296edc19694bd195af5396c36787ae",
            "33c1a4964ec045218f32535f0ce9968b",
            "c61e73ba88ba426cbbed8cfed13269be",
            "62694866e04b47928a2722c9b29bc07e",
            "2c8478edfaf94d7f912022b6d1185ad8",
            "b7cd40e9f8284f459d592efde209ec53",
            "593a5f8c069e4061995b8a0fbeddc33a",
            "092e670f9b834b239aa0bf5ef521cb53",
            "3222d4fb5c1e4138a594cf9edbaa0748",
            "d409974836ee4c2a965e721ad347c460",
            "de1a11fa427646d3becc2e8c99ab2812",
            "847b518cb96948009cb97e0ae3681e4b",
            "6d6a5470f567411289ce5def9c347abd",
            "51dbdea738db4b688df8d4d2971babfc",
            "e928878511c742c2989a9e888e99797f",
            "b6fceaefb91545068c9be746981e9cf7",
            "f6c793e725154a2298631adf6d807ce7",
            "a7332fe1349248c0bb14222040716e88",
            "2afeee701b564b6eaec77ebe5c06e34d",
            "24d2633945b14ff0932130d6dc6b5171",
            "8bed4efd79e543049a04c1df3c72a50f",
            "6e91c4f5807241a09441a1f7c1b1bf84",
            "9bac08d2ef8849fbbba1549429f404a0",
            "494affada79849c4a974652fe7a070c6",
            "9584861d1474450da2aac186b1a4481c",
            "04784ca9825a4257a788a76835bd7493",
            "32d0722e22284d2999579e244303881e",
            "b4465fd38a444c68b0f67fe7ab67457a",
            "dc3686da512447a6ac66663c11e39abf",
            "2786b5a4367e4213a765e3963b277f51",
            "097e842afc9e4bc8bab29ca22ba6c07b",
            "4ef6c8c5c1c64d25b3252f9f509bf7a3",
            "f006cb096af84ce5b75adbd83043108f",
            "5ba19ca33a174db68883aa8712d71578",
            "d22b5d395913418aa608ea277ceafa3a",
            "9e4ab1aaac1e44e290d27f59d9be0bc2",
            "b87174a312154bb0b21f1fa9b4a10ca3",
            "0b83efb382274762948ec1b1c2b21c7f",
            "2ea64f2fbd8644dea97dd6ee08fe22bb",
            "1157881e56804c99a5f1f8ef1b4d1850",
            "d26b4c62ddb04a9bb824b9ce2ce306ce",
            "71d421482b4a4860b56eb229f708eb33",
            "e5f7f55cd2414b5883aed40094410e01",
            "adcf3930810b4875ac661551e21bc852",
            "e5106a7f20a845479749080ee3f6ffa4",
            "c15b5a7938b345e683e0c62506fa87b5",
            "ce88efe7cbd04ce28e42da378097baf5",
            "e82419e2f78c4765a900e8f7a2ec581f",
            "c35f63e727cf43ce9472b8dde6be478d",
            "afd56917323c48ddbd8659d5a5153fd6",
            "d4d3da4f4034427f82d2a7c1a14c6fdd",
            "21f23b86578b43029711ab0c738bc1da",
            "86c337421d204c7dbd1e7b5d3ea6b6ea",
            "f1fcb04c6cd643b3b68973c983929a4e",
            "70e1c0808ecd4e418817eba1b2ffc1dd",
            "98915d7e2be14836a5e833ee30db1402",
            "292cdd96db474a32b72c0d7d19865225",
            "1051c382444545a499c6c163113d973e",
            "6b6a1b2ed63f417a8712d79c54ba09e0",
            "36102fdd4844493b8c06ce22f319fac2",
            "946d76c552b448279f2c0f2a2f372387",
            "584346a65568423da688e3063e5fb840",
            "cdfca94dd0254e92a87f2a0ba02b706b",
            "f03d3c0e8f3a47c5a836b9ecd1f16bd1",
            "45661577f69949c0a69dac54bd7383cf",
            "b87506a2d19943709a4a31d60f13feda",
            "81f71060f06a467f938037fdd945a03b",
            "0c513d02f1914314a848683355ef1c6d",
            "8cabff7d374b4ad7b4467c8baa6b23c8",
            "d9c0b3a5334f40508989952c82e744fb",
            "5db0ca6dc6d64f2b9461c2fa7ac88c36",
            "7ffe7c666e02400f82a48375ecc0109c",
            "1a141e8ccd144bf7bbd16a39863e4064",
            "ccbaf9630780425ea9e222053bd354c6",
            "385259253b7f4fd0aa376c07a5c7969a",
            "d16186e75a0c4b4fb69fa013c0f42eb9",
            "1104c93b8cc346f5ab9173529259151e",
            "106bd52209224d9d9ec6d5f8dcf96772",
            "4f06057bd64346f98593a9a0623cc66f",
            "a41d2abb301e48ba8df8da670e53e977",
            "2ac66f097ee84257bd60ca0914f7bffe",
            "7da971c7dd484944bc560e4506a42bcd",
            "6076b3b8c10940ce996f1b2c7b67f73f",
            "602fc543709348fba1b4f5099c9fa036",
            "37176ba8f8124ad6b29ac35c461306e3",
            "717ab25606834e3397916968e02b4ab8",
            "9e2d9c45ecc94212be84ffbea37d7831",
            "0646db9a4ab042c483e275c931196059",
            "32005eb156cd43fe9459d2f966a2210b",
            "1f0a26b01f2e4e9fa4b7dc7eebce1f57",
            "2225a8db84734179bcb20dc56c0a61d2",
            "93468cc808a24690be54fbf39b707f60",
            "7d7dae87ed09499f9d6b270d65c64fbf",
            "bd838c8b1b7b4a2682d27ee36a83e838",
            "ae9a0274cbcd4e8388a3793d2cf59ce0",
            "856dbf2c27954e1bb3f243c6611c46bc",
            "6dda1545ab3b4f2e9a2ee2df2bed6f9a",
            "82e8bd61836e4bd0b0cf941a31838c42",
            "22ba85f34e1841379299bcd675bedfd3",
            "9cd63676a2f84d8a883b0261c17ada56"
          ]
        },
        "outputId": "2b04b260-1fd0-440b-a4fe-0f2f9263488a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fc0afe58bdf453b9fc9a8d253d9b15e"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edd237a2675c4e02b880078db1cf977a"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2e2646a3891471b8807094aac79a861"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f430a6c173b4747a76e9507b7de9304"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "670dc6aeae984c3b86a690f03651ad34"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4276874e205f4773a23b128ecf1d0251"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3eb0a55f8044831aa45e116d4a6730e"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98241a67dd0e4d9592eff31baf72ef30"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce88efe7cbd04ce28e42da378097baf5"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c35f63e727cf43ce9472b8dde6be478d"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "/* Ancho y tipografa de los widgets */\n",
              ".widget-dropdown, .widget-select-multiple, .widget-button {\n",
              "  width: 400px !important;       /* cajas ms anchas */\n",
              "  font-size: 14px !important;    /* texto de 14px */\n",
              "}\n",
              ".widget-dropdown > label, .widget-select-multiple > label {\n",
              "  font-size: 14px !important;    /* etiquetas tambin grandes */\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value=\"<h3 style='font-size:1.3rem;margin-bottom:5px;'> Men Principal</h3>\"), VBox(child"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cabff7d374b4ad7b4467c8baa6b23c8"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='warning', description=' Limpiar pantalla', layout=Layout(width='150px'), style=ButtonSty"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a141e8ccd144bf7bbd16a39863e4064"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================================\n",
        "# PROGRAMA COMPLETO PMS\n",
        "# Programa para el entrenamiento, comparacin y optimizacin de modelos matemticos para llevar a cabo el Gemelo Digital del Proceso de Fabricacin de Tableros MDF\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# 0. INSTALACION DE COMPONENTES PYTHON NECESARIOS PARA LA EJECUCIN DEL PROGRAMA\n",
        "# ============================================================================\n",
        "#Antes de ejecutar el programa asegurar que estn instaladas las siguientes librerias:\n",
        "#!pip install ipywidgets\n",
        "#!pip install keras-tuner scikit-learn openpyxl -q\n",
        "#!pip install keras-tuner -q\n",
        "#!pip install shap\n",
        "#!pip install lime\n",
        "#!pip install interpret\n",
        "#!pip install shap lime xgboost -q\n",
        "#!pip install boruta\n",
        "#!pip install umap-learn\n",
        "#!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
        "#!pip install ipywidgets openpyxl\n",
        "#!jupyter nbextension enable --py widgetsnbextension\n",
        "#!pip install --quiet ipywidgets openpyxl\n",
        "#!pip install optuna scikit-optimize --quiet\n",
        "#!pip install optuna optuna-integration[sklearn]\n",
        "#!pip install scikeras\n",
        "#!pip install --upgrade scikeras\n",
        "#!pip install -U interpret\n",
        "#!pip install PyPDF2\n",
        "#!pip install python-docx\n",
        "#!pip install fpdf\n",
        "#!pip install pypandoc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 1. LIBRERAS\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle, os\n",
        "from io import StringIO\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from keras_tuner import BayesianOptimization\n",
        "from sklearn.model_selection import KFold\n",
        "import pickle, os\n",
        "import shutil\n",
        "import time\n",
        "import threading\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import keras_tuner as kt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "import threading\n",
        "import pickle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Ftrl\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import time\n",
        "import random\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost.callback import EarlyStopping as XgbEarlyStopping\n",
        "import pkgutil\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "import threading, time, pickle, os\n",
        "from ipywidgets import Output\n",
        "import io\n",
        "from google.colab import output\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "import re\n",
        "\n",
        "def sanitize_name(s):\n",
        "    \"\"\"\n",
        "    Unifica la sanitizacin de cualquier string de columna:\n",
        "    Reemplaza espacios, puntos, comas, punto y coma, dos puntos,\n",
        "    barras (/ \\\\), parntesis (), corchetes [], llaves {},\n",
        "    signo %, +, -, *, &, ^, $, #, @, !, ?, =, <, >, |, `, ~\n",
        "    por guin bajo y colapsa mltiples guiones bajos.\n",
        "    \"\"\"\n",
        "    # Reemplaza todo carcter no alfanumrico o guin bajo por '_'\n",
        "    t = re.sub(r\"[^\\w]\", \"_\", str(s))\n",
        "    # Colapsa mltiples guiones bajos consecutivos\n",
        "    t = re.sub(r\"_+\", \"_\", t)\n",
        "    return t.strip(\"_\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 2. VARIABLES DE SALIDA\n",
        "# ===============================================================\n",
        "out_carga = widgets.Output()\n",
        "out_svr = widgets.Output()\n",
        "out_nn = widgets.Output()\n",
        "out_xgb = widgets.Output()\n",
        "out_pred = widgets.Output()\n",
        "out_graf = widgets.Output()\n",
        "out_xai = widgets.Output()\n",
        "out_bienvenida = widgets.Output()\n",
        "out_ayuda = widgets.Output()\n",
        "out_nn_opt = widgets.Output()\n",
        "stop_flag = threading.Event()\n",
        "stop_flag_nn = threading.Event()\n",
        "out_svr_opt = widgets.Output()\n",
        "stop_flag_svr = threading.Event()\n",
        "out_xgb_opt = widgets.Output()\n",
        "stop_flag_xgb = threading.Event()\n",
        "out_rf = widgets.Output()\n",
        "rf_timer_stop_event = threading.Event()\n",
        "\n",
        "if 'out_rnn' not in globals():\n",
        "    out_rnn = widgets.Output()\n",
        "\n",
        "ayuda_visible = [False]  # Flag global para mostrar/ocultar ayuda\n",
        "\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 3. AYUDA GLOBAL\n",
        "# Este mdulo sirve para proporcionar al usuario del programa toda la ayuda que podria necesitar para entender el funcionamiento del programa y sus diferentes parmetros.\n",
        "# Crea un Centro de Ayuda todo-en-uno:\n",
        "#  Un Accordion con 4 paneles (uno por Bloque)\n",
        "#  Cada panel contiene HTML muy detallado\n",
        "#  Se aaden imgenes/figuras ilustrativas (base-64 embebidas)\n",
        "#  Incluye un botn flotante Necesitas ayuda? para abrir/cerrar\n",
        "# ===============================================================\n",
        "def mostrar_ayuda_completa() -> None:\n",
        "    \"\"\"\n",
        "    Despliega la ayuda global, dividida en 4 bloques:\n",
        "\n",
        "    Bloque 1  Carga, Segmentacin y Seleccin de Variables\n",
        "    Bloque 2  Entrenamiento y Visualizacin de Modelos\n",
        "    Bloque 3  Optimizacin Multicapa (SVR, NN, XGB, RF, RNN)\n",
        "    Bloque 4  Interpretabilidad xIA, Navegacin y Men Principal\n",
        "\n",
        "    Cada bloque se explica en profundidad con texto, tablas e\n",
        "    imgenes incrustadas (generadas al vuelo).  El usuario puede\n",
        "    cambiar de bloque pulsando los botones de navegacin.\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    #  util_img_blocks.py    4 generadores de imagen\n",
        "    # -----------------------------------------------\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.patches import FancyArrowPatch, Rectangle\n",
        "    import numpy as np, io, base64, textwrap, itertools, random\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "\n",
        "    # \n",
        "    def _fig_to_b64(fig) -> str:\n",
        "        \"Convierte una figura matplotlib en cadena base64\"\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format=\"png\", bbox_inches=\"tight\", dpi=140)\n",
        "        plt.close(fig); buf.seek(0)\n",
        "        return base64.b64encode(buf.read()).decode()\n",
        "\n",
        "    # \n",
        "    # BLOQUE 1    Tubo de carga  split  seleccin\n",
        "    def _img_block1() -> str:\n",
        "        fig, ax = plt.subplots(figsize=(6, 2))\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        # 1) CSV/Excel\n",
        "        ax.add_patch(Rectangle((0.05, .25), .18, .5, fc=\"#f0f8ff\", ec=\"#4682b4\", lw=1.5))\n",
        "        ax.text(.14, .5, \"Excel/CSV\", ha=\"center\", va=\"center\", weight=\"bold\")\n",
        "\n",
        "        # 2) DataFrame\n",
        "        ax.add_patch(Rectangle((.30, .25), .18, .5, fc=\"#e6ffe6\", ec=\"#2e8b57\", lw=1.5))\n",
        "        ax.text(.39, .5, \"DataFrame\\n(pandas)\", ha=\"center\", va=\"center\")\n",
        "\n",
        "        # 3) Train/Test\n",
        "        ax.add_patch(Rectangle((.55, .45), .18, .3, fc=\"#fff4e6\", ec=\"#ff8c00\", lw=1.5))\n",
        "        ax.text(.64, .60, \"Train\", ha=\"center\", va=\"center\", size=8)\n",
        "        ax.add_patch(Rectangle((.55, .25), .18, .15, fc=\"#ffe6e6\", ec=\"#d80027\", lw=1.5))\n",
        "        ax.text(.64, .325, \"Test\", ha=\"center\", va=\"center\", size=8)\n",
        "\n",
        "        # 4) Seleccin de variables (nodos pequeos)\n",
        "        methods = [\"Pearson\", \"Mutual\\nInfo\", \"Boruta\", \"UMAP\"]\n",
        "        for i, m in enumerate(methods):\n",
        "            x = .82; y = .6 - i*0.15\n",
        "            ax.add_patch(Rectangle((x, y), .13, .1, fc=\"#fafafa\", ec=\"#555\", lw=1))\n",
        "            ax.text(x+.065, y+.05, m, ha=\"center\", va=\"center\", size=7)\n",
        "\n",
        "        # Flechas\n",
        "        def arrow(xy1, xy2):\n",
        "            ax.add_patch(FancyArrowPatch(xy1, xy2, arrowstyle=\"->\", lw=1, color=\"#444\"))\n",
        "        arrow((.23, .5), (.30, .5))\n",
        "        arrow((.48, .5), (.55, .5))\n",
        "        arrow((.73, .5), (.82, .55))\n",
        "        arrow((.73, .5), (.82, .4))\n",
        "        arrow((.73, .5), (.82, .25))\n",
        "        fig.suptitle(\"Pipeline Bloque 1\", fontweight=\"bold\")\n",
        "        return _fig_to_b64(fig)\n",
        "\n",
        "    # \n",
        "    # BLOQUE 2    Comparativa de desempeo de modelos\n",
        "    def _img_block2() -> str:\n",
        "        models = [\"SVR\", \"NN\", \"XGB\", \"RF\", \"RNN\"]\n",
        "        scores = [0.82, 0.88, 0.91, 0.86, 0.84]  # ejemplo R\n",
        "        fig, ax = plt.subplots(figsize=(5, 3))\n",
        "        bars = ax.bar(models, scores, color=\"#4c9be8\")\n",
        "        ax.set_ylim(0, 1.0)\n",
        "        ax.set_ylabel(\"R en Test\")\n",
        "        ax.set_title(\"Rendimiento modelos (ejemplo)\")\n",
        "        for b, s in zip(bars, scores):\n",
        "            ax.text(b.get_x() + b.get_width()/2, s+0.02, f\"{s:.2f}\", ha=\"center\", va=\"bottom\", size=8)\n",
        "        return _fig_to_b64(fig)\n",
        "\n",
        "    # \n",
        "    # BLOQUE 3    Convergencia de bsqueda HPO\n",
        "    def _img_block3() -> str:\n",
        "        fig, ax = plt.subplots(figsize=(5.5, 3))\n",
        "        n_iter = 30\n",
        "        # curva score ficticia para 3 tcnicas\n",
        "        rng = np.random.RandomState(0)\n",
        "        for label, c in zip((\"GridSearch\", \"BayesSearch\", \"Optuna\"), (\"#999\", \"#2e8b57\", \"#d62728\")):\n",
        "            best_so_far = np.maximum.accumulate(rng.uniform(.5, .9, n_iter))\n",
        "            ax.plot(range(1, n_iter+1), best_so_far, label=label, lw=1.8, color=c)\n",
        "        ax.set_xlabel(\"Iteracin\")\n",
        "        ax.set_ylabel(\"Score acumulado (R)\")\n",
        "        ax.set_title(\"Evolucin bsqueda de hiperparmetros\")\n",
        "        ax.legend(frameon=False, fontsize=8)\n",
        "        return _fig_to_b64(fig)\n",
        "\n",
        "    # \n",
        "    # BLOQUE 4    Mini-summary de interpretabilidad (SHAP simulado)\n",
        "    def _img_block4() -> str:\n",
        "        feats = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"]\n",
        "        shap_vals = np.array([0.4, -0.35, 0.25, -0.15, 0.05])\n",
        "        colors = ['#d62728' if v<0 else '#2ca02c' for v in shap_vals]\n",
        "        fig, ax = plt.subplots(figsize=(4.5, 3))\n",
        "        ax.barh(feats, shap_vals, color=colors)\n",
        "        ax.axvline(0, color=\"#444\", lw=0.8)\n",
        "        ax.set_xlabel(\"Valor SHAP (impacto en la prediccin)\")\n",
        "        ax.set_title(\"Contribucin de variables (ejemplo)\")\n",
        "        plt.tight_layout()\n",
        "        return _fig_to_b64(fig)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # 1  TEXTOS DE AYUDA  (puedes ampliarlos todo lo que quieras)\n",
        "    #      Cada bloque es un gran HTML con ttulos, listas,\n",
        "    #       tablas <table>, imgenes <img>\n",
        "    # ----------------------------------------------------------\n",
        "    bloque1_html = HTML(f\"\"\"\n",
        "    <h3 style='color:#2E8B57;'> Bloque 1  Carga, segmentacin y seleccin de variables</h3>\n",
        "    <p><b>Objetivo:</b> transformar hojas Excel o .csv en matrices <code>X</code> (predictoras) y\n",
        "    <code>Y</code> (objetivo).</p>\n",
        "\n",
        "    <h4> Carga de datos</h4>\n",
        "    <ul>\n",
        "      <li>Importacin directa desde rea de transferencia (<i>copypaste</i> de Excel) o desde fichero.</li>\n",
        "      <li>Validacin de encabezados, tipos y <code>NaN</code>.</li>\n",
        "      <li>Ejemplo rpido:<br>\n",
        "      <code>X, Y = cargar_desde_clipboard(sep='\\\\t')</code></li>\n",
        "    </ul>\n",
        "\n",
        "    <h4> Segmentacin Train/Test</h4>\n",
        "    <ul>\n",
        "      <li>Divisin estratificada opcional (<i>stratify=Y</i> cuando <code>Y</code> es discreta).</li>\n",
        "      <li>Seed reproducible (<code>random_state=42</code>).</li>\n",
        "      <li>Visualizacin: tabla de tamaos y grfico barra apilada para comparar distribucin de\n",
        "          objetivos.</li>\n",
        "    </ul>\n",
        "\n",
        "    <h4> Seleccin de variables <i>X</i></h4>\n",
        "    <table>\n",
        "    <thead><tr><th>Mtodo</th><th>Descripcin resumida</th><th>Mtrica interna</th></tr></thead>\n",
        "    <tbody>\n",
        "    <tr><td>Pearson / Spearman</td><td>Descarta colinealidad lineal / montona</td><td>|| &gt; </td></tr>\n",
        "    <tr><td>Mutual Info</td><td>Informacin mutua no-lineal</td><td>MI &gt; </td></tr>\n",
        "    <tr><td>Boruta</td><td>Seleccin envolvente basada en RF</td><td>Importancia &gt; shadow</td></tr>\n",
        "    <tr><td>UMAP</td><td>Reduccin de dimensin no lineal (embedding)</td><td>Varianza retenida</td></tr>\n",
        "    </tbody></table>\n",
        "\n",
        "    <p><i>Resultado:</i> diccionario <code>RESUMEN_METODOS</code> con el subconjunto de columnas\n",
        "    aprobado por cada tcnica.</p>\n",
        "    <<img src=\"data:image/png;base64,{_img_block1()}\"  ></td></tr>>\n",
        "    \"\"\")\n",
        "\n",
        "    bloque2_html = HTML(f\"\"\"\n",
        "    <h3 style='color:#1E90FF;'> Bloque 2  Entrenamiento y visualizacin de modelos</h3>\n",
        "    <p>Incluye:</p>\n",
        "    <ul>\n",
        "      <li><b>SVR</b> (kernels lineal / RBF)</li>\n",
        "      <li><b>Red Neuronal densa</b> (Keras/TensorFlow)</li>\n",
        "      <li><b>XGBoost</b> (regresin)</li>\n",
        "      <li><b>Random Forest</b> (sklearn)</li>\n",
        "      <li><b>RNN</b> / LSTM para series temporales</li>\n",
        "    </ul>\n",
        "\n",
        "    <h4>Flujo de trabajo general</h4>\n",
        "    <ol>\n",
        "      <li>Escalado (<code>StandardScaler</code>) de <code>X</code> y <code>Y</code>.</li>\n",
        "      <li>Entrenamiento con <code>X_train</code>, evaluacin con <code>X_test</code>.</li>\n",
        "      <li>Mtricas trazadas: R, MSE, RMSE, MAE, MedAE.</li>\n",
        "      <li>Grficos:\n",
        "        <ul>\n",
        "            <li>Y real vs Y predicho (scatter y lnea)</li>\n",
        "            <li>Residuos: histograma + QQ + <i>residual vs fitted</i></li>\n",
        "        </ul>\n",
        "      </li>\n",
        "    </ol>\n",
        "\n",
        "    <h4>Ejemplo mnimo  SVR RBF</h4>\n",
        "    <pre>\n",
        "    svr = SVR(kernel='rbf', C=10, epsilon=0.1, gamma='scale')\n",
        "    svr.fit(X_train_scaled, y_train_scaled)\n",
        "    pred = scaler_y.inverse_transform(svr.predict(X_test_scaled).reshape(-1,1))\n",
        "    </pre>\n",
        "    <img src=\"data:image/png;base64,{_img_block2()}\"  ></td></tr>\n",
        "    \"\"\")\n",
        "\n",
        "    bloque3_html = HTML(f\"\"\"\n",
        "    <h3 style='color:#FFA500;'> Bloque 3  Optimizacin multicapa (HPO)</h3>\n",
        "    <p>Se soportan cinco motores por tipo de modelo:</p>\n",
        "    <ul>\n",
        "      <li><b>GridSearchCV</b></li>\n",
        "      <li><b>RandomizedSearchCV</b></li>\n",
        "      <li><b>BayesSearchCV</b> (scikit-optimize)</li>\n",
        "      <li><b>Optuna</b></li>\n",
        "      <li><b>Hyperband / HalvingSearchCV</b></li>\n",
        "    </ul>\n",
        "\n",
        "    <table>\n",
        "    <thead><tr><th>Modelo</th><th>Espacio de bsqueda  </th><th>Trials por defecto</th></tr></thead>\n",
        "    <tbody>\n",
        "    <tr><td>SVR</td><td>C, , kernel</td><td>30</td></tr>\n",
        "    <tr><td>Neural Net</td><td>#capas, neuronas, LR, dropout, </td><td>50</td></tr>\n",
        "    <tr><td>XGBoost</td><td>depth, lr, n_estim, , subsample</td><td>100</td></tr>\n",
        "    <tr><td>Random Forest</td><td>n_estim, depth, mtry, bootstrap</td><td>70</td></tr>\n",
        "    <tr><td>RNN</td><td>units, batch, epochs, LR</td><td>50</td></tr>\n",
        "    </tbody></table>\n",
        "\n",
        "    <p>Cada ejecucin devuelve:</p>\n",
        "    <ul>\n",
        "      <li>TOP-5 configuraciones (tabla ordenada)</li>\n",
        "      <li>Mejor curva de prediccin y residuos</li>\n",
        "      <li>Heat-map de mtricas normalizadas + Radar chart</li>\n",
        "    </ul>\n",
        "    <img src=\"data:image/png;base64,{_img_block3()}\"  ></td></tr>\n",
        "    \"\"\")\n",
        "\n",
        "    bloque4_html = HTML(f\"\"\"\n",
        "    <h3 style='color:#8A2BE2;'> Bloque 4  Interpretabilidad xIA &amp; Navegacin</h3>\n",
        "    <p>El mdulo integra <b>14</b> tcnicas:</p>\n",
        "    <ol>\n",
        "      <li>SHAP (Tree / Kernel / Deep)</li>\n",
        "      <li>LIME (tabular)</li>\n",
        "      <li>KernelExplainer (SHAP caja negra)</li>\n",
        "      <li>Integrated Gradients</li>\n",
        "      <li>DeepLIFT / LRP</li>\n",
        "      <li>Permutation Feature Importance</li>\n",
        "      <li>Partial Dependence Plots (PDP)</li>\n",
        "      <li>Accumulated Local Effects (ALE)</li>\n",
        "      <li>ICE plots</li>\n",
        "      <li>Counterfactual Explainer</li>\n",
        "      <li>Anchors (rboles locales)</li>\n",
        "      <li>Modelos sustitutos (rbol global + lneas locales)</li>\n",
        "      <li>Explainable Boosting Machine (EBM)</li>\n",
        "      <li>Optuna Hyper-parameter Importance</li>\n",
        "    </ol>\n",
        "\n",
        "    <p>Para cada tcnica se generan:</p>\n",
        "    <ul>\n",
        "      <li>Grfico principal (summary, barras, scatter)</li>\n",
        "      <li>Tabla local (primeras 10 muestras)</li>\n",
        "      <li>Tabla de importancia global</li>\n",
        "      <li>Bloque de interpretacin con <i>tips</i> y lectura guiada</li>\n",
        "    </ul>\n",
        "    <img src=\"data:image/png;base64,{_img_block4()}\"  ></td></tr>\n",
        "    <p><i>Consejo:</i> combina SHAP + PDP + Permutation para tener una visin 360: explicaciones locales,\n",
        "    efecto medio y robustez de cada variable.</p>\n",
        "    \"\"\")\n",
        "\n",
        "    # \n",
        "    # Botones + enlace a callback\n",
        "    # \n",
        "    # ----------------------------------------------------------\n",
        "    # 2  REA DE SALIDA (visible a todas las funciones)\n",
        "    # ----------------------------------------------------------\n",
        "    _help_output = widgets.Output()\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # 3  RENDER FUNCTIONS (deben existir ANTES de .on_click)\n",
        "    # ----------------------------------------------------------\n",
        "    def _render_bloque_1(*_):\n",
        "        _help_output.clear_output()\n",
        "        with _help_output:\n",
        "            display(bloque1_html)\n",
        "\n",
        "    def _render_bloque_2(*_):\n",
        "        _help_output.clear_output()\n",
        "        with _help_output:\n",
        "            display(bloque2_html)\n",
        "\n",
        "    def _render_bloque_3(*_):\n",
        "        _help_output.clear_output()\n",
        "        with _help_output:\n",
        "            display(bloque3_html)\n",
        "\n",
        "    def _render_bloque_4(*_):\n",
        "        _help_output.clear_output()\n",
        "        with _help_output:\n",
        "            display(bloque4_html)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # 4  BOTONES  (creados DESPUS de las funciones)\n",
        "    # ----------------------------------------------------------\n",
        "    btn_b1 = widgets.Button(description=\" Bloque 1  DATOS\",           button_style='info')\n",
        "    btn_b2 = widgets.Button(description=\" Bloque 2  ENTRENAMIENTO\",   button_style='info')\n",
        "    btn_b3 = widgets.Button(description=\" Bloque 3  OPTIMIZACIN\",    button_style='info')\n",
        "    btn_b4 = widgets.Button(description=\" Bloque 4  INTERPRETABILIDAD\", button_style='info')\n",
        "\n",
        "    # Enlazar callbacks  (ya no produce NameError)\n",
        "    btn_b1.on_click(_render_bloque_1)\n",
        "    btn_b2.on_click(_render_bloque_2)\n",
        "    btn_b3.on_click(_render_bloque_3)\n",
        "    btn_b4.on_click(_render_bloque_4)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # 5  INTERFAZ  se muestra la cabecera + botones + rea\n",
        "    # ----------------------------------------------------------\n",
        "    display(\n",
        "        widgets.VBox([\n",
        "            widgets.HTML(\"<h2 style='color:#1E90FF;'> AYUDA GLOBAL  Gua Completa de la Aplicacin</h2>\"),\n",
        "            widgets.HTML(\"<p>Selecciona el bloque sobre el que necesitas informacin detallada.</p>\"),\n",
        "            widgets.HBox([btn_b1, btn_b2, btn_b3, btn_b4]),\n",
        "            _help_output\n",
        "        ])\n",
        "    )\n",
        "    #btn.on_click(mostrar_ayuda_completa)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 4. BIENVENIDA\n",
        "# Este mdulo sirve como apoyo al usuario en la entrada al programa proporcionando una descripcin bastante detallada de todo lo que hace el programa.\n",
        "# ===============================================================\n",
        "import io, base64, numpy as np, matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# \n",
        "# UTILIDADES DE IMAGEN EMBEBIDA\n",
        "# \n",
        "def _fig_to_b64(fig):\n",
        "    \"\"\"Devuelve la figura matplotlib codificada en base-64 (PNG).\"\"\"\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\", dpi=130)\n",
        "    plt.close(fig)\n",
        "    buf.seek(0)\n",
        "    return base64.b64encode(buf.read()).decode()\n",
        "\n",
        "#   Mini-diagrama Bloque 2: ENTRENAMIENTO  VALIDACIN  PREDICCIN\n",
        "def _block2_img_training_prediction():\n",
        "    fig, ax = plt.subplots(figsize=(6.2, 1.0))\n",
        "    ax.axis(\"off\")\n",
        "    boxes = [\n",
        "        (\"ENTRENAMIENTO\", \"#e6ffe6\", \"#2e8b57\"),\n",
        "        (\"VALIDACIN\",    \"#fffbd5\", \"#ff8c00\"),\n",
        "        (\"PREDICCIN\",    \"#d6e0ff\", \"#4169e1\")\n",
        "    ]\n",
        "    xs = [.02, .36, .70]\n",
        "    for x, (txt, fc, ec) in zip(xs, boxes):\n",
        "        ax.annotate(\n",
        "            txt, (x, .5), ha=\"left\", va=\"center\",\n",
        "            bbox=dict(fc=fc, ec=ec, lw=1.4, boxstyle=\"round,pad=0.28\"))\n",
        "    for x in [.28, .62]:\n",
        "        ax.annotate(\n",
        "            \"\", xy=(x+.03, .5), xytext=(x-.03,.5),\n",
        "            arrowprops=dict(arrowstyle=\"->\", lw=1.5))\n",
        "    ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
        "    return _fig_to_b64(fig)\n",
        "\n",
        "#   Flujos Entrenar  -vs-  Optimizar  -vs-  Explicar\n",
        "def _img_flow_trainpred():\n",
        "    fig, ax = plt.subplots(figsize=(4.5, .9)); ax.axis(\"off\")\n",
        "    ax.annotate(\"Bloque 1\\nCarga, \\nSegmentacion y \\nSeleccin\", (.05,.5), va=\"center\",\n",
        "                bbox=dict(fc=\"#f0f8ff\", ec=\"#4682b4\"))\n",
        "    ax.annotate(\"Bloque 2\\nEntrenamiento, \\nPrediccin y \\nComparacin IA\", (.55,.5), va=\"center\",\n",
        "                bbox=dict(fc=\"#e6ffe6\", ec=\"#2e8b57\"))\n",
        "    ax.annotate(\"\", xy=(.43,.5), xytext=(.33,.5),\n",
        "                arrowprops=dict(arrowstyle=\"->\", lw=1.6))\n",
        "    ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
        "    return _fig_to_b64(fig)\n",
        "\n",
        "def _img_flow_opt():\n",
        "    fig, ax = plt.subplots(figsize=(5.2, .9)); ax.axis(\"off\")\n",
        "    ax.annotate(\"Bloque 1\\nCarga, \\nSegmentacion y \\nSeleccin\", (.05,.5), va=\"center\",\n",
        "                bbox=dict(fc=\"#f0f8ff\", ec=\"#4682b4\"))\n",
        "    ax.annotate(\"Bloque 3\\nOptimizacin \\nModelos \\nde IA\", (.55,.5), va=\"center\",\n",
        "                bbox=dict(fc=\"#fff4e6\", ec=\"#ff8c00\"))\n",
        "    ax.annotate(\"\", xy=(.43,.5), xytext=(.33,.5),\n",
        "                arrowprops=dict(arrowstyle=\"->\", lw=1.6))\n",
        "    ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
        "    return _fig_to_b64(fig)\n",
        "\n",
        "def _img_flow_xai():\n",
        "    fig, ax = plt.subplots(figsize=(6.8, .9)); ax.axis(\"off\")\n",
        "    xs = [.03,.29,.55,.80]\n",
        "    labels  = [\"Bloque 1\\nCarga, \\nSegmentacion y \\nSeleccin\",\"Bloque 2\\nEntrenamiento, \\nPrediccin y \\nComparacin IA\",\"Bloque 3\\nOptimizacin \\nModelos \\nde IA\",\"Bloque 4\\nExplicacin \\nModelos \\nde IA\"]\n",
        "    fcs     = [\"#f0f8ff\",\"#e6ffe6\",\"#fff4e6\",\"#fde2ff\"]\n",
        "    frames  = [\"#4682b4\",\"#2e8b57\",\"#ff8c00\",\"#ba55d3\"]\n",
        "    for x,l,fc,ec in zip(xs, labels, fcs, frames):\n",
        "        ax.annotate(l, (x,.5), va=\"center\",\n",
        "                    bbox=dict(fc=fc, ec=ec))\n",
        "    for x in [.21,.47,.72]:\n",
        "        ax.annotate(\"\", xy=(x+.05,.5), xytext=(x-.05,.5),\n",
        "                    arrowprops=dict(arrowstyle=\"->\", lw=1.6))\n",
        "    ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
        "    return _fig_to_b64(fig)\n",
        "\n",
        "# \n",
        "#  FUNCIN PRINCIPAL DE CELDA\n",
        "# \n",
        "def mostrar_bienvenida():\n",
        "    \"\"\"Pantalla de bienvenida profesional con itinerarios y accesos rpidos.\"\"\"\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    html_intro = HTML(f\"\"\"\n",
        "    <style>\n",
        "      .pms h1   {{font-family:'Segoe UI',Roboto,sans-serif;font-size:1.95rem;margin:.2em 0}}\n",
        "      .pms h2   {{font-size:1.15rem;color:#2E8B57;margin:.8em 0 .3em}}\n",
        "      .pms h3   {{font-size:1.05rem;margin:1.0em 0 .5em}}\n",
        "      .pms p, .pms li {{font-size:.95rem;line-height:1.55em}}\n",
        "      .pms .flow img {{border:1px solid #d0d0d0;border-radius:6px}}\n",
        "      .pms ul   {{margin-top:.2em;margin-bottom:1em}}\n",
        "    </style>\n",
        "\n",
        "    <div class='pms'>\n",
        "\n",
        "      <h1> Pipeline Modeling Suite (PMS)</h1>\n",
        "      <p><em>Entorno interactivo pasta-y-ejecuta para ciencia de datos end-to-end:\n",
        "         desde la preparacin de datos hasta la explicacin xAI.</em></p>\n",
        "\n",
        "      <!--  ITINERARIO 1: ENTRENAR + PREDICCIONES -->\n",
        "      <h2>Itinerarios sugeridos de uso</h2>\n",
        "\n",
        "      <h3> Entrenar <b>y predecir</b> &nbsp;(<i>Bloques 1  2</i>)</h3>\n",
        "      <div class='flow'><img src=\"data:image/png;base64,{_img_flow_trainpred()}\" /></div>\n",
        "      <ul>\n",
        "        <li><b>Funcionalidades Bloque 1 - Carga, segmentacin y seleccin:</b></li>\n",
        "        <li><b> Cargar Datos:</b> pegar o cargar X&nbsp;/ Y desde Excel/CSV.</li>\n",
        "        <li><b> Segmentar:</b> Train / Test (<code>stratify</code> opcional).</li>\n",
        "        <li><b> Seleccionar Variables:</b> Pearson, Spearman, Mutual Info, Boruta, UMAP.</li>\n",
        "      </ul>\n",
        "        <li><b>Funcionalidades Bloque 2 - Entrenamiento, prediccin y comparacin de modelos de IA:</b></li>\n",
        "        <li><b> Entrenar SVR:</b> SVR (Support Vector Regression) es un algoritmo de aprendizaje supervisado que busca predecir valores continuos ajustando una funcin que mantenga los errores dentro de un margen tolerable (psilon) y maximizando la generalizacin del modelo.</li>\n",
        "        <li><b> Entrenar NN:</b> Una red neuronal (NN) es un modelo de aprendizaje automtico inspirado en el cerebro humano, compuesto por capas de nodos interconectados que procesan datos para reconocer patrones y hacer predicciones.</li>\n",
        "        <li><b> Entrenar XGBoost:</b> Algoritmo de aprendizaje automtico basado en rboles de decisin que utiliza tcnicas de gradiente boosting para lograr alta precisin y eficiencia en tareas de clasificacin y regresin. </li>\n",
        "        <li><b> Entrenar Random Forest.</b> Algoritmo de aprendizaje automtico basado en conjuntos que construye mltiples rboles de decisin y combina sus predicciones para mejorar la precisin y reducir el sobreajuste. </li>\n",
        "        <li><b> Entrenar RNN:</b> La Red Neuronal Recurrente (RNN)) es un tipo de red neuronal diseada para procesar secuencias de datos, donde cada salida depende no solo de la entrada actual sino tambin del estado anterior, lo que la hace ideal para tareas como series temporales, texto o audio</li>\n",
        "        <li><b> Prediccin con modelos SVR, NN, XGBoostm RandomForest y RNN:</b> compara Y-real vs Y-predicho, residuos y curvas.</li>\n",
        "        <li><b> Comparador:</b> ranking mtrico y visual entre modelos.</li>\n",
        "      </ul>\n",
        "      <div style=\"margin-bottom:1em;text-align:center\">\n",
        "        <img src=\"data:image/png;base64,{_block2_img_training_prediction()}\" />\n",
        "        <div style=\"font-size:.86rem;margin-top:.3em;color:#555\">\n",
        "          Flujo interno de Bloque 2: entrenamiento  validacin  <b>prediccin</b>\n",
        "        </div>\n",
        "      </div>\n",
        "\n",
        "      <!--  ITINERARIO 2: OPTIMIZAR -->\n",
        "      <h3> Optimizar modelos &nbsp;(<i>Bloques 1  3</i>)</h3>\n",
        "      <div class='flow'><img src=\"data:image/png;base64,{_img_flow_opt()}\" /></div>\n",
        "      <ul>\n",
        "        <li><b>Funcionalidades Bloque 3 - Optimizacin de Modelos de Inteligencia Artificial:</b></li>\n",
        "        <li><b> Optimizar SVR:</b> Grid, Random, Optuna, BayesSearch.</li>\n",
        "        <li><b> Optimizar NN:</b> RandomSearch, Bayesian, Hyperband, Optuna.</li>\n",
        "        <li><b> Optimizar XGB:</b> RandomSearch, Bayesian, Hyperband, Optuna.</li>\n",
        "        <li><b> Optimizar RF:</b> RandomSearch, Bayesian, Hyperband, Optuna.</li>\n",
        "        <li><b> Optimizar RNN:</b> RandomSearch, Bayesian, Hyperband, Optuna.</li>\n",
        "      </ul>\n",
        "\n",
        "      <!--  ITINERARIO 3: EXPLICAR -->\n",
        "      <h3> Explicar modelos &nbsp;(<i>Bloques 1  2  3  4</i>)</h3>\n",
        "      <div class='flow'><img src=\"data:image/png;base64,{_img_flow_xai()}\" /></div>\n",
        "      <ul>\n",
        "        <li><b>Funcionalidades Bloque 4 - Inteligencia Artificial Explicativa xIA:</b></li>\n",
        "        <li><b> SHAP, LIME, KernelExplainer.</b></li>\n",
        "        <li><b> PDP, ALE, ICE.</b></li>\n",
        "        <li><b> Surrogate Models &amp; Anchors.</b></li>\n",
        "        <li><b> EBM, Contrafactuales.</b></li>\n",
        "      </ul>\n",
        "    \"\"\")\n",
        "\n",
        "    # Muestra el HTML + botones\n",
        "    display(html_intro)\n",
        "\n",
        "    #btn.on_click(mostrar_bienvenida)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 5. CARGA Y SEGMENTACION DE DATOS\n",
        "# Este mdulo sirve para cargar los datos a modelar y para segmentar dichos datos en el grupo de Entrenamiento y de Test\n",
        "# ===============================================================\n",
        "# ===============================================================\n",
        "# 5.1. CARGA DE DATOS (X, Y y fechas) - FUNCIONA CORRECTAMENTE\n",
        "# Permite pegar desde Excel, procesar con validaciones y mostrar estadsticas.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import io\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "out_carga = widgets.Output()\n",
        "\n",
        "# Variables globales\n",
        "X_data, Y_data, FECHAS, y_variable_name = None, None, None, None\n",
        "\n",
        "def mostrar_carga(b=None):\n",
        "    out_carga.clear_output()\n",
        "\n",
        "    text_X = widgets.Textarea(\n",
        "        placeholder='Pega aqu la matriz X desde Excel (sin ndice).',\n",
        "        layout=widgets.Layout(width='100%', height='150px')\n",
        "    )\n",
        "    text_Y = widgets.Textarea(\n",
        "        placeholder='Pega aqu la matriz Y (una o ms columnas).',\n",
        "        layout=widgets.Layout(width='100%', height='100px')\n",
        "    )\n",
        "    text_F = widgets.Textarea(\n",
        "        placeholder='Pega aqu la columna de fechas (una sola columna con encabezado).',\n",
        "        layout=widgets.Layout(width='100%', height='100px')\n",
        "    )\n",
        "\n",
        "    boton_importar = widgets.Button(description=\" Importar Datos\", button_style='success')\n",
        "    salida = widgets.Output()\n",
        "\n",
        "    def importar_datos(_):\n",
        "        salida.clear_output()\n",
        "        global X_data, Y_data, FECHAS, y_variable_name\n",
        "\n",
        "        try:\n",
        "            # ------------------ Matriz X ------------------\n",
        "            X = pd.read_csv(io.StringIO(text_X.value.strip()), sep='\\t', header=0)\n",
        "            X = X.apply(lambda col: col.str.replace(',', '.', regex=False) if col.dtype == 'object' else col)\n",
        "            X = X.apply(pd.to_numeric, errors='raise')\n",
        "\n",
        "            # ------------------ Matriz Y ------------------\n",
        "            Y = pd.read_csv(io.StringIO(text_Y.value.strip()), sep='\\t', header=0)\n",
        "            Y = Y.apply(lambda col: col.str.replace(',', '.', regex=False) if col.dtype == 'object' else col)\n",
        "            Y = Y.apply(pd.to_numeric, errors='raise')\n",
        "            y_variable_name = ', '.join(Y.columns) if Y.shape[1] > 1 else Y.columns[0]\n",
        "\n",
        "            # ------------------ Columna de Fechas ------------------\n",
        "            raw_text = text_F.value.strip()\n",
        "            F = pd.read_csv(io.StringIO(raw_text), sep='\\t', header=0)\n",
        "            if F.shape[1] != 1:\n",
        "                raise ValueError(\" La columna de fechas debe tener solo una columna.\")\n",
        "\n",
        "            fecha_raw = F.iloc[:, 0].astype(str).str.strip()\n",
        "            FECHAS = pd.to_datetime(fecha_raw, errors='coerce', format=\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "            if FECHAS.isna().sum() > 0:\n",
        "                display(HTML(\"<b style='color:red;'> Estas son las fechas no reconocidas:</b>\"))\n",
        "                display(pd.DataFrame({\"Fecha original\": fecha_raw[FECHAS.isna()]}))\n",
        "                raise ValueError(\" Algunas fechas no se pudieron interpretar. Revisa el formato o caracteres ocultos.\")\n",
        "\n",
        "            # ------------------ Validar dimensiones ------------------\n",
        "            if not (len(X) == len(Y) == len(FECHAS)):\n",
        "                raise ValueError(f\" Dimensiones incompatibles: X({len(X)}), Y({len(Y)}), Fechas({len(FECHAS)})\")\n",
        "\n",
        "            # ------------------ Guardar ------------------\n",
        "            X_data = X.copy()\n",
        "            Y_data = Y.copy()\n",
        "            mostrar_estadisticas_datos()  #  Aadir esta lnea al final\n",
        "\n",
        "            display(HTML(\"<b style='color:green;'> Datos cargados correctamente.</b>\"))\n",
        "            mostrar_estadisticas_datos()\n",
        "\n",
        "        except Exception as e:\n",
        "            display(HTML(f\"<span style='color:red;'> Error al importar: {str(e)}</span>\"))\n",
        "\n",
        "    boton_importar.on_click(importar_datos)\n",
        "\n",
        "    with out_carga:\n",
        "        display(HTML(\"<h3> Carga de Datos</h3>\"))\n",
        "        display(widgets.VBox([\n",
        "            widgets.Label(\" Matriz X (variables predictoras):\"), text_X,\n",
        "            widgets.Label(\" Matriz Y (variable/s a predecir):\"), text_Y,\n",
        "            widgets.Label(\" Columna de Fechas (una sola columna):\"), text_F,\n",
        "            boton_importar, salida\n",
        "        ]))\n",
        "\n",
        "    #  Mostrar el output\n",
        "    display(out_carga)\n",
        "\n",
        "def mostrar_estadisticas_datos():\n",
        "    with out_carga:\n",
        "        try:\n",
        "            display(HTML(\"<h3> Estadsticas de los Datos Cargados</h3>\"))\n",
        "\n",
        "            # Estadsticas de X\n",
        "            if X_data is not None:\n",
        "                display(HTML(\"<h4> Estadsticas de X (Variables Independientes):</h4>\"))\n",
        "                display(X_data.describe(include='all').T.style.set_caption(\"Resumen Estadstico de X\").format(precision=3))\n",
        "\n",
        "            # Estadsticas de Y\n",
        "            if Y_data is not None:\n",
        "                display(HTML(\"<h4> Estadsticas de Y (Variable Objetivo):</h4>\"))\n",
        "                display(Y_data.describe(include='all').T.style.set_caption(\"Resumen Estadstico de Y\").format(precision=3))\n",
        "\n",
        "            # Estadsticas de Fechas\n",
        "            if FECHAS is not None and not FECHAS.isna().all():\n",
        "                display(HTML(\"<h4> Estadsticas de Fechas:</h4>\"))\n",
        "                display(pd.DataFrame({\n",
        "                    'Primera fecha': [FECHAS.min()],\n",
        "                    'ltima fecha': [FECHAS.max()],\n",
        "                    'Total registros': [len(FECHAS)],\n",
        "                    'Frecuencia media (das)': [FECHAS.diff().dt.total_seconds().dropna().mean() / 86400]\n",
        "                }).T.rename(columns={0: 'Valor'}))\n",
        "\n",
        "        except Exception as e:\n",
        "            display(HTML(f\"<span style='color:red;'> Error al mostrar estadsticas: {str(e)}</span>\"))\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5.2. SEGMENTACION DE DATOS PARA ENTRENAMIENTO Y TEST - FUNCIONA CORRECTAMENTE\n",
        "# Split %entrenamiento / % pruebas estndar o estratificado segn distribucin de Y\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, Javascript, clear_output\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Widget de salida\n",
        "out_split = widgets.Output()\n",
        "\n",
        "def mostrar_split(event=None):\n",
        "    clear_output(wait=True)           #  Borra todo lo visible antes de pintar de nuevo\n",
        "#    out_split.clear_output()          #  Borra lo que haba dentro del widget\n",
        "    with out_split:\n",
        "        #out_split.clear_output()\n",
        "        # Validar que los datos estn cargados\n",
        "        if 'X_data' not in globals() or 'Y_data' not in globals():\n",
        "            print(\" Debes cargar primero los datos (X_data, Y_data)\")\n",
        "            return\n",
        "        X = X_data.copy()\n",
        "        Y = Y_data.copy()\n",
        "\n",
        "        # Ttulo e instrucciones\n",
        "        display(HTML(\"<h3 style='color:#2E8B57;'> Filtrado de Datos para Train/Test</h3>\"))\n",
        "        print(\"Configura el split de tu dataset (recomendado test_size=0.2, random_state=42)\")\n",
        "\n",
        "        # Controles de usuario\n",
        "        test_size = widgets.FloatSlider(\n",
        "            value=0.2, min=0.05, max=0.5, step=0.05,\n",
        "            description='test_size:', tooltip='reserva el % de filas para test'\n",
        "        )\n",
        "        random_state = widgets.BoundedIntText(\n",
        "            value=42, min=0, description='random_state:'\n",
        "        )\n",
        "        stratify_chk = widgets.Checkbox(\n",
        "            value=False, description='Estratificar segn Y'\n",
        "        )\n",
        "        bin_method = widgets.Dropdown(\n",
        "            options=['qcut','cut'], value='qcut', description='Mtodo bins:'\n",
        "        )\n",
        "        q_bins = widgets.BoundedIntText(\n",
        "            value=5, min=2, max=20, description='q (bins):'\n",
        "        )\n",
        "        btn_split = widgets.Button(\n",
        "            description='Aplicar Split', button_style='success'\n",
        "        )\n",
        "\n",
        "        display(widgets.VBox([test_size, random_state,\n",
        "                              stratify_chk, bin_method, q_bins, btn_split]))\n",
        "\n",
        "        def run_split(b):\n",
        "            out_split.clear_output()      # <-- CAMBIO: borra cualquier contenido previo en out_split cuando se pulsa de nuevo\n",
        "            global X_train, X_test, Y_train, Y_test, FECHAS_train, FECHAS_test\n",
        "            # Preparar stratify\n",
        "            stratify = None\n",
        "            if stratify_chk.value:\n",
        "                try:\n",
        "                    # Asegurar que usamos una sola columna para la estratificacin\n",
        "                    if isinstance(Y, pd.DataFrame):\n",
        "                        Y_strat = Y.iloc[:, 0]\n",
        "                    else:\n",
        "                        Y_strat = pd.Series(Y)\n",
        "\n",
        "                    if bin_method.value == 'qcut':\n",
        "                        bins = pd.qcut(Y_strat, q=q_bins.value, duplicates='drop')\n",
        "                    else:\n",
        "                        bins = pd.cut(Y_strat, bins=q_bins.value)\n",
        "                    stratify = bins\n",
        "                except Exception as e:\n",
        "                    print(f\" Error al crear bins: {e}\")\n",
        "                    return\n",
        "\n",
        "            # Realizar split\n",
        "            try:\n",
        "                X_train, X_test, Y_train, Y_test, FECHAS_train, FECHAS_test = train_test_split(\n",
        "                    X, Y, FECHAS,\n",
        "                    test_size=test_size.value,\n",
        "                    random_state=random_state.value,\n",
        "                    stratify=stratify if stratify_chk.value else None\n",
        "                )\n",
        "                # justo aqu, guardo los parmetros en globals\n",
        "                global SPLIT_PARAMS                           # Nuevo desde aqui: Creado para poder generar el informe final\n",
        "                SPLIT_PARAMS = {\n",
        "                    \"test_size\": test_size.value,\n",
        "                    \"random_state\": random_state.value,\n",
        "                    \"stratify\": stratify_chk.value,\n",
        "                    \"bin_method\": bin_method.value,\n",
        "                    \"q_bins\": q_bins.value\n",
        "                }                                             # Nuevo hasta aqui\n",
        "            except Exception as e:\n",
        "                print(f\" Error en train_test_split: {e}\")\n",
        "                return\n",
        "\n",
        "            y_col = Y.columns[0]\n",
        "            df_train = X_train.copy()\n",
        "            df_train[y_col] = Y_train.values\n",
        "            df_train['fecha'] = FECHAS_train.values\n",
        "            df_train['set'] = 'train'\n",
        "\n",
        "            df_test = X_test.copy()\n",
        "            df_test[y_col] = Y_test.values\n",
        "            df_test['fecha'] = FECHAS_test.values\n",
        "            df_test['set'] = 'test'\n",
        "\n",
        "            df_out = pd.concat([df_train, df_test], axis=0)\n",
        "\n",
        "            # Mostrar resumen de particin\n",
        "            print(f\" Total registros: {len(X)}\")\n",
        "            print(f\" Registros en Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "            print(f\" Registros en Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "            # Mostrar DataFrame final\n",
        "            print(\" Resultado del split (train/test) para copiar y pegar:\")\n",
        "            display(df_out)\n",
        "\n",
        "            # Botones de copia X e Y\n",
        "            # Entrenamiento Y\n",
        "            btn_copy_y_train = widgets.Button(\n",
        "                description=' Copiar Y Train', icon='clipboard', button_style='info'\n",
        "            )\n",
        "            # Entrenamiento X\n",
        "            btn_copy_x_train = widgets.Button(\n",
        "                description=' Copiar X Train', icon='clipboard', button_style='info'\n",
        "            )\n",
        "            # Entrenamiento Fechas\n",
        "            btn_copy_f_train = widgets.Button(\n",
        "                description=' Copiar Fechas Train', icon='calendar', button_style='info'\n",
        "            )\n",
        "            # Test Y\n",
        "            btn_copy_y_test = widgets.Button(\n",
        "                description=' Copiar Y Test', icon='clipboard', button_style='info'\n",
        "            )\n",
        "            # Test X\n",
        "            btn_copy_x_test = widgets.Button(\n",
        "                description=' Copiar X Test', icon='clipboard', button_style='info'\n",
        "            )\n",
        "            # Test Fechas\n",
        "            btn_copy_f_test = widgets.Button(\n",
        "                description=' Copiar Fechas Test', icon='calendar', button_style='info'\n",
        "            )\n",
        "            msg_train_y = widgets.Output()\n",
        "            msg_train_x = widgets.Output()\n",
        "            msg_test_y = widgets.Output()\n",
        "            msg_test_x = widgets.Output()\n",
        "            msg_f_train = widgets.Output()\n",
        "            msg_f_test = widgets.Output()\n",
        "\n",
        "            def copy_y_train(_):\n",
        "                try:\n",
        "                    Y_train.to_frame() .to_clipboard(index=False)\n",
        "                    with msg_train_y:\n",
        "                        display(HTML(\"<span style='color:green;'> Y Train copiada</span>\"))\n",
        "                except Exception:\n",
        "                    csv = Y_train.to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_train_y:\n",
        "                        display(HTML(\"<span style='color:green;'> Y Train copiada via JS</span>\"))\n",
        "            def copy_x_train(_):\n",
        "                try:\n",
        "                    df_train.drop(columns=[Y.name,'set']).to_clipboard(index=False)\n",
        "                    with msg_train_x:\n",
        "                        display(HTML(\"<span style='color:green;'> X Train copiada</span>\"))\n",
        "                except Exception:\n",
        "                    csv = df_train.drop(columns=[Y.name,'set']).to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_train_x:\n",
        "                        display(HTML(\"<span style='color:green;'> X Train copiada via JS</span>\"))\n",
        "            def copy_f_train(_):\n",
        "                try:\n",
        "                    FECHAS.iloc[X_train.index].to_frame().to_clipboard(index=False)\n",
        "                    with msg_f_train:\n",
        "                        msg_f_train.clear_output()\n",
        "                        display(HTML(\"<span style='color:green;'> Fechas Train copiadas</span>\"))\n",
        "                except Exception:\n",
        "                    csv = FECHAS.iloc[X_train.index].to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_f_train:\n",
        "                        msg_f_train.clear_output()\n",
        "                        display(HTML(\"<span style='color:green;'> Fechas Train copiadas va JS</span>\"))\n",
        "\n",
        "            def copy_y_test(_):\n",
        "                try:\n",
        "                    Y_test.to_frame().to_clipboard(index=False)\n",
        "                    with msg_test_y:\n",
        "                        display(HTML(\"<span style='color:green;'> Y Test copiada</span>\"))\n",
        "                except Exception:\n",
        "                    csv = Y_test.to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_test_y:\n",
        "                        display(HTML(\"<span style='color:green;'> Y Test copiada via JS</span>\"))\n",
        "            def copy_x_test(_):\n",
        "                try:\n",
        "                    df_test.drop(columns=[Y.name,'set']).to_clipboard(index=False)\n",
        "                    with msg_test_x:\n",
        "                        display(HTML(\"<span style='color:green;'> X Test copiada</span>\"))\n",
        "                except Exception:\n",
        "                    csv = df_test.drop(columns=[Y.name,'set']).to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_test_x:\n",
        "                        display(HTML(\"<span style='color:green;'> X Test copiada via JS</span>\"))\n",
        "            def copy_f_test(_):\n",
        "                try:\n",
        "                    FECHAS.iloc[X_test.index].to_frame().to_clipboard(index=False)\n",
        "                    with msg_f_test:\n",
        "                        msg_f_test.clear_output()\n",
        "                        display(HTML(\"<span style='color:green;'> Fechas Test copiadas</span>\"))\n",
        "                except Exception:\n",
        "                    csv = FECHAS.iloc[X_test.index].to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_f_test:\n",
        "                        msg_f_test.clear_output()\n",
        "                        display(HTML(\"<span style='color:green;'> Fechas Test copiadas va JS</span>\"))\n",
        "\n",
        "            btn_copy_y_train.on_click(copy_y_train)\n",
        "            btn_copy_x_train.on_click(copy_x_train)\n",
        "            btn_copy_f_train.on_click(copy_f_train)\n",
        "            btn_copy_y_test.on_click(copy_y_test)\n",
        "            btn_copy_x_test.on_click(copy_x_test)\n",
        "            btn_copy_f_test.on_click(copy_f_test)\n",
        "\n",
        "            #display(widgets.HBox([btn_copy_y_train, btn_copy_x_train, btn_copy_y_test, btn_copy_x_test]))\n",
        "            display(widgets.HBox([\n",
        "                btn_copy_y_train, btn_copy_x_train, btn_copy_y_test, btn_copy_x_test,\n",
        "                btn_copy_f_train, btn_copy_f_test\n",
        "            ]))\n",
        "            #display(widgets.HBox([msg_train_y, msg_train_x, msg_test_y, msg_test_x]))\n",
        "            display(widgets.HBox([\n",
        "                msg_train_y, msg_train_x, msg_test_y, msg_test_x, msg_f_train, msg_f_test\n",
        "            ]))\n",
        "\n",
        "        btn_split.on_click(run_split)\n",
        "\n",
        "    display(out_split)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 6. SELECCION DE VARIABLES X QUE CORRELAN CON LA VARIABLE Y - FUNCIONA CORRECTAMENTE\n",
        "# Este mdulo permite seleccionar las variables X que tienen influencia en Y, usando los datos cargados, previa segmentacin entre Datos para Entrenamiento y datos para test\n",
        "# ===============================================================================\n",
        "import pkgutil\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import threading\n",
        "import time\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "# Libreras opcionales\n",
        "BORUTA_AVAILABLE = pkgutil.find_loader('boruta') is not None\n",
        "UMAP_AVAILABLE   = pkgutil.find_loader('umap')   is not None\n",
        "if BORUTA_AVAILABLE:\n",
        "    from boruta import BorutaPy\n",
        "if UMAP_AVAILABLE:\n",
        "    import umap\n",
        "\n",
        "# Temporizador\n",
        "_timer_stop_event = threading.Event()\n",
        "def _update_timer(event, widget):\n",
        "    start = time.time()\n",
        "    while not event.is_set():\n",
        "        widget.value = f\" {time.time() - start:.1f}s\"\n",
        "        time.sleep(0.5)\n",
        "\n",
        "# Salidas globales\n",
        "dlg_out = widgets.Output()\n",
        "res_out = widgets.Output()\n",
        "ayuda_out = widgets.Output()\n",
        "\n",
        "# Variables globales\n",
        "VARIABLES_SELECCIONADAS = {}\n",
        "METODO_SELECCION = \"\"\n",
        "VALORES_CORRELACION = pd.Series(dtype=float)\n",
        "RESUMEN_METODOS = {}\n",
        "\n",
        "\n",
        "def mostrar_seleccion_variables(event=None):\n",
        "    global X_train, Y_train\n",
        "    dlg_out.clear_output()\n",
        "    res_out.clear_output()\n",
        "    display(dlg_out, res_out)\n",
        "\n",
        "    with dlg_out:\n",
        "        clear_output()\n",
        "        if 'X_train' not in globals() or 'Y_train' not in globals():\n",
        "            print(\" Debes segmentar antes los datos (X_train, Y_train).\")\n",
        "            return\n",
        "\n",
        "        X = X_train.copy()\n",
        "        Y = Y_train.copy()\n",
        "        if isinstance(Y, pd.DataFrame):\n",
        "            Y_corr = Y.iloc[:, 0]\n",
        "        else:\n",
        "            Y_corr = pd.Series(Y)\n",
        "\n",
        "        print(f\" Variables disponibles: {X.shape[1]}\")\n",
        "        print(f\" Observaciones en entrenamiento: {X.shape[0]}\")\n",
        "\n",
        "        display(HTML(\"<h3 style='color:#2E8B57;'> Seleccin de Variables</h3>\"))\n",
        "\n",
        "        ayuda_pearson = widgets.HTML(\"\"\"\n",
        "        <ul>\n",
        "            <li><b>Umbral XY:</b> Selecciona las variables cuya correlacin absoluta con Y supera un valor determinado.</li>\n",
        "            <li><b>Valores recomendados:</b> entre 0.3 y 0.6 para evitar colinealidad excesiva.</li>\n",
        "            <li><i>Pearson mide relaciones lineales.</i></li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "        ayuda_spearman = widgets.HTML(\"\"\"\n",
        "        <ul>\n",
        "            <li><b>Umbral XY:</b> Selecciona variables con alta correlacin de rangos con Y.</li>\n",
        "            <li><b>Recomendado:</b> igual a Pearson (0.3  0.6).</li>\n",
        "            <li><i>Spearman es til para relaciones no lineales montonas.</i></li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "        ayuda_mi = widgets.HTML(\"\"\"\n",
        "        <ul>\n",
        "            <li><b>Top k MI:</b> Nmero de variables con mayor informacin mutua respecto a Y.</li>\n",
        "            <li><b>Valores recomendados:</b> entre 5 y 10 para datasets medianos.</li>\n",
        "            <li><i>Captura relaciones no lineales.</i></li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "        ayuda_boruta = widgets.HTML(\"\"\"\n",
        "        <ul>\n",
        "            <li><b>RF est:</b> Nmero de rboles del bosque aleatorio. (Recomendado:  100).</li>\n",
        "            <li><b>Iter BOR:</b> Iteraciones del algoritmo Boruta. (Recomendado: 50100).</li>\n",
        "            <li><b>Alpha:</b> Nivel de significancia estadstica. (Recomendado: 0.01  0.1).</li>\n",
        "            <li><i>Selecciona solo variables relevantes con base en importancia del modelo.</i></li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "        ayuda_umap = widgets.HTML(\"\"\"\n",
        "        <ul>\n",
        "            <li><b>Dims UMAP:</b> Dimensiones latentes en la proyeccin (tpicamente 25).</li>\n",
        "            <li><b>Top k UMAP:</b> Variables con mayor correlacin a las dimensiones proyectadas.</li>\n",
        "            <li><i>UMAP revela estructuras no lineales complejas en los datos.</i></li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        metodo = widgets.Dropdown(\n",
        "            options=[\"Pearson\", \"Spearman\", \"MutualInfo\"]\n",
        "                    + ([\"Boruta\"] if BORUTA_AVAILABLE else [])\n",
        "                    + ([\"UMAP\"] if UMAP_AVAILABLE else []),\n",
        "            description='Mtodo:'\n",
        "        )\n",
        "        th_xy = widgets.FloatSlider(\n",
        "            value=0.1, min=0.0, max=1.0, step=0.01, description='Umbral XY:', readout=False\n",
        "        )\n",
        "        th_xy_lbl = widgets.Label(value=f\"{th_xy.value:.2f}\")\n",
        "        def actualizar_umbral(change):\n",
        "            th_xy_lbl.value = f\"{change['new']:.2f}\"\n",
        "        th_xy.observe(actualizar_umbral, names='value')\n",
        "        fila_th_xy = widgets.HBox([th_xy, th_xy_lbl])\n",
        "\n",
        "        mi_k     = widgets.BoundedIntText(value=5, min=1, max=X.shape[1], description='Top k MI:')\n",
        "        bor_n    = widgets.BoundedIntText(value=100, min=1, max=1000, description='RF est:')\n",
        "        bor_iter = widgets.BoundedIntText(value=50, min=1, max=500, description='Iter BOR:')\n",
        "        bor_a    = widgets.BoundedFloatText(value=0.05, min=0.0, max=1.0, description='Alpha:')\n",
        "        umap_d   = widgets.BoundedIntText(value=2, min=1, max=min(10, X.shape[1]), description='Dims UMAP:')\n",
        "        umap_k   = widgets.BoundedIntText(value=5, min=1, max=X.shape[1], description='Top k UMAP:')\n",
        "        btn_run  = widgets.Button(description='Ejecutar', button_style='success')\n",
        "        btn_resumen = widgets.Button(description=' Ver Resumen', button_style='info')\n",
        "        timer_lbl= widgets.Label(' 0.0s')\n",
        "\n",
        "        ayudas = {\n",
        "            'Pearson': ayuda_pearson,\n",
        "            'Spearman': ayuda_spearman,\n",
        "            'MutualInfo': ayuda_mi,\n",
        "            'Boruta': ayuda_boruta,\n",
        "            'UMAP': ayuda_umap\n",
        "        }\n",
        "\n",
        "        fila_th_xy.layout.display = 'none'\n",
        "        mi_k.layout.display = 'none'\n",
        "        bor_n.layout.display = 'none'\n",
        "        bor_iter.layout.display = 'none'\n",
        "        bor_a.layout.display = 'none'\n",
        "        umap_d.layout.display = 'none'\n",
        "        umap_k.layout.display = 'none'\n",
        "\n",
        "        def toggle_params(_=None):\n",
        "            m = metodo.value\n",
        "            corr_visible = m in ['Pearson', 'Spearman']\n",
        "            fila_th_xy.layout.display = 'flex' if corr_visible else 'none'\n",
        "            mi_k.layout.display     = 'block' if m == 'MutualInfo' else 'none'\n",
        "            bor_n.layout.display    = 'block' if m == 'Boruta' else 'none'\n",
        "            bor_iter.layout.display = 'block' if m == 'Boruta' else 'none'\n",
        "            bor_a.layout.display    = 'block' if m == 'Boruta' else 'none'\n",
        "            umap_d.layout.display   = 'block' if m == 'UMAP' else 'none'\n",
        "            umap_k.layout.display   = 'block' if m == 'UMAP' else 'none'\n",
        "            with ayuda_out:\n",
        "                clear_output()\n",
        "                if m in ayudas:\n",
        "                    display(ayudas[m])\n",
        "\n",
        "        metodo.observe(toggle_params, names='value')\n",
        "        toggle_params()\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo, fila_th_xy, mi_k, bor_n, bor_iter, bor_a, umap_d, umap_k, ayuda_out, btn_run, btn_resumen, timer_lbl\n",
        "        ]))\n",
        "\n",
        "    def run_selection(_):\n",
        "        global VARIABLES_SELECCIONADAS, METODO_SELECCION, VALORES_CORRELACION, RESUMEN_METODOS\n",
        "        res_out.clear_output()\n",
        "        _timer_stop_event.clear()\n",
        "        threading.Thread(target=_update_timer, args=(_timer_stop_event, timer_lbl), daemon=True).start()\n",
        "\n",
        "        with res_out:\n",
        "            X = X_train.copy()\n",
        "            Y = Y_train.copy()\n",
        "            Y_corr = Y.iloc[:, 0] if isinstance(Y, pd.DataFrame) else pd.Series(Y)\n",
        "            method = metodo.value\n",
        "            selected = []\n",
        "            correlaciones = pd.Series(dtype=float)\n",
        "\n",
        "            if method in ['Pearson', 'Spearman']:\n",
        "                correlaciones = X.apply(lambda col: col.corr(Y_corr, method=method.lower()))\n",
        "                correlaciones_abs = correlaciones.abs()\n",
        "                display(HTML(f\"<h4> Correlaciones XY (abs) usando <u>{method}</u>:</h4>\"))\n",
        "                display(correlaciones_abs.to_frame(name=f'|correlacin {method}|'))\n",
        "                selected = correlaciones_abs[correlaciones_abs >= th_xy.value].index.tolist()\n",
        "                not_selected = correlaciones_abs[~correlaciones_abs.index.isin(selected)].index.tolist()\n",
        "\n",
        "                if selected:\n",
        "                    display(HTML(f\"<h4 style='color:green;'> Variables Seleccionadas ({method}, umbral  {th_xy.value}):</h4>\"))\n",
        "                    display(correlaciones[selected].to_frame(name=f'correlacin {method}').sort_values(by=f'correlacin {method}', ascending=False))\n",
        "                else:\n",
        "                    display(HTML(f\"<h4 style='color:red;'> No se seleccionaron variables con {method}  {th_xy.value}</h4>\"))\n",
        "\n",
        "                if not_selected:\n",
        "                    display(HTML(f\"<h4> Variables No Seleccionadas ({method}):</h4>\"))\n",
        "                    display(correlaciones[not_selected].to_frame(name=f'correlacin {method}').sort_values(by=f'correlacin {method}', ascending=False))\n",
        "\n",
        "            elif method == 'MutualInfo':\n",
        "                mi = mutual_info_regression(X, Y_corr)\n",
        "                correlaciones = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
        "                display(HTML('<b>Top MutualInfo:</b>'))\n",
        "                display(correlaciones.head(mi_k.value).to_frame('MI'))\n",
        "                selected = correlaciones.head(mi_k.value).index.tolist()\n",
        "\n",
        "            elif method == 'Boruta' and BORUTA_AVAILABLE:\n",
        "                #rf = RandomForestRegressor(n_estimators=bor_n.value, random_state=42)\n",
        "                #bor = BorutaPy(rf, alpha=bor_a.value, max_iter=bor_iter.value, random_state=42)\n",
        "                #  AADIDO: usar todos los cores \n",
        "                rf = RandomForestRegressor(\n",
        "                    n_estimators=bor_n.value,\n",
        "                    random_state=42,\n",
        "                    n_jobs=-1              # Paraleliza el entrenamiento del RF\n",
        "                )\n",
        "                bor = BorutaPy(\n",
        "                    estimator=rf,\n",
        "                    alpha=bor_a.value,\n",
        "                    max_iter=bor_iter.value,\n",
        "                    random_state=42,\n",
        "                    verbose=2          # < 1 para resumen, 2 o 3 para detalle completo\n",
        "                )\n",
        "                bor.fit(X.values, Y_corr.values)\n",
        "                support_mask = bor.support_\n",
        "                correlaciones = pd.Series(bor.ranking_, index=X.columns)\n",
        "                dfb = pd.DataFrame({'Feature': X.columns, 'Rank': bor.ranking_, 'Support': support_mask})\n",
        "                display(HTML('<b>Ranking Boruta Top10:</b>'))\n",
        "                display(dfb.sort_values('Rank').head(10))\n",
        "                selected = list(X.columns[support_mask])\n",
        "\n",
        "            elif method == 'UMAP' and UMAP_AVAILABLE:\n",
        "                reducer = umap.UMAP(n_components=umap_d.value, random_state=42)\n",
        "                emb = reducer.fit_transform(X)\n",
        "                dfemb = pd.DataFrame(emb)\n",
        "                corr = X.apply(lambda c: np.mean([abs(np.corrcoef(c, dfemb[i])[0,1]) for i in range(dfemb.shape[1])]), axis=0)\n",
        "                correlaciones = corr\n",
        "                display(HTML('<b>Correlacin media UMAP:</b>'))\n",
        "                display(correlaciones.to_frame('mean_corr'))\n",
        "                selected = correlaciones.sort_values(ascending=False).head(umap_k.value).index.tolist()\n",
        "\n",
        "            if not selected:\n",
        "                print(\" No se seleccionaron variables. Ajusta el umbral o mtodo.\")\n",
        "            else:\n",
        "                VARIABLES_SELECCIONADAS = selected\n",
        "                METODO_SELECCION = method\n",
        "                VALORES_CORRELACION = correlaciones\n",
        "                RESUMEN_METODOS[method] = selected\n",
        "\n",
        "                #  AADIDO: sanitizar nombres para que no haya corchetes, % ni espacios raros \n",
        "                #import re\n",
        "                #clean = lambda c: re.sub(r'[\\[\\]<>%]', '_', str(c))\n",
        "                #selected = [ clean(c) for c in selected ]\n",
        "                #  FIN AADIDO \n",
        "                #RESUMEN_METODOS[method] = selected\n",
        "\n",
        "                dfout = X[selected].copy()\n",
        "                dfout[Y_corr.name] = Y_corr.values\n",
        "                display(HTML('<b>Variables Seleccionadas:</b>'))\n",
        "                display(dfout)\n",
        "                txt = widgets.Textarea(value=dfout.drop(columns=[Y_corr.name]).to_csv(index=False),\n",
        "                                       layout=widgets.Layout(width='100%', height='150px'))\n",
        "                display(HTML('<b>CSV X:</b>'))\n",
        "                display(txt)\n",
        "\n",
        "        _timer_stop_event.set()\n",
        "\n",
        "    def mostrar_resumen(b):\n",
        "        with res_out:\n",
        "            clear_output()\n",
        "            if not RESUMEN_METODOS:\n",
        "                display(HTML(\"<b> No hay mtodos ejecutados an.</b>\"))\n",
        "            else:\n",
        "                for metodo, variables in RESUMEN_METODOS.items():\n",
        "                    display(HTML(f\"<h4> {metodo}:</h4>\"))\n",
        "                    display(pd.DataFrame(variables, columns=[\"Variables Seleccionadas\"]))\n",
        "\n",
        "    btn_run.on_click(run_selection, remove=True)\n",
        "    btn_run.on_click(run_selection)\n",
        "    btn_resumen.on_click(mostrar_resumen)\n",
        "\n",
        "# Mostrar panel en ejecucin directa\n",
        "#display(dlg_out, res_out)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 7. ENTRENAMIENTO MODELOS Y VALIDACION DE ENTRENAMIENTO - FUNCIONA CORRECTAMENTE\n",
        "# Este mdulo se descompone en varios sub-modulos para el entrenamiento de cada uno de los modelos con los datos cargados (segmento entrenamiento) y para las variables que correlan.\n",
        "# Posteriormente se valida el entrenamiento con el segmento de datos de test, para la prueba de los entrenamientos.\n",
        "# ===============================================================\n",
        "# ===============================================================\n",
        "# 7.1 ENTRENAMIENTO SVR - FUNCIONA CORRECTAMENTE\n",
        "# Este mdulo se usa para entrenar el modelo SVR, permitiendo al usuario ajustar los principales parmetros de diseo.\n",
        "# ===============================================================\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Widget principal de salida (si no existe)\n",
        "if 'out_svr' not in globals():\n",
        "    out_svr = widgets.Output()\n",
        "\n",
        "def mostrar_svr(b=None):\n",
        "    if b is None:\n",
        "        display(out_svr)\n",
        "\n",
        "    with out_svr:\n",
        "        clear_output()\n",
        "\n",
        "        if 'X_train' not in globals() or 'X_test' not in globals():\n",
        "            display(widgets.HTML(\"\"\"<span style='color:red;'> Primero debes segmentar los datos en train/test.</span>\"\"\"))\n",
        "            return\n",
        "\n",
        "        # Bloque para sincronizar variables individuales a partir del resumen RESUMEN_METODOS\n",
        "        if 'RESUMEN_METODOS' in globals():\n",
        "            for metodo, variables in RESUMEN_METODOS.items():\n",
        "                globals()[f\"selected_vars_{metodo.lower()}\"] = variables\n",
        "\n",
        "        # Widgets para configuracin\n",
        "        metodo_sel = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selector:'\n",
        "        )\n",
        "        C_val = widgets.FloatLogSlider(value=1.0, base=10, min=-2, max=2, step=0.1, description='C:')\n",
        "        epsilon_val = widgets.FloatLogSlider(value=0.1, base=10, min=-3, max=0, step=0.1, description='Epsilon:')\n",
        "        kernel_val = widgets.Dropdown(options=['rbf', 'linear', 'poly', 'sigmoid'], value='rbf', description='Kernel:')\n",
        "        gamma_val = widgets.Dropdown(options=['scale', 'auto'], description='Gamma:')\n",
        "\n",
        "        btn_train = widgets.Button(description=\" Entrenar SVR\", button_style='success')\n",
        "        output_area = widgets.Output()\n",
        "        tiempo_lbl = widgets.Label()\n",
        "\n",
        "        def entrenar_svr(_):\n",
        "            output_area.clear_output()\n",
        "            resumen_modelos = []\n",
        "            inicio = time.time()\n",
        "\n",
        "            metodos = [metodo_sel.value] if metodo_sel.value != 'Todos' else ['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "            colores = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "            fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "            with output_area:\n",
        "                for i, metodo in enumerate(metodos):\n",
        "                    var_key = f\"selected_vars_{metodo.lower()}\"\n",
        "                    if var_key not in globals():\n",
        "                        print(f\" No hay variables seleccionadas para el mtodo: {metodo}\")\n",
        "                        continue\n",
        "                    selected_vars = globals()[var_key]\n",
        "\n",
        "                    Xtr, Xts = X_train[selected_vars], X_test[selected_vars]\n",
        "                    ytr, yts = Y_train.values.ravel(), Y_test.values.ravel()\n",
        "\n",
        "                    sx, sy = StandardScaler(), StandardScaler()\n",
        "                    Xtr_scaled = sx.fit_transform(Xtr)\n",
        "                    Xts_scaled = sx.transform(Xts)\n",
        "                    ytr_scaled = sy.fit_transform(ytr.reshape(-1,1)).ravel()\n",
        "\n",
        "                    model = SVR(C=C_val.value, epsilon=epsilon_val.value, kernel=kernel_val.value, gamma=gamma_val.value)\n",
        "                    model.fit(Xtr_scaled, ytr_scaled)\n",
        "\n",
        "                    y_pred_scaled = model.predict(Xts_scaled)\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "\n",
        "                    r2 = r2_score(yts, y_pred)\n",
        "                    mse = mean_squared_error(yts, y_pred)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    mae = mean_absolute_error(yts, y_pred)\n",
        "\n",
        "                    resumen_modelos.append({\n",
        "                        'Mtodo': metodo,\n",
        "                        'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae\n",
        "                    })\n",
        "\n",
        "                    # Guardar modelo\n",
        "                    nombre_archivo = f\"modelo_svr_{metodo.lower()}.pkl\"\n",
        "                    with open(nombre_archivo, 'wb') as f:\n",
        "                        pickle.dump({'model': model, 'sx': sx, 'sy': sy, 'cols': selected_vars, 'yname': y_variable_name}, f)\n",
        "\n",
        "                    ax.plot(y_pred, label=f'{metodo}', alpha=0.7, linestyle='--', color=colores[i])\n",
        "                    print(f\" Modelo {metodo} entrenado. R: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "                if resumen_modelos:\n",
        "                    ax.plot(yts, label='Y Real', color='black', linewidth=2)\n",
        "                    ax.set_title('Comparacin Y Real vs Predicciones SVR por Mtodo')\n",
        "                    ax.grid(); ax.legend()\n",
        "                    plt.show()\n",
        "\n",
        "                    # Tabla resumen\n",
        "                    print(\"\\n Resumen comparativo de mtricas:\")\n",
        "                    df = pd.DataFrame(resumen_modelos)\n",
        "                    display(df.set_index('Mtodo'))\n",
        "\n",
        "            tiempo_lbl.value = f\" Duracin total: {time.time()-inicio:.2f} segundos\"\n",
        "\n",
        "        btn_train.on_click(entrenar_svr)\n",
        "\n",
        "        # Ayuda extendida\n",
        "        ayuda = widgets.HTML(\"\"\"\n",
        "        <h4> Ayuda - Parmetros del modelo SVR</h4>\n",
        "        <ul>\n",
        "            <li><b>C:</b> Penalizacin al error. Valores altos = bajo sesgo, alto sobreajuste. Recomendado: 0.1 a 100</li>\n",
        "            <li><b>Epsilon:</b> Margen de tolerancia para el error. Cuanto mayor, ms simple el modelo. Recomendado: 0.001 a 0.1</li>\n",
        "            <li><b>Kernel:</b> Funcin para proyectar los datos. rbf es el ms comn. Otras: linear, poly, sigmoid</li>\n",
        "            <li><b>Gamma:</b> Influencia de un punto de entrenamiento. 'scale' es recomendado.</li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            widgets.HBox([metodo_sel]),\n",
        "            widgets.HBox([C_val, epsilon_val]),\n",
        "            widgets.HBox([kernel_val, gamma_val]),\n",
        "            btn_train, tiempo_lbl,\n",
        "            ayuda, output_area\n",
        "        ]))\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 7.2. ENTRENAMIENTO RED NEURONAL - FUNCIONA CORRECTAMENTE\n",
        "# Este mdulo permite entrenar una red neuronal con las variables seleccionadas\n",
        "# por diferentes mtodos (Pearson, Spearman, Mutual Info, Boruta, UMAP, o Todos)\n",
        "# sobre los datos previamente segmentados como Train y evaluar sobre Test.\n",
        "# ===============================================================\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "if 'out_nn' not in globals():\n",
        "    out_nn = widgets.Output()\n",
        "\n",
        "def mostrar_nn(b=None):\n",
        "    if b is None:\n",
        "        display(out_nn)\n",
        "\n",
        "    with out_nn:\n",
        "        clear_output()\n",
        "\n",
        "        if 'X_train' not in globals() or 'X_test' not in globals():\n",
        "            display(widgets.HTML(\"<span style='color:red;'> Primero debes segmentar los datos en train/test.</span>\"))\n",
        "            return\n",
        "\n",
        "        if 'RESUMEN_METODOS' in globals():\n",
        "            for metodo, variables in RESUMEN_METODOS.items():\n",
        "                globals()[f\"selected_vars_{metodo.lower()}\"] = variables\n",
        "\n",
        "        metodo_sel = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selector:'\n",
        "        )\n",
        "\n",
        "        capas = widgets.BoundedIntText(value=2, min=1, max=10, description='Capas ocultas:')\n",
        "        neuronas = widgets.Text(value='64,32', description='Neuronas por capa:', placeholder='Ej: 64,32')\n",
        "        activacion = widgets.Dropdown(options=['relu','tanh','sigmoid'], value='relu', description='Activacin:')\n",
        "        loss_fn = widgets.Dropdown(options=[('MSE','mse'),('MAE','mae'),('Huber','huber')], value='mse', description='Prdida:')\n",
        "        tasa = widgets.FloatText(value=0.001, description='Learning Rate:')\n",
        "        epocas = widgets.BoundedIntText(value=100, min=1, description='Epocas:')\n",
        "        batch = widgets.BoundedIntText(value=32, min=1, description='Batch size:')\n",
        "        opt = widgets.Dropdown(options=['adam','sgd','rmsprop'], value='adam', description='Optimizador:')\n",
        "\n",
        "        btn_train = widgets.Button(description=' Entrenar NN', button_style='success')\n",
        "        output_area = widgets.Output()\n",
        "        tiempo_lbl = widgets.Label()\n",
        "\n",
        "        def entrenar_nn(_):\n",
        "            output_area.clear_output()\n",
        "            resumen_modelos = []\n",
        "            inicio = time.time()\n",
        "\n",
        "            metodos = [metodo_sel.value] if metodo_sel.value != 'Todos' else ['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "            colores = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "            fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "            with output_area:\n",
        "                for i, metodo in enumerate(metodos):\n",
        "                    var_key = f\"selected_vars_{metodo.lower()}\"\n",
        "                    if var_key not in globals():\n",
        "                        print(f\" No hay variables seleccionadas para el mtodo: {metodo}\")\n",
        "                        continue\n",
        "\n",
        "                    selected_vars = globals()[var_key]\n",
        "                    Xtr, Xts = X_train[selected_vars], X_test[selected_vars]\n",
        "                    ytr, yts = Y_train.values.ravel(), Y_test.values.ravel()\n",
        "\n",
        "                    sx, sy = StandardScaler(), StandardScaler()\n",
        "                    Xtr_scaled = sx.fit_transform(Xtr)\n",
        "                    Xts_scaled = sx.transform(Xts)\n",
        "                    ytr_scaled = sy.fit_transform(ytr.reshape(-1,1)).ravel()\n",
        "\n",
        "                    model = Sequential()\n",
        "                    try:\n",
        "                        layers = [int(n) for n in neuronas.value.split(',')]\n",
        "                        if len(layers) != capas.value:\n",
        "                            raise ValueError(\" Especifica tantas capas como valores de neuronas.\")\n",
        "                        model.add(Dense(layers[0], activation=activacion.value, input_shape=(Xtr_scaled.shape[1],)))\n",
        "                        for units in layers[1:]:\n",
        "                            model.add(Dense(units, activation=activacion.value))\n",
        "                        model.add(Dense(1))\n",
        "                    except Exception as e:\n",
        "                        print(f\" Error en la definicin de la arquitectura: {e}\")\n",
        "                        return\n",
        "\n",
        "                    opt_dict = {'adam': Adam, 'sgd': SGD, 'rmsprop': RMSprop}\n",
        "                    optimizer = opt_dict[opt.value](learning_rate=tasa.value)\n",
        "\n",
        "                    loss = {'mse':'mean_squared_error','mae':'mean_absolute_error','huber':tf.keras.losses.Huber()}[loss_fn.value]\n",
        "                    model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n",
        "\n",
        "                    # === Aqu guardamos los hiperparmetros antes de entrenar ===\n",
        "                    hp = {\n",
        "                        'capas_ocultas': capas.value,\n",
        "                        'neuronas_por_capa': neuronas.value,  # e.g. '64,32'\n",
        "                        'activacion': activacion.value,\n",
        "                        'loss_fn': loss_fn.value,\n",
        "                        'learning_rate': tasa.value,\n",
        "                        'epocas': epocas.value,\n",
        "                        'batch_size': batch.value,\n",
        "                        'optimizador': opt.value\n",
        "                    }\n",
        "                    import pickle\n",
        "                    hp_fname = f\"hyperparams_nn_{metodo.lower()}.pkl\"\n",
        "                    try:\n",
        "                        with open(hp_fname, \"wb\") as f_hp:\n",
        "                            pickle.dump(hp, f_hp)\n",
        "                        print(f\" Hiperparmetros guardados en {hp_fname}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\" No se pudo guardar hiperparmetros en {hp_fname}: {e}\")\n",
        "\n",
        "                    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "                    history = model.fit(Xtr_scaled, ytr_scaled, validation_split=0.2, epochs=epocas.value, batch_size=batch.value, verbose=0, callbacks=[es])\n",
        "\n",
        "                    y_pred_scaled = model.predict(Xts_scaled).ravel()\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "\n",
        "                    r2 = r2_score(yts, y_pred)\n",
        "                    mse = mean_squared_error(yts, y_pred)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    mae = mean_absolute_error(yts, y_pred)\n",
        "\n",
        "                    resumen_modelos.append({'Mtodo': metodo, 'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae})\n",
        "\n",
        "                    nombre_archivo = f\"modelo_nn_{metodo.lower()}.h5\"\n",
        "                    model.save(nombre_archivo)\n",
        "\n",
        "                    with open(f\"escaladores_nn_{metodo.lower()}.pkl\", \"wb\") as f:\n",
        "                        pickle.dump({\n",
        "                            'scaler_X': sx,\n",
        "                            'scaler_Y': sy,\n",
        "                            'cols': selected_vars,\n",
        "                            'yname': y_variable_name\n",
        "                        }, f)\n",
        "\n",
        "                    ax.plot(y_pred, label=f'{metodo}', alpha=0.7, linestyle='--', color=colores[i])\n",
        "                    print(f\" Modelo {metodo} entrenado. R: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "\n",
        "                if resumen_modelos:\n",
        "                    ax.plot(yts, label='Y Real', color='black', linewidth=2)\n",
        "                    ax.set_title('Comparacin Y Real vs Predicciones NN por Mtodo')\n",
        "                    ax.grid(); ax.legend()\n",
        "                    plt.show()\n",
        "\n",
        "                    print(\"\\n Resumen comparativo de mtricas:\")\n",
        "                    df = pd.DataFrame(resumen_modelos)\n",
        "                    display(df.set_index('Mtodo'))\n",
        "\n",
        "            tiempo_lbl.value = f\" Duracin total: {time.time()-inicio:.2f} segundos\"\n",
        "\n",
        "        btn_train.on_click(entrenar_nn)\n",
        "\n",
        "        ayuda = widgets.HTML(\"\"\"\n",
        "        <h4> Ayuda - Parmetros de la Red Neuronal</h4>\n",
        "        <ul>\n",
        "            <li><b>Capas ocultas:</b> Nmero de capas intermedias. Ms capas pueden mejorar la expresividad.</li>\n",
        "            <li><b>Neuronas por capa:</b> Lista separada por comas. Cada valor representa una capa. Ej: 64,32</li>\n",
        "            <li><b>Activacin:</b> Funciones como relu (recomendado), tanh, sigmoid. Afectan la no linealidad.</li>\n",
        "            <li><b>Learning Rate:</b> Tamao del paso. Valores tpicos: 0.001, 0.01</li>\n",
        "            <li><b>Epocas:</b> Nmero de iteraciones sobre el dataset. Demasiadas pueden sobreajustar.</li>\n",
        "            <li><b>Batch size:</b> Tamao de lote en cada actualizacin de gradiente.</li>\n",
        "            <li><b>Optimizador:</b> Algoritmo de ajuste. Adam es general. SGD es ms simple. RMSprop es bueno en secuencias.</li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            widgets.HBox([metodo_sel]),\n",
        "            widgets.HBox([capas, neuronas]),\n",
        "            widgets.HBox([activacion, opt]),\n",
        "            widgets.HBox([tasa, epocas, batch]),\n",
        "            widgets.HBox([loss_fn]),\n",
        "            btn_train, tiempo_lbl,\n",
        "            ayuda, output_area\n",
        "        ]))\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 7.3. ENTRENAMIENTO XGBOOST - FUNCIONA CORRECTAMENTE\n",
        "# Este mdulo permite entrenar el modelo XGBoost con las variables seleccionadas\n",
        "# por diferentes mtodos (Pearson, Spearman, Mutual Info, Boruta, UMAP, o Todos)\n",
        "# sobre los datos previamente segmentados como Train y evaluar sobre Test.\n",
        "# ===============================================================\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Widget principal de salida (si no existe)\n",
        "if 'out_xgb' not in globals():\n",
        "    out_xgb = widgets.Output()\n",
        "\n",
        "def mostrar_xgb(b=None):\n",
        "    if b is None:\n",
        "        display(out_xgb)\n",
        "\n",
        "    with out_xgb:\n",
        "        clear_output()\n",
        "\n",
        "        if 'X_train' not in globals() or 'X_test' not in globals():\n",
        "            display(widgets.HTML(\"\"\"<span style='color:red;'> Primero debes segmentar los datos en train/test.</span>\"\"\"))\n",
        "            return\n",
        "\n",
        "        # Bloque para sincronizar variables individuales a partir del resumen RESUMEN_METODOS\n",
        "        if 'RESUMEN_METODOS' in globals():\n",
        "            for metodo, variables in RESUMEN_METODOS.items():\n",
        "                globals()[f\"selected_vars_{metodo.lower()}\"] = variables\n",
        "\n",
        "        metodo_sel = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selector:'\n",
        "        )\n",
        "        n_estimators = widgets.IntSlider(value=100, min=10, max=1000, step=10, description='rboles:')\n",
        "        learning_rate = widgets.FloatLogSlider(value=0.1, base=10, min=-3, max=0, step=0.01, description='Learning Rate:')\n",
        "        max_depth = widgets.IntSlider(value=3, min=1, max=20, step=1, description='Profundidad:')\n",
        "        subsample = widgets.FloatSlider(value=1.0, min=0.1, max=1.0, step=0.1, description='Subsample:')\n",
        "\n",
        "        btn_train = widgets.Button(description=\" Entrenar XGBoost\", button_style='success')\n",
        "        output_area = widgets.Output()\n",
        "        tiempo_lbl = widgets.Label()\n",
        "\n",
        "        def entrenar_xgb(_):\n",
        "            output_area.clear_output()\n",
        "            resumen_modelos = []\n",
        "            inicio = time.time()\n",
        "\n",
        "            metodos = [metodo_sel.value] if metodo_sel.value != 'Todos' else ['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "            colores = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "            fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "            with output_area:\n",
        "                for i, metodo in enumerate(metodos):\n",
        "                    var_key = f\"selected_vars_{metodo.lower()}\"\n",
        "                    if var_key not in globals():\n",
        "                        print(f\" No hay variables seleccionadas para el mtodo: {metodo}\")\n",
        "                        continue\n",
        "                    selected_vars = globals()[var_key]\n",
        "\n",
        "                    Xtr, Xts = X_train[selected_vars], X_test[selected_vars]\n",
        "                    ytr, yts = Y_train.values.ravel(), Y_test.values.ravel()\n",
        "\n",
        "                    sx, sy = StandardScaler(), StandardScaler()\n",
        "                    Xtr_scaled = sx.fit_transform(Xtr)\n",
        "                    Xts_scaled = sx.transform(Xts)\n",
        "                    ytr_scaled = sy.fit_transform(ytr.reshape(-1,1)).ravel()\n",
        "\n",
        "                    model = XGBRegressor(\n",
        "                        n_estimators=n_estimators.value,\n",
        "                        learning_rate=learning_rate.value,\n",
        "                        max_depth=max_depth.value,\n",
        "                        subsample=subsample.value,\n",
        "                        verbosity=0\n",
        "                    )\n",
        "                    model.fit(Xtr_scaled, ytr_scaled)\n",
        "\n",
        "                    y_pred_scaled = model.predict(Xts_scaled)\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "\n",
        "                    r2 = r2_score(yts, y_pred)\n",
        "                    mse = mean_squared_error(yts, y_pred)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    mae = mean_absolute_error(yts, y_pred)\n",
        "\n",
        "                    resumen_modelos.append({\n",
        "                        'Mtodo': metodo,\n",
        "                        'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae\n",
        "                    })\n",
        "\n",
        "                    # Guardar modelo\n",
        "                    nombre_archivo = f\"modelo_xgb_{metodo.lower()}.pkl\"\n",
        "                    with open(nombre_archivo, 'wb') as f:\n",
        "                        pickle.dump({'model': model, 'sx': sx, 'sy': sy, 'cols': selected_vars, 'yname': y_variable_name}, f)\n",
        "\n",
        "                    ax.plot(y_pred, label=f'{metodo}', alpha=0.7, linestyle='--', color=colores[i])\n",
        "                    print(f\" Modelo {metodo} entrenado. R: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "                if resumen_modelos:\n",
        "                    ax.plot(yts, label='Y Real', color='black', linewidth=2)\n",
        "                    ax.set_title('Comparacin Y Real vs Predicciones XGBoost por Mtodo')\n",
        "                    ax.grid(); ax.legend()\n",
        "                    plt.show()\n",
        "\n",
        "                    print(\"\\n Resumen comparativo de mtricas:\")\n",
        "                    df = pd.DataFrame(resumen_modelos)\n",
        "                    display(df.set_index('Mtodo'))\n",
        "\n",
        "            tiempo_lbl.value = f\" Duracin total: {time.time()-inicio:.2f} segundos\"\n",
        "\n",
        "        btn_train.on_click(entrenar_xgb)\n",
        "\n",
        "        ayuda = widgets.HTML(\"\"\"\n",
        "        <h4> Ayuda - Parmetros del modelo XGBoost</h4>\n",
        "        <ul>\n",
        "            <li><b>n_estimators:</b> nmero de rboles. Mayor nmero = mayor precisin pero ms tiempo.</li>\n",
        "            <li><b>learning_rate:</b> tasa de aprendizaje. Pequeos valores mejoran precisin, pero requieren ms rboles.</li>\n",
        "            <li><b>max_depth:</b> profundidad de rboles. Mayor profundidad permite ms complejidad, pero riesgo de sobreajuste.</li>\n",
        "            <li><b>subsample:</b> fraccin de muestras usadas por rbol. Menor valor ayuda a regularizacin.</li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            widgets.HBox([metodo_sel]),\n",
        "            widgets.HBox([n_estimators, learning_rate]),\n",
        "            widgets.HBox([max_depth, subsample]),\n",
        "            btn_train, tiempo_lbl,\n",
        "            ayuda, output_area\n",
        "        ]))\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 7.4. ENTRENAMIENTO RANDOM FOREST - FUNCIONA CORRECTAMENTE\n",
        "# Este mdulo permite entrenar Random Forest con variables seleccionadas\n",
        "# por diferentes mtodos (Pearson, Spearman, Mutual Info, Boruta, UMAP, o Todos),\n",
        "# usando datos de entrenamiento y validando sobre test.\n",
        "# ===============================================================\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Widget principal de salida (si no existe)\n",
        "if 'out_rf' not in globals():\n",
        "    out_rf = widgets.Output()\n",
        "\n",
        "def mostrar_rf(b=None):\n",
        "    if b is None:\n",
        "        display(out_rf)\n",
        "\n",
        "    with out_rf:\n",
        "        clear_output()\n",
        "\n",
        "        if 'X_train' not in globals() or 'X_test' not in globals():\n",
        "            display(widgets.HTML(\"\"\"<span style='color:red;'> Primero debes segmentar los datos en train/test.</span>\"\"\"))\n",
        "            return\n",
        "\n",
        "        if 'RESUMEN_METODOS' in globals():\n",
        "            for metodo, variables in RESUMEN_METODOS.items():\n",
        "                globals()[f\"selected_vars_{metodo.lower()}\"] = variables\n",
        "\n",
        "        metodo_sel = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selector:'\n",
        "        )\n",
        "\n",
        "        n_estimators = widgets.IntSlider(value=100, min=10, max=500, step=10, description='rboles:')\n",
        "        max_depth = widgets.IntSlider(value=5, min=1, max=50, step=1, description='Profundidad:')\n",
        "        min_samples_split = widgets.IntSlider(value=2, min=2, max=20, step=1, description='Min Split:')\n",
        "        min_samples_leaf = widgets.IntSlider(value=1, min=1, max=20, step=1, description='Min Leaf:')\n",
        "        max_features = widgets.Dropdown(options=['auto', 'sqrt', 'log2'], value='sqrt', description='Max Features:')\n",
        "        bootstrap = widgets.Checkbox(value=True, description='Bootstrap:')\n",
        "\n",
        "        btn_train = widgets.Button(description=\" Entrenar RF\", button_style='success')\n",
        "        output_area = widgets.Output()\n",
        "        tiempo_lbl = widgets.Label()\n",
        "\n",
        "        def entrenar_rf(_):\n",
        "            output_area.clear_output()\n",
        "            resumen_modelos = []\n",
        "            inicio = time.time()\n",
        "\n",
        "            metodos = [metodo_sel.value] if metodo_sel.value != 'Todos' else ['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "            colores = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "            fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "            with output_area:\n",
        "                for i, metodo in enumerate(metodos):\n",
        "                    var_key = f\"selected_vars_{metodo.lower()}\"\n",
        "                    if var_key not in globals():\n",
        "                        print(f\" No hay variables seleccionadas para el mtodo: {metodo}\")\n",
        "                        continue\n",
        "                    selected_vars = globals()[var_key]\n",
        "\n",
        "                    Xtr, Xts = X_train[selected_vars], X_test[selected_vars]\n",
        "                    ytr, yts = Y_train.values.ravel(), Y_test.values.ravel()\n",
        "\n",
        "                    sx, sy = StandardScaler(), StandardScaler()\n",
        "                    Xtr_scaled = sx.fit_transform(Xtr)\n",
        "                    Xts_scaled = sx.transform(Xts)\n",
        "                    ytr_scaled = sy.fit_transform(ytr.reshape(-1,1)).ravel()\n",
        "\n",
        "                    model = RandomForestRegressor(\n",
        "                        n_estimators=n_estimators.value,\n",
        "                        max_depth=max_depth.value,\n",
        "                        min_samples_split=min_samples_split.value,\n",
        "                        min_samples_leaf=min_samples_leaf.value,\n",
        "                        max_features=max_features.value,\n",
        "                        bootstrap=bootstrap.value,\n",
        "                        random_state=42,\n",
        "                        n_jobs=-1\n",
        "                    )\n",
        "                    model.fit(Xtr_scaled, ytr_scaled)\n",
        "                    y_pred_scaled = model.predict(Xts_scaled)\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "\n",
        "                    r2 = r2_score(yts, y_pred)\n",
        "                    mse = mean_squared_error(yts, y_pred)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    mae = mean_absolute_error(yts, y_pred)\n",
        "\n",
        "                    resumen_modelos.append({\n",
        "                        'Mtodo': metodo,\n",
        "                        'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae\n",
        "                    })\n",
        "\n",
        "                    nombre_archivo = f\"modelo_rf_{metodo.lower()}.pkl\"\n",
        "                    with open(nombre_archivo, 'wb') as f:\n",
        "                        pickle.dump({'model': model, 'sx': sx, 'sy': sy, 'cols': selected_vars, 'yname': y_variable_name}, f)\n",
        "\n",
        "                    ax.plot(y_pred, label=f'{metodo}', alpha=0.7, linestyle='--', color=colores[i])\n",
        "                    print(f\" Modelo {metodo} entrenado. R: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "                if resumen_modelos:\n",
        "                    ax.plot(yts, label='Y Real', color='black', linewidth=2)\n",
        "                    ax.set_title('Comparacin Y Real vs Predicciones Random Forest por Mtodo')\n",
        "                    ax.grid(); ax.legend()\n",
        "                    plt.show()\n",
        "\n",
        "                    print(\"\\n Resumen comparativo de mtricas:\")\n",
        "                    df = pd.DataFrame(resumen_modelos)\n",
        "                    display(df.set_index('Mtodo'))\n",
        "\n",
        "            tiempo_lbl.value = f\" Duracin total: {time.time()-inicio:.2f} segundos\"\n",
        "\n",
        "        btn_train.on_click(entrenar_rf)\n",
        "\n",
        "        ayuda = widgets.HTML(\"\"\"\n",
        "        <h4> Ayuda - Parmetros del modelo Random Forest</h4>\n",
        "        <ul>\n",
        "            <li><b>n_estimators:</b> nmero de rboles. Mayor nmero = mejor precisin, mayor tiempo.</li>\n",
        "            <li><b>max_depth:</b> profundidad mxima. Limita la complejidad del modelo.</li>\n",
        "            <li><b>min_samples_split:</b> tamao mnimo para dividir nodo. Mayor = menos sobreajuste.</li>\n",
        "            <li><b>min_samples_leaf:</b> mnimo de muestras en hoja. Controla profundidad mnima.</li>\n",
        "            <li><b>max_features:</b> n de variables evaluadas por divisin. 'sqrt' es tpico para regresin.</li>\n",
        "            <li><b>bootstrap:</b> si se usan muestras con reemplazo. True mejora diversidad.</li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            widgets.HBox([metodo_sel]),\n",
        "            widgets.HBox([n_estimators, max_depth]),\n",
        "            widgets.HBox([min_samples_split, min_samples_leaf]),\n",
        "            widgets.HBox([max_features, bootstrap]),\n",
        "            btn_train, tiempo_lbl,\n",
        "            ayuda, output_area\n",
        "        ]))\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 7.5. ENTRENAMIENTO REDES NEURONALES RECURRENTES (RNN) - FUNCIONA CORRECTAMENTE\n",
        "# Este mdulo permite entrenar distintos tipos de RNN sobre los datos\n",
        "# segmentados (X_train, X_test, Y_train, Y_test, FECHAS_train, FECHAS_test),\n",
        "# con variables seleccionadas por cada mtodo (Pearson, Spearman, etc.).\n",
        "# ===============================================================\n",
        "import time, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Bidirectional, TimeDistributed, RepeatVector\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widget de salida global\n",
        "if 'out_rnn' not in globals():\n",
        "    out_rnn = widgets.Output()\n",
        "\n",
        "# Output de resultados (asegura persistencia)\n",
        "if 'output_area_rnn' not in globals():\n",
        "    output_area_rnn = widgets.Output()\n",
        "\n",
        "def mostrar_rnn(b=None):\n",
        "    if b is None:\n",
        "        display(out_rnn)\n",
        "\n",
        "    with out_rnn:\n",
        "        clear_output()\n",
        "\n",
        "        if not all(k in globals() for k in ['X_train', 'X_test', 'Y_train', 'Y_test', 'FECHAS_train', 'FECHAS_test']):\n",
        "            display(widgets.HTML(\"\"\"<span style='color:red;'> Primero debes segmentar los datos en train/test incluyendo fechas.</span>\"\"\"))\n",
        "            return\n",
        "\n",
        "        # Actualizar variables seleccionadas si existen\n",
        "        if 'RESUMEN_METODOS' in globals():\n",
        "            for metodo, variables in RESUMEN_METODOS.items():\n",
        "                globals()[f\"selected_vars_{metodo.lower()}\"] = variables\n",
        "\n",
        "        metodo_sel = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selector:'\n",
        "        )\n",
        "        tipo_rnn = widgets.Dropdown(\n",
        "            options=['Vanilla RNN', 'LSTM', 'GRU', 'BiLSTM', 'BiGRU', 'Deep RNN', 'Encoder-Decoder'],\n",
        "            description='Tipo RNN:'\n",
        "        )\n",
        "        window_size = widgets.IntSlider(value=10, min=5, max=100, description='Ventana:')\n",
        "        units = widgets.IntSlider(value=50, min=10, max=200, step=10, description='Unidades:')\n",
        "        layers = widgets.IntSlider(value=1, min=1, max=5, description='Capas:')\n",
        "        batch = widgets.IntSlider(value=32, min=8, max=128, step=8, description='Batch:')\n",
        "        epochs = widgets.IntSlider(value=30, min=10, max=500, step=10, description='pocas:')\n",
        "        learning_rate = widgets.FloatLogSlider(value=0.001, base=10, min=-5, max=-1, step=0.1, description='LR:')\n",
        "        optimizer = widgets.Dropdown(options=['adam', 'sgd', 'rmsprop', 'adagrad'], description='Optimizador:')\n",
        "        loss_fn = widgets.Dropdown(options=['mse', 'mae', 'huber'], description='Prdida:')\n",
        "        activation = widgets.Dropdown(options=['tanh', 'relu', 'sigmoid'], description='Activacin:')\n",
        "        drop = widgets.FloatSlider(value=0.0, min=0.0, max=0.5, step=0.05, description='Dropout:')\n",
        "        boton_entrenar = widgets.Button(description=' Entrenar RNN', button_style='success')\n",
        "        tiempo_lbl = widgets.Label()\n",
        "\n",
        "        def entrenar_rnn(_):\n",
        "            output_area_rnn.clear_output()\n",
        "            resumen_modelos = []\n",
        "            inicio = time.time()\n",
        "\n",
        "            metodos = [metodo_sel.value] if metodo_sel.value != 'Todos' else ['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "            colores = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "            fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "            for i, metodo in enumerate(metodos):\n",
        "                var_key = f\"selected_vars_{metodo.lower()}\"\n",
        "                if var_key not in globals():\n",
        "                    with output_area_rnn:\n",
        "                        print(f\" No hay variables seleccionadas para el mtodo: {metodo}\")\n",
        "                    continue\n",
        "                selected_vars = globals()[var_key]\n",
        "\n",
        "                df_train = X_train[selected_vars].copy()\n",
        "                df_test = X_test[selected_vars].copy()\n",
        "                y_train = Y_train.values.ravel()\n",
        "                y_test = Y_test.values.ravel()\n",
        "\n",
        "                sx, sy = StandardScaler(), StandardScaler()\n",
        "                Xtr = sx.fit_transform(df_train)\n",
        "                Xts = sx.transform(df_test)\n",
        "                ytr = sy.fit_transform(y_train.reshape(-1,1)).ravel()\n",
        "\n",
        "                def create_sequences(X, Y, window):\n",
        "                    X_seq, Y_seq = [], []\n",
        "                    for j in range(len(X) - window):\n",
        "                        X_seq.append(X[j:j+window])\n",
        "                        Y_seq.append(Y[j+window])\n",
        "                    return np.array(X_seq), np.array(Y_seq)\n",
        "\n",
        "                X_seq, Y_seq = create_sequences(Xtr, ytr, window_size.value)\n",
        "                Xts_seq, Yts_seq = create_sequences(Xts, y_test, window_size.value)\n",
        "                input_shape = (X_seq.shape[1], X_seq.shape[2])\n",
        "\n",
        "                # \n",
        "                # Exportar las secuencias al namespace global para el motor IG\n",
        "                globals()['X_seq'] = X_seq.copy()\n",
        "                globals()['Y_seq'] = Y_seq.copy()\n",
        "                # \n",
        "\n",
        "                model = Sequential()\n",
        "                RNNLayer = {\n",
        "                    'Vanilla RNN': SimpleRNN,\n",
        "                    'LSTM': LSTM,\n",
        "                    'GRU': GRU\n",
        "                }.get(tipo_rnn.value.split()[0], LSTM)\n",
        "\n",
        "                if 'Bi' in tipo_rnn.value:\n",
        "                    for _ in range(layers.value - 1):\n",
        "                        model.add(Bidirectional(RNNLayer(units.value, activation=activation.value, return_sequences=True), input_shape=input_shape))\n",
        "                    model.add(Bidirectional(RNNLayer(units.value, activation=activation.value)))\n",
        "                elif 'Deep' in tipo_rnn.value:\n",
        "                    for _ in range(layers.value - 1):\n",
        "                        model.add(RNNLayer(units.value, activation=activation.value, return_sequences=True))\n",
        "                    model.add(RNNLayer(units.value, activation=activation.value))\n",
        "                elif 'Encoder' in tipo_rnn.value:\n",
        "                    model.add(LSTM(units.value, activation=activation.value, input_shape=input_shape))\n",
        "                    model.add(RepeatVector(1))\n",
        "                    model.add(LSTM(units.value, activation=activation.value, return_sequences=True))\n",
        "                    model.add(TimeDistributed(Dense(1)))\n",
        "                else:\n",
        "                    for _ in range(layers.value - 1):\n",
        "                        model.add(RNNLayer(units.value, activation=activation.value, return_sequences=True, input_shape=input_shape))\n",
        "                    model.add(RNNLayer(units.value, activation=activation.value))\n",
        "\n",
        "                if 'Encoder' not in tipo_rnn.value:\n",
        "                    model.add(Dense(1))\n",
        "\n",
        "                opt_dict = {\n",
        "                    'adam': Adam(learning_rate=learning_rate.value),\n",
        "                    'sgd': SGD(learning_rate=learning_rate.value),\n",
        "                    'rmsprop': RMSprop(learning_rate=learning_rate.value),\n",
        "                    'adagrad': Adagrad(learning_rate=learning_rate.value)\n",
        "                }\n",
        "\n",
        "                #model.compile(loss=loss_fn.value, optimizer=opt_dict[optimizer.value])\n",
        "                from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, Huber\n",
        "\n",
        "                loss_function = {\n",
        "                    'mse': MeanSquaredError(),\n",
        "                    'mae': MeanAbsoluteError(),\n",
        "                    'huber': Huber()\n",
        "                }[loss_fn.value]\n",
        "\n",
        "                model.compile(loss=loss_function, optimizer=opt_dict[optimizer.value])\n",
        "\n",
        "                model.fit(X_seq, Y_seq, epochs=epochs.value, batch_size=batch.value, verbose=0)\n",
        "                Y_pred_scaled = model.predict(Xts_seq).ravel()\n",
        "                Y_pred = sy.inverse_transform(Y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                Y_real = Yts_seq\n",
        "\n",
        "                r2 = r2_score(Y_real, Y_pred)\n",
        "                mse = mean_squared_error(Y_real, Y_pred)\n",
        "                rmse = np.sqrt(mse)\n",
        "                mae = mean_absolute_error(Y_real, Y_pred)\n",
        "\n",
        "                resumen_modelos.append({\n",
        "                    'Mtodo': metodo,\n",
        "                    'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae\n",
        "                })\n",
        "\n",
        "                # Dentro de la funcin entrenar_rnn, despus de model.fit(...) y antes de model.save(...):\n",
        "                hp = {\n",
        "                    'tipo_rnn': tipo_rnn.value,\n",
        "                    'window_size': window_size.value,\n",
        "                    'units': units.value,\n",
        "                    'layers': layers.value,\n",
        "                    'batch_size': batch.value,\n",
        "                    'epochs': epochs.value,\n",
        "                    'learning_rate': learning_rate.value,\n",
        "                    'optimizer': optimizer.value,\n",
        "                    'loss_fn': loss_fn.value,\n",
        "                    'activation': activation.value,\n",
        "                    'dropout': drop.value\n",
        "                }\n",
        "                # Serializar en pickle:\n",
        "                import pickle\n",
        "                hp_fname = f\"hyperparams_rnn_{metodo.lower()}.pkl\"\n",
        "                with open(hp_fname, \"wb\") as f_hp:\n",
        "                    pickle.dump(hp, f_hp)\n",
        "                print(f\" Hiperparmetros RNN guardados en {hp_fname}\")\n",
        "\n",
        "                nombre_archivo = f\"modelo_rnn_{metodo.lower()}.h5\"\n",
        "                model.save(nombre_archivo)\n",
        "\n",
        "                with open(f\"escaladores_rnn_{metodo.lower()}.pkl\", \"wb\") as f:\n",
        "                    pickle.dump({\n",
        "                        'scaler_X': sx,\n",
        "                        'scaler_Y': sy,\n",
        "                        'cols': selected_vars,\n",
        "                        'yname': y_variable_name\n",
        "                    }, f)\n",
        "\n",
        "                ax.plot(Y_pred, label=f'{metodo}', alpha=0.7, linestyle='--', color=colores[i])\n",
        "\n",
        "                with output_area_rnn:\n",
        "                    print(f\" Modelo {metodo} entrenado. R: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "            if resumen_modelos:\n",
        "                ax.plot(Y_real, label='Y Real', color='black', linewidth=2)\n",
        "                ax.set_title('Comparacin Y Real vs Predicciones RNN por Mtodo')\n",
        "                ax.grid(); ax.legend()\n",
        "\n",
        "                with output_area_rnn:\n",
        "                    plt.show()\n",
        "                    print(\"\\n Resumen comparativo de mtricas:\")\n",
        "                    df = pd.DataFrame(resumen_modelos)\n",
        "                    display(df.set_index('Mtodo'))\n",
        "\n",
        "            tiempo_lbl.value = f\" Duracin total: {time.time()-inicio:.2f} segundos\"\n",
        "\n",
        "        boton_entrenar.on_click(entrenar_rnn)\n",
        "\n",
        "        ayuda = widgets.HTML(\"\"\"\n",
        "        <h4> Ayuda - Parmetros del modelo RNN</h4>\n",
        "        <ul>\n",
        "            <li><b>Tipo RNN:</b> Define la arquitectura de red. LSTM y GRU son recomendados para series con dependencia larga.</li>\n",
        "            <li><b>Ventana:</b> Nmero de pasos de tiempo usados para predecir el siguiente valor.</li>\n",
        "            <li><b>Capas / Unidades:</b> Ms capas y unidades aumentan capacidad, pero tambin el riesgo de sobreajuste.</li>\n",
        "            <li><b>Batch, Epochs:</b> Controlan tamao del lote y nmero de iteraciones de entrenamiento.</li>\n",
        "            <li><b>LR:</b> Tasa de aprendizaje. Valores muy altos o bajos pueden afectar la convergencia.</li>\n",
        "            <li><b>Dropout:</b> Regulariza y previene sobreajuste. 0.10.3 comn.</li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_sel,\n",
        "            widgets.HBox([tipo_rnn, window_size]),\n",
        "            widgets.HBox([units, layers, batch]),\n",
        "            widgets.HBox([epochs, learning_rate, optimizer]),\n",
        "            widgets.HBox([activation, drop, loss_fn]),\n",
        "            boton_entrenar, tiempo_lbl,\n",
        "            ayuda, output_area_rnn\n",
        "        ]))\n",
        "\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# 7.6. COMPARADOR DE MODELOS ENTRENADOS - FUNCIONA CORRECTAMENTE\n",
        "# Esta cell compara los resultados obtenidos en la ejecucin de los entrenamientos anteriores (SVR, NN, XGBoost, Random Forest y RNN)\n",
        "# ===========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle, os\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "out_model_comparator = widgets.Output()\n",
        "\n",
        "# Botn para mostrar el comparador\n",
        "btn_lanzar_comparador = widgets.Button(description=' Comparar Modelos Entrenados', button_style='success')\n",
        "\n",
        "def mostrar_comparador_modelos(b=None):\n",
        "    with out_model_comparator:\n",
        "        clear_output()\n",
        "        display(btn_lanzar_comparador)\n",
        "\n",
        "# Accin al pulsar botn de ejecutar comparador\n",
        "\n",
        "def ejecutar_comparador(b=None):\n",
        "    with out_model_comparator:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'> Comparador de Modelos Entrenados</h3>\n",
        "        <p>Este comparador analiza los modelos entrenados con distintos algoritmos y mtodos de seleccin de variables.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        ruta_modelos = '.'\n",
        "        modelos_validos = []\n",
        "\n",
        "        print(\" Buscando modelos guardados...\")\n",
        "        for archivo in os.listdir(ruta_modelos):\n",
        "            if archivo.startswith(\"modelo_\") and (archivo.endswith(\".pkl\") or archivo.endswith(\".h5\")):\n",
        "                nombre_modelo = archivo.replace(\"modelo_\", \"\").replace(\".pkl\", \"\").replace(\".h5\", \"\")\n",
        "                try:\n",
        "                    if archivo.endswith(\".pkl\"):\n",
        "                        with open(os.path.join(ruta_modelos, archivo), 'rb') as f:\n",
        "                            modelo_guardado = pickle.load(f)\n",
        "                        modelo = modelo_guardado.get('model')\n",
        "                        scaler_X = modelo_guardado.get('scaler_X', modelo_guardado.get('sx'))\n",
        "                        scaler_Y = modelo_guardado.get('scaler_Y', modelo_guardado.get('sy'))\n",
        "                        cols = modelo_guardado.get('cols')\n",
        "                        yname = modelo_guardado.get('yname')\n",
        "                        if scaler_X is None or scaler_Y is None:\n",
        "                            raise KeyError(\"Faltan escaladores\")\n",
        "                    else:\n",
        "                        modelo = load_model(os.path.join(ruta_modelos, archivo))\n",
        "                        escalador_path = f\"escaladores_{nombre_modelo}.pkl\"\n",
        "                        with open(escalador_path, 'rb') as f:\n",
        "                            escaladores = pickle.load(f)\n",
        "                        scaler_X = escaladores['scaler_X']\n",
        "                        scaler_Y = escaladores['scaler_Y']\n",
        "                        cols = escaladores['cols']\n",
        "                        yname = escaladores['yname']\n",
        "\n",
        "                    modelos_validos.append({\n",
        "                        'nombre': nombre_modelo,\n",
        "                        'modelo': modelo,\n",
        "                        'scaler_X': scaler_X,\n",
        "                        'scaler_Y': scaler_Y,\n",
        "                        'cols': cols,\n",
        "                        'yname': yname\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\" Error al procesar {archivo}: {e}\")\n",
        "\n",
        "        if not modelos_validos:\n",
        "            display(HTML(\"<b style='color:red;'> No se encontraron modelos entrenados vlidos.</b>\"))\n",
        "            return\n",
        "\n",
        "        resultados = []\n",
        "        detalles_modelos = []\n",
        "\n",
        "        for entry in modelos_validos:\n",
        "            nombre = entry['nombre']\n",
        "            model = entry['modelo']\n",
        "            sx = entry['scaler_X']\n",
        "            sy = entry['scaler_Y']\n",
        "            cols = entry['cols']\n",
        "            yname = entry['yname']\n",
        "\n",
        "            Xtest = X_test[cols].copy()\n",
        "            ytest = Y_test.values.ravel()\n",
        "\n",
        "            # Detectar si es un modelo RNN por la forma esperada\n",
        "            try:\n",
        "                input_shape = model.input_shape\n",
        "                is_rnn = len(input_shape) == 3\n",
        "            except:\n",
        "                is_rnn = False\n",
        "\n",
        "            if is_rnn:\n",
        "                window = input_shape[1]\n",
        "                X_full = X_test[cols].copy()\n",
        "                y_full = Y_test.values.ravel()\n",
        "                X_scaled = sx.transform(X_full)\n",
        "                y_scaled = sy.transform(y_full.reshape(-1, 1)).ravel()\n",
        "\n",
        "                X_seq, y_seq = [], []\n",
        "                for i in range(len(X_scaled) - window):\n",
        "                    X_seq.append(X_scaled[i:i+window])\n",
        "                    y_seq.append(y_full[i+window])\n",
        "                X_seq = np.array(X_seq)\n",
        "                y_seq = np.array(y_seq)\n",
        "\n",
        "                pred_scaled = model.predict(X_seq).ravel()\n",
        "                pred = sy.inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
        "                ytest = y_seq  # ajustar Y real a misma longitud\n",
        "            else:\n",
        "                Xtest_scaled = sx.transform(Xtest)\n",
        "                pred_scaled = model.predict(Xtest_scaled)\n",
        "                if isinstance(pred_scaled, tuple): pred_scaled = pred_scaled[0]\n",
        "                pred = sy.inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
        "                ytest = ytest  # no cambia\n",
        "\n",
        "            r2 = r2_score(ytest, pred)\n",
        "            mse = mean_squared_error(ytest, pred)\n",
        "            rmse = np.sqrt(mse)\n",
        "            mae = mean_absolute_error(ytest, pred)\n",
        "\n",
        "            resultados.append({\n",
        "                'Modelo': nombre,\n",
        "                'R2': r2,\n",
        "                'MSE': mse,\n",
        "                'RMSE': rmse,\n",
        "                'MAE': mae,\n",
        "                'Pred': pred\n",
        "            })\n",
        "\n",
        "            detalles_modelos.append({\n",
        "                'Modelo': nombre,\n",
        "                'Variables X': ', '.join(cols),\n",
        "                'Variable Y': yname,\n",
        "                'N Variables': len(cols),\n",
        "                'Tipo Modelo': nombre.split('_')[0].upper(),\n",
        "                'Mtodo Seleccin': nombre.split('_')[-1].capitalize()\n",
        "            })\n",
        "\n",
        "        df_resultados = pd.DataFrame(resultados)\n",
        "        df_detalles = pd.DataFrame(detalles_modelos)\n",
        "\n",
        "        display(HTML(\"<h4> Comparativa de Resultados</h4>\"))\n",
        "        display(df_resultados[['Modelo', 'R2', 'RMSE', 'MAE', 'MSE']]\n",
        "                .sort_values(by='R2', ascending=False)\n",
        "                .style.set_caption(\"Ranking de Modelos por R\")\n",
        "                .set_properties(**{'border': '1px solid gray', 'padding': '6px'})\n",
        "                .set_table_styles([\n",
        "                    {'selector': 'th', 'props': [('background-color', '#f0f0f0'), ('font-weight', 'bold')]},\n",
        "                ]))\n",
        "\n",
        "        df_sorted = df_resultados.sort_values(by='R2', ascending=False)\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        colors = ['gold', 'silver', 'peru'] + ['lightblue']*(len(df_sorted)-3)\n",
        "        ax.barh(df_sorted['Modelo'], df_sorted['R2'], color=colors[:len(df_sorted)])\n",
        "        ax.set_title(\" Ranking de Modelos por R\")\n",
        "        ax.invert_yaxis()\n",
        "        for i, v in enumerate(df_sorted['R2']):\n",
        "            ax.text(v + 0.01, i, f\"{v:.3f}\", va='center')\n",
        "        plt.grid(axis='x')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        display(HTML(\"<h4> Parmetros de Entrenamiento de los Modelos</h4>\"))\n",
        "        display(df_detalles.style.set_caption(\"Resumen de Configuracin de Modelos\")\n",
        "                .set_properties(**{'border': '1px solid gray', 'padding': '6px'})\n",
        "                .set_table_styles([\n",
        "                    {'selector': 'th', 'props': [('background-color', '#e0f7fa'), ('font-weight', 'bold')]},\n",
        "                ]))\n",
        "\n",
        "        # Mostrar tabla con parmetros tcnicos (si existen)\n",
        "        tabla_parametros = []\n",
        "\n",
        "        for entry in modelos_validos:\n",
        "            nombre = entry['nombre']\n",
        "            modelo = entry['modelo']\n",
        "\n",
        "            fila = {'Modelo': nombre}\n",
        "            try:\n",
        "                if hasattr(modelo, 'get_params'):\n",
        "                    fila.update(modelo.get_params())\n",
        "                elif isinstance(modelo, tf.keras.Model):\n",
        "                    config = modelo.get_config()\n",
        "                    fila['Capas'] = len(config['layers'])\n",
        "                    fila['Optimizador'] = config.get('optimizer_config', {}).get('class_name', 'Desconocido')\n",
        "                    fila['Prdida'] = config.get('loss', 'Desconocida')\n",
        "                else:\n",
        "                    fila['Info'] = ' Tipo de modelo no reconocido'\n",
        "            except Exception as e:\n",
        "                fila['Error'] = str(e)\n",
        "\n",
        "            tabla_parametros.append(fila)\n",
        "\n",
        "        df_parametros = pd.DataFrame(tabla_parametros)\n",
        "        display(HTML(\"<h4> Parmetros de Configuracin (Detalles Tcnicos)</h4>\"))\n",
        "        if not df_parametros.empty:\n",
        "            display(df_parametros.style.set_caption(\"Parmetros usados en cada modelo\")\n",
        "                    .set_properties(**{'border': '1px solid #ccc', 'padding': '4px'})\n",
        "                    .set_table_styles([{'selector': 'th', 'props': [('background-color', '#f8f8f8'), ('font-weight', 'bold')]}]))\n",
        "        else:\n",
        "            display(HTML(\"<i>No se pudieron recuperar parmetros para los modelos cargados.</i>\"))\n",
        "\n",
        "        display(HTML(\"<h4> Grficos Comparativos Y Real vs. Prediccin</h4>\"))\n",
        "        for res in resultados:\n",
        "            nombre = res['Modelo']\n",
        "            pred = res['Pred']\n",
        "            fig, ax = plt.subplots(figsize=(10,4))\n",
        "            ax.plot(Y_test.values, label='Y Real', color='black')\n",
        "            ax.plot(pred, label=f'{nombre}', linestyle='--')\n",
        "            ax.set_title(f'{nombre}: Real vs Predicho')\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        # Tabla de parmetros completos si estn disponibles\n",
        "        display(HTML(\"<h4> Parmetros de Configuracin (Detalles Tcnicos)</h4>\"))\n",
        "\n",
        "        tabla_parametros = []\n",
        "\n",
        "        for entry in modelos_validos:\n",
        "            modelo = entry['modelo']\n",
        "            nombre = entry['nombre']\n",
        "            tipo   = nombre.split('_')[0].upper()\n",
        "\n",
        "            if hasattr(modelo, 'get_params'):\n",
        "                try:\n",
        "                    params = modelo.get_params()\n",
        "                    tabla_parametros.append({\n",
        "                        'Modelo': nombre,\n",
        "                        **params\n",
        "                    })\n",
        "                except:\n",
        "                    tabla_parametros.append({'Modelo': nombre, 'Info': ' No se pudieron extraer los parmetros'})\n",
        "            elif isinstance(modelo, tf.keras.Model):\n",
        "                config = modelo.get_config()\n",
        "                tabla_parametros.append({\n",
        "                    'Modelo': nombre,\n",
        "                    'Capas': len(config['layers']),\n",
        "                    'Optim.': config.get('optimizer_config', {}).get('class_name', 'N/A'),\n",
        "                    'Loss': config.get('loss', 'N/A') if isinstance(config.get('loss'), str) else str(config.get('loss')),\n",
        "                    'Tipo': tipo\n",
        "                })\n",
        "            else:\n",
        "                tabla_parametros.append({'Modelo': nombre, 'Info': ' Modelo no compatible'})\n",
        "\n",
        "        df_params = pd.DataFrame(tabla_parametros)\n",
        "        display(df_params.style.set_caption(\" Parmetros de Ajuste de Cada Modelo\")\n",
        "                .set_properties(**{'border': '1px solid #999', 'padding': '5px'})\n",
        "                .set_table_styles([{'selector': 'th', 'props': [('background-color', '#f9f9f9'), ('font-weight', 'bold')]}]))\n",
        "\n",
        "btn_lanzar_comparador.on_click(ejecutar_comparador)\n",
        "display(out_model_comparator)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8. PREDICCION Y VISUALIZACION DE MODELOS - PENDIENTE CORREGIR\n",
        "# Este mdulo se descompone en varios sub-modulos para la realizacin de predicciones usando los diferentes modelos entrenados y visualizar los resultados de los modelos\n",
        "# ===============================================================\n",
        "# ===============================================================\n",
        "# 8.1. PREDICCIN SVR  FUNCIONA CORRECTAMENTE\n",
        "# Permite al usuario seleccionar mtodo de seleccin, introducir datos\n",
        "# manualmente o generarlos automticamente, y obtener predicciones SVR.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "import pickle, os\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "out_pred_svr = widgets.Output()\n",
        "\n",
        "def mostrar_prediccion_svr(b=None):\n",
        "    with out_pred_svr:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'> Prediccin con Modelo SVR</h3>\n",
        "        <p>Este mdulo permite realizar predicciones con modelos SVR entrenados previamente\n",
        "        utilizando variables seleccionadas automticamente o introducidas manualmente.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Seleccin de Variables:'\n",
        "        )\n",
        "\n",
        "        num_casos = widgets.BoundedIntText(value=5, min=1, max=100, description='N Casos:')\n",
        "\n",
        "        modo_datos = widgets.ToggleButtons(\n",
        "            options=['Automtico', 'Manual'],\n",
        "            description='Modo de entrada:'\n",
        "        )\n",
        "\n",
        "        btn_generar = widgets.Button(description=' Generar Variables X')\n",
        "        btn_predecir = widgets.Button(description=' Predecir Y', button_style='success')\n",
        "        btn_copiar = widgets.Button(description=' Copiar Tabla', button_style='info')\n",
        "\n",
        "        tabla_x = widgets.Output()\n",
        "        tabla_pred = widgets.Output()\n",
        "        grafico_pred = widgets.Output()\n",
        "        datos_generados = {}\n",
        "        resultados = {}\n",
        "\n",
        "        def generar_valores(_):\n",
        "            tabla_x.clear_output()\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            datos_generados.clear()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                nombre_modelo = f\"modelo_svr_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(nombre_modelo):\n",
        "                    continue\n",
        "                with open(nombre_modelo, 'rb') as f:\n",
        "                    modelo_dict = pickle.load(f)\n",
        "                cols = modelo_dict['cols']\n",
        "\n",
        "                df_x = pd.DataFrame()\n",
        "                for col in cols:\n",
        "                    if col not in X_data.columns or X_data[col].isnull().all():\n",
        "                        continue\n",
        "                    serie = X_data[col]\n",
        "                    minimo, maximo = serie.min(), serie.max()\n",
        "                    tendencia = 'asc' if serie.corr(Y_data.iloc[:, 0]) > 0 else 'desc'\n",
        "                    if modo_datos.value == 'Automtico':\n",
        "                        vals = np.linspace(minimo, maximo, num_casos.value) if tendencia == 'asc' \\\n",
        "                            else np.linspace(maximo, minimo, num_casos.value)\n",
        "                        df_x[col] = vals\n",
        "                    else:\n",
        "                        df_x[col] = [widgets.FloatText(value=0.0, layout=widgets.Layout(width='80px')) for _ in range(num_casos.value)]\n",
        "                datos_generados[metodo] = df_x\n",
        "\n",
        "            with tabla_x:\n",
        "                clear_output()\n",
        "                display(HTML(f\"<h4> Variables X Generadas ({modo_datos.value})</h4>\"))\n",
        "                for metodo in metodos:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    df = datos_generados[metodo]\n",
        "                    display(HTML(f\"<b style='color:#2E8B57;'>{metodo}</b>\"))\n",
        "\n",
        "                    if isinstance(df, pd.DataFrame) and modo_datos.value == 'Automtico':\n",
        "                        display(df)\n",
        "                    elif modo_datos.value == 'Manual':\n",
        "                        grid = widgets.GridspecLayout(num_casos.value + 1, len(df.columns))\n",
        "                        for j, col in enumerate(df.columns):\n",
        "                            grid[0, j] = widgets.Label(value=col)\n",
        "                            for i in range(num_casos.value):\n",
        "                                grid[i+1, j] = df[col][i]\n",
        "                        display(grid)\n",
        "\n",
        "        def realizar_prediccion(_):\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                if metodo not in datos_generados:\n",
        "                    continue\n",
        "                df = datos_generados[metodo]\n",
        "                nombre_modelo = f\"modelo_svr_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(nombre_modelo):\n",
        "                    continue\n",
        "                with open(nombre_modelo, 'rb') as f:\n",
        "                    modelo_dict = pickle.load(f)\n",
        "\n",
        "                sx, sy, model = modelo_dict['sx'], modelo_dict['sy'], modelo_dict['model']\n",
        "\n",
        "                if modo_datos.value == 'Manual':\n",
        "                    df_manual = pd.DataFrame()\n",
        "                    for col in df.columns:\n",
        "                        df_manual[col] = [w.value for w in df[col]]\n",
        "                    df_to_use = df_manual\n",
        "                else:\n",
        "                    df_to_use = df\n",
        "\n",
        "                x_scaled = sx.transform(df_to_use.values)\n",
        "                y_pred = sy.inverse_transform(model.predict(x_scaled).reshape(-1, 1)).ravel()\n",
        "                df_pred = df_to_use.copy()\n",
        "                df_pred['Y_pred'] = y_pred\n",
        "                resultados[metodo] = df_pred\n",
        "\n",
        "            tabla_pred.clear_output()\n",
        "            contenedor_tablas = []\n",
        "            for metodo in metodos:\n",
        "                if metodo in resultados:\n",
        "                    df = resultados[metodo]\n",
        "                    contenedor_tablas.append(HTML(f\"<h4 style='color:#2E8B57;'> {metodo}</h4>\"))\n",
        "                    contenedor_tablas.append(widgets.Output())\n",
        "                    with contenedor_tablas[-1]:\n",
        "                        display(df)\n",
        "\n",
        "            with tabla_pred:\n",
        "                clear_output()\n",
        "                display(HTML(\"<h3> Resultados de la Prediccin</h3>\"))\n",
        "                display(widgets.VBox(contenedor_tablas))\n",
        "\n",
        "            with grafico_pred:\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        y_vals = resultados[metodo]['Y_pred'].values\n",
        "                        if y_vals.size > 0:\n",
        "                            plt.plot(y_vals, label=str(metodo), linestyle='--')\n",
        "\n",
        "                plt.title(\" Predicciones Y por Mtodo\")\n",
        "                plt.xlabel(\"Caso\")\n",
        "                plt.ylabel(\"Y_predicho\")\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        def copiar_al_portapapeles(_):\n",
        "            from IPython.display import Javascript\n",
        "            contenido = \"\"\n",
        "            for metodo, df in resultados.items():\n",
        "                contenido += f\"\\n### {metodo}\\n\" + df.to_csv(index=False)\n",
        "            js_code = f\"navigator.clipboard.writeText(`{contenido}`)\"\n",
        "            display(Javascript(js_code))\n",
        "            print(\" Tabla copiada al portapapeles.\")\n",
        "\n",
        "        btn_generar.on_click(generar_valores)\n",
        "        btn_predecir.on_click(realizar_prediccion)\n",
        "        btn_copiar.on_click(copiar_al_portapapeles)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            num_casos,\n",
        "            modo_datos,\n",
        "            btn_generar,\n",
        "            tabla_x,\n",
        "            btn_predecir,\n",
        "            tabla_pred,\n",
        "            grafico_pred,\n",
        "            btn_copiar\n",
        "        ]))\n",
        "\n",
        "# Mostrar en ejecucin directa\n",
        "display(out_pred_svr)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8.2. PREDICCIN NN  FUNCIONA CORRECTAMENTE\n",
        "# Permite al usuario seleccionar mtodo de seleccin, introducir datos\n",
        "# manualmente o generarlos automticamente, y obtener predicciones NN.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "import pickle, os\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "out_pred_nn = widgets.Output()\n",
        "\n",
        "def mostrar_prediccion_nn(b=None):\n",
        "    with out_pred_nn:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'> Prediccin con Red Neuronal</h3>\n",
        "        <p>Este mdulo permite realizar predicciones con redes neuronales previamente entrenadas\n",
        "        utilizando variables seleccionadas automticamente o introducidas manualmente.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Seleccin de Variables:'\n",
        "        )\n",
        "\n",
        "        num_casos = widgets.BoundedIntText(value=5, min=1, max=100, description='N Casos:')\n",
        "\n",
        "        modo_datos = widgets.ToggleButtons(\n",
        "            options=['Automtico', 'Manual'],\n",
        "            description='Modo de entrada:'\n",
        "        )\n",
        "\n",
        "        btn_generar = widgets.Button(description=' Generar Variables X')\n",
        "        btn_predecir = widgets.Button(description=' Predecir Y', button_style='success')\n",
        "        btn_copiar = widgets.Button(description=' Copiar Tabla', button_style='info')\n",
        "\n",
        "        tabla_x = widgets.Output()\n",
        "        tabla_pred = widgets.Output()\n",
        "        grafico_pred = widgets.Output()\n",
        "        datos_generados = {}\n",
        "        resultados = {}\n",
        "\n",
        "        def generar_valores(_):\n",
        "            tabla_x.clear_output()\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            datos_generados.clear()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                modelo_path = f\"modelo_nn_{metodo.lower()}.h5\"\n",
        "                scaler_path = f\"escaladores_nn_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(modelo_path) or not os.path.exists(scaler_path):\n",
        "                    continue\n",
        "                with open(scaler_path, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                cols = datos['cols']\n",
        "\n",
        "                df_x = pd.DataFrame()\n",
        "                for col in cols:\n",
        "                    if col not in X_data.columns or X_data[col].isnull().all():\n",
        "                        continue\n",
        "                    serie = X_data[col]\n",
        "                    minimo, maximo = serie.min(), serie.max()\n",
        "                    tendencia = 'asc' if serie.corr(Y_data.iloc[:, 0]) > 0 else 'desc'\n",
        "                    if modo_datos.value == 'Automtico':\n",
        "                        vals = np.linspace(minimo, maximo, num_casos.value) if tendencia == 'asc' \\\n",
        "                            else np.linspace(maximo, minimo, num_casos.value)\n",
        "                        df_x[col] = vals\n",
        "                    else:\n",
        "                        df_x[col] = [widgets.FloatText(value=0.0, layout=widgets.Layout(width='80px')) for _ in range(num_casos.value)]\n",
        "                datos_generados[metodo] = df_x\n",
        "\n",
        "            with tabla_x:\n",
        "                clear_output()\n",
        "                display(HTML(f\"<h4> Variables X Generadas ({modo_datos.value})</h4>\"))\n",
        "                for metodo in metodos:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    df = datos_generados[metodo]\n",
        "                    display(HTML(f\"<b style='color:#2E8B57;'>{metodo}</b>\"))\n",
        "                    if isinstance(df, pd.DataFrame) and modo_datos.value == 'Automtico':\n",
        "                        display(df)\n",
        "                    elif modo_datos.value == 'Manual':\n",
        "                        grid = widgets.GridspecLayout(num_casos.value + 1, len(df.columns))\n",
        "                        for j, col in enumerate(df.columns):\n",
        "                            grid[0, j] = widgets.Label(value=col)\n",
        "                            for i in range(num_casos.value):\n",
        "                                grid[i+1, j] = df[col][i]\n",
        "                        display(grid)\n",
        "\n",
        "        def realizar_prediccion(_):\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                if metodo not in datos_generados:\n",
        "                    continue\n",
        "                df = datos_generados[metodo]\n",
        "                modelo_path = f\"modelo_nn_{metodo.lower()}.h5\"\n",
        "                scaler_path = f\"escaladores_nn_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(modelo_path) or not os.path.exists(scaler_path):\n",
        "                    continue\n",
        "\n",
        "                model = load_model(modelo_path)\n",
        "                with open(scaler_path, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                sx, sy = datos['scaler_X'], datos['scaler_Y']\n",
        "\n",
        "                if modo_datos.value == 'Manual':\n",
        "                    df_manual = pd.DataFrame()\n",
        "                    for col in df.columns:\n",
        "                        df_manual[col] = [w.value for w in df[col]]\n",
        "                    df_to_use = df_manual\n",
        "                else:\n",
        "                    df_to_use = df\n",
        "\n",
        "                x_scaled = sx.transform(df_to_use.values)\n",
        "                y_pred = sy.inverse_transform(model.predict(x_scaled)).ravel()\n",
        "                df_pred = df_to_use.copy()\n",
        "                df_pred['Y_pred'] = y_pred\n",
        "                resultados[metodo] = df_pred\n",
        "\n",
        "            tabla_pred.clear_output()\n",
        "            contenedor_tablas = []\n",
        "            for metodo in metodos:\n",
        "                if metodo in resultados:\n",
        "                    df = resultados[metodo]\n",
        "                    contenedor_tablas.append(HTML(f\"<h4 style='color:#2E8B57;'> {metodo}</h4>\"))\n",
        "                    contenedor_tablas.append(widgets.Output())\n",
        "                    with contenedor_tablas[-1]:\n",
        "                        display(df)\n",
        "\n",
        "            with tabla_pred:\n",
        "                clear_output()\n",
        "                display(HTML(\"<h3> Resultados de la Prediccin</h3>\"))\n",
        "                display(widgets.VBox(contenedor_tablas))\n",
        "\n",
        "            with grafico_pred:\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        plt.plot(resultados[metodo]['Y_pred'].values, label=metodo, linestyle='--')\n",
        "                plt.title(\" Predicciones Y por Mtodo (NN)\")\n",
        "                plt.xlabel(\"Caso\")\n",
        "                plt.ylabel(\"Y_predicho\")\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        def copiar_al_portapapeles(_):\n",
        "            from IPython.display import Javascript\n",
        "            contenido = \"\"\n",
        "            for metodo, df in resultados.items():\n",
        "                contenido += f\"\\n### {metodo}\\n\" + df.to_csv(index=False)\n",
        "            js_code = f\"navigator.clipboard.writeText(`{contenido}`)\"\n",
        "            display(Javascript(js_code))\n",
        "            print(\" Tabla copiada al portapapeles.\")\n",
        "\n",
        "        btn_generar.on_click(generar_valores)\n",
        "        btn_predecir.on_click(realizar_prediccion)\n",
        "        btn_copiar.on_click(copiar_al_portapapeles)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            num_casos,\n",
        "            modo_datos,\n",
        "            btn_generar,\n",
        "            tabla_x,\n",
        "            btn_predecir,\n",
        "            tabla_pred,\n",
        "            grafico_pred,\n",
        "            btn_copiar\n",
        "        ]))\n",
        "\n",
        "# Mostrar en ejecucin directa\n",
        "display(out_pred_nn)\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8.3. PREDICCIN XGBOOST  FUNCIONA CORRECTAMENTE\n",
        "# Permite al usuario seleccionar mtodo de seleccin, introducir datos\n",
        "# manualmente o generarlos automticamente, y obtener predicciones con XGBoost.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "import pickle, os\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "out_pred_xgb = widgets.Output()\n",
        "\n",
        "def mostrar_prediccion_xgboost(b=None):\n",
        "    with out_pred_xgb:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'> Prediccin con XGBoost</h3>\n",
        "        <p>Este mdulo permite realizar predicciones con modelos XGBoost previamente entrenados\n",
        "        utilizando variables seleccionadas automticamente o introducidas manualmente.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Seleccin de Variables:'\n",
        "        )\n",
        "\n",
        "        num_casos = widgets.BoundedIntText(value=5, min=1, max=100, description='N Casos:')\n",
        "\n",
        "        modo_datos = widgets.ToggleButtons(\n",
        "            options=['Automtico', 'Manual'],\n",
        "            description='Modo de entrada:'\n",
        "        )\n",
        "\n",
        "        btn_generar = widgets.Button(description=' Generar Variables X')\n",
        "        btn_predecir = widgets.Button(description=' Predecir Y', button_style='success')\n",
        "        btn_copiar = widgets.Button(description=' Copiar Tabla', button_style='info')\n",
        "\n",
        "        tabla_x = widgets.Output()\n",
        "        tabla_pred = widgets.Output()\n",
        "        grafico_pred = widgets.Output()\n",
        "        datos_generados = {}\n",
        "        resultados = {}\n",
        "\n",
        "        def generar_valores(_):\n",
        "            tabla_x.clear_output()\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            datos_generados.clear()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                path_model = f\"modelo_xgb_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(path_model):\n",
        "                    continue\n",
        "                with open(path_model, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                cols = datos['cols']\n",
        "\n",
        "                df_x = pd.DataFrame()\n",
        "                for col in cols:\n",
        "                    if col not in X_data.columns or X_data[col].isnull().all():\n",
        "                        continue\n",
        "                    serie = X_data[col]\n",
        "                    minimo, maximo = serie.min(), serie.max()\n",
        "                    tendencia = 'asc' if serie.corr(Y_data.iloc[:, 0]) > 0 else 'desc'\n",
        "                    if modo_datos.value == 'Automtico':\n",
        "                        vals = np.linspace(minimo, maximo, num_casos.value) if tendencia == 'asc' \\\n",
        "                            else np.linspace(maximo, minimo, num_casos.value)\n",
        "                        df_x[col] = vals\n",
        "                    else:\n",
        "                        df_x[col] = [widgets.FloatText(value=0.0, layout=widgets.Layout(width='80px')) for _ in range(num_casos.value)]\n",
        "                datos_generados[metodo] = df_x\n",
        "\n",
        "            with tabla_x:\n",
        "                clear_output()\n",
        "                display(HTML(f\"<h4> Variables X Generadas ({modo_datos.value})</h4>\"))\n",
        "                for metodo in metodos:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    df = datos_generados[metodo]\n",
        "                    display(HTML(f\"<b style='color:#2E8B57;'>{metodo}</b>\"))\n",
        "                    if isinstance(df, pd.DataFrame) and modo_datos.value == 'Automtico':\n",
        "                        display(df)\n",
        "                    elif modo_datos.value == 'Manual':\n",
        "                        grid = widgets.GridspecLayout(num_casos.value + 1, len(df.columns))\n",
        "                        for j, col in enumerate(df.columns):\n",
        "                            grid[0, j] = widgets.Label(value=col)\n",
        "                            for i in range(num_casos.value):\n",
        "                                grid[i+1, j] = df[col][i]\n",
        "                        display(grid)\n",
        "\n",
        "        def realizar_prediccion(_):\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                if metodo not in datos_generados:\n",
        "                    continue\n",
        "                df = datos_generados[metodo]\n",
        "                path_model = f\"modelo_xgb_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(path_model):\n",
        "                    continue\n",
        "\n",
        "                with open(path_model, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                model = datos['model']\n",
        "                sx, sy = datos['sx'], datos['sy']\n",
        "\n",
        "                if modo_datos.value == 'Manual':\n",
        "                    df_manual = pd.DataFrame()\n",
        "                    for col in df.columns:\n",
        "                        df_manual[col] = [w.value for w in df[col]]\n",
        "                    df_to_use = df_manual\n",
        "                else:\n",
        "                    df_to_use = df\n",
        "\n",
        "                x_scaled = sx.transform(df_to_use.values)\n",
        "                y_pred = sy.inverse_transform(model.predict(x_scaled).reshape(-1, 1)).ravel()\n",
        "                df_pred = df_to_use.copy()\n",
        "                df_pred['Y_pred'] = y_pred\n",
        "                resultados[metodo] = df_pred\n",
        "\n",
        "            contenedor_tablas = []\n",
        "            for metodo in metodos:\n",
        "                if metodo in resultados:\n",
        "                    df = resultados[metodo]\n",
        "                    contenedor_tablas.append(HTML(f\"<h4 style='color:#2E8B57;'> {metodo}</h4>\"))\n",
        "                    contenedor_tablas.append(widgets.Output())\n",
        "                    with contenedor_tablas[-1]:\n",
        "                        display(df)\n",
        "\n",
        "            with tabla_pred:\n",
        "                clear_output()\n",
        "                display(HTML(\"<h3> Resultados de la Prediccin (XGBoost)</h3>\"))\n",
        "                display(widgets.VBox(contenedor_tablas))\n",
        "\n",
        "            with grafico_pred:\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        plt.plot(resultados[metodo]['Y_pred'].values, label=metodo, linestyle='--')\n",
        "                plt.title(\" Predicciones Y por Mtodo (XGBoost)\")\n",
        "                plt.xlabel(\"Caso\")\n",
        "                plt.ylabel(\"Y_predicho\")\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        def copiar_al_portapapeles(_):\n",
        "            from IPython.display import Javascript\n",
        "            contenido = \"\"\n",
        "            for metodo, df in resultados.items():\n",
        "                contenido += f\"\\n### {metodo}\\n\" + df.to_csv(index=False)\n",
        "            js_code = f\"navigator.clipboard.writeText(`{contenido}`)\"\n",
        "            display(Javascript(js_code))\n",
        "            print(\" Tabla copiada al portapapeles.\")\n",
        "\n",
        "        btn_generar.on_click(generar_valores)\n",
        "        btn_predecir.on_click(realizar_prediccion)\n",
        "        btn_copiar.on_click(copiar_al_portapapeles)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            num_casos,\n",
        "            modo_datos,\n",
        "            btn_generar,\n",
        "            tabla_x,\n",
        "            btn_predecir,\n",
        "            tabla_pred,\n",
        "            grafico_pred,\n",
        "            btn_copiar\n",
        "        ]))\n",
        "\n",
        "# Mostrar en ejecucin directa\n",
        "display(out_pred_xgb)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8.4. PREDICCIN RANDOM FOREST  FUNCIONA CORRECTAMENTE\n",
        "# Permite al usuario seleccionar mtodo de seleccin, introducir datos\n",
        "# manualmente o generarlos automticamente, y obtener predicciones con RF.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "import pickle, os, traceback, time\n",
        "from IPython.display import display, HTML, clear_output, Javascript\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "out_pred_rf = widgets.Output()\n",
        "\n",
        "def mostrar_prediccion_rf(b=None):\n",
        "    with out_pred_rf:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'> Prediccin con Random Forest</h3>\n",
        "        <p>Este mdulo permite realizar predicciones con modelos Random Forest previamente entrenados\n",
        "        utilizando variables seleccionadas automticamente o introducidas manualmente.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Seleccin de Variables:'\n",
        "        )\n",
        "\n",
        "        num_casos = widgets.BoundedIntText(value=5, min=1, max=100, description='N Casos:')\n",
        "\n",
        "        modo_datos = widgets.ToggleButtons(\n",
        "            options=['Automtico', 'Manual'],\n",
        "            description='Modo de entrada:'\n",
        "        )\n",
        "\n",
        "        btn_generar = widgets.Button(description=' Generar Variables X')\n",
        "        btn_predecir = widgets.Button(description=' Predecir Y', button_style='success')\n",
        "        btn_copiar = widgets.Button(description=' Copiar Tabla', button_style='info')\n",
        "\n",
        "        tabla_x = widgets.Output()\n",
        "        tabla_pred = widgets.Output()\n",
        "        grafico_pred = widgets.Output()\n",
        "        progreso = widgets.Label()\n",
        "\n",
        "        datos_generados = {}\n",
        "        resultados = {}\n",
        "\n",
        "        def generar_valores(_):\n",
        "            tabla_x.clear_output()\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            datos_generados.clear()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                path_model = f\"modelo_rf_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(path_model):\n",
        "                    continue\n",
        "                try:\n",
        "                    with open(path_model, 'rb') as f:\n",
        "                        datos = pickle.load(f)\n",
        "                    cols = datos.get('cols', [])\n",
        "                    if not cols:\n",
        "                        continue\n",
        "\n",
        "                    df_x = pd.DataFrame()\n",
        "                    for col in cols:\n",
        "                        if col not in X_data.columns or X_data[col].isnull().all():\n",
        "                            continue\n",
        "                        serie = X_data[col]\n",
        "                        minimo, maximo = serie.min(), serie.max()\n",
        "                        tendencia = 'asc' if serie.corr(Y_data.iloc[:, 0]) > 0 else 'desc'\n",
        "                        if modo_datos.value == 'Automtico':\n",
        "                            vals = np.linspace(minimo, maximo, num_casos.value) if tendencia == 'asc' \\\n",
        "                                else np.linspace(maximo, minimo, num_casos.value)\n",
        "                            df_x[col] = vals\n",
        "                        else:\n",
        "                            df_x[col] = [widgets.FloatText(value=0.0, layout=widgets.Layout(width='80px')) for _ in range(num_casos.value)]\n",
        "                    datos_generados[metodo] = df_x\n",
        "                except Exception as e:\n",
        "                    print(f\" Error al generar variables para {metodo}: {e}\")\n",
        "\n",
        "            with tabla_x:\n",
        "                clear_output()\n",
        "                display(HTML(f\"<h4> Variables X Generadas ({modo_datos.value})</h4>\"))\n",
        "                for metodo in metodos:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    df = datos_generados[metodo]\n",
        "                    display(HTML(f\"<b style='color:#2E8B57;'>{metodo}</b>\"))\n",
        "                    if isinstance(df, pd.DataFrame) and modo_datos.value == 'Automtico':\n",
        "                        display(df)\n",
        "                    elif modo_datos.value == 'Manual':\n",
        "                        grid = widgets.GridspecLayout(num_casos.value + 1, len(df.columns))\n",
        "                        for j, col in enumerate(df.columns):\n",
        "                            grid[0, j] = widgets.Label(value=col)\n",
        "                            for i in range(num_casos.value):\n",
        "                                grid[i+1, j] = df[col][i]\n",
        "                        display(grid)\n",
        "\n",
        "        def realizar_prediccion(_):\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            progreso.value = \" Realizando predicciones...\"\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            t_ini = time.time()\n",
        "            errores = []  #  para mostrar errores al final\n",
        "\n",
        "            for metodo in metodos:\n",
        "                if metodo not in datos_generados:\n",
        "                    continue\n",
        "                df = datos_generados[metodo]\n",
        "                path_model = f\"modelo_rf_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(path_model):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    with open(path_model, 'rb') as f:\n",
        "                        datos = pickle.load(f)\n",
        "                    model = datos['model']\n",
        "                    #sx, sy = datos['scaler_X'], datos['scaler_Y']\n",
        "                    sx = datos.get('scaler_X', None)\n",
        "                    sy = datos.get('scaler_Y', None)\n",
        "\n",
        "\n",
        "                    if modo_datos.value == 'Manual':\n",
        "                        df_manual = pd.DataFrame()\n",
        "                        for col in df.columns:\n",
        "                            df_manual[col] = [w.value for w in df[col]]\n",
        "                        df_to_use = df_manual\n",
        "                    else:\n",
        "                        df_to_use = df\n",
        "\n",
        "                    if set(df_to_use.columns) != set(datos['cols']):\n",
        "                        errores.append(f\" Columnas incompatibles para {metodo}. Se omite.\")\n",
        "                        continue\n",
        "                    df_to_use = df_to_use[datos['cols']]\n",
        "\n",
        "                    if df_to_use.empty:\n",
        "                        errores.append(f\" DataFrame vaco para {metodo}. Se omite.\")\n",
        "                        continue\n",
        "\n",
        "                    print(f\"[DEBUG] Prediciendo para mtodo: {metodo}, df.shape = {df_to_use.shape}\")\n",
        "\n",
        "                    # === Prediccin con o sin escalado ===\n",
        "                    if sx is not None:\n",
        "                        x_scaled = sx.transform(df_to_use.values)\n",
        "                    else:\n",
        "                        x_scaled = df_to_use.values\n",
        "\n",
        "                    y_pred_scaled = model.predict(x_scaled).reshape(-1, 1)\n",
        "\n",
        "                    if sy is not None:\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled).ravel()\n",
        "                    else:\n",
        "                        y_pred = y_pred_scaled.ravel()\n",
        "\n",
        "                    df_pred = df_to_use.copy()\n",
        "                    df_pred['Y_pred'] = y_pred\n",
        "                    resultados[metodo] = df_pred\n",
        "\n",
        "                except Exception as e:\n",
        "                    errores.append(f\" Error al predecir para {metodo}:\\n{traceback.format_exc()}\")\n",
        "\n",
        "            progreso.value = f\" Predicciones completadas en {time.time() - t_ini:.1f}s\"\n",
        "\n",
        "            contenedor_tablas = []\n",
        "            for metodo in metodos:\n",
        "                if metodo in resultados:\n",
        "                    df = resultados[metodo]\n",
        "                    contenedor_tablas.append(HTML(f\"<h4 style='color:#2E8B57;'> {metodo}</h4>\"))\n",
        "                    contenedor_tablas.append(widgets.Output())\n",
        "                    with contenedor_tablas[-1]:\n",
        "                        display(df)\n",
        "\n",
        "            with tabla_pred:\n",
        "                clear_output()\n",
        "                display(HTML(\"<h3> Resultados de la Prediccin (Random Forest)</h3>\"))\n",
        "                if contenedor_tablas:\n",
        "                    display(widgets.VBox(contenedor_tablas))\n",
        "                if errores:\n",
        "                    display(HTML(\"<h4 style='color:red;'> Errores detectados:</h4>\"))\n",
        "                    for err in errores:\n",
        "                        display(HTML(f\"<pre style='color:darkred;'>{err}</pre>\"))\n",
        "\n",
        "            with grafico_pred:\n",
        "                clear_output()\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                hay_datos = False\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        plt.plot(resultados[metodo]['Y_pred'].values, label=metodo, linestyle='--')\n",
        "                        hay_datos = True\n",
        "                if hay_datos:\n",
        "                    plt.title(\" Predicciones Y por Mtodo (Random Forest)\")\n",
        "                    plt.xlabel(\"Caso\")\n",
        "                    plt.ylabel(\"Y_predicho\")\n",
        "                    plt.legend()\n",
        "                    plt.grid(True)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "        def copiar_al_portapapeles(_):\n",
        "            contenido = \"\"\n",
        "            for metodo, df in resultados.items():\n",
        "                contenido += f\"\\n### {metodo}\\n\" + df.to_csv(index=False)\n",
        "            js_code = f\"navigator.clipboard.writeText(`{contenido}`)\"\n",
        "            display(Javascript(js_code))\n",
        "            print(\" Tabla copiada al portapapeles.\")\n",
        "\n",
        "        btn_generar.on_click(generar_valores)\n",
        "        btn_predecir.on_click(realizar_prediccion)\n",
        "        btn_copiar.on_click(copiar_al_portapapeles)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            num_casos,\n",
        "            modo_datos,\n",
        "            btn_generar,\n",
        "            tabla_x,\n",
        "            btn_predecir,\n",
        "            progreso,\n",
        "            tabla_pred,\n",
        "            grafico_pred,\n",
        "            btn_copiar\n",
        "        ]))\n",
        "\n",
        "\n",
        "# Mostrar en ejecucin directa\n",
        "display(out_pred_rf)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8.5. PREDICCIN CON RNN  FUNCIONA CORRECTAMENTE\n",
        "# Permite al usuario seleccionar mtodo de seleccin, introducir datos\n",
        "# manualmente o generarlos automticamente, y obtener predicciones con RNN.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "import pickle, os, traceback\n",
        "from IPython.display import display, HTML, clear_output, Javascript\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "out_pred_rnn = widgets.Output()\n",
        "resultados = {}\n",
        "datos_generados = {}\n",
        "\n",
        "def mostrar_prediccion_rnn(b=None):\n",
        "    with out_pred_rnn:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'> Prediccin con RNN</h3>\n",
        "        <p>Este mdulo permite realizar predicciones con modelos RNN previamente entrenados\n",
        "        utilizando variables seleccionadas automticamente o introducidas manualmente.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Seleccin de Variables:'\n",
        "        )\n",
        "\n",
        "        num_casos = widgets.BoundedIntText(value=5, min=1, max=100, description='N Casos:')\n",
        "        modo_datos = widgets.ToggleButtons(options=['Automtico', 'Manual'], description='Modo de entrada:')\n",
        "\n",
        "        btn_generar = widgets.Button(description=' Generar Variables X')\n",
        "        btn_predecir = widgets.Button(description=' Predecir Y', button_style='success')\n",
        "        btn_copiar = widgets.Button(description=' Copiar Tabla', button_style='info')\n",
        "\n",
        "        tabla_x = widgets.Output()\n",
        "        tabla_pred = widgets.Output()\n",
        "        grafico_pred = widgets.Output()\n",
        "        progreso = widgets.Label()\n",
        "\n",
        "        def generar_valores(_):\n",
        "            tabla_x.clear_output()\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            progreso.value = \"\"\n",
        "            datos_generados.clear()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                modelo_path = f\"modelo_rnn_{metodo.lower()}.h5\"\n",
        "                scaler_path = f\"escaladores_rnn_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(modelo_path) or not os.path.exists(scaler_path):\n",
        "                    continue\n",
        "                with open(scaler_path, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                cols = datos.get('cols', [])\n",
        "                if not cols:\n",
        "                    continue\n",
        "\n",
        "                df_x = pd.DataFrame()\n",
        "                for col in cols:\n",
        "                    if col not in X_data.columns or X_data[col].isnull().all():\n",
        "                        continue\n",
        "                    serie = X_data[col]\n",
        "                    minimo, maximo = serie.min(), serie.max()\n",
        "                    tendencia = 'asc' if serie.corr(Y_data.iloc[:, 0]) > 0 else 'desc'\n",
        "                    if modo_datos.value == 'Automtico':\n",
        "                        valores = np.linspace(minimo, maximo, num_casos.value) if tendencia == 'asc' else np.linspace(maximo, minimo, num_casos.value)\n",
        "                        df_x[col] = valores\n",
        "                    else:\n",
        "                        df_x[col] = [widgets.FloatText(value=0.0, layout=widgets.Layout(width='80px')) for _ in range(num_casos.value)]\n",
        "                datos_generados[metodo] = df_x\n",
        "\n",
        "            with tabla_x:\n",
        "                clear_output()\n",
        "                display(HTML(f\"<h4> Variables X Generadas ({modo_datos.value})</h4>\"))\n",
        "                for metodo in metodos:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    display(HTML(f\"<b style='color:#2E8B57;'>{metodo}</b>\"))\n",
        "                    df = datos_generados[metodo]\n",
        "                    if modo_datos.value == 'Automtico':\n",
        "                        display(df)\n",
        "                    else:\n",
        "                        grid = widgets.GridspecLayout(num_casos.value + 1, len(df.columns))\n",
        "                        for j, col in enumerate(df.columns):\n",
        "                            grid[0, j] = widgets.Label(value=col)\n",
        "                            for i in range(num_casos.value):\n",
        "                                grid[i + 1, j] = df[col][i]\n",
        "                        display(grid)\n",
        "\n",
        "        def realizar_prediccion(_):\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            progreso.value = \" Realizando predicciones...\"\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            errores = []\n",
        "\n",
        "            for metodo in metodos:\n",
        "                try:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    df = datos_generados[metodo]\n",
        "\n",
        "                    modelo_path = f\"modelo_rnn_{metodo.lower()}.h5\"\n",
        "                    scaler_path = f\"escaladores_rnn_{metodo.lower()}.pkl\"\n",
        "                    if not os.path.exists(modelo_path) or not os.path.exists(scaler_path):\n",
        "                        errores.append(f\" Archivos no encontrados para {metodo}\")\n",
        "                        continue\n",
        "\n",
        "                    with open(scaler_path, 'rb') as f:\n",
        "                        datos = pickle.load(f)\n",
        "                    sx, sy = datos.get('scaler_X'), datos.get('scaler_Y')\n",
        "                    model = load_model(modelo_path)\n",
        "\n",
        "                    if modo_datos.value == 'Manual':\n",
        "                        df_manual = pd.DataFrame()\n",
        "                        for col in df.columns:\n",
        "                            df_manual[col] = [w.value for w in df[col]]\n",
        "                        df_to_use = df_manual\n",
        "                    else:\n",
        "                        df_to_use = df\n",
        "\n",
        "                    if set(df_to_use.columns) != set(datos['cols']):\n",
        "                        errores.append(f\" Columnas incompatibles para {metodo}. Se omite.\")\n",
        "                        continue\n",
        "                    df_to_use = df_to_use[datos['cols']]\n",
        "\n",
        "                    x_scaled = sx.transform(df_to_use.values)\n",
        "                    x_scaled_rnn = x_scaled.reshape((x_scaled.shape[0], 1, x_scaled.shape[1]))\n",
        "                    y_pred_scaled = model.predict(x_scaled_rnn).reshape(-1, 1)\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled).ravel()\n",
        "\n",
        "                    df_pred = df_to_use.copy()\n",
        "                    df_pred['Y_pred'] = y_pred\n",
        "                    resultados[metodo] = df_pred\n",
        "\n",
        "                except Exception as e:\n",
        "                    errores.append(f\" Error al predecir para {metodo}:\\n{traceback.format_exc()}\")\n",
        "\n",
        "            progreso.value = \" Predicciones completadas\"\n",
        "\n",
        "            with tabla_pred:\n",
        "                clear_output()\n",
        "                contenedor_tablas = []\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        df = resultados[metodo]\n",
        "                        contenedor_tablas.append(HTML(f\"<h4 style='color:#2E8B57;'> {metodo}</h4>\"))\n",
        "                        contenedor_tablas.append(widgets.Output())\n",
        "                        with contenedor_tablas[-1]:\n",
        "                            display(df)\n",
        "                if contenedor_tablas:\n",
        "                    display(HTML(\"<h3> Resultados de la Prediccin (RNN)</h3>\"))\n",
        "                    display(widgets.VBox(contenedor_tablas))\n",
        "                if errores:\n",
        "                    display(HTML(\"<h4 style='color:red;'> Errores detectados:</h4>\"))\n",
        "                    for err in errores:\n",
        "                        display(HTML(f\"<pre style='color:darkred;'>{err}</pre>\"))\n",
        "\n",
        "            with grafico_pred:\n",
        "                clear_output()\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        plt.plot(resultados[metodo]['Y_pred'].values, label=metodo, linestyle='--')\n",
        "                plt.title(\" Predicciones Y por Mtodo (RNN)\")\n",
        "                plt.xlabel(\"Caso\")\n",
        "                plt.ylabel(\"Y_predicho\")\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        def copiar_al_portapapeles(_):\n",
        "            contenido = \"\"\n",
        "            for metodo, df in resultados.items():\n",
        "                contenido += f\"\\n### {metodo}\\n\" + df.to_csv(index=False)\n",
        "            js_code = f\"navigator.clipboard.writeText(`{contenido}`)\"\n",
        "            display(Javascript(js_code))\n",
        "            print(\" Tabla copiada al portapapeles.\")\n",
        "\n",
        "        btn_generar.on_click(generar_valores)\n",
        "        btn_predecir.on_click(realizar_prediccion)\n",
        "        btn_copiar.on_click(copiar_al_portapapeles)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            num_casos,\n",
        "            modo_datos,\n",
        "            btn_generar,\n",
        "            tabla_x,\n",
        "            btn_predecir,\n",
        "            progreso,\n",
        "            tabla_pred,\n",
        "            grafico_pred,\n",
        "            btn_copiar\n",
        "        ]))\n",
        "\n",
        "# Mostrar en ejecucin directa\n",
        "display(out_pred_rnn)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8.6. VISUALIZACION Y VS X  NUEVA LGICA IMPLEMENTADA\n",
        "# Permite al usuario seleccionar variables X que correlacionan o no con Y, ver las curvas Y_real vs Y_pred con SVR, NN, XGB, RF y RNN,\n",
        "# y copiar tabla de datos generada al portapapeles. Compatible con los mtodos de seleccin (Pearson, Spearman, etc.)\n",
        "# ===============================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import glob\n",
        "from keras.models import load_model\n",
        "\n",
        "# Output global\n",
        "out_graf_86 = widgets.Output()\n",
        "\n",
        "def listar_modelos():\n",
        "    modelos = {}\n",
        "    for path in glob.glob(\"modelo_*.pkl\"):\n",
        "        nombre = os.path.splitext(os.path.basename(path))[0].replace(\"modelo_\", \"\").upper()\n",
        "        modelos[nombre] = path\n",
        "\n",
        "    for h5_path in glob.glob(\"modelo_*.h5\"):\n",
        "        nombre = os.path.splitext(os.path.basename(h5_path))[0].replace(\"modelo_\", \"\").upper()\n",
        "        pkl_path = f\"escaladores_{nombre.lower()}.pkl\"\n",
        "        if os.path.exists(pkl_path):\n",
        "            modelos[nombre] = (h5_path, pkl_path)\n",
        "        else:\n",
        "            modelos[nombre] = h5_path\n",
        "    return modelos\n",
        "\n",
        "def mostrar_grafico_y_vs_x():\n",
        "    with out_graf_86:\n",
        "        clear_output()\n",
        "\n",
        "        if 'X_data' not in globals() or 'Y_data' not in globals():\n",
        "            print(\" Faltan datos cargados (X_data o Y_data).\")\n",
        "            return\n",
        "\n",
        "        modelos_disponibles = listar_modelos()\n",
        "\n",
        "        selector_variable = widgets.Dropdown(\n",
        "            options=X_data.columns.tolist(),\n",
        "            description='Variable X:',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        selector_dataset = widgets.ToggleButtons(\n",
        "            options=['Train', 'Test'],\n",
        "            description='Dataset:',\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        selector_modelos = widgets.SelectMultiple(\n",
        "            options=list(modelos_disponibles.keys()),\n",
        "            description='Modelos:',\n",
        "            layout=widgets.Layout(width='50%', height='150px')\n",
        "        )\n",
        "\n",
        "        boton_ver = widgets.Button(description=' Comparar Y', button_style='success')\n",
        "        resumen_out = widgets.Output()\n",
        "        tabla_out = widgets.Output()\n",
        "        debug_out = widgets.Output()\n",
        "        grafico_out = widgets.Output()\n",
        "\n",
        "        def graficar(_):\n",
        "            resumen_out.clear_output()\n",
        "            tabla_out.clear_output()\n",
        "            grafico_out.clear_output()\n",
        "            debug_out.clear_output()\n",
        "\n",
        "            var_x = selector_variable.value\n",
        "            if var_x is None:\n",
        "                print(\" No se ha seleccionado variable X\")\n",
        "                return\n",
        "\n",
        "            dataset = selector_dataset.value\n",
        "            if dataset == 'Train':\n",
        "                X_base = X_train.copy()\n",
        "                Y_base = Y_train.copy()\n",
        "            else:\n",
        "                X_base = X_test.copy()\n",
        "                Y_base = Y_test.copy()\n",
        "\n",
        "            x_vals = X_base[var_x].values\n",
        "            y_vals = Y_base.values.ravel()\n",
        "            df = pd.DataFrame({\"X\": x_vals, \"Y_real\": y_vals})\n",
        "            metricas = {}\n",
        "\n",
        "            modelos_seleccionados = selector_modelos.value\n",
        "            if isinstance(modelos_seleccionados, str):\n",
        "                modelos_seleccionados = [modelos_seleccionados]\n",
        "            elif isinstance(modelos_seleccionados, tuple):\n",
        "                modelos_seleccionados = list(modelos_seleccionados)\n",
        "\n",
        "            for modelo_key in modelos_seleccionados:\n",
        "                modelo_path = modelos_disponibles.get(modelo_key)\n",
        "                if not modelo_path:\n",
        "                    with debug_out:\n",
        "                        print(f\" Modelo {modelo_key} no encontrado.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    if isinstance(modelo_path, tuple):\n",
        "                        h5_file, pkl_file = modelo_path\n",
        "                        model = load_model(h5_file)\n",
        "                        with open(pkl_file, 'rb') as f:\n",
        "                            datos = pickle.load(f)\n",
        "                    else:\n",
        "                        with open(modelo_path, 'rb') as f:\n",
        "                            datos = pickle.load(f)\n",
        "                        model = datos.get('model')\n",
        "\n",
        "                    if model is None:\n",
        "                        raise ValueError(\" No se encontr el modelo entrenado en el archivo.\")\n",
        "\n",
        "                    sx = datos.get('scaler_X', datos.get('sx'))\n",
        "                    sy = datos.get('scaler_Y', datos.get('sy'))\n",
        "\n",
        "                    if sx is None or sy is None:\n",
        "                        raise ValueError(\" No se encontraron los escaladores (sx/sy o scaler_X/scaler_Y) en el modelo.\")\n",
        "\n",
        "                    cols = datos.get('cols', None)\n",
        "                    if cols is None:\n",
        "                        cols = list(set(X_base.columns) & set(sx.feature_names_in_))\n",
        "                        if not cols:\n",
        "                            raise ValueError(\" No se pudo inferir columnas para aplicar scaler_X\")\n",
        "\n",
        "                    X_scaled = sx.transform(X_base[cols])\n",
        "\n",
        "                    # Si el modelo requiere entrada 3D (ej. RNN)\n",
        "                    if hasattr(model, 'input_shape') and len(model.input_shape) == 3:\n",
        "                        X_scaled = np.expand_dims(X_scaled, axis=1)  # Convertir a (batch_size, 1, features)\n",
        "\n",
        "                    y_pred_scaled = model.predict(X_scaled).reshape(-1, 1)\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled).ravel()\n",
        "\n",
        "                    df[f\"Y_{modelo_key}\"] = y_pred\n",
        "                    metricas[modelo_key] = {\n",
        "                        'R2': r2_score(y_vals, y_pred),\n",
        "                        'MSE': mean_squared_error(y_vals, y_pred),\n",
        "                        'MAE': mean_absolute_error(y_vals, y_pred)\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    with debug_out:\n",
        "                        print(f\" Error al procesar {modelo_key}: {e}\")\n",
        "                        print(f\" Contenido del modelo cargado: {list(datos.keys()) if 'datos' in locals() else ' No cargado'}\")\n",
        "\n",
        "            with resumen_out:\n",
        "                display(HTML(\"<h4> Mtricas comparativas:</h4>\"))\n",
        "                filas = [[m, f\"{v['R2']:.3f}\", f\"{v['MSE']:.3f}\", f\"{v['MAE']:.3f}\"] for m, v in metricas.items()]\n",
        "                display(pd.DataFrame(filas, columns=['Modelo', 'R', 'MSE', 'MAE']))\n",
        "\n",
        "            with tabla_out:\n",
        "                display(HTML(\"<h4> Tabla X, Y real y predicho (Top 20 casos):</h4>\"))\n",
        "                display(df.head(20))\n",
        "                try:\n",
        "                    import pyperclip\n",
        "                    pyperclip.copy(df.to_csv(sep=';', index=False))\n",
        "                    print(\" Copiado al portapapeles\")\n",
        "                except:\n",
        "                    print(\" pyperclip no disponible\")\n",
        "\n",
        "            with grafico_out:\n",
        "                plt.figure(figsize=(10,6))\n",
        "                plt.scatter(df['X'], df['Y_real'], label='Y Real', color='black', s=50, alpha=0.6)\n",
        "                for col in df.columns:\n",
        "                    if col.startswith(\"Y_\"):\n",
        "                        modelo = col[2:]\n",
        "                        if modelo in metricas:\n",
        "                            plt.scatter(df['X'], df[col], label=f\"{modelo} (R={metricas[modelo]['R2']:.2f})\", alpha=0.6)\n",
        "                plt.xlabel(var_x)\n",
        "                plt.ylabel('Y')\n",
        "                plt.title(f\"Comparacin Y real vs prediccin - {var_x} ({dataset})\")\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        boton_ver.on_click(graficar)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            selector_variable,\n",
        "            selector_dataset,\n",
        "            selector_modelos,\n",
        "            boton_ver,\n",
        "            resumen_out,\n",
        "            tabla_out,\n",
        "            grafico_out,\n",
        "            debug_out\n",
        "        ]))\n",
        "\n",
        "# Mostrar salida\n",
        "display(out_graf_86)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 9. OPTIMIZACION - FUNCIONA CORRECTAMENTE\n",
        "# Este mdulo es el responsable de la optimizacin completa de los diferentes modelos entrenados, que busca minimizar el Minimo Error Cuadrtico (MAE) y la prdida (LOSS) optimizando el HPO .\n",
        "# ===============================================================\n",
        "# ===============================================================\n",
        "# 9.1. OPTIMIZACIN SVR  FUNCIONA CORRECTAMENTE\n",
        "# Optimiza hiperparmetros del modelo SVR basndose en RESUMEN_METODOS\n",
        "# Incluye GridSearchCV, RandomizedSearchCV, Optuna y BayesSearchCV (scikit-optimize)\n",
        "# Ampliado con mltiples configuraciones de motores para bsqueda jerrquica\n",
        "# Aade trazabilidad visual detallada paso a paso\n",
        "# Incluye tabla comparativa de los 5 mejores modelos\n",
        "# ===============================================================\n",
        "# Importaciones\n",
        "import time\n",
        "import re       #  AADIDO \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, median_absolute_error\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML, Javascript\n",
        "import optuna\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "# Widget principal\n",
        "out_opt_svr = widgets.Output()\n",
        "\n",
        "def mostrar_optimizacion_svr():\n",
        "    with out_opt_svr:\n",
        "        clear_output()\n",
        "\n",
        "        # 1) Verifico que ya se haya segmentado X_train/X_test\n",
        "        if 'X_train' not in globals() or 'X_test' not in globals():\n",
        "            print(\" Ejecute primero la segmentacin para definir X_train y X_test.\")\n",
        "            return\n",
        "\n",
        "        # 2) Ahora s puedo sanear columnas\n",
        "        # Usar sanitize_name para limpiar columnas en el payload\n",
        "        def clean_cols(col_list):\n",
        "            return [sanitize_name(c) for c in col_list]\n",
        "        # Ejemplo de sanitizacin de X_train antes de fit\n",
        "        X_train.columns = [sanitize_name(col) for col in X_train.columns]\n",
        "        X_test.columns = [sanitize_name(col) for col in X_test.columns]\n",
        "\n",
        "        display(HTML(\"<h3 style='color:#2E8B57;'> Optimizacin de Hiperparmetros - Modelo SVR</h3>\"))\n",
        "        display(HTML(\"\"\"\n",
        "            <h3 style='color:#2E8B57;'> Optimizacin de Hiperparmetros - Modelo SVR</h3>\n",
        "            <p>Este mdulo permite encontrar la mejor configuracin de hiperparmetros del modelo SVR\n",
        "            usando distintos motores de optimizacin. Cada motor aplica estrategias diferentes de bsqueda del ptimo:</p>\n",
        "            <ul>\n",
        "                <li><b>GridSearchCV</b>: bsqueda exhaustiva sobre combinaciones definidas. Garantiza el hallazgo del mejor resultado entre todas las combinaciones, pero puede ser computacionalmente costoso.</li>\n",
        "                <li><b>RandomizedSearchCV</b>: muestreo aleatorio sobre el espacio de bsqueda. Acelera el proceso seleccionando combinaciones al azar.</li>\n",
        "                <li><b>Optuna</b>: optimizacin bayesiana con estrategia de aprendizaje secuencial. Aprende de cada iteracin para mejorar la siguiente.</li>\n",
        "                <li><b>BayesSearchCV</b>: bsqueda bayesiana usando scikit-optimize. Muy eficaz para espacios amplios con hiperparmetros complejos.</li>\n",
        "                <li><b>HalvingGridSearchCV</b>: bsqueda jerrquica que evala inicialmente muchas configuraciones con pocos recursos y reserva los recursos mayores solo a las mejores.</li>\n",
        "            </ul>\n",
        "\n",
        "            <h4 style=\"color:#1E90FF;\"> Qu es la Validacin Cruzada?</h4>\n",
        "            <p>La validacin cruzada (CV) evala la capacidad de generalizacin de un modelo dividiendo los datos en varias particiones (\"folds\").\n",
        "            En cada iteracin, uno de los folds se usa como conjunto de validacin mientras los restantes se usan para entrenamiento.\n",
        "            El modelo se entrena y valida mltiples veces y luego se promedia la mtrica para obtener una evaluacin ms robusta.</p>\n",
        "            <p>Esto reduce el riesgo de sobreajuste y asegura que el modelo funciona correctamente en datos que no ha visto.</p>\n",
        "\n",
        "            <h4 style=\"color:#1E90FF;\"> Qu son los residuos?</h4>\n",
        "            <p>Los residuos son la diferencia entre los valores reales (observados) y los predichos por el modelo.\n",
        "            Se calculan como:</p>\n",
        "            <pre>residuo = valor_real - valor_predicho</pre>\n",
        "            <p>Interpretacin:</p>\n",
        "            <ul>\n",
        "                <li> Residuos cercanos a cero indican buen ajuste.</li>\n",
        "                <li> Una distribucin normal de los residuos es deseable: implica que los errores son aleatorios.</li>\n",
        "                <li> La presencia de sesgos, asimetras o colas en los residuos puede indicar fallos estructurales del modelo.</li>\n",
        "            </ul>\n",
        "            <p>Adems de los histogramas, se utiliza el test de Shapiro-Wilk para evaluar si los residuos siguen una distribucin normal:</p>\n",
        "            <pre>p-value > 0.05  los residuos se consideran normales.</pre>\n",
        "\n",
        "            <h4 style=\"color:#1E90FF;\"> Comparativa Visual entre Modelos</h4>\n",
        "            <p>Una vez obtenidos los 5 mejores modelos, se genera una comparativa visual con las mtricas R, MSE, MAE, RMSE y MedAE\n",
        "            para facilitar la seleccin del modelo ms robusto en funcin de las prioridades del usuario.</p>\n",
        "            <p>Tambin se generan histogramas de residuos para evaluar el comportamiento del error y grficos Q-Q para validar la normalidad de dichos residuos.</p>\n",
        "            <p>Se incluirn grficos de barras para comparar mtricas entre modelos y residuos superpuestos para identificar el ms preciso.</p>\n",
        "            <hr>\n",
        "            <p style=\"color:gray;\">Puedes lanzar la optimizacin seleccionando el mtodo de seleccin de variables (Pearson, MutualInfo, etc.) y los motores deseados.</p>\n",
        "            \"\"\"))\n",
        "\n",
        "        metodos = list(RESUMEN_METODOS.keys()) if isinstance(RESUMEN_METODOS, dict) else []\n",
        "        if not metodos:\n",
        "            display(HTML(\"<span style='color:red;'> No se encontraron variables seleccionadas por ningn mtodo en RESUMEN_METODOS.</span>\"))\n",
        "            return\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=metodos + ['Todos'],\n",
        "            description='Variables X:',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        funcion_selector = widgets.Dropdown(\n",
        "            options=['R2', 'MSE', 'MAE', 'RMSE', 'MedAE'],\n",
        "            value='R2',\n",
        "            description='Funcin objetivo:',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        motor_selector = widgets.SelectMultiple(\n",
        "            options=['GridSearchCV', 'RandomizedSearchCV', 'Optuna', 'BayesSearchCV', 'Todos'],\n",
        "            value=['GridSearchCV'],\n",
        "            description='Motores de Optimizacin:',\n",
        "            layout=widgets.Layout(width='70%', height='100px')\n",
        "        )\n",
        "\n",
        "        btn_ejecutar = widgets.Button(description=' Ejecutar Optimizacin', button_style='success')\n",
        "        progreso = widgets.HTML()\n",
        "        traza = widgets.Output()\n",
        "        salida_resultados = widgets.Output()\n",
        "\n",
        "        def ejecutar_optimizacion(_):\n",
        "            with salida_resultados:\n",
        "                clear_output()\n",
        "            with traza:\n",
        "                clear_output()\n",
        "                print(\" Iniciando optimizacin...\")\n",
        "\n",
        "            inicio = time.time()\n",
        "            metodos_a_probar = metodos if metodo_selector.value == 'Todos' else [metodo_selector.value]\n",
        "            motores = ['GridSearchCV', 'RandomizedSearchCV', 'Optuna', 'BayesSearchCV'] if 'Todos' in motor_selector.value else list(motor_selector.value)\n",
        "\n",
        "            mejores_modelos = []\n",
        "\n",
        "            for metodo in metodos_a_probar:\n",
        "                with traza:\n",
        "                    print(f\"\\n Optimizando para variables seleccionadas por: {metodo}\")\n",
        "                #vars_x = RESUMEN_METODOS.get(metodo, [])\n",
        "                #if not vars_x:\n",
        "                #    with traza:\n",
        "                #        print(f\" No hay variables seleccionadas por {metodo}. Se omite.\")\n",
        "                #    continue\n",
        "                #  AADIDO: limpiar lista de variables antes de indexar \n",
        "                raw_vars = RESUMEN_METODOS.get(metodo, [])\n",
        "                if not raw_vars:\n",
        "                    with traza:\n",
        "                        print(f\" No hay variables para {metodo}, omito.\")\n",
        "                    continue\n",
        "                vars_x = clean_cols(raw_vars)\n",
        "                #  FIN AADIDO \n",
        "                try:\n",
        "                    X_train_sel = X_train[vars_x].copy()\n",
        "                    X_test_sel = X_test[vars_x].copy()\n",
        "                    y_train_sel = Y_train.values.ravel()\n",
        "                    y_test_sel = Y_test.values.ravel()\n",
        "\n",
        "                    sx = StandardScaler()\n",
        "                    sy = StandardScaler()\n",
        "                    X_train_scaled = sx.fit_transform(X_train_sel)\n",
        "                    X_test_scaled = sx.transform(X_test_sel)\n",
        "                    y_train_scaled = sy.fit_transform(y_train_sel.reshape(-1, 1)).ravel()\n",
        "\n",
        "                    def calcular_score(y_real, y_pred):\n",
        "                        if funcion_selector.value == 'R2': return r2_score(y_real, y_pred)\n",
        "                        elif funcion_selector.value == 'MSE': return mean_squared_error(y_real, y_pred)\n",
        "                        elif funcion_selector.value == 'MAE': return mean_absolute_error(y_real, y_pred)\n",
        "                        elif funcion_selector.value == 'RMSE': return np.sqrt(mean_squared_error(y_real, y_pred))\n",
        "                        elif funcion_selector.value == 'MedAE': return median_absolute_error(y_real, y_pred)\n",
        "\n",
        "                    def objetivo_optuna(trial):\n",
        "                        C = trial.suggest_float('C', 1e-2, 1e3, log=True)\n",
        "                        epsilon = trial.suggest_float('epsilon', 1e-4, 0.5, log=True)\n",
        "                        kernel = trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly'])\n",
        "                        degree = trial.suggest_int('degree', 2, 4) if kernel == 'poly' else 3\n",
        "\n",
        "                        #C = trial.suggest_float('C', 0.1, 100, log=True)\n",
        "                        #epsilon = trial.suggest_float('epsilon', 0.01, 1.0, log=True)\n",
        "                        #kernel = trial.suggest_categorical('kernel', ['rbf', 'linear'])\n",
        "\n",
        "                        #svr = SVR(C=C, epsilon=epsilon, kernel=kernel)\n",
        "\n",
        "                        svr = SVR(C=C, epsilon=epsilon, kernel=kernel, degree=degree)\n",
        "                        svr.fit(X_train_scaled, y_train_scaled)\n",
        "                        y_pred = sy.inverse_transform(svr.predict(X_test_scaled).reshape(-1, 1)).ravel()\n",
        "                        score = calcular_score(y_test_sel, y_pred)\n",
        "                        return score if funcion_selector.value == 'R2' else -score\n",
        "\n",
        "                    for motor in motores:\n",
        "                        with traza:\n",
        "                            print(f\" Motor: {motor}  Variables: {len(vars_x)}  Datos: {X_train_scaled.shape}\")\n",
        "\n",
        "                        best_model = None\n",
        "\n",
        "                        if motor == 'GridSearchCV':\n",
        "                            grid = GridSearchCV(\n",
        "                                SVR(),\n",
        "#                                param_grid={\n",
        "#                                    'C': [0.1, 1, 10, 100],\n",
        "#                                    'epsilon': [0.01, 0.1, 0.5, 1],\n",
        "#                                    'kernel': ['rbf', 'linear']\n",
        "#                                },\n",
        "                                param_grid= {\n",
        "                                    'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
        "                                    'epsilon': [0.001, 0.01, 0.1, 0.5],\n",
        "                                    'kernel': ['rbf', 'linear', 'poly'],\n",
        "                                    'degree': [2, 3]  # solo si kernel = poly\n",
        "                                },\n",
        "                                scoring='r2', cv=3, n_jobs=-1\n",
        "                            )\n",
        "                            grid.fit(X_train_scaled, y_train_scaled)\n",
        "                            best_model = grid.best_estimator_\n",
        "\n",
        "                        elif motor == 'RandomizedSearchCV':\n",
        "                            rand = RandomizedSearchCV(\n",
        "                                SVR(),\n",
        "#                                param_distributions={\n",
        "#                                    'C': np.logspace(-1, 2, 20),\n",
        "#                                    'epsilon': np.logspace(-2, 0, 20),\n",
        "#                                    'kernel': ['rbf', 'linear']\n",
        "#                                },\n",
        "                                param_distributions={\n",
        "                                    'C': stats.reciprocal(1e-2, 1e3),\n",
        "                                    'epsilon': stats.reciprocal(1e-4, 0.5),\n",
        "                                    'kernel': ['rbf', 'linear', 'poly'],\n",
        "                                    'degree': stats.randint(2, 4)\n",
        "                                },\n",
        "                                scoring='r2', n_iter=30, cv=3, n_jobs=-1, random_state=42\n",
        "                            )\n",
        "                            rand.fit(X_train_scaled, y_train_scaled)\n",
        "                            best_model = rand.best_estimator_\n",
        "\n",
        "                        elif motor == 'Optuna':\n",
        "                            study = optuna.create_study(direction='maximize' if funcion_selector.value == 'R2' else 'minimize')\n",
        "                            study.optimize(objetivo_optuna, n_trials=30)\n",
        "                            best_model = SVR(**study.best_params)\n",
        "                            best_model.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "                        elif motor == 'BayesSearchCV':\n",
        "                            bayes = BayesSearchCV(\n",
        "                                SVR(),\n",
        "#                                search_spaces={\n",
        "#                                    'C': Real(0.1, 100, prior='log-uniform'),\n",
        "#                                    'epsilon': Real(0.01, 1.0, prior='log-uniform'),\n",
        "#                                    'kernel': Categorical(['rbf', 'linear'])\n",
        "#                                },\n",
        "                                search_spaces={\n",
        "                                    'C': Real(1e-2, 1e3, prior='log-uniform'),\n",
        "                                    'epsilon': Real(1e-4, 0.5, prior='log-uniform'),\n",
        "                                    'kernel': Categorical(['rbf', 'linear', 'poly']),\n",
        "                                    'degree': (2, 4)\n",
        "                                },\n",
        "                                scoring='r2', cv=3, n_iter=30, n_jobs=-1, random_state=42\n",
        "                            )\n",
        "                            bayes.fit(X_train_scaled, y_train_scaled)\n",
        "                            best_model = bayes.best_estimator_\n",
        "\n",
        "                        y_pred = sy.inverse_transform(best_model.predict(X_test_scaled).reshape(-1, 1)).ravel()\n",
        "                        score = calcular_score(y_test_sel, y_pred)\n",
        "                        mejores_modelos.append((f\"{metodo} - {motor}\", best_model, score, y_test_sel, y_pred))\n",
        "\n",
        "                        with traza:\n",
        "                            print(f\" {metodo} [{motor}]  {funcion_selector.value}: {score:.4f}\")\n",
        "\n",
        "                        # \n",
        "                        #   Bloque de Grabacin de resultados - persistencia\n",
        "                        # \n",
        "                        try:\n",
        "                            from pathlib import Path\n",
        "                            import pickle\n",
        "\n",
        "                            save_dir = Path(\"modelos_opt\")\n",
        "                            save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "                            modelo_fname = (\n",
        "                                save_dir / f\"modelo_svr_{metodo.lower()}_opt_{motor.lower()}.pkl\"\n",
        "                            )\n",
        "                            study_fname = (\n",
        "                                save_dir / f\"optuna_svr_{metodo.lower()}_opt_{motor.lower()}.pkl\"\n",
        "                            )\n",
        "\n",
        "                            # Obtener nombre Y de forma robusta\n",
        "                            if isinstance(Y_train, pd.Series):\n",
        "                                y_name = Y_train.name or \"target\"\n",
        "                            else:  # DataFrame (una sola columna)\n",
        "                                y_name = Y_train.columns[0] if Y_train.shape[1] == 1 else \"target\"\n",
        "\n",
        "                            payload = {\n",
        "                                \"model\":  best_model,\n",
        "                                \"sx\":     sx,\n",
        "                                \"sy\":     sy,\n",
        "                                \"cols\":   vars_x,\n",
        "                                \"yname\":  y_name,\n",
        "                                \"score\":  score,\n",
        "                                \"metric\": funcion_selector.value,\n",
        "                            }\n",
        "\n",
        "                            with open(modelo_fname, \"wb\") as f:\n",
        "                                pickle.dump(payload, f)\n",
        "\n",
        "                            study_fname = \"optuna_study.pkl\"   # nombre que espera la celda 10\n",
        "                            if motor.lower() == \"optuna\" and \"study\" in locals():\n",
        "                                with open(study_fname, \"wb\") as f:\n",
        "                                    pickle.dump(study, f)\n",
        "                                with traza: print(f\" Estudio Optuna guardado en: {study_fname}\")\n",
        "\n",
        "                            with traza: print(f\" Modelo guardado en: {modelo_fname}\")\n",
        "\n",
        "                            # Registrar en un diccionario global opcional\n",
        "                            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "                            OPT_MODELS[(\"svr\", metodo.lower(), motor.lower())] = payload\n",
        "                            if motor.lower() == \"optuna\" and \"study\" in locals():\n",
        "                                OPT_MODELS[(\"svr\", metodo.lower(), \"optuna_study\")] = study\n",
        "\n",
        "                        except Exception as e:\n",
        "                            with traza: print(f\" No se pudo guardar el modelo o estudio: {e}\")\n",
        "                        # \n",
        "                        #   FIN DEL BLOQUE DE PERSISTENCIA\n",
        "                        # \n",
        "                except Exception as e:\n",
        "                    with traza:\n",
        "                        print(f\" Error al optimizar {metodo}: {e}\")\n",
        "\n",
        "            with salida_resultados:\n",
        "                if mejores_modelos:\n",
        "                    mejores_modelos.sort(key=lambda x: x[2], reverse=(funcion_selector.value == 'R2'))\n",
        "                    top5 = mejores_modelos[:5]\n",
        "\n",
        "                    df_top5 = pd.DataFrame([{\n",
        "                        'Mtodo-Motor': m[0],\n",
        "                        funcion_selector.value: round(m[2], 4),\n",
        "                        'C': m[1].C,\n",
        "                        'Epsilon': m[1].epsilon,\n",
        "                        'Kernel': m[1].kernel\n",
        "                    } for m in top5])\n",
        "\n",
        "                    display(HTML(\"<h4 style='color:#2E8B57;'> Top 5 Modelos Optimizado...</h4>\"))\n",
        "                    display(df_top5.style.set_caption(\"Top 5 configuraciones SVR\")\n",
        "                            .set_properties(**{'border': '1px solid gray', 'text-align': 'center'})\n",
        "                            .set_table_styles([{'selector': 'th', 'props': [('background-color', '#2E8B57'), ('color', 'white')]}]))\n",
        "\n",
        "                    mejor = top5[0]\n",
        "                    metodo_motor, model, score, y_real, y_pred = mejor\n",
        "\n",
        "                    display(HTML(f\"\"\"\n",
        "                    <h4 style='color:green;'> Mejor configuracin encontrada:</h4>\n",
        "                    <b>Mtodo:</b> {metodo_motor}<br>\n",
        "                    <b>{funcion_selector.value}:</b> {score:.4f}\n",
        "                    <hr>\n",
        "                    <h4> Detalle de Hiperparmetros:</h4>\n",
        "                    \"\"\"))\n",
        "\n",
        "                    df_hp = pd.DataFrame({\n",
        "                        'Parmetro': ['C', 'epsilon', 'kernel'],\n",
        "                        'Valor ptimo': [model.C, model.epsilon, model.kernel],\n",
        "                        'Descripcin': [\n",
        "                            'Penalizacin del margen. Equilibra error y generalizacin',\n",
        "                            'Zona de tolerancia sin penalizacin en el error',\n",
        "                            'Funcin que transforma el espacio (lineal o no lineal)'\n",
        "                        ],\n",
        "                        'Rango Tpico': ['0.1  100', '0.01  1.0', 'rbf / linear']\n",
        "                    })\n",
        "\n",
        "                    display(df_hp.style.set_table_styles([\n",
        "                        {'selector': 'th', 'props': [('background-color', '#2E8B57'), ('color', 'white')]},\n",
        "                        {'selector': 'td', 'props': [('text-align', 'center')]}\n",
        "                    ]).set_properties(**{'border': '1px solid gray', 'padding': '6px'}))\n",
        "\n",
        "                    plt.figure(figsize=(8, 5))\n",
        "                    plt.plot(y_real, label='Real', marker='o')\n",
        "                    plt.plot(y_pred, label='Prediccin', marker='x')\n",
        "                    plt.title(f'Y Real vs Y Predicho (Mejor SVR - {metodo_motor})')\n",
        "                    plt.legend()\n",
        "                    plt.grid()\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                    # ===============================================================\n",
        "                    # Anlisis de Residuos del Mejor Modelo\n",
        "                    # ===============================================================\n",
        "                    residuos = y_real - y_pred\n",
        "\n",
        "                    display(HTML(\"<h4 style='color:#2E8B57;'> Anlisis de Residuos del Mejor Modelo</h4>\"))\n",
        "                    display(HTML(\"\"\"\n",
        "                    <p>Los <b>residuos</b> representan la diferencia entre los valores reales (observados) y los predichos por el modelo.\n",
        "                    Evaluar su comportamiento ayuda a determinar si el modelo ha capturado correctamente la estructura de los datos.</p>\n",
        "                    <ul>\n",
        "                    <li><b>Residuos = Valor Real  Valor Predicho</b></li>\n",
        "                    <li>Distribucin simtrica centrada en cero es seal de buen ajuste</li>\n",
        "                    <li>Asimetra, colas largas o concentraciones pueden indicar sobreajuste, variables omitidas u otros problemas.</li>\n",
        "                    </ul>\n",
        "                    \"\"\"))\n",
        "\n",
        "                    # Estadsticas bsicas\n",
        "                    res_stats = pd.DataFrame({\n",
        "                        'Mtrica': ['Media', 'Desviacin estndar', 'Mnimo', 'Mximo'],\n",
        "                        'Valor': [np.mean(residuos), np.std(residuos), np.min(residuos), np.max(residuos)]\n",
        "                    })\n",
        "                    display(res_stats.style.set_caption(\" Estadsticas de los Residuos\")\n",
        "                            .set_properties(**{'border': '1px solid gray', 'text-align': 'center'})\n",
        "                            .set_table_styles([{'selector': 'th', 'props': [('background-color', '#2E8B57'), ('color', 'white')]}]))\n",
        "\n",
        "                    # Histograma de residuos\n",
        "                    plt.figure(figsize=(8,4))\n",
        "                    plt.hist(residuos, bins=25, color='skyblue', edgecolor='black')\n",
        "                    plt.title(' Histograma de Residuos')\n",
        "                    plt.xlabel('Residuo')\n",
        "                    plt.ylabel('Frecuencia')\n",
        "                    plt.grid(True)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                    # Grfico Q-Q\n",
        "                    plt.figure(figsize=(6, 6))\n",
        "                    stats.probplot(residuos, dist=\"norm\", plot=plt)\n",
        "                    plt.title(' Grfico Q-Q de los Residuos')\n",
        "                    plt.grid(True)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                    # Test de normalidad de Shapiro-Wilk\n",
        "                    residuos_validos = residuos[~np.isnan(residuos) & ~np.isinf(residuos)]\n",
        "\n",
        "                    display(HTML(\"<h4> Test de Normalidad (Shapiro-Wilk)</h4>\"))\n",
        "\n",
        "                    print(f\"Nmero de residuos vlidos: {len(residuos_validos)}\")\n",
        "                    print(\"Primeros residuos vlidos:\", residuos_validos[:10])\n",
        "\n",
        "                    if residuos_validos.size >= 3:\n",
        "                        try:\n",
        "                            stat, p = shapiro(residuos_validos)\n",
        "                            interpretacion = ' Residuos normales (p > 0.05)' if p > 0.05 else ' Residuos no normales (p  0.05)'\n",
        "                            display(HTML(f\"\"\"\n",
        "                                <ul>\n",
        "                                    <li><b>Estadstico:</b> {stat:.4f}</li>\n",
        "                                    <li><b>p-valor:</b> {p:.4f}</li>\n",
        "                                    <li>{interpretacion}</li>\n",
        "                                </ul>\n",
        "                            \"\"\"))\n",
        "                        except Exception as e:\n",
        "                            display(HTML(f\"<span style='color:red;'> Error al ejecutar el test de Shapiro: {e}</span>\"))\n",
        "                    else:\n",
        "                        display(HTML(\"<span style='color:red;'> No hay suficientes datos vlidos para realizar el test de normalidad.</span>\"))\n",
        "\n",
        "                    # ==========================================================\n",
        "                    #  ANALISIS COMPARATIVO AVANZADO\n",
        "                    # ==========================================================\n",
        "                    # ============================================\n",
        "                    #  CREACIN DE DATAFRAME DE MTRICAS PARA COMPARATIVA\n",
        "                    # ============================================\n",
        "                    metricas_df = pd.DataFrame([\n",
        "                        {\n",
        "                            'Modelo': nombre,\n",
        "                            'R2': r2_score(y_real, y_pred),\n",
        "                            'MSE': mean_squared_error(y_real, y_pred),\n",
        "                            'MAE': mean_absolute_error(y_real, y_pred),\n",
        "                            'RMSE': np.sqrt(mean_squared_error(y_real, y_pred)),\n",
        "                            'MedAE': median_absolute_error(y_real, y_pred)\n",
        "                        }\n",
        "                        for nombre, modelo, _, y_real, y_pred in top5\n",
        "                    ])\n",
        "\n",
        "                    # ============================================\n",
        "                    #  MAPA DE CALOR DE MTRICAS\n",
        "                    # ============================================\n",
        "                    metricas_norm = (metricas_df.drop('Modelo', axis=1) - metricas_df.drop('Modelo', axis=1).min()) / (\n",
        "                        metricas_df.drop('Modelo', axis=1).max() - metricas_df.drop('Modelo', axis=1).min())\n",
        "                    plt.figure(figsize=(10, 5))\n",
        "                    sns.heatmap(metricas_norm.T, annot=True, cmap='YlGnBu', xticklabels=metricas_df['Modelo'], fmt=\".2f\")\n",
        "                    plt.title(\" Mapa de Calor de Mtricas Normalizadas\")\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                    # ============================================\n",
        "                    #  RADAR CHART DE MTRICAS\n",
        "                    # ============================================\n",
        "                    #import matplotlib.pyplot as plt\n",
        "                    from math import pi\n",
        "\n",
        "                    # Preparar datos\n",
        "                    categorias = list(metricas_df.columns[1:])\n",
        "                    N = len(categorias)\n",
        "                    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "                    angles += angles[:1]\n",
        "\n",
        "                    plt.figure(figsize=(8, 6))\n",
        "                    for i in range(len(metricas_df)):\n",
        "                        valores = metricas_df.iloc[i, 1:].values.flatten().tolist()\n",
        "                        valores += valores[:1]\n",
        "                        plt.polar(angles, valores, label=metricas_df.iloc[i, 0], marker='o')\n",
        "\n",
        "                    plt.xticks(angles[:-1], categorias, color='grey', size=8)\n",
        "                    plt.title(\" Radar Chart - Comparativa Multimtrica\")\n",
        "                    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                    # ===============================================================\n",
        "                    # ACopiar resultados al portapapeles\n",
        "                    # ===============================================================\n",
        "                    btn_copiar = widgets.Button(description=' Copiar Hiperparmetros', button_style='info')\n",
        "                    def copiar(_):\n",
        "                        texto = str(model.get_params())\n",
        "                        js = f\"navigator.clipboard.writeText(`{texto}`)\"\n",
        "                        display(Javascript(js))\n",
        "                        print(\" Copiados al portapapeles.\")\n",
        "                    btn_copiar.on_click(copiar)\n",
        "                    display(btn_copiar)\n",
        "\n",
        "                else:\n",
        "                    print(\" No se encontr ninguna configuracin ptima vlida.\")\n",
        "\n",
        "            progreso.value = f\" Tiempo total de optimizacin: {time.time() - inicio:.2f} segundos\"\n",
        "\n",
        "        btn_ejecutar.on_click(ejecutar_optimizacion)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            funcion_selector,\n",
        "            motor_selector,\n",
        "            btn_ejecutar,\n",
        "            progreso,\n",
        "            traza,\n",
        "            salida_resultados\n",
        "        ]))\n",
        "\n",
        "#mostrar_optimizacion_svr()\n",
        "display(out_opt_svr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 9.2. OPTIMIZACION NN - FUNCIONA CORRECTAMENTE\n",
        "# Este mdulo es el responsable de la optimizacin completa de la red neuronal, que busca minimizar el Minimo Error Cuadrtico (MAE) y la prdida (LOSS) optimizando el HPO que incluye: capas ocultas,\n",
        "# neuronas por capa, funcin de activacin, velocidad de aprendizaje, epocas, funacin de optimizacin.\n",
        "# Incluye el grabado de datos\n",
        "# ===============================================================\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import shapiro\n",
        "from ipywidgets import VBox, HBox, Dropdown, IntSlider, IntText, IntProgress, FloatSlider, SelectMultiple, Button, Output, HTML, Accordion\n",
        "from IPython.display import display, clear_output\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "#  AADIDO: Mixed-Precision Training \n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "#  FIN AADIDO \n",
        "\n",
        "#  AADIDO: import de EarlyStopping + definicin de TimeStopping \n",
        "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
        "\n",
        "class TimeStopping(Callback):\n",
        "    \"\"\"Detiene el entrenamiento si supera un tiempo mximo (en segundos).\"\"\"\n",
        "    def __init__(self, max_seconds=600):\n",
        "        super().__init__()\n",
        "        self.max_seconds = max_seconds\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if time.time() - self.start_time > self.max_seconds:\n",
        "            self.model.stop_training = True\n",
        "#  FIN AADIDO \n",
        "\n",
        "import signal\n",
        "\n",
        "#  AADIDO: Timeout para tuner.search con signal.alarm \n",
        "class TimeoutException(Exception):\n",
        "    pass\n",
        "\n",
        "def _timeout_handler(signum, frame):\n",
        "    raise TimeoutException()\n",
        "\n",
        "# Registramos el manejador\n",
        "signal.signal(signal.SIGALRM, _timeout_handler)\n",
        "#  FIN AADIDO \n",
        "\n",
        "#  AADIDO: Custom R2 metric para Keras \n",
        "class R2(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='r2', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.sse = self.add_weight(name='sse', initializer='zeros')\n",
        "        self.sst = self.add_weight(name='sst', initializer='zeros')\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "        resid = tf.reduce_sum(tf.square(y_true - y_pred))\n",
        "        mean_true = tf.reduce_mean(y_true)\n",
        "        sst_value = tf.reduce_sum(tf.square(y_true - mean_true))\n",
        "        self.sse.assign_add(resid)\n",
        "        self.sst.assign_add(sst_value)\n",
        "    def result(self):\n",
        "        return 1.0 - (self.sse / (self.sst + tf.keras.backend.epsilon()))\n",
        "    def reset_states(self):\n",
        "        self.sse.assign(0.0)\n",
        "        self.sst.assign(0.0)\n",
        "#  FIN AADIDO \n",
        "\n",
        "import optuna\n",
        "from keras_tuner import RandomSearch, BayesianOptimization, Hyperband, Objective\n",
        "\n",
        "# ========== Widgets de configuracin ==========\n",
        "\n",
        "# Ayuda extendida sobre los parmetros\n",
        "ayuda_parametros = HTML(\"\"\"\n",
        "<h4> Explicacin de Parmetros</h4>\n",
        "<ul>\n",
        "  <li><b>Mtodos X:</b> Mtodos de seleccin de variables predictoras. Usan correlaciones estadsticas o algoritmos de reduccin de dimensin. <br>\n",
        "      <i>Pearson</i> y <i>Spearman</i>: correlaciones lineales y montonas.<br>\n",
        "      <i>MutualInfo</i>: mide dependencia informacional. <br>\n",
        "      <i>Boruta</i>: seleccin envolvente basada en rboles. <br>\n",
        "      <i>UMAP</i>: reduccin no lineal de dimensiones. <br>\n",
        "      <b>Todos</b> ejecuta cada uno secuencialmente.</li>\n",
        "  <li><b>Motores:</b> Algoritmos de optimizacin de hiperparmetros. <br>\n",
        "      <i>RandomSearch</i>: bsqueda aleatoria. <br>\n",
        "      <i>BayesianOptimization</i>: estima regiones ptimas. <br>\n",
        "      <i>Hyperband</i>: eficiente para grandes espacios de bsqueda. <br>\n",
        "      <i>Optuna</i>: flexible y potente. <br>\n",
        "      <b>Todos</b> ejecuta todos los motores.</li>\n",
        "  <li><b>Funcin objetivo:</b> Mtrica a maximizar o minimizar: <br>\n",
        "      <i>R2</i>: se desea maximizar. <i>MAE</i> y <i>MSE</i>: se minimizan.</li>\n",
        "  <li><b>Trials:</b> Nmero de combinaciones a evaluar en la bsqueda.</li>\n",
        "  <li><b>pocas:</b> Iteraciones completas sobre el dataset de entrenamiento (100 a 2000 recomendado).</li>\n",
        "  <li><b>Capas:</b> Cantidad de capas ocultas en la red (1 a 20 habitual, mximo 100 para pruebas avanzadas).</li>\n",
        "  <li><b>Neuronas/capa:</b> Nmero de neuronas por capa (32 a 512 recomendado).</li>\n",
        "  <li><b>Dropout:</b> Fraccin de neuronas descartadas en entrenamiento (0.1 a 0.4 recomendado).</li>\n",
        "  <li><b>L2 Reg:</b> Regularizacin L2 para evitar sobreajuste (0.001 a 0.01 habitual).</li>\n",
        "</ul>\n",
        "\"\"\")\n",
        "\n",
        "select_metodos = SelectMultiple(\n",
        "    options=['Pearson', 'Spearman', 'Mutualinfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "    description='Mtodos Seleccin Variables X:',\n",
        "    layout={'width': '50%'}\n",
        ")\n",
        "\n",
        "select_motores = SelectMultiple(\n",
        "    options=['RandomSearch', 'BayesianOptimization', 'Hyperband', 'Optuna', 'Todos'],\n",
        "    description='Motores:',\n",
        "    layout={'width': '50%'}\n",
        ")\n",
        "\n",
        "func_objetivo = Dropdown(\n",
        "    options=['R2', 'MAE', 'MSE'],\n",
        "    value='R2',\n",
        "    description='Funcin objetivo:',\n",
        "    layout={'width': '40%'}\n",
        ")\n",
        "\n",
        "# Rango de hiperparmetros\n",
        "n_trials_slider = IntSlider(value=10, min=1, max=50, step=1, description='Trials:')\n",
        "rango_epocas = IntSlider(value=500, min=1, max=200, step=10, description='pocas:')\n",
        "rango_capas = IntSlider(value=3, min=1, max=6, step=1, description='Capas:')\n",
        "rango_neuronas = IntSlider(value=64, min=256, max=512, step=8, description='Neuronas/capa:')\n",
        "dropout_rate = FloatSlider(value=0.2, min=0.0, max=0.7, step=0.05, description='Dropout:')\n",
        "l2_reg = FloatSlider(value=0.001, min=0.0, max=0.01, step=0.0005, description='L2 Reg:')\n",
        "\n",
        "btn_ejecutar = Button(description=' Ejecutar Optimizacin NN', button_style='success')\n",
        "progreso_bar = IntProgress(min=0, max=1, description='Progreso:', style={'bar_color': 'green'})\n",
        "#progreso_label = Label(value=\"\")\n",
        "out_nn = Output()\n",
        "\n",
        "# ========== Funcin principal ==========\n",
        "def ejecutar_optimizacion(_):\n",
        "    with out_nn:\n",
        "        clear_output()\n",
        "\n",
        "        # 2) Ahora s puedo sanear columnas\n",
        "        # Usar sanitize_name para limpiar columnas en el payload\n",
        "        def clean_cols(col_list):\n",
        "            return [sanitize_name(c) for c in col_list]\n",
        "        # Ejemplo de sanitizacin de X_train antes de fit\n",
        "        X_train.columns = [sanitize_name(col) for col in X_train.columns]\n",
        "        X_test.columns = [sanitize_name(col) for col in X_test.columns]\n",
        "\n",
        "        print(\" Iniciando optimizacin...\")\n",
        "        start_time = time.time()\n",
        "        max_total_time = 6000   # 100 minutos en total para TODO el tuning\n",
        "        resultados_modelos = []\n",
        "\n",
        "        for metodo in select_metodos.value:\n",
        "            print(f\" Mtodo: {metodo}\")\n",
        "            # Simulacin de filtrado de variables segn el mtodo seleccionado\n",
        "            X_train_sel, X_test_sel = X_train.copy(), X_test.copy()\n",
        "            y_train_sel, y_test_sel = Y_train.copy(), Y_test.copy()\n",
        "\n",
        "            # \n",
        "            # Si el usuario marca Todos, reemplazamos esa opcin\n",
        "            motores = list(select_motores.value)\n",
        "            if \"Todos\" in motores:\n",
        "                motores = [\"RandomSearch\", \"BayesianOptimization\", \"Hyperband\", \"Optuna\"]\n",
        "            # \n",
        "            from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
        "            #from tensorflow_addons.metrics import RSquare  # si lo tienes instalado\n",
        "\n",
        "            #for motor in select_motores.value:\n",
        "            for motor in motores:\n",
        "                print(f\" Motor: {motor}\")\n",
        "\n",
        "                #  AADIDO: tope global de tiempo \n",
        "                # 1. Chequeo de tiempo PARA ESTE motor\n",
        "                elapsed = time.time() - start_time\n",
        "                if elapsed > max_total_time:\n",
        "                    print(f\" Tiempo agotado antes de {motor} en {metodo}; sigo con el siguiente mtodo.\")\n",
        "                    break   # solo sale del bucle 'motor'\n",
        "                #  FIN AADIDO \n",
        "\n",
        "                metric_name = func_objetivo.value.lower()\n",
        "                direction = 'max' if metric_name == 'r2' else 'min'\n",
        "                # Ahora monitorizamos la mtrica de validacin correcta:\n",
        "                tuner_metric = f\"val_{metric_name}\"                 # 'val_r2', 'val_mae' o 'val_mse'\n",
        "\n",
        "                #tuner_metric = 'val_loss' if metric_name == 'r2' else metric_name\n",
        "                #tuner_metric = 'val_r2'   if metric_name=='r2' else f'val_{metric_name}'\n",
        "\n",
        "                def build_model(hp):\n",
        "                    model = keras.Sequential()\n",
        "                    model.add(layers.Input(shape=(X_train_sel.shape[1],)))\n",
        "                    for i in range(hp.Int('layers', 1, rango_capas.max)):\n",
        "                        model.add(layers.Dense(hp.Int(f'units_{i}', 8, rango_neuronas.max), activation='relu'))\n",
        "                        model.add(layers.Dropout(hp.Float(f'dropout_{i}', 0.0, dropout_rate.max)))\n",
        "                    model.add(layers.Dense(1))\n",
        "                    #model.compile(optimizer='adam', loss='mse')\n",
        "                    model.compile(\n",
        "                        optimizer='adam',\n",
        "                        loss='mse',\n",
        "                        metrics=[R2(name='r2'),\n",
        "                                tf.keras.metrics.MeanSquaredError(name='mse'),\n",
        "                                tf.keras.metrics.MeanAbsoluteError(name='mae')]\n",
        "                    )\n",
        "                    return model\n",
        "\n",
        "                #callbacks = [EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]\n",
        "                #  AADIDO: EarlyStopping con min_delta + TimeStopping \n",
        "                callbacks = [\n",
        "                    EarlyStopping(\n",
        "                        monitor=tuner_metric,      # mejorar segn la mtrica que toque\n",
        "                        mode = 'max' if metric_name == 'r2' else 'min',\n",
        "                        min_delta=1e-1,            # mejora mnima del 10%\n",
        "                        patience=3,               # si no mejora tras 3 pocas, cortar\n",
        "                        restore_best_weights=True\n",
        "                    ),\n",
        "                    TimeStopping(max_seconds=120)  # tope de 120 s (~2 min) por modelo\n",
        "                ]\n",
        "                #  FIN AADIDO \n",
        "\n",
        "                # Eliminar directorios previos si existen\n",
        "                if motor == 'RandomSearch':\n",
        "                    tuner_dir = f'randomsearch_dir/random_{metodo}'\n",
        "                    if os.path.exists(tuner_dir):\n",
        "                        shutil.rmtree(tuner_dir)\n",
        "                    tuner = RandomSearch(\n",
        "                        build_model,\n",
        "                        objective=Objective(tuner_metric, direction=direction),\n",
        "                        max_trials=n_trials_slider.value,\n",
        "                        executions_per_trial=1,\n",
        "                        directory='randomsearch_dir',\n",
        "                        project_name=f'random_{metodo}'\n",
        "                    )\n",
        "                    #tuner.search(X_train_sel, y_train_sel, epochs=rango_epocas.value, validation_split=0.2, verbose=1, callbacks=callbacks)\n",
        "                    #  AADIDO: timeout para este tuner \n",
        "                    remaining = max_total_time - (time.time() - start_time)\n",
        "                    # no menos de 1 segundo\n",
        "                    timeout_secs = int(max(1, remaining))\n",
        "                    signal.alarm(timeout_secs)\n",
        "                    try:\n",
        "                        tuner.search(\n",
        "                          X_train_sel, y_train_sel,\n",
        "                          epochs=rango_epocas.value,\n",
        "                          validation_split=0.2,\n",
        "                          verbose=1,\n",
        "                          callbacks=callbacks\n",
        "                        )\n",
        "                    except TimeoutException:\n",
        "                        print(f\" RandomSearch ({metodo}) interrumpido tras {timeout_secs}s\")\n",
        "                    finally:\n",
        "                        signal.alarm(0)\n",
        "                  #  FIN AADIDO \n",
        "                    best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "                elif motor == 'BayesianOptimization':\n",
        "                    tuner_dir = f'bo_dir/bo_{metodo}'\n",
        "                    if os.path.exists(tuner_dir):\n",
        "                        shutil.rmtree(tuner_dir)\n",
        "                    tuner = BayesianOptimization(\n",
        "                        build_model,\n",
        "                        objective=Objective(tuner_metric, direction=direction),\n",
        "                        max_trials=n_trials_slider.value,\n",
        "                        directory='bo_dir',\n",
        "                        project_name=f'bo_{metodo}'\n",
        "                    )\n",
        "                    #tuner.search(X_train_sel, y_train_sel, epochs=rango_epocas.value, validation_split=0.2, verbose=1, callbacks=callbacks)\n",
        "                    #  AADIDO: timeout para este tuner \n",
        "                    remaining = max_total_time - (time.time() - start_time)\n",
        "                    # no menos de 1 segundo\n",
        "                    timeout_secs = int(max(1, remaining))\n",
        "                    signal.alarm(timeout_secs)\n",
        "                    try:\n",
        "                        tuner.search(\n",
        "                           X_train_sel, y_train_sel,\n",
        "                           epochs=rango_epocas.value,\n",
        "                           validation_split=0.2,\n",
        "                           verbose=1,\n",
        "                           callbacks=callbacks\n",
        "                        )\n",
        "                    except TimeoutException:\n",
        "                        print(f\" RandomSearch ({metodo}) interrumpido tras {timeout_secs}s\")\n",
        "                    finally:\n",
        "                        signal.alarm(0)\n",
        "                    #  FIN AADIDO \n",
        "                    best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "                elif motor == 'Hyperband':\n",
        "                    tuner_dir = f'hyperband_dir/hyper_{metodo}'\n",
        "                    if os.path.exists(tuner_dir):\n",
        "                        shutil.rmtree(tuner_dir)\n",
        "                    tuner = Hyperband(\n",
        "                        build_model,\n",
        "                        objective=Objective(tuner_metric, direction=direction),\n",
        "                        #max_epochs=rango_epocas.max,\n",
        "                        max_epochs=rango_epocas.value,\n",
        "                        factor=4,                         #  de 2 a 4  menos brackets\n",
        "                        directory='hyperband_dir',\n",
        "                        project_name=f'hyper_{metodo}'\n",
        "                    )\n",
        "                    #  AADIDO AQU: callbacks especficos para Hyperband \n",
        "                    callbacks = [\n",
        "                        EarlyStopping(\n",
        "                            monitor=tuner_metric,\n",
        "                            mode = 'max' if metric_name == 'r2' else 'min',  # < aqu le decimos a Keras qu queremos\n",
        "                            min_delta=1e-2,\n",
        "                            patience=2,\n",
        "                            restore_best_weights=True\n",
        "                        ),\n",
        "                        TimeStopping(max_seconds=120)\n",
        "                    ]\n",
        "                    #  FIN AADIDO \n",
        "\n",
        "                    #tuner.search(X_train_sel, y_train_sel, validation_split=0.2, verbose=1, callbacks=callbacks)\n",
        "                    #tuner.search(X_train_sel, y_train_sel, epochs=rango_epocas.value, validation_split=0.2, verbose=1, callbacks=callbacks)\n",
        "\n",
        "                    #  AADIDO: timeout para este tuner \n",
        "                    remaining = max_total_time - (time.time() - start_time)\n",
        "                    # no menos de 1 segundo\n",
        "                    timeout_secs = int(max(1, remaining))\n",
        "                    signal.alarm(timeout_secs)\n",
        "                    try:\n",
        "                        tuner.search(\n",
        "                           X_train_sel, y_train_sel,\n",
        "                           epochs=rango_epocas.value,\n",
        "                           validation_split=0.2,\n",
        "                           verbose=1,\n",
        "                           callbacks=callbacks\n",
        "                        )\n",
        "                    except TimeoutException:\n",
        "                        print(f\" RandomSearch ({metodo}) interrumpido tras {timeout_secs}s\")\n",
        "                    finally:\n",
        "                        signal.alarm(0)\n",
        "                    #  FIN AADIDO \n",
        "                    best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "                elif motor == 'Optuna':\n",
        "                    def objective(trial):\n",
        "                        model = keras.Sequential()\n",
        "                        model.add(layers.Input(shape=(X_train_sel.shape[1],)))\n",
        "                        for i in range(trial.suggest_int('n_layers', 1, rango_capas.max)):\n",
        "                            model.add(layers.Dense(trial.suggest_int(f'n_units_l{i}', 8, rango_neuronas.max), activation='relu'))\n",
        "                            model.add(layers.Dropout(trial.suggest_float(f'dropout_l{i}', 0.0, dropout_rate.max)))\n",
        "                        model.add(layers.Dense(1))\n",
        "                        model.compile(optimizer='adam', loss='mse')\n",
        "                        #model.fit(X_train_sel, y_train_sel, epochs=rango_epocas.value, batch_size=32, verbose=0, validation_split=0.2)\n",
        "                        model.fit(X_train_sel, y_train_sel, epochs=rango_epocas.value, batch_size=32, verbose=1, validation_split=0.2, callbacks=callbacks)\n",
        "                        preds = model.predict(X_test_sel).ravel()\n",
        "                        return -r2_score(y_test_sel, preds) if func_objetivo.value == 'R2' else mean_absolute_error(y_test_sel, preds)\n",
        "\n",
        "                    direction = 'maximize' if func_objetivo.value == 'R2' else 'minimize'\n",
        "\n",
        "                    #study = optuna.create_study(direction=direction)\n",
        "                    #  AADIDO: Pruner para cortar trials poco prometedores \n",
        "                    from optuna.pruners import MedianPruner\n",
        "                    remaining = max_total_time - (time.time() - start_time)\n",
        "                    if remaining <= 0:\n",
        "                        print(\" Ya no queda tiempo para Optuna.\")\n",
        "                        continue\n",
        "\n",
        "                    study = optuna.create_study(direction=direction,\n",
        "                                              pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=10))\n",
        "                    # timeout detiene el optimize tras X segundos, sin esperar a n_trials\n",
        "                    study.optimize(objective,\n",
        "                                  n_trials=n_trials_slider.value,\n",
        "                                  timeout=remaining)\n",
        "                    #  FIN AADIDO \n",
        "                    #study.optimize(objective, n_trials=n_trials_slider.value)\n",
        "\n",
        "                    best_params = study.best_params\n",
        "\n",
        "                progreso_bar.value += 1\n",
        "                print(f\" Completado: Mtodo {metodo}, Motor {motor}\")\n",
        "                print(f\" Optimizacin completada en {time.time() - start_time:.2f} segundos.\")\n",
        "\n",
        "                # Placeholder para evaluacin final\n",
        "                model = keras.Sequential()\n",
        "                model.add(layers.Input(shape=(X_train_sel.shape[1],)))\n",
        "                for _ in range(rango_capas.value):\n",
        "                    model.add(layers.Dense(rango_neuronas.value, activation='relu'))\n",
        "                    model.add(layers.Dropout(dropout_rate.value))\n",
        "                model.add(layers.Dense(1))\n",
        "                model.compile(optimizer='adam', loss='mse')\n",
        "                model.fit(X_train_sel, y_train_sel, epochs=rango_epocas.value, batch_size=32, verbose=0, callbacks=callbacks)\n",
        "\n",
        "                y_pred = model.predict(X_test_sel).ravel()\n",
        "                r2 = r2_score(y_test_sel, y_pred)\n",
        "                mae = mean_absolute_error(y_test_sel, y_pred)\n",
        "                mse = mean_squared_error(y_test_sel, y_pred)\n",
        "\n",
        "                # *********************************************************\n",
        "                # Visualizacin del modelo optimizado\n",
        "                # *********************************************************\n",
        "                fig, ax = plt.subplots(figsize=(6, 4))\n",
        "                ax.scatter(range(len(y_pred)), y_test_sel.values.ravel(), label='Y Real')\n",
        "                ax.plot(range(len(y_pred)), y_pred, color='orange', label='Y Predicho')\n",
        "                ax.set_title(f\"XY-Y Real vs. Predicho: {metodo}-{motor}\")\n",
        "                ax.set_xlabel(\"Casos\")\n",
        "                ax.set_ylabel(\"Y\")\n",
        "                ax.legend()\n",
        "                ax.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # \n",
        "                #   BLOQUE DE PERSISTENCIA  (NO ALTERA LA LGICA EXISTENTE)\n",
        "                #      Guarda el mejor modelo de cada motor en /modelos_opt\n",
        "                #      Guarda escaladores y columnas en un .pkl auxiliar\n",
        "                #      Guarda el study de Optuna, si existe\n",
        "                # \n",
        "                try:\n",
        "                    from pathlib import Path\n",
        "                    import pickle\n",
        "\n",
        "                    # 1  Mtrica que queremos persistir como score\n",
        "                    if func_objetivo.value == \"R2\":\n",
        "                        score_val = r2\n",
        "                    elif func_objetivo.value == \"MAE\":\n",
        "                        score_val = mae\n",
        "                    else:                                 # \"MSE\"\n",
        "                        score_val = mse\n",
        "\n",
        "                    # ---------- rutas ----------\n",
        "                    save_dir = Path(\"modelos_opt\")\n",
        "                    save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "                    # nombre robusto variable-objetivo\n",
        "                    if isinstance(Y_train, pd.Series):\n",
        "                        y_name = Y_train.name or \"target\"\n",
        "                    else:                                # DataFrame\n",
        "                        y_name = Y_train.columns[0] if Y_train.shape[1] == 1 else \"target\"\n",
        "\n",
        "                    base_fname  = f\"nn_{metodo.lower()}_opt_{motor.lower()}\"\n",
        "                    model_fname = save_dir / f\"modelo_{base_fname}.h5\"          # modelo\n",
        "                    meta_fname  = save_dir / f\"meta_{base_fname}.pkl\"           # metadatos\n",
        "                    study_fname = save_dir / f\"optuna_{base_fname}.pkl\"         # estudio Optuna\n",
        "\n",
        "                    # ---------- modelo ----------\n",
        "                    model_to_save = model                # alias universal\n",
        "                    model_to_save.save(model_fname, include_optimizer=True)\n",
        "\n",
        "                    best_hps_dict = {}\n",
        "                    if motor in (\"RandomSearch\",\"BayesianOptimization\",\"Hyperband\"):\n",
        "                        # para Keras Tuner:\n",
        "                        best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "                        best_hps_dict = {\n",
        "                          \"layers\":  best_hps.get(\"layers\"),\n",
        "                          \"neurons\": best_hps.get(\"units_0\"),\n",
        "                          \"dropout\": best_hps.get(\"dropout_0\"),\n",
        "                          \"epochs\":  rango_epocas.value\n",
        "                        }\n",
        "                    elif motor == \"Optuna\":\n",
        "                        # Optuna:\n",
        "                        best_hps_dict = study.best_params.copy()\n",
        "                        best_hps_dict[\"epochs\"] = rango_epocas.value\n",
        "\n",
        "                    # Comprobacin de scalers\n",
        "                    if 'sx' not in locals():\n",
        "                        sx = None\n",
        "                    if 'sy' not in locals():\n",
        "                        sy = None\n",
        "\n",
        "                    # Extrae los hiperparmetros relevantes asegurando ambos nombres\n",
        "                    # (esto funciona tanto para Optuna como KerasTuner)\n",
        "                    layers_ = best_hps_dict.get(\"layers\") or best_hps_dict.get(\"n_layers\")\n",
        "                    neurons_ = best_hps_dict.get(\"neurons\") or best_hps_dict.get(\"n_units_l0\")\n",
        "                    dropout_ = best_hps_dict.get(\"dropout\") or best_hps_dict.get(\"dropout_l0\")\n",
        "                    epochs_ = best_hps_dict.get(\"epochs\")\n",
        "\n",
        "                    meta_payload = {\n",
        "                        \"sx\":    locals().get(\"sx\", None),\n",
        "                        \"sy\":    locals().get(\"sy\", None),\n",
        "                        \"cols\":  X_train_sel.columns.tolist(),\n",
        "                        \"yname\": y_name,\n",
        "                        \"score\": float(score_val),\n",
        "                        \"metric\": func_objetivo.value,\n",
        "                        \"motor\": motor,\n",
        "                        \"metodo\": metodo,\n",
        "                        # Nombres duplicados para compatibilidad mxima\n",
        "                        \"layers\": layers_,\n",
        "                        \"n_layers\": layers_,\n",
        "                        \"neurons\": neurons_,\n",
        "                        \"n_units_l0\": neurons_,\n",
        "                        \"dropout\": dropout_,\n",
        "                        \"dropout_l0\": dropout_,\n",
        "                        \"epochs\": epochs_,\n",
        "                        **{k: v for k, v in best_hps_dict.items() if k not in [\"layers\", \"n_layers\", \"neurons\", \"n_units_l0\", \"dropout\", \"dropout_l0\", \"epochs\"]}\n",
        "                    }\n",
        "\n",
        "                    with open(meta_fname, \"wb\") as f_meta:\n",
        "                        pickle.dump(meta_payload, f_meta)\n",
        "\n",
        "                    # ---------- Optuna ----------\n",
        "                    msg_opt = \"\"\n",
        "                    if motor == \"Optuna\" and \"study\" in locals():\n",
        "                        with open(study_fname, \"wb\") as f_st:\n",
        "                            pickle.dump(study, f_st)\n",
        "                        msg_opt = f\"  Estudio guardado  {study_fname}\"\n",
        "\n",
        "                    # ---------- feedback ----------\n",
        "                    try:                                        # usa traza si existe\n",
        "                        with traza:\n",
        "                            print(f\" Modelo guardado  {model_fname}\")\n",
        "                            print(f\"  Metadatos      {meta_fname}{msg_opt}\")\n",
        "                    except NameError:\n",
        "                        print(f\" Modelo guardado  {model_fname}\")\n",
        "                        print(f\"  Metadatos      {meta_fname}{msg_opt}\")\n",
        "\n",
        "                    # ---------- registro global opcional ----------\n",
        "                    OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "                    OPT_MODELS[(\"nn\", metodo.lower(), motor.lower())] = {\n",
        "                        # 1 rutas de fichero obligatorias\n",
        "                        \"model_path\":  str(model_fname),\n",
        "                        \"meta_path\":   str(meta_fname),\n",
        "                        # guardamos el propio objeto (o su path si prefieres)\n",
        "                        \"model\":       model_to_save,\n",
        "                        # escaladores\n",
        "                        \"sx\":          sx,\n",
        "                        \"sy\":          sy,\n",
        "                        # columnas utilizadas\n",
        "                        \"cols\":        X_train_sel.columns.tolist(),\n",
        "                        # mtrica y score\n",
        "                        \"metric\":      func_objetivo.value,\n",
        "                        \"score\":       float(score_val),\n",
        "                        # metadatos de optimizacin\n",
        "                        \"motor\":       motor,\n",
        "                        \"metodo\":      metodo,\n",
        "                        # hiperparmetros, duplicados para compat\n",
        "                        \"layers\":      layers_,\n",
        "                        \"n_layers\":    layers_,\n",
        "                        \"neurons\":     neurons_,\n",
        "                        \"n_units_l0\":  neurons_,\n",
        "                        \"dropout\":     dropout_,\n",
        "                        \"dropout_l0\":  dropout_,\n",
        "                        \"epochs\":      epochs_,\n",
        "                        # cualquier otro parmetro de best_hps_dict\n",
        "                        **{k: v for k, v in best_hps_dict.items()\n",
        "                            if k not in {\"layers\",\"neurons\",\"dropout\",\"epochs\"}}\n",
        "                    }\n",
        "\n",
        "                    if motor == \"Optuna\" and \"study\" in locals():\n",
        "                        OPT_MODELS[(\"nn\", metodo.lower(), \"optuna_study\")] = study\n",
        "\n",
        "                except Exception as e:\n",
        "                    # si algo falla, avisa pero NO interrumpe la optimizacin\n",
        "                    try:\n",
        "                        with traza:\n",
        "                            print(f\"  No se pudo guardar el modelo o estudio: {e}\")\n",
        "                    except NameError:\n",
        "                        print(f\"  No se pudo guardar el modelo o estudio: {e}\")\n",
        "                # \n",
        "                #   FIN BLOQUE DE PERSISTENCIA\n",
        "                # \n",
        "\n",
        "                resultados_modelos.append({\n",
        "                    'Motor': motor, 'Mtodo': metodo,\n",
        "                    'R2': r2, 'MAE': mae, 'MSE': mse,\n",
        "                    'pocas': rango_epocas.value,\n",
        "                    'Capas': rango_capas.value,\n",
        "                    'Neuronas': rango_neuronas.value,\n",
        "                    'Dropout': dropout_rate.value,\n",
        "                    'L2': l2_reg.value\n",
        "                })\n",
        "\n",
        "            # tras bucle motores, chequeo global de tiempo  # MODIFICADO\n",
        "            if time.time() - start_time > max_total_time:\n",
        "                print(\" Tiempo total agotado; salgo de mtodos.\")\n",
        "                break  # rompe bucle mtodos\n",
        "\n",
        "        global df_results\n",
        "        df_results = pd.DataFrame(resultados_modelos)\n",
        "        df_top5 = df_results.sort_values(by=func_objetivo.value, ascending=(func_objetivo.value != 'R2')).head(5)\n",
        "        best = df_top5.iloc[0]\n",
        "\n",
        "        display(HTML(\"<h4> Top 5 Modelos Optim.</h4>\"))\n",
        "        display(df_top5.style.set_caption(\"Modelos ptimos\").format(precision=4))\n",
        "\n",
        "        y_pred = model.predict(X_test_sel).ravel()\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        ax.scatter(y_test_sel, y_pred, alpha=0.6)\n",
        "        ax.plot([y_test_sel.min(), y_test_sel.max()], [y_test_sel.min(), y_test_sel.max()], 'r--')\n",
        "        ax.set_title(\"Y Real vs. Y Predicho\")\n",
        "        ax.set_xlabel(\"Y Real\")\n",
        "        ax.set_ylabel(\"Y Predicho\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        #residuos = y_test_sel - y_pred\n",
        "        residuos = y_test_sel.values.ravel() - y_pred\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.scatter(y_pred, residuos, alpha=0.6)\n",
        "        plt.axhline(0, color='red', linestyle='--')\n",
        "        plt.title(\"Residuos vs. Prediccin\")\n",
        "        plt.xlabel(\"Y Predicho\")\n",
        "        plt.ylabel(\"Residuos\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        stat, p_value = shapiro(residuos)\n",
        "        display(HTML(f\"<b> Test de Shapiro:</b> p = {p_value:.5f}  {'Normal' if p_value > 0.05 else 'No normal'}\"))\n",
        "\n",
        "        from math import pi\n",
        "        categorias = ['R2', 'MAE', 'MSE']\n",
        "        valores = [best['R2'], best['MAE'], best['MSE']]\n",
        "        valores += valores[:1]\n",
        "        angles = [n / float(len(categorias)) * 2 * pi for n in range(len(categorias))]\n",
        "        angles += angles[:1]\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        ax = plt.subplot(111, polar=True)\n",
        "        plt.xticks(angles[:-1], categorias)\n",
        "        ax.plot(angles, valores, linewidth=2)\n",
        "        ax.fill(angles, valores, alpha=0.3)\n",
        "        plt.title(\"Radar de Mtricas\")\n",
        "        plt.show()\n",
        "\n",
        "        global NN_RESULTADOS_TOP5, NN_MEJOR_MODELO, NN_RESIDUOS, NN_METODO_MEJOR, NN_MOTOR_MEJOR\n",
        "        NN_RESULTADOS_TOP5 = df_top5\n",
        "        NN_MEJOR_MODELO = best\n",
        "        NN_RESIDUOS = residuos\n",
        "        NN_METODO_MEJOR = best['Mtodo']\n",
        "        NN_MOTOR_MEJOR = best['Motor']\n",
        "\n",
        "# ***********************************************************************\n",
        "# Visualizacin de los mejores modelos optimizados\n",
        "# ***********************************************************************\n",
        "        for idx, row in df_top5.iterrows():\n",
        "            metodo, motor = row['Mtodo'], row['Motor']\n",
        "            model = keras.Sequential()\n",
        "            model.add(layers.Input(shape=(X_train_sel.shape[1],)))\n",
        "            for _ in range(int(row['Capas'])):\n",
        "                model.add(layers.Dense(int(row['Neuronas']), activation='relu'))\n",
        "                model.add(layers.Dropout(row['Dropout']))\n",
        "            model.add(layers.Dense(1))\n",
        "            model.compile(optimizer='adam', loss='mse')\n",
        "            model.fit(X_train_sel, y_train_sel, epochs=rango_epocas.value, batch_size=32, verbose=0, callbacks=callbacks)\n",
        "\n",
        "            y_pred = model.predict(X_test_sel).ravel()\n",
        "            fig, ax = plt.subplots(figsize=(6, 4))\n",
        "            ax.scatter(range(len(y_pred)), y_test_sel.values.ravel(), label='Y Real')\n",
        "            ax.plot(range(len(y_pred)), y_pred, color='orange', label='Y Predicho')\n",
        "            ax.set_title(f\"XY-Y Real vs. Predicho: {metodo}-{motor}\")\n",
        "            ax.set_xlabel(\"Casos\")\n",
        "            ax.set_ylabel(\"Y\")\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        print(f\" Optimizacin completada en {time.time() - start_time:.2f} segundos.\")\n",
        "\n",
        "# ========== Visualizacin y conexin ==========\n",
        "control_panel = VBox([\n",
        "    HTML(\"<h3> Configuracin Optimizacin NN</h3>\"),\n",
        "    ayuda_parametros,\n",
        "    HBox([select_metodos, select_motores]),\n",
        "    func_objetivo,\n",
        "    n_trials_slider,\n",
        "    HBox([rango_epocas, rango_capas]),\n",
        "    HBox([rango_neuronas, dropout_rate, l2_reg]),\n",
        "    btn_ejecutar,\n",
        "    progreso_bar,\n",
        "    out_nn\n",
        "])\n",
        "\n",
        "def mostrar_optimizacion_nn():\n",
        "    display(control_panel)\n",
        "\n",
        "try:\n",
        "    btn_ejecutar._click_handlers.callbacks.clear()\n",
        "except:\n",
        "    pass\n",
        "btn_ejecutar.on_click(ejecutar_optimizacion)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================\n",
        "# Cell 9.3: OPTIMIZACIN XGBOOST - FUNCIONA CORRECTAMENTE\n",
        "# Optimizacin mltiple de hiperparmetros del modelo XGBoost\n",
        "# Compatible con motores: RandomSearch, Bayesian, Hyperband y Optuna\n",
        "# Variables: seleccionadas desde Cell 6 (Pearson, Spearman, MutualInfo, Boruta, UMAP, Todos)\n",
        "# Datos: segmentados desde Cell 5.2 (X_train, Y_train, X_test, Y_test)\n",
        "# ===============================================\n",
        "def mostrar_optimizacion_xgb():\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import pandas as pd\n",
        "    import ipywidgets as widgets\n",
        "    import traceback\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    from xgboost import XGBRegressor\n",
        "    import numpy as np\n",
        "    from skopt import BayesSearchCV  # Motor de optimizacin bayesiano\n",
        "    # Importar Hyperband\n",
        "    from sklearn.experimental import enable_halving_search_cv  # noqa\n",
        "    from sklearn.model_selection import HalvingRandomSearchCV\n",
        "    # Importar Optuna\n",
        "    from optuna.integration import OptunaSearchCV\n",
        "    import optuna\n",
        "    optuna.logging.set_verbosity(optuna.logging.INFO)\n",
        "    from optuna.distributions import IntDistribution, FloatDistribution\n",
        "\n",
        "    # Mostrar progreso en RandomSearch mediante logging\n",
        "    import logging\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    global out_opt_xgb, traza_xgb\n",
        "    out_opt_xgb   = widgets.Output()      # panel principal\n",
        "    traza_xgb     = widgets.Output()      # trazas de cada motor\n",
        "\n",
        "    # ========= AADIDOS: Funciones de saneamiento =========\n",
        "\n",
        "    # 2) Ahora s puedo sanear columnas\n",
        "    # Usar sanitize_name para limpiar columnas en el payload\n",
        "    def clean_cols(col_list):\n",
        "        return [sanitize_name(c) for c in col_list]\n",
        "    # Ejemplo de sanitizacin de X_train antes de fit\n",
        "    X_train.columns = [sanitize_name(col) for col in X_train.columns]\n",
        "    X_test.columns = [sanitize_name(col) for col in X_test.columns]\n",
        "\n",
        "    #  SANITIZAR LISTAS DE RESUMEN_METODOS \n",
        "    for metodo, vars_ in RESUMEN_METODOS.items():\n",
        "        if isinstance(vars_, list):\n",
        "            RESUMEN_METODOS[metodo] = [sanitize_name(v) for v in vars_]\n",
        "        elif isinstance(vars_, pd.DataFrame) and not vars_.empty:\n",
        "            # asumimos que la columna de variable se llama \"Variable\" o la primera\n",
        "            col = 'Variable' if 'Variable' in vars_.columns else vars_.columns[0]\n",
        "            # saneamos esa columna inplace\n",
        "            RESUMEN_METODOS[metodo][col] = vars_[col].astype(str).map(sanitize_name)\n",
        "    #  FIN SANITIZACIN RESUMEN_METODOS \n",
        "\n",
        "    #import re\n",
        "\n",
        "    #def clean_columns(df):\n",
        "    #    \"\"\"\n",
        "    #    Transforma todos los nombres de columna a str y sustituye\n",
        "    #    corchetes, %, <, > y espacios por guiones bajos.\n",
        "    #    \"\"\"\n",
        "    #    df = df.copy()\n",
        "    #    df.columns = (\n",
        "    #        df.columns\n",
        "    #          .astype(str)\n",
        "    #          .str.replace(r'[\\[\\]<>%]', '_', regex=True)\n",
        "    #          .str.replace(r'\\s+', '_', regex=True)\n",
        "    #          .str.strip('_')\n",
        "    #    )\n",
        "    #    return df\n",
        "\n",
        "    #def clean_cols(var_list):\n",
        "    #    \"\"\"\n",
        "    #    Limpia una lista de nombres de columna con las mismas reglas.\n",
        "    #    \"\"\"\n",
        "    #    return [\n",
        "    #        re.sub(r'[\\[\\]<>%]', '_', str(v))\n",
        "    #          .replace(' ', '_')\n",
        "    #          .strip('_')\n",
        "    #        for v in var_list\n",
        "    #    ]\n",
        "\n",
        "    # ======================================================\n",
        "\n",
        "    #  Parmetros configurables del motor de optimizacin\n",
        "    slider_n_iter = widgets.IntSlider(value=50, min=10, max=300, step=10,\n",
        "                                      description='n_iter:', layout=widgets.Layout(width='45%'))\n",
        "    ayuda_n_iter = widgets.HTML(\"<small><b>n_iter:</b> nmero de combinaciones aleatorias a probar (mayor = ms preciso, pero ms lento). No aplica a  Hyperbrand.</small>\")\n",
        "\n",
        "    slider_cv = widgets.IntSlider(value=3, min=2, max=10, step=1,\n",
        "                                  description='cv:', layout=widgets.Layout(width='45%'))\n",
        "    ayuda_cv = widgets.HTML(\"<small><b>cv:</b> nmero de particiones para validacin cruzada (mnimo 2)</small>\")\n",
        "\n",
        "    selector_funcion_objetivo = widgets.Dropdown(\n",
        "        options=['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
        "        value='r2',\n",
        "        description='Funcin:',\n",
        "        layout=widgets.Layout(width='45%')\n",
        "    )\n",
        "    ayuda_funcion = widgets.HTML(\"<small><b>Funcin:</b> mtrica a optimizar. R2 para ajuste, MSE o MAE para error</small>\")\n",
        "\n",
        "    def seleccionar_variables_filtradas(metodo):\n",
        "        global X_train, Y_train, X_test, RESUMEN_METODOS\n",
        "\n",
        "        # --- AADIDO: sanear columnas globales antes de todo ---\n",
        "        #X_train = clean_columns(X_train)\n",
        "        #X_test  = clean_columns(X_test)\n",
        "        # --- FIN AADIDO -----------------------------------------\n",
        "\n",
        "        print(f\"\\n [seleccionar_variables_filtradas] Iniciando con mtodo: '{metodo}'\")\n",
        "        try:\n",
        "            assert 'X_train' in globals(), \" 'X_train' no est definido\"\n",
        "            assert 'Y_train' in globals(), \" 'Y_train' no est definido\"\n",
        "            assert 'RESUMEN_METODOS' in globals(), \" 'RESUMEN_METODOS' no est definido\"\n",
        "\n",
        "            vars_sel = []\n",
        "            if metodo.strip().lower() == \"todos\":\n",
        "                all_vars = []\n",
        "                for k, df in RESUMEN_METODOS.items():\n",
        "                    if isinstance(df, list):\n",
        "                        all_vars.extend(df)\n",
        "                    elif isinstance(df, pd.DataFrame) and not df.empty:\n",
        "                        col = 'Variable' if 'Variable' in df.columns else df.columns[0]\n",
        "                        all_vars.extend(df[col].dropna().tolist())\n",
        "                vars_sel = list(set(all_vars))\n",
        "            elif metodo in RESUMEN_METODOS:\n",
        "                df_vars = RESUMEN_METODOS[metodo]\n",
        "                if isinstance(df_vars, list):\n",
        "                    vars_sel = df_vars\n",
        "                elif isinstance(df_vars, pd.DataFrame):\n",
        "                    if not df_vars.empty:\n",
        "                        col = 'Variable' if 'Variable' in df_vars.columns else df_vars.columns[0]\n",
        "                        vars_sel = df_vars[col].dropna().tolist()\n",
        "                    else:\n",
        "                        return\n",
        "                else:\n",
        "                    return\n",
        "            else:\n",
        "                return\n",
        "            if not vars_sel:\n",
        "                return\n",
        "\n",
        "            columnas_faltantes = [col for col in vars_sel if col not in X_train.columns]\n",
        "            if columnas_faltantes:\n",
        "                print(f\" Columnas no existentes en X_train: {columnas_faltantes}\")\n",
        "                return\n",
        "\n",
        "\n",
        "        #    #  AADIDO: obtener y limpiar lista raw_vars \n",
        "        #    raw_vars = []\n",
        "        #    if metodo.strip().lower() == \"todos\":\n",
        "        #        all_vars = []\n",
        "        #        for df in RESUMEN_METODOS.values():\n",
        "        #            if isinstance(df, list):\n",
        "        #                all_vars += df\n",
        "        #            elif isinstance(df, pd.DataFrame) and not df.empty:\n",
        "        #                col = 'Variable' if 'Variable' in df.columns else df.columns[0]\n",
        "        #                all_vars += df[col].dropna().tolist()\n",
        "        #        raw_vars = list(set(all_vars))\n",
        "        #    elif metodo in RESUMEN_METODOS:\n",
        "        #        df_vars = RESUMEN_METODOS[metodo]\n",
        "        #        if isinstance(df_vars, list):\n",
        "        #            raw_vars = df_vars\n",
        "        #        elif isinstance(df_vars, pd.DataFrame) and not df_vars.empty:\n",
        "        #            col = 'Variable' if 'Variable' in df_vars.columns else df_vars.columns[0]\n",
        "        #            raw_vars = df_vars[col].dropna().tolist()\n",
        "        #    else:\n",
        "        #        return\n",
        "\n",
        "        #    if not raw_vars:\n",
        "        #        with traza_xgb:\n",
        "        #            print(f\" No hay variables para '{metodo}'.\")\n",
        "        #        return\n",
        "\n",
        "            # limpiar lista de nombres\n",
        "        #    vars_sel = clean_cols(raw_vars)\n",
        "\n",
        "        #    # comprobar que existen tras limpiar\n",
        "        #    faltantes = [c for c in vars_sel if c not in X_train.columns]\n",
        "        #    if faltantes:\n",
        "        #        with traza_xgb:\n",
        "        #            print(f\" Columnas no existentes en X_train: {faltantes}\")\n",
        "        #        return\n",
        "        #    #  FIN AADIDO \n",
        "\n",
        "            X_sel = X_train[vars_sel].copy()\n",
        "            Y_sel = Y_train.copy()\n",
        "            globals()['X_train_filtrado'] = X_sel\n",
        "            globals()['Y_train_filtrado'] = Y_sel\n",
        "            globals()['metodo_usado_xgb'] = metodo\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\" Excepcin atrapada desde consola principal:\")\n",
        "            print(traceback.format_exc())\n",
        "\n",
        "    def ejecutar_metricas_finales(modelo, nombre_motor=\"Desconocido\"):\n",
        "        try:\n",
        "            X_test_filtrado = X_test[X_train_filtrado.columns]\n",
        "            preds = best_model.predict(X_test_filtrado)\n",
        "\n",
        "            r2 = r2_score(Y_test, preds)\n",
        "            mse = mean_squared_error(Y_test, preds)\n",
        "            mae = mean_absolute_error(Y_test, preds)\n",
        "\n",
        "            df_metricas = pd.DataFrame({\n",
        "                'Mtrica': ['R2', 'MSE', 'MAE'],\n",
        "                'Valor': [r2, mse, mae]\n",
        "            })\n",
        "            display(df_metricas.style.set_caption(\" Rendimiento del Modelo ptimo\").format(precision=4))\n",
        "\n",
        "            # Clculo de residuos y anlisis\n",
        "            residuos = Y_test.values.ravel() - preds.ravel()\n",
        "            df_residuos = pd.DataFrame({\n",
        "                'ndice': range(len(residuos)),\n",
        "                'Y_real': Y_test.values.ravel(),\n",
        "                'Y_predicho': preds.ravel(),\n",
        "                'Residuo': residuos\n",
        "            })\n",
        "            display(df_residuos.head().style.set_caption(\" Ejemplo de Clculo de Residuos\"))\n",
        "\n",
        "            # Histograma de residuos\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            sns.histplot(residuos, bins=30, kde=True, color='skyblue')\n",
        "            plt.axvline(0, color='red', linestyle='--')\n",
        "            plt.title(\"Histograma de Residuos\")\n",
        "            plt.xlabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "            plt.ylabel(\"Frecuencia\")\n",
        "            plt.show()\n",
        "\n",
        "            #  Prueba de normalidad de Shapiro-Wilk\n",
        "            stat, p_value = shapiro(residuos)\n",
        "            display(HTML(f\"<h4> Prueba de Normalidad (Shapiro-Wilk)</h4><ul><li>Estadstico: {stat:.4f}</li><li>p-valor: {p_value:.4f}</li><li>{' Los residuos siguen una distribucin normal (p > 0.05)' if p_value > 0.05 else ' Los residuos no siguen una distribucin normal (p  0.05)'}</li></ul>\"))\n",
        "\n",
        "            # Explicacin de los resultados\n",
        "            display(HTML(\"\"\"\n",
        "                <h4> Explicacin de Resultados:</h4>\n",
        "                <ul>\n",
        "                    <li><b>R2:</b> mide el grado de ajuste del modelo. Valores cercanos a 1 indican buen ajuste.</li>\n",
        "                    <li><b>MSE:</b> error cuadrtico medio. Penaliza ms los errores grandes.</li>\n",
        "                    <li><b>MAE:</b> error absoluto medio. Ms robusto ante valores atpicos.</li>\n",
        "                    <li><b>Residuos:</b> diferencia entre el valor real y el predicho. Deben estar centrados en 0.</li>\n",
        "                    <li><b>Histograma:</b> ayuda a evaluar si los residuos siguen una distribucin normal.</li>\n",
        "                    <li><b>Shapiro-Wilk:</b> prueba estadstica que indica si los residuos son normales. Se acepta normalidad si p > 0.05.</li>\n",
        "                </ul>\n",
        "            \"\"\"))\n",
        "\n",
        "            # Grficas modelo optimo\n",
        "            plt.figure(figsize=(10, 4))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(Y_test.values, label='Real')\n",
        "            plt.plot(preds, label='Predicho')\n",
        "            plt.legend()\n",
        "            plt.title(\"X-Y Real vs X-Y Predicho\")\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.scatter(Y_test, preds, alpha=0.6)\n",
        "            min_val = min(Y_test.values.min(), preds.min())\n",
        "            max_val = max(Y_test.values.max(), preds.max())\n",
        "            plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal')\n",
        "            plt.xlabel(\"Y real\")\n",
        "            plt.ylabel(\"Y predicho\")\n",
        "            plt.title(\"Y Real vs Y Predicho\")\n",
        "            plt.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\" Error en mtricas finales:\", traceback.format_exc())\n",
        "    # \n",
        "    #  FUNCIN DE PERSISTENCIA  (XGBoost)\n",
        "    # \n",
        "    import pickle, pathlib, datetime\n",
        "    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "    def guardar_xgb(best_model, best_params, nombre_motor,\n",
        "                    metodo_usado_xgb, selector_funcion_objetivo,\n",
        "                    X_train_filtrado, X_test, Y_train, Y_test,\n",
        "                    study=None, traza_out=None):\n",
        "        score_val = None\n",
        "        try:\n",
        "            # 1) carpeta\n",
        "            pathlib.Path(\"modelos_opt\").mkdir(exist_ok=True)\n",
        "\n",
        "            # 3) nombre robusto de variable-objetivo\n",
        "            y_name = getattr(Y_train, \"name\", None) or \\\n",
        "                    (Y_train.columns[0] if hasattr(Y_train, \"columns\") else \"Y\")\n",
        "\n",
        "            # 4) rutas\n",
        "            ts      = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            tag     = f\"xgb_{metodo_usado_xgb.lower()}_{nombre_motor.lower()}_opt_{ts}\"\n",
        "            model_f = f\"modelos_opt/{tag}.pkl\"\n",
        "            meta_f  = f\"modelos_opt/{tag}_meta.pkl\"\n",
        "            study_f = f\"modelos_opt/{tag}_study.pkl\"\n",
        "\n",
        "            # 5) modelo\n",
        "            with open(model_f, \"wb\") as f:\n",
        "                pickle.dump(best_model, f)\n",
        "\n",
        "            # 6) metadatos\n",
        "            meta = dict(\n",
        "                score       = float(score_val),\n",
        "                func_obj    = selector_funcion_objetivo.value,\n",
        "                motor       = nombre_motor,\n",
        "                metodo_x    = metodo_usado_xgb,\n",
        "                cols        = list(X_train_filtrado.columns),\n",
        "                yname       = y_name,\n",
        "                best_params = best_params,\n",
        "                fecha       = ts,\n",
        "            )\n",
        "            with open(meta_f, \"wb\") as f:\n",
        "                pickle.dump(meta, f)\n",
        "\n",
        "            # 7) estudio Optuna (si procede)\n",
        "            if nombre_motor.lower() == \"optuna\" and study is not None:\n",
        "                with open(study_f, \"wb\") as f:\n",
        "                    pickle.dump(study, f)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Si score_val an no se ha calculado, no emitir warning\n",
        "            if score_val is None:\n",
        "                return\n",
        "            if traza_out is not None:\n",
        "                with traza_out:\n",
        "                    print(f\"  No se pudo guardar modelo/estudio: {e}\")\n",
        "            else:\n",
        "                print(f\"  No se pudo guardar modelo/estudio: {e}\")\n",
        "\n",
        "    # ===================================================\n",
        "    # Motor de Optimizacin RandomSearch CV\n",
        "    # ===================================================\n",
        "    def optimizar_randomsearch():\n",
        "        try:\n",
        "            assert 'X_train_filtrado' in globals()\n",
        "            assert 'Y_train_filtrado' in globals()\n",
        "\n",
        "            print(\" Iniciando optimizacin con RandomSearch...\")\n",
        "            funcion_objetivo = selector_funcion_objetivo.value\n",
        "            n_iter_val = slider_n_iter.value\n",
        "            cv_val = slider_cv.value\n",
        "            print(f\" Funcin de optimizacin seleccionada: {funcion_objetivo} (n_iter={n_iter_val}, cv={cv_val})\")\n",
        "\n",
        "            param_dist = {\n",
        "                'n_estimators': list(range(50, 300)),\n",
        "                'max_depth': list(range(3, 15)),\n",
        "                'learning_rate': np.linspace(0.01, 0.3, 30),\n",
        "                'subsample': np.linspace(0.5, 1.0, 20),\n",
        "                'colsample_bytree': np.linspace(0.5, 1.0, 20),\n",
        "                'gamma': np.linspace(0, 5, 20)\n",
        "            }\n",
        "\n",
        "            print(f\" Hiperparmetros a optimizar: {list(param_dist.keys())}\")\n",
        "            print(f\" Nmero de iteraciones: {n_iter_val}, Validacin cruzada (cv): {cv_val}\")\n",
        "\n",
        "            model = XGBRegressor(random_state=42, verbosity=0)\n",
        "            search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
        "                                        n_iter=n_iter_val, scoring=funcion_objetivo, cv=cv_val, random_state=42,\n",
        "                                        n_jobs=-1, verbose=3)\n",
        "\n",
        "            #  AADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TRAIN \n",
        "            # 1) Forzar que todos los nombres sean str\n",
        "            #X_train_filtrado.columns = X_train_filtrado.columns.astype(str)\n",
        "            ## 2) Reemplazar corchetes y '<', '>' por '_'\n",
        "            #X_train_filtrado.columns = (\n",
        "            #    X_train_filtrado\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            #  FIN AADIDO \n",
        "            # #  AADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TEST \n",
        "            #X_test.columns = X_test.columns.astype(str)\n",
        "            #X_test.columns = (\n",
        "            #    X_test\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            #  FIN AADIDOS \n",
        "\n",
        "            search.fit(X_train_filtrado, Y_train_filtrado)\n",
        "\n",
        "            global best_model, best_params\n",
        "            best_model = search.best_estimator_\n",
        "            best_params = search.best_params_\n",
        "            tabla_resultados = pd.DataFrame(best_params.items(), columns=['Hiperparmetro', 'Valor ptimo'])\n",
        "            display(tabla_resultados.style.set_caption(\" Tabla de Hiperparmetros ptimos\").format(precision=4))\n",
        "\n",
        "            ejecutar_metricas_finales(best_model, nombre_motor=\"RandomSearch\")\n",
        "\n",
        "            #  Tras best_model, best_params en optimizar_randomsearch() \n",
        "            # Calcular el score real con los datos de test\n",
        "            preds = best_model.predict(X_test[X_train_filtrado.columns])\n",
        "            if selector_funcion_objetivo.value == \"r2\":\n",
        "                score_val = r2_score(Y_test, preds)\n",
        "            elif selector_funcion_objetivo.value == \"neg_mean_absolute_error\":\n",
        "                score_val = mean_absolute_error(Y_test, preds)\n",
        "            else:\n",
        "                score_val = mean_squared_error(Y_test, preds)\n",
        "\n",
        "            # Guardar en OPT_MODELS\n",
        "            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "            OPT_MODELS[(\"xgb\", metodo_usado_xgb.lower(), \"randomsearch\")] = {\n",
        "                \"model\":       best_model,\n",
        "                \"score\":       float(score_val),\n",
        "                \"metric\":      selector_funcion_objetivo.value,\n",
        "                \"param_dist\":  param_dist,           # <- espacio RandomSearch\n",
        "                \"best_params\": best_params,\n",
        "                \"cols\":        list(X_train_filtrado.columns)\n",
        "            }\n",
        "            # \n",
        "\n",
        "            # --- llamada de persistencia ----\n",
        "            guardar_xgb(best_model, best_params, \"RandomSearch\",\n",
        "                        metodo_usado_xgb, selector_funcion_objetivo,\n",
        "                        X_train_filtrado, X_test, Y_train, Y_test,\n",
        "                        study=None, traza_out=traza_xgb)\n",
        "            print(\" Optimizacin con RandomSearch completada.\")\n",
        "            #  3) Imprime la confirmacin aqu dentro \n",
        "            if traza_xgb is not None:\n",
        "                with traza_xgb:\n",
        "                    print(f\" Modelo guardado      modelos_opt/xgb_{metodo_usado_xgb.lower()}_randomsearch_opt_*.pkl\")\n",
        "                    print(f\" Metadatos guardados  modelos_opt/xgb_{metodo_usado_xgb.lower()}_randomsearch_opt_*_meta.pkl\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\" Error en optimizacin RandomSearch:\", traceback.format_exc())\n",
        "\n",
        "    # ===================================================\n",
        "    # Motor de Optimizacin Bayesian\n",
        "    # ===================================================\n",
        "    def optimizar_bayesian():\n",
        "        try:\n",
        "            assert 'X_train_filtrado' in globals()\n",
        "            assert 'Y_train_filtrado' in globals()\n",
        "\n",
        "            print(\" Iniciando optimizacin con Bayesian Optimization...\")\n",
        "            funcion_objetivo = selector_funcion_objetivo.value\n",
        "            n_iter_val = slider_n_iter.value\n",
        "            cv_val = slider_cv.value\n",
        "\n",
        "            param_spaces = {\n",
        "                'n_estimators': (50, 300),\n",
        "                'max_depth': (3, 15),\n",
        "                'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
        "                'subsample': (0.5, 1.0),\n",
        "                'colsample_bytree': (0.5, 1.0),\n",
        "                'gamma': (0.0, 5.0)\n",
        "            }\n",
        "\n",
        "            print(f\" Hiperparmetros a optimizar: {list(param_spaces.keys())}\")\n",
        "            print(f\" Nmero de iteraciones: {n_iter_val}, Validacin cruzada (cv): {cv_val}\")\n",
        "\n",
        "            model = XGBRegressor(random_state=42, verbosity=0)\n",
        "\n",
        "            opt = BayesSearchCV(model, search_spaces=param_spaces,\n",
        "                n_iter=n_iter_val, scoring=funcion_objetivo, cv=cv_val,\n",
        "                n_jobs=-1, verbose=3, random_state=42)\n",
        "            opt.fit(X_train_filtrado, Y_train_filtrado)\n",
        "\n",
        "            global best_model, best_params\n",
        "            if opt.best_estimator_ is not None:\n",
        "                best_model = opt.best_estimator_\n",
        "                best_params = opt.best_params_\n",
        "\n",
        "            tabla_resultados = pd.DataFrame(best_params.items(), columns=['Hiperparmetro', 'Valor ptimo'])\n",
        "            display(tabla_resultados.style.set_caption(\" Tabla de Hiperparmetros ptimos (Bayesian)\").format(precision=4))\n",
        "\n",
        "            ejecutar_metricas_finales(best_model, nombre_motor=\"Bayesian\")\n",
        "\n",
        "            #  Tras best_model, best_params en optimizar_bayesian() \n",
        "            preds = best_model.predict(X_test[X_train_filtrado.columns])\n",
        "            if selector_funcion_objetivo.value == \"r2\":\n",
        "                score_val = r2_score(Y_test, preds)\n",
        "            elif selector_funcion_objetivo.value == \"neg_mean_absolute_error\":\n",
        "                score_val = mean_absolute_error(Y_test, preds)\n",
        "            else:\n",
        "                score_val = mean_squared_error(Y_test, preds)\n",
        "\n",
        "            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "            OPT_MODELS[(\"xgb\", metodo_usado_xgb.lower(), \"bayesian\")] = {\n",
        "                \"model\":       best_model,\n",
        "                \"score\":       float(score_val),\n",
        "                \"metric\":      selector_funcion_objetivo.value,\n",
        "                \"param_spaces\": param_spaces,      # <- espacio BayesSearchCV\n",
        "                \"best_params\": best_params,\n",
        "                \"cols\":        list(X_train_filtrado.columns)\n",
        "            }\n",
        "\n",
        "            # --- llamada de persistencia ----\n",
        "            guardar_xgb(best_model, best_params, \"Bayesian\",\n",
        "                        metodo_usado_xgb, selector_funcion_objetivo,\n",
        "                        X_train_filtrado, X_test, Y_train, Y_test,\n",
        "                        study=None, traza_out=traza_xgb)\n",
        "            print(\" Optimizacin con Bayesian Optimization completada.\")\n",
        "            with traza_xgb:\n",
        "                print(f\" Modelo guardado      modelos_opt/xgb_{metodo_usado_xgb.lower()}_bayesian_opt_*.pkl\")\n",
        "                print(f\" Metadatos guardados  modelos_opt/xgb_{metodo_usado_xgb.lower()}_bayesian_opt_*_meta.pkl\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\" Error en optimizacin Bayesian:\", traceback.format_exc())\n",
        "\n",
        "    # ===================================================\n",
        "    # Motor de Optimizacin HyperBrand\n",
        "    # ===================================================\n",
        "    def optimizar_hyperband():\n",
        "        try:\n",
        "            assert 'X_train_filtrado' in globals()\n",
        "            assert 'Y_train_filtrado' in globals()\n",
        "\n",
        "            print(\" Iniciando optimizacin con Hyperband...\")\n",
        "            funcion_objetivo = selector_funcion_objetivo.value\n",
        "            cv_val = slider_cv.value\n",
        "\n",
        "            #  Corregido: se elimina 'n_estimators' de los hiperparmetros buscados\n",
        "            param_dist = {\n",
        "                'max_depth': list(range(3, 15)),\n",
        "                'learning_rate': np.linspace(0.01, 0.3, 30),\n",
        "                'subsample': np.linspace(0.5, 1.0, 20),\n",
        "                'colsample_bytree': np.linspace(0.5, 1.0, 20),\n",
        "                'gamma': np.linspace(0, 5, 20)\n",
        "            }\n",
        "\n",
        "            model = XGBRegressor(random_state=42, verbosity=0)\n",
        "            search = HalvingRandomSearchCV(model, param_dist,\n",
        "                                          scoring=funcion_objetivo, cv=cv_val,\n",
        "                                          factor=3, resource='n_estimators',\n",
        "                                          max_resources=300, random_state=42,\n",
        "                                          verbose=2, n_jobs=-1)\n",
        "\n",
        "            #  AADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TRAIN \n",
        "            # 1) Forzar que todos los nombres sean str\n",
        "            #X_train_filtrado.columns = X_train_filtrado.columns.astype(str)\n",
        "            # 2) Reemplazar corchetes y '<', '>' por '_'\n",
        "            #X_train_filtrado.columns = (\n",
        "            #    X_train_filtrado\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            #  FIN AADIDO \n",
        "            # #  AADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TEST \n",
        "            #X_test.columns = X_test.columns.astype(str)\n",
        "            #X_test.columns = (\n",
        "            #    X_test\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            #  FIN AADIDOS \n",
        "\n",
        "            search.fit(X_train_filtrado, Y_train_filtrado)\n",
        "\n",
        "            global best_model, best_params\n",
        "            best_model = search.best_estimator_\n",
        "            best_params = search.best_params_\n",
        "\n",
        "            tabla_resultados = pd.DataFrame(best_params.items(), columns=['Hiperparmetro', 'Valor ptimo'])\n",
        "            display(tabla_resultados.style.set_caption(\" Tabla de Hiperparmetros ptimos (Hyperband)\").format(precision=4))\n",
        "\n",
        "            ejecutar_metricas_finales(best_model, nombre_motor=\"Hyperband\")\n",
        "\n",
        "            #  Tras best_model, best_params en optimizar_hyperband() \n",
        "            preds = best_model.predict(X_test[X_train_filtrado.columns])\n",
        "            if selector_funcion_objetivo.value == \"r2\":\n",
        "                score_val = r2_score(Y_test, preds)\n",
        "            elif selector_funcion_objetivo.value == \"neg_mean_absolute_error\":\n",
        "                score_val = mean_absolute_error(Y_test, preds)\n",
        "            else:\n",
        "                score_val = mean_squared_error(Y_test, preds)\n",
        "\n",
        "            # Hyperband usaba param_dist tambin\n",
        "            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "            OPT_MODELS[(\"xgb\", metodo_usado_xgb.lower(), \"hyperband\")] = {\n",
        "                \"model\":       best_model,\n",
        "                \"score\":       float(score_val),\n",
        "                \"metric\":      selector_funcion_objetivo.value,\n",
        "                \"param_dist\":  param_dist,           # <- espacio Hyperband\n",
        "                \"best_params\": best_params,\n",
        "                \"cols\":        list(X_train_filtrado.columns)\n",
        "            }\n",
        "\n",
        "            # --- llamada de persistencia ----\n",
        "            guardar_xgb(best_model, best_params, \"Hyperband\",\n",
        "                        metodo_usado_xgb, selector_funcion_objetivo,\n",
        "                        X_train_filtrado, X_test, Y_train, Y_test,\n",
        "                        study=None, traza_out=traza_xgb)\n",
        "            print(\" Optimizacin con Hyperband completada.\")\n",
        "            with traza_xgb:\n",
        "               print(f\" Modelo guardado      modelos_opt/xgb_{metodo_usado_xgb.lower()}_hyperband_opt_*.pkl\")\n",
        "               print(f\" Metadatos guardados  modelos_opt/xgb_{metodo_usado_xgb.lower()}_hyperband_opt_*_meta.pkl\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\" Error en optimizacin Hyperband:\", traceback.format_exc())\n",
        "\n",
        "    # ===================================================\n",
        "    # Motor de Optimizacin Optuna\n",
        "    # ===================================================\n",
        "    def optimizar_optuna():\n",
        "        try:\n",
        "            print(\"\\n Iniciando optimizacin con Optuna...\")\n",
        "            assert 'X_train_filtrado' in globals()\n",
        "            assert 'Y_train_filtrado' in globals()\n",
        "\n",
        "            #  AADIDO: Definir escaladores para Optuna \n",
        "            from sklearn.preprocessing import StandardScaler\n",
        "            # Ajustamos escalador de X sobre el train filtrado\n",
        "            X_scaler = StandardScaler().fit(X_train_filtrado)\n",
        "            # Ajustamos escalador de Y (reshape para vector columna)\n",
        "            y_scaler = StandardScaler().fit(\n",
        "                Y_train_filtrado.values.reshape(-1, 1)\n",
        "            )\n",
        "            #  FIN AADIDO \n",
        "\n",
        "            param_dist = {\n",
        "                'n_estimators': IntDistribution(50, 300),\n",
        "                'max_depth': IntDistribution(3, 15),\n",
        "                'learning_rate': FloatDistribution(0.01, 0.3),\n",
        "                'subsample': FloatDistribution(0.5, 1.0),\n",
        "                'colsample_bytree': FloatDistribution(0.5, 1.0),\n",
        "                'gamma': FloatDistribution(0, 5)\n",
        "            }\n",
        "\n",
        "            model = XGBRegressor(random_state=42, verbosity=0)\n",
        "            search = OptunaSearchCV(\n",
        "                estimator=model,\n",
        "                param_distributions=param_dist,\n",
        "                scoring=selector_funcion_objetivo.value,\n",
        "                n_trials=slider_n_iter.value,\n",
        "                cv=slider_cv.value,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            #  AADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TRAIN \n",
        "            # 1) Forzar que todos los nombres sean str\n",
        "            #X_train_filtrado.columns = X_train_filtrado.columns.astype(str)\n",
        "            # 2) Reemplazar corchetes y '<', '>' por '_'\n",
        "            #X_train_filtrado.columns = (\n",
        "            #    X_train_filtrado\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            #  FIN AADIDO \n",
        "            # #  AADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TEST \n",
        "            #X_test.columns = X_test.columns.astype(str)\n",
        "            #X_test.columns = (\n",
        "            #    X_test\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            #  FIN AADIDOS \n",
        "\n",
        "            search.fit(X_train_filtrado, Y_train_filtrado)\n",
        "            global best_model\n",
        "            best_model = search.best_estimator_\n",
        "\n",
        "            print(\"\\n Hiperparmetros ptimos encontrados con Optuna:\")\n",
        "            global best_params\n",
        "            best_params = search.best_params_\n",
        "            display(pd.DataFrame([best_params]).T.rename(columns={0: 'Valor ptimo'}).style.set_caption(\" Hiperparmetros ptimos (Optuna)\").format(precision=4))\n",
        "\n",
        "            ejecutar_metricas_finales(best_model, nombre_motor=\"Optuna\")\n",
        "\n",
        "            #  Tras best_model, best_params en optimizar_optuna() \n",
        "            preds = best_model.predict(X_test[X_train_filtrado.columns])\n",
        "            if selector_funcion_objetivo.value == \"r2\":\n",
        "                score_val = r2_score(Y_test, preds)\n",
        "            elif selector_funcion_objetivo.value == \"neg_mean_absolute_error\":\n",
        "                score_val = mean_absolute_error(Y_test, preds)\n",
        "            else:\n",
        "                score_val = mean_squared_error(Y_test, preds)\n",
        "\n",
        "            # Capturamos el espacio usado por Optuna (param_dist) y el estudio\n",
        "            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "            OPT_MODELS[(\"xgb\", metodo_usado_xgb.lower(), \"optuna\")] = {\n",
        "                \"model\":       best_model,\n",
        "                'sx':          X_scaler,        # tu StandardScaler de X\n",
        "                'sy':          y_scaler,        # si existe\n",
        "                \"score\":       float(score_val),\n",
        "                \"metric\":      selector_funcion_objetivo.value,\n",
        "                \"param_dist\":  param_dist,           # <- espacio de Optuna\n",
        "                \"best_params\": best_params,\n",
        "                \"cols\":        list(X_train_filtrado.columns)\n",
        "            }\n",
        "            # Si quieres guardar el study:\n",
        "            if 'study' in locals():\n",
        "                OPT_MODELS[(\"xgb\", metodo_usado_xgb.lower(), \"optuna_study\")] = study\n",
        "\n",
        "            # --- llamada de persistencia ----\n",
        "            guardar_xgb(best_model, best_params, \"Optuna\",\n",
        "                        metodo_usado_xgb, selector_funcion_objetivo,\n",
        "                        X_train_filtrado, X_test, Y_train, Y_test,\n",
        "                        study=search.study_, traza_out=traza_xgb)\n",
        "            print(\" Optimizacin con Hyperband completada.\")\n",
        "            with traza_xgb:\n",
        "                print(f\" Modelo guardado      modelos_opt/xgb_{metodo_usado_xgb.lower()}_optuna_opt_*.pkl\")\n",
        "                print(f\" Metadatos guardados  modelos_opt/xgb_{metodo_usado_xgb.lower()}_optuna_opt_*_meta.pkl\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\" Error en optimizacin Optuna:\", traceback.format_exc())\n",
        "\n",
        "    opciones_metodos = sorted(list(RESUMEN_METODOS.keys()) + ['Todos'])\n",
        "    selector_metodo = widgets.Dropdown(\n",
        "        options=opciones_metodos,\n",
        "        description='Mtodo:',\n",
        "        layout=widgets.Layout(width='50%')\n",
        "    )\n",
        "\n",
        "    boton_confirmar = widgets.Button(description=\" Cargar variables seleccionadas\", button_style='primary')\n",
        "    boton_confirmar.on_click(lambda b: seleccionar_variables_filtradas(selector_metodo.value))\n",
        "\n",
        "    selector_motor = widgets.SelectMultiple(\n",
        "        options=['RandomSearch', 'Bayesian', 'Hyperband', 'Optuna', 'Todos'],\n",
        "        value=['RandomSearch'],\n",
        "        description='Motores:',\n",
        "        layout=widgets.Layout(width='50%', height='120px')\n",
        "    )\n",
        "\n",
        "    boton_opt = widgets.Button(description=\" Iniciar Optimizacin XGBoost\", button_style='success')\n",
        "    #boton_opt.on_click(lambda b: optimizar_randomsearch() if 'RandomSearch' in selector_motor.value or 'Todos' in selector_motor.value else None)\n",
        "\n",
        "    def lanzar_optimizaciones(_):\n",
        "        if 'RandomSearch' in selector_motor.value or 'Todos' in selector_motor.value:\n",
        "            optimizar_randomsearch()\n",
        "        if 'Bayesian' in selector_motor.value or 'Todos' in selector_motor.value:\n",
        "            optimizar_bayesian()\n",
        "        if 'Hyperband' in selector_motor.value or 'Todos' in selector_motor.value:\n",
        "            optimizar_hyperband()\n",
        "        if 'Optuna' in selector_motor.value or 'Todos' in selector_motor.value:\n",
        "            optimizar_optuna()\n",
        "\n",
        "    boton_opt.on_click(lanzar_optimizaciones)\n",
        "\n",
        "    out_opt_xgb.clear_output()\n",
        "    with out_opt_xgb:\n",
        "        display(HTML(\"<h3> Seleccin de Variables para Optimizacin XGBoost</h3>\"))\n",
        "        display(widgets.HBox([selector_metodo, boton_confirmar]))\n",
        "        display(HTML(\"<h3> Parmetros de Optimizacin</h3>\"))\n",
        "        display(widgets.VBox([\n",
        "            widgets.HBox([slider_n_iter, slider_cv]),\n",
        "            widgets.HBox([ayuda_n_iter, ayuda_cv]),\n",
        "            widgets.HBox([selector_funcion_objetivo]),\n",
        "            ayuda_funcion\n",
        "        ]))\n",
        "        display(HTML(\"<h3> Motores de Optimizacin</h3>\"))\n",
        "        display(widgets.VBox([selector_motor, boton_opt]))\n",
        "\n",
        "        display(traza_xgb)             #  se muestra el panel de trazas\n",
        "\n",
        "    display(out_opt_xgb)\n",
        "\n",
        "\n",
        "# ===============================================\n",
        "# Cell 9.4: OPTIMIZACIN RANDOM FOREST - AADIR GRABADO\n",
        "# Optimizacin mltiple de hiperparmetros del modelo RF\n",
        "# Compatible con motores: RandomSearch, Bayesian, Hyperband y Optuna\n",
        "# Variables: seleccionadas desde Cell 6 (Pearson, Spearman, MutualInfo, Boruta, UMAP, Todos)\n",
        "# Datos: segmentados desde Cell 5.2 (X_train, Y_train, X_test, Y_test)\n",
        "# ===============================================\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
        "from sklearn.model_selection import HalvingRandomSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.integration import OptunaSearchCV\n",
        "from skopt.space import Integer, Categorical\n",
        "from scipy.stats import shapiro\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "# \n",
        "# >>> INICIO DEL BLOQUE DE PERSISTENCIA  (Random-Forest) EN CELDA 9.4\n",
        "# \n",
        "import pathlib, datetime, pickle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "def guardar_rf(best_model,\n",
        "               optimizations_results,\n",
        "               scaler_X, scaler_Y,\n",
        "               cols,\n",
        "               X_test, Y_test,\n",
        "               func_objetivo,\n",
        "               study=None,\n",
        "               traza_out=None):\n",
        "    try:\n",
        "        # 1) Asegurar carpeta de destino\n",
        "        pathlib.Path(\"modelos_opt\").mkdir(exist_ok=True)\n",
        "\n",
        "        # 2) Preparar datos de prueba filtrados y escalados\n",
        "        X_test_sel = X_test[cols]\n",
        "        X_scaled   = scaler_X.transform(X_test_sel)\n",
        "        preds      = best_model.predict(X_scaled)\n",
        "        preds_inv  = scaler_Y.inverse_transform(preds.reshape(-1,1)).ravel()\n",
        "\n",
        "        # 3) Calcular mtrica segn func_objetivo\n",
        "        if func_objetivo == \"r2\":\n",
        "            score_val = r2_score(Y_test, preds_inv)\n",
        "        elif func_objetivo == \"neg_mean_absolute_error\":\n",
        "            score_val = mean_absolute_error(Y_test, preds_inv)\n",
        "        else:\n",
        "            score_val = mean_squared_error(Y_test, preds_inv)\n",
        "\n",
        "        # 4) Definir rutas de archivo\n",
        "        ts      = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        tag     = f\"rf_{func_objetivo}_opt_{ts}\"\n",
        "        model_f = f\"modelos_opt/{tag}.pkl\"\n",
        "        meta_f  = f\"modelos_opt/{tag}_meta.pkl\"\n",
        "        study_f = f\"modelos_opt/{tag}_study.pkl\"\n",
        "\n",
        "        # 5) Guardar modelo+escaladores+cols\n",
        "        with open(model_f, \"wb\") as f:\n",
        "            pickle.dump({\n",
        "                \"model\": best_model,\n",
        "                \"sx\":    scaler_X,\n",
        "                \"sy\":    scaler_Y,\n",
        "                \"cols\":  cols\n",
        "            }, f)\n",
        "\n",
        "        # 6) Guardar metadatos\n",
        "        meta = {\n",
        "            \"score\":  float(score_val),\n",
        "            \"metric\": func_objetivo,\n",
        "            \"cols\":   cols\n",
        "        }\n",
        "        with open(meta_f, \"wb\") as f:\n",
        "            pickle.dump(meta, f)\n",
        "\n",
        "        # 7) Guardar estudio Optuna si existe\n",
        "        if study is not None:\n",
        "            with open(study_f, \"wb\") as f:\n",
        "                pickle.dump(study, f)\n",
        "\n",
        "        # 8) Mensaje de confirmacin\n",
        "        if traza_out is not None:\n",
        "            with traza_out:\n",
        "                print(f\" Modelo guardado  {model_f}\")\n",
        "                print(f\" Metadatos guardados  {meta_f}\")\n",
        "                if study is not None:\n",
        "                    print(f\" Study guardado  {study_f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = f\" No se pudo guardar modelo/estudio: {e}\"\n",
        "        if traza_out is not None:\n",
        "            with traza_out:\n",
        "                print(msg)\n",
        "        else:\n",
        "            print(msg)\n",
        "\n",
        "# Logger para depuracin\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Salida global para la tabla y las grficas\n",
        "out_rf_opt = widgets.Output()\n",
        "\n",
        "# Lista para almacenar los resultados de las optimizaciones\n",
        "optimizations_results = []\n",
        "\n",
        "def mostrar_optimizacion_rf():\n",
        "    with out_rf_opt:\n",
        "        clear_output()\n",
        "\n",
        "        # Aseguramos que las variables X e Y estn definidas globalmente\n",
        "        global X_train, Y_train, X_test, Y_test\n",
        "        if 'X_train' not in globals() or 'Y_train' not in globals() or 'X_test' not in globals() or 'Y_test' not in globals():\n",
        "            print(\" Asegrate de que las variables X_train, Y_train, X_test y Y_test estn correctamente definidas.\")\n",
        "            return\n",
        "\n",
        "        #  AADIDO: saneamiento global de nombres \n",
        "        #import re\n",
        "        #def clean_name(s):\n",
        "        #    t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))  # corchetes, %, /, . y espacios  _\n",
        "        #    t = re.sub(r'_+', '_', t)                     # colapsar guiones bajos repetidos\n",
        "        #    return t.strip('_')\n",
        "\n",
        "        # 1) Limpio las columnas de X_train y X_test\n",
        "        #X_train.columns = [clean_name(c) for c in X_train.columns]\n",
        "        #X_test.columns  = [clean_name(c) for c in X_test.columns]\n",
        "\n",
        "        # 2) Limpio todas las listas de RESUMEN_METODOS\n",
        "        #for m, lst in RESUMEN_METODOS.items():\n",
        "        #    if isinstance(lst, list):\n",
        "        #        RESUMEN_METODOS[m] = [clean_name(c) for c in lst]\n",
        "        #  FIN AADIDO \n",
        "\n",
        "        # 2) Ahora s puedo sanear columnas\n",
        "        # Usar sanitize_name para limpiar columnas en el payload\n",
        "        def clean_cols(col_list):\n",
        "            return [sanitize_name(c) for c in col_list]\n",
        "        # Ejemplo de sanitizacin de X_train antes de fit\n",
        "        X_train.columns = [sanitize_name(col) for col in X_train.columns]\n",
        "        X_test.columns = [sanitize_name(col) for col in X_test.columns]\n",
        "\n",
        "        # Mostrar men para seleccionar el motor de optimizacin, el mtodo y la funcin de optimizacin\n",
        "        selector_motor = widgets.Dropdown(\n",
        "            options=['RandomSearch', 'BayesianOptimization', 'Hyperband', 'Optuna', 'Todos'],\n",
        "            description='Motor:',\n",
        "            value='RandomSearch',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        selector_metodo = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'Mutualinfo', 'Boruta', 'UAMP', 'Todos'],\n",
        "            description='Mtodo:',\n",
        "            value='Todos',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        func_objetivo = widgets.Dropdown(\n",
        "            options=['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
        "            value='r2',\n",
        "            description='Funcin:',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        # Sliders para la configuracin de la optimizacin\n",
        "        n_iter = widgets.IntSlider(value=50, min=10, max=300, step=10, description='n_iter:')\n",
        "        cv = widgets.IntSlider(value=5, min=2, max=10, step=1, description='cv:')\n",
        "\n",
        "        # Botn de ejecutar optimizacin\n",
        "        btn_ejecutar = widgets.Button(description=\" Ejecutar Optimizacin\", button_style='success')\n",
        "        barra_progreso = widgets.IntProgress(min=0, max=1, description='Progreso:')\n",
        "        salida_resultados = widgets.Output()\n",
        "\n",
        "        def ejecutar_optimizacin(_):\n",
        "            global OPT_MODELS\n",
        "            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "            salida_resultados.clear_output()\n",
        "            barra_progreso.bar_style = 'info'\n",
        "            barra_progreso.value = 0\n",
        "            tiempo_transcurrido = widgets.HTML(' Tiempo: 0.0s')\n",
        "            mensaje_estado = widgets.HTML(value=\" <b>Optimizacin en curso...</b>\")\n",
        "            inicio = time.time()\n",
        "\n",
        "            with salida_resultados:\n",
        "                print(\" Ejecutando optimizacin...\")\n",
        "\n",
        "                # Escaladores para entrenamiento/test\n",
        "                scaler_X = StandardScaler().fit(X_train)\n",
        "                scaler_Y = StandardScaler().fit(Y_train.values.reshape(-1,1))\n",
        "                X_train_scaled = scaler_X.transform(X_train)\n",
        "                X_test_scaled  = scaler_X.transform(X_test)\n",
        "                Y_train_scaled = scaler_Y.transform(Y_train.values.reshape(-1,1)).ravel()\n",
        "                Y_test_scaled  = scaler_Y.transform(Y_test.values.reshape(-1,1)).ravel()\n",
        "\n",
        "                # Inicializar motores de optimizacin\n",
        "                search_random = None\n",
        "                search_bayes = None\n",
        "                search_hyperband = None\n",
        "                best_rf_random = None\n",
        "                best_rf_bayes = None\n",
        "                best_rf_hyperband = None\n",
        "                best_rf_optuna = None\n",
        "                best_motor = None\n",
        "                best_metodo = None\n",
        "                best_rf = None\n",
        "                best_score = -np.inf\n",
        "                optimizations_results = []\n",
        "                best_score_random = best_score_bayes = best_score_hyperband = best_score_optuna = None\n",
        "\n",
        "                # Configurar lista de motores y mtodos\n",
        "                motors = (['RandomSearch','BayesianOptimization','Hyperband','Optuna']\n",
        "                          if selector_motor.value=='Todos'\n",
        "                          else [selector_motor.value])\n",
        "                study = None          # placeholder que existir en todos los motores\n",
        "                for motor in motors:\n",
        "                    # Para cada mtodo de seleccin\n",
        "                    methods = list(RESUMEN_METODOS.keys()) if selector_metodo.value=='Todos' else [selector_metodo.value]\n",
        "                    for metodo in methods:\n",
        "                        print(f\" Motor: {motor} | Mtodo: {metodo}\")\n",
        "                        #cols = RESUMEN_METODOS.get(metodo, X_train.columns.tolist())  # columnas de este mtodo\n",
        "\n",
        "                        #  REEMPLAZO: obtengo directamente las columnas saneadas \n",
        "                        cols = RESUMEN_METODOS.get(metodo, [])\n",
        "                        if not cols:\n",
        "                            print(f\" No hay variables para '{metodo}', omito.\")\n",
        "                            continue\n",
        "                        #  FIN REEMPLAZO \n",
        "\n",
        "                        X_train_sel = X_train[cols]\n",
        "                        X_test_sel  = X_test[cols]\n",
        "                        scaler_X = StandardScaler().fit(X_train_sel)\n",
        "                        X_train_scaled = scaler_X.transform(X_train_sel)\n",
        "                        X_test_scaled  = scaler_X.transform(X_test_sel)\n",
        "\n",
        "                        # Aqu nos aseguramos de que Y_train tambin se escala\n",
        "                        sy = StandardScaler()\n",
        "                        Y_train_scaled = sy.fit_transform(Y_train.values.reshape(-1, 1)).ravel()\n",
        "                        Y_test_scaled = sy.transform(Y_test.values.reshape(-1, 1)).ravel()\n",
        "\n",
        "                        # =================================================\n",
        "                        # Motor Random Search\n",
        "                        # =================================================\n",
        "                        if motor == 'RandomSearch':\n",
        "                            param_dist = {\n",
        "                                'n_estimators': np.arange(50, 501, 50),\n",
        "                                'max_depth': np.arange(3, 15, 1),\n",
        "                                'min_samples_split': np.arange(2, 21, 1),\n",
        "                                'min_samples_leaf': np.arange(1, 21, 1),\n",
        "                                'max_features': ['sqrt', 'log2', None],\n",
        "                                'bootstrap': [True, False]\n",
        "                            }\n",
        "                            search_random = RandomizedSearchCV(\n",
        "                                RandomForestRegressor(random_state=42),\n",
        "                                param_distributions=param_dist,\n",
        "                                n_iter=n_iter.value,\n",
        "                                cv=cv.value, n_jobs=-1, scoring=func_objetivo.value, random_state=42,\n",
        "                                return_train_score=True, verbose=3\n",
        "                            )\n",
        "                            search_random.fit(X_train_scaled, Y_train_scaled)\n",
        "                            best_rf_random = search_random.best_estimator_\n",
        "                            best_score_random = search_random.best_score_\n",
        "                            if best_score_random > best_score:\n",
        "                                best_score = best_score_random\n",
        "                                best_rf = best_rf_random\n",
        "                                best_metodo = metodo\n",
        "                                best_motor  = motor\n",
        "                            # Almacenar los resultados del motor RandomSearch\n",
        "                            #  Tras best_estimator_ y best_score de cada motor \n",
        "                            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "                            # RandomSearch\n",
        "                            OPT_MODELS[(\"rf\", metodo.lower(), \"randomsearch\")] = {\n",
        "                                \"model\":       best_rf_random,\n",
        "                                \"sx\":          scaler_X,\n",
        "                                \"sy\":          sy,\n",
        "                                \"score\":       float(best_score_random),\n",
        "                                \"metric\":      func_objetivo.value,\n",
        "                                \"param_dist\":  param_dist,\n",
        "                                \"best_params\": search_random.best_params_,\n",
        "                                \"cols\":        cols\n",
        "                            }\n",
        "                            optimizations_results.append({\n",
        "                                'motor' : motor,\n",
        "                                'metodo': metodo,\n",
        "                                'puntuacin': best_score_random,\n",
        "                                'R2': r2_score(Y_test_scaled, search_random.predict(X_test_scaled)),\n",
        "                                'MSE': mean_squared_error(Y_test_scaled, search_random.predict(X_test_scaled)),\n",
        "                                'MAE': mean_absolute_error(Y_test_scaled, search_random.predict(X_test_scaled)),\n",
        "                                'params': search_random.best_params_\n",
        "                            })\n",
        "\n",
        "                            pass\n",
        "                        # =================================================\n",
        "                        # Motor Bayesian\n",
        "                        # =================================================\n",
        "                        elif motor == 'BayesianOptimization':\n",
        "                            param_space = {\n",
        "                                'n_estimators': Integer(50, 500),\n",
        "                                'max_depth': Integer(3, 15),\n",
        "                                'min_samples_split': Integer(2, 20),\n",
        "                                'min_samples_leaf': Integer(1, 20),\n",
        "                                'max_features': Categorical(['sqrt', 'log2', None]),\n",
        "                                'bootstrap': Categorical([True, False])\n",
        "                            }\n",
        "                            search_bayes = BayesSearchCV(\n",
        "                                RandomForestRegressor(random_state=42),\n",
        "                                param_space,\n",
        "                                n_iter=n_iter.value,\n",
        "                                cv=cv.value, n_jobs=-1, scoring=func_objetivo.value, random_state=42,\n",
        "                                return_train_score=True, verbose=3\n",
        "                            )\n",
        "                            search_bayes.fit(X_train_scaled, Y_train_scaled)\n",
        "                            best_rf_bayes = search_bayes.best_estimator_\n",
        "                            best_score_bayes = search_bayes.best_score_\n",
        "                            if best_score_bayes > best_score:\n",
        "                                best_score = best_score_bayes\n",
        "                                best_rf = best_rf_bayes\n",
        "                                best_metodo = metodo\n",
        "                                best_motor  = motor\n",
        "                            # Almacenar los resultados del motor Bayesian\n",
        "                            OPT_MODELS[(\"rf\", metodo.lower(), \"bayesianoptimization\")] = {\n",
        "                                \"model\":       best_rf_bayes,\n",
        "                                \"sx\":          scaler_X,\n",
        "                                \"sy\":          sy,\n",
        "                                \"score\":       float(best_score_bayes),\n",
        "                                \"metric\":      func_objetivo.value,\n",
        "                                \"param_dist\":  param_space,\n",
        "                                \"best_params\": search_bayes.best_params_,\n",
        "                                \"cols\":        cols\n",
        "                            }\n",
        "                            optimizations_results.append({\n",
        "                                'motor' : motor,\n",
        "                                'metodo': metodo,\n",
        "                                'puntuacin': best_score_bayes,\n",
        "                                'R2': r2_score(Y_test_scaled, search_bayes.predict(X_test_scaled)),\n",
        "                                'MSE': mean_squared_error(Y_test_scaled, search_bayes.predict(X_test_scaled)),\n",
        "                                'MAE': mean_absolute_error(Y_test_scaled, search_bayes.predict(X_test_scaled)),\n",
        "                                'params': search_bayes.best_params_\n",
        "                            })\n",
        "\n",
        "                            pass\n",
        "                        # =================================================\n",
        "                        # Motor Hyperband\n",
        "                        # =================================================\n",
        "                        elif motor == 'Hyperband':\n",
        "                            param_dist = {\n",
        "                                'n_estimators': np.arange(50, 501, 50),\n",
        "                                'max_depth': np.arange(3, 15, 1),\n",
        "                                'min_samples_split': np.arange(2, 21, 1),\n",
        "                                'min_samples_leaf': np.arange(1, 21, 1),\n",
        "                                'max_features': ['sqrt', 'log2', None],\n",
        "                                'bootstrap': [True, False]\n",
        "                            }\n",
        "                            search_hyperband = HalvingRandomSearchCV(\n",
        "                                RandomForestRegressor(random_state=42),\n",
        "                                param_distributions=param_dist,\n",
        "                                factor=3,  # Aumenta recursos a medida que mejora el modelo\n",
        "                                max_resources=300,  # Mximo nmero de recursos para la optimizacin\n",
        "                                min_resources=50,  # Nmero mnimo de recursos\n",
        "                                cv=cv.value, n_jobs=-1, scoring=func_objetivo.value, random_state=42,\n",
        "                                return_train_score=True, verbose=3\n",
        "                            )\n",
        "                            search_hyperband.fit(X_train_scaled, Y_train_scaled)\n",
        "                            best_rf_hyperband = search_hyperband.best_estimator_\n",
        "                            best_score_hyperband = search_hyperband.best_score_\n",
        "                            if best_score_hyperband > best_score:\n",
        "                                best_score = best_score_hyperband\n",
        "                                best_rf = best_rf_hyperband\n",
        "                                best_metodo = metodo\n",
        "                                best_motor  = motor\n",
        "                            # Almacenar los resultados del motor Hyperband\n",
        "                            OPT_MODELS[(\"rf\", metodo.lower(), \"hyperband\")] = {\n",
        "                                \"model\":       best_rf_hyperband,\n",
        "                                \"sx\":          scaler_X,\n",
        "                                \"sy\":          sy,\n",
        "                                \"score\":       float(best_score_hyperband),\n",
        "                                \"metric\":      func_objetivo.value,\n",
        "                                \"param_dist\":  param_dist,\n",
        "                                \"best_params\": search_hyperband.best_params_,\n",
        "                                \"cols\":        cols\n",
        "                            }\n",
        "                            optimizations_results.append({\n",
        "                                'motor' : motor,\n",
        "                                'metodo': metodo,\n",
        "                                'puntuacin': best_score_hyperband,\n",
        "                                'R2': r2_score(Y_test_scaled, search_hyperband.predict(X_test_scaled)),\n",
        "                                'MSE': mean_squared_error(Y_test_scaled, search_hyperband.predict(X_test_scaled)),\n",
        "                                'MAE': mean_absolute_error(Y_test_scaled, search_hyperband.predict(X_test_scaled)),\n",
        "                                'params': search_hyperband.best_params_\n",
        "                            })\n",
        "\n",
        "                            pass\n",
        "                        # =================================================\n",
        "                        # Motor Optuna\n",
        "                        # =================================================\n",
        "                        elif motor == 'Optuna':\n",
        "                            def objective(trial: Trial):\n",
        "                                param = {\n",
        "                                    'n_estimators': trial.suggest_int('n_estimators', 50, 500, step=50),\n",
        "                                    'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "                                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "                                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
        "                                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
        "                                    'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
        "                                }\n",
        "                                model = RandomForestRegressor(**param, random_state=42)\n",
        "                                model.fit(X_train_scaled, Y_train_scaled)\n",
        "                                preds = model.predict(X_test_scaled)\n",
        "                                return mean_squared_error(Y_test_scaled, preds)\n",
        "\n",
        "                            #study = optuna.create_study(direction='maximize')  # Maximizar la puntuacin\n",
        "                            direction = 'minimize' if func_objetivo.value != 'r2' else 'maximize'\n",
        "                            study = optuna.create_study(direction=direction)\n",
        "                            study.optimize(objective, n_trials=n_iter.value)\n",
        "\n",
        "                            best_rf_optuna = RandomForestRegressor(**study.best_params, random_state=42)\n",
        "                            best_rf_optuna.fit(X_train_scaled, Y_train_scaled)\n",
        "                            best_score_optuna = study.best_value\n",
        "                            if best_score_optuna > best_score:\n",
        "                                best_score = best_score_optuna\n",
        "                                best_rf = best_rf_optuna\n",
        "                                best_metodo = metodo\n",
        "                                best_motor  = motor\n",
        "\n",
        "                            # Almacenar los resultados del motor Optuna\n",
        "                            #  despus de entrenar best_rf_optuna y antes de optimizations_results.append\n",
        "                            pred_opt = best_rf_optuna.predict(X_test_scaled)   #  calcula una sola vez\n",
        "\n",
        "                            OPT_MODELS[(\"rf\", metodo.lower(), \"optuna\")] = {\n",
        "                                \"model\":       best_rf_optuna,\n",
        "                                \"sx\":          scaler_X,\n",
        "                                \"sy\":          sy,\n",
        "                                \"score\":       float(best_score_optuna),\n",
        "                                \"metric\":      func_objetivo.value,\n",
        "                                \"param_dist\":  study.best_params,\n",
        "                                \"best_params\": study.best_params,\n",
        "                                \"cols\":        cols\n",
        "                            }\n",
        "                            optimizations_results.append({\n",
        "                                'motor' : motor,\n",
        "                                'metodo': metodo,\n",
        "                                'puntuacin': best_score_optuna,\n",
        "                                'R2'    : r2_score(Y_test_scaled, pred_opt),\n",
        "                                'MSE'   : mean_squared_error(Y_test_scaled, pred_opt),\n",
        "                                'MAE'   : mean_absolute_error(Y_test_scaled, pred_opt),\n",
        "                                'params': study.best_params\n",
        "                            })\n",
        "\n",
        "                            study = study\n",
        "\n",
        "                            pass\n",
        "\n",
        "                # Elegir el mejor modelo segn el motor seleccionado\n",
        "                best_rf = best_rf_random if best_rf_random else best_rf_bayes if best_rf_bayes else best_rf_hyperband if best_rf_hyperband else best_rf_optuna\n",
        "\n",
        "                # Recopilamos slo los scores que s tenemos\n",
        "                scores = [\n",
        "                    best_score_random,\n",
        "                    best_score_bayes,\n",
        "                    best_score_hyperband,\n",
        "                    best_score_optuna\n",
        "                ]\n",
        "                # Filtramos los None\n",
        "                scores = [s for s in scores if s is not None]\n",
        "                # Si hay al menos uno, tomamos el mximo; si no, dejamos None o 0\n",
        "                best_score = max(scores) if scores else None\n",
        "\n",
        "                # Registrar en memoria el mejor RF en OPT_MODELS\n",
        "                OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "                OPT_MODELS[(\"rf\", best_metodo.lower(), best_motor.lower())] = {\n",
        "                    \"model\":       best_rf,\n",
        "                    \"sx\":          scaler_X,\n",
        "                    \"sy\":          sy,\n",
        "                    \"score\":       float(best_score),\n",
        "                    \"metric\":      func_objetivo.value,\n",
        "                    \"param_dist\":  optimizations_results[-1][\"params\"],\n",
        "                    \"best_params\": optimizations_results[-1][\"params\"],\n",
        "                    \"cols\":        RESUMEN_METODOS.get(best_metodo, X_train.columns.tolist())\n",
        "                }\n",
        "\n",
        "                #import re\n",
        "                # tras bucles, desescalamos y graficamos con el mejor de todos\n",
        "                # Ajuste de columnas segn seleccin de variables\n",
        "                cols_rf = RESUMEN_METODOS.get(best_metodo, X_train.columns.tolist())\n",
        "                #  AADIDO: sanitizar nombres de cols_rf \n",
        "                # despus de determinar cols_rf original\n",
        "                #raw_cols_rf = RESUMEN_METODOS.get(best_metodo, X_train.columns.tolist())\n",
        "\n",
        "                # 1) Sanitizamos:\n",
        "                #cols_rf = [ re.sub(r'[\\[\\]<>]', '_', str(c)) for c in raw_cols_rf ]\n",
        "\n",
        "                # 2) Filtramos para quedarnos solo con los que existen:\n",
        "                #cols_rf = [ c for c in cols_rf if c in X_train.columns ]\n",
        "                #  FIN AADIDO \n",
        "\n",
        "                scaler_X_final = StandardScaler().fit(X_train[cols_rf])\n",
        "                scaler_Y_final = StandardScaler().fit(Y_train.values.reshape(-1,1))\n",
        "                X_test_sel = X_test[cols_rf]\n",
        "                X_test_scaled_final = scaler_X_final.transform(X_test_sel)\n",
        "\n",
        "                # Prediccin sobre columnas seleccionadas\n",
        "                preds_scaled = best_rf.predict(X_test_scaled_final)\n",
        "                preds_final = scaler_Y_final.inverse_transform(preds_scaled.reshape(-1,1)).ravel()\n",
        "\n",
        "                # >>> LLAMADA A PERSISTENCIA \n",
        "                guardar_rf(\n",
        "                    best_model=best_rf,\n",
        "                    optimizations_results=optimizations_results,\n",
        "                    scaler_X=scaler_X_final,\n",
        "                    scaler_Y=scaler_Y_final,\n",
        "                    cols=cols,\n",
        "                    X_test=X_test,\n",
        "                    Y_test=Y_test,\n",
        "                    func_objetivo=func_objetivo.value,\n",
        "                    study=study if study is not None else None,\n",
        "                    traza_out=salida_resultados\n",
        "                )\n",
        "                # \n",
        "                # Mostrar el mejor modelo y los hiperparmetros\n",
        "                print(\" Optimizacin completada.\")\n",
        "                print(f\" Mejor Modelo: {best_rf}\")\n",
        "                print(f\" Mejor Puntuacin: {best_score:.4f}\")\n",
        "\n",
        "                plt.figure(figsize=(10,4))\n",
        "                plt.subplot(1,2,1)\n",
        "                plt.scatter(Y_test, preds_final, alpha=0.6)\n",
        "                plt.plot([Y_test.min(),Y_test.max()],[Y_test.min(),Y_test.max()],'r--', label='Ideal')\n",
        "                plt.title(f\"Real vs Predicho ({best_motor} - {best_metodo})\")\n",
        "                plt.xlabel(\"Y Real\"); plt.ylabel(\"Y Predicho\"); plt.legend(); plt.grid()\n",
        "\n",
        "                # Histograma residuos y curvas de distribucin\n",
        "                residuos = Y_test.values.ravel() - preds_final\n",
        "                plt.figure(figsize=(6,4))\n",
        "                # Histograma\n",
        "                n,bins,patches=plt.hist(residuos,bins=20,density=True,alpha=0.6,edgecolor='black',label='Distribucin Residuos')\n",
        "                # Curva normal\n",
        "                mu,sd=norm.fit(residuos)\n",
        "                x=np.linspace(bins.min(),bins.max(),100)\n",
        "                plt.plot(x,norm.pdf(x,mu,sd),linewidth=2, label='Curva normal')\n",
        "                # Curva KDE de la distribucin real\n",
        "                kde = gaussian_kde(residuos)\n",
        "                x = np.linspace(bins.min(),bins.max(),100)\n",
        "                plt.plot(x, kde(x), lw=2, label='Densidad KDE')  # aadido KDE\n",
        "                plt.title('Histograma de residuos con curva normal y curva de densidad'); plt.tight_layout()\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "\n",
        "                # Test normalidad Shapiro\n",
        "                stat, p = shapiro(residuos)\n",
        "                display(HTML(f\"<h4> Prueba de Normalidad Shapiro-Wilk:</h4><ul><li>Estadstico: {stat:.4f}</li><li>p-valor: {p:.4f}</li><li>{' Residuos normales' if p>0.05 else ' Residuos no normales'}</li></ul>\"))\n",
        "\n",
        "                # Eje de casos\n",
        "                n_casos = len(Y_test)\n",
        "                casos = range(n_casos)\n",
        "\n",
        "                # Extraer valores\n",
        "                y_real = Y_test.values if hasattr(Y_test, \"values\") else Y_test\n",
        "                y_pred = preds_final  # ajusta el nombre si usas otro\n",
        "\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                plt.scatter(casos, y_real, marker='o', label='Y real')\n",
        "                plt.scatter(casos, y_pred, marker='x', label='Y predicho')\n",
        "                plt.xlabel('Caso')\n",
        "                plt.ylabel('Y')\n",
        "                plt.title('Comparacin de puntos: Y real vs. Y predicho')\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Mostrar los hiperparmetros ptimos\n",
        "                print(\"\\n Mejores hiperparmetros encontrados:\")\n",
        "                optimal_params_df = pd.DataFrame.from_dict(best_rf.get_params(), orient='index', columns=['Valor ptimo'])\n",
        "                display(optimal_params_df.style.set_caption(\" Parmetros ptimos\").format(precision=4))\n",
        "\n",
        "                # Calcular y mostrar mtricas\n",
        "                r2 = r2_score(Y_test, preds_final)\n",
        "                mse = mean_squared_error(Y_test, preds_final)\n",
        "                rmse = np.sqrt(mse)\n",
        "                mae = mean_absolute_error(Y_test, preds_final)\n",
        "\n",
        "                print(f\" Valores de ajuste para los datos de prueba\")\n",
        "                print(f\" R: {r2:.4f}\")\n",
        "                print(f\" MSE: {mse:.4f}\")\n",
        "                print(f\" RMSE: {rmse:.4f}\")\n",
        "                print(f\" MAE: {mae:.4f}\")\n",
        "\n",
        "                # Tabla completa de optimizaciones\n",
        "                df_all=pd.DataFrame(optimizations_results).rename(columns={'motor':'Motor','metodo':'Mtodo'})\n",
        "                display(df_all.style.set_caption(\" Todos los resultados de optimizacin\").format({'R2':'{:.4f}','MSE':'{:.4f}','MAE':'{:.4f}'}))\n",
        "\n",
        "                # Ranking top 5\n",
        "                df_rank = pd.DataFrame(sorted(optimizations_results, key=lambda x: x['puntuacin'], reverse=True)[:5])\n",
        "                display(df_rank.style.set_caption(\" Top 5 Optimizacin\").format({\"R2\": \"{:.4f}\", \"MSE\":\"{:.4f}\", \"MAE\":\"{:.4f}\"}))\n",
        "\n",
        "                barra_progreso.bar_style = 'success'\n",
        "                barra_progreso.value = 1\n",
        "                tiempo_transcurrido.value = f' Tiempo total: {time.time() - inicio:.1f}s'\n",
        "                mensaje_estado = widgets.HTML(value=\" Optimizacin completada\")\n",
        "\n",
        "        # Conectar la accin del botn con la ejecucin de la optimizacin\n",
        "        btn_ejecutar.on_click(ejecutar_optimizacin)\n",
        "\n",
        "        # Mostrar widgets\n",
        "        display(HTML(\"<h3> Optimizacin</h3>\"))\n",
        "        display(widgets.VBox([\n",
        "            selector_motor,\n",
        "            selector_metodo,\n",
        "            func_objetivo,\n",
        "            n_iter,\n",
        "            cv,\n",
        "            btn_ejecutar,\n",
        "            barra_progreso,\n",
        "            salida_resultados\n",
        "        ]))\n",
        "\n",
        "# Llamar la funcin para que el men se active desde el men principal\n",
        "display(out_rf_opt)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================\n",
        "# Cell 9.5: OPTIMIZACIN REDES NEURONALES RECURRENTES - AADIR GRABADO\n",
        "# Optimizacin mltiple de hiperparmetros del modelo RNN\n",
        "# Compatible con motores: RandomSearch, Bayesian, Hyperband y Optuna\n",
        "# Variables: seleccionadas desde Cell 6 (Pearson, Spearman, MutualInfo, Boruta, UMAP, Todos)\n",
        "# Datos: segmentados desde Cell 5.2 (X_train, Y_train, X_test, Y_test)\n",
        "# ===============================================\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import threading, time\n",
        "\n",
        "# SKLEARN & KERAS imports\n",
        "from sklearn.base import BaseEstimator, RegressorMixin                        # wrapper base\n",
        "from sklearn.preprocessing import StandardScaler      #  AADIDO\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, HalvingRandomSearchCV  # Import Hyperband\n",
        "# TensorFlow imports\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# skopt imports\n",
        "from skopt import BayesSearchCV  # Motor Bayesian\n",
        "from skopt.space import Real, Integer, Categorical  # Espacios de bsqueda para Bayesian\n",
        "\n",
        "# Salida global\n",
        "out_opt_rnn = widgets.Output()\n",
        "import re\n",
        "\n",
        "# === NEW: Wrapper para integrar RNN con la API de scikit-learn ===\n",
        "class RNNRegressor(BaseEstimator, RegressorMixin):                             # NEW\n",
        "    def __init__(self, units=32, dropout_rate=0.2, learning_rate=1e-3,\n",
        "        epochs=50, batch_size=32, verbose=0, sy=None):\n",
        "        self.units = units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.model_ = None\n",
        "        self.sy = sy\n",
        "\n",
        "    #def fit(self, X, y):\n",
        "    def fit(self, X, y, **kwargs):\n",
        "        # Si alguien pasa sy por fit, lo recogemos\n",
        "        sy = kwargs.pop(\"sy\", None)\n",
        "        if sy is not None:\n",
        "            self.sy = sy\n",
        "\n",
        "        # Absorber posibles argumentos de recurso (epochs) de Hyperband\n",
        "        kwargs.pop('epochs', None)\n",
        "        # Aseguramos tipos nativos\n",
        "        self.units = int(self.units)            # NOW ensures Python int\n",
        "        self.dropout_rate = float(self.dropout_rate)\n",
        "        self.learning_rate = float(self.learning_rate)\n",
        "        self.epochs = int(self.epochs)\n",
        "        self.batch_size = int(self.batch_size)\n",
        "\n",
        "        # Damos formato 3D para la capa recurrente: (n_samples, timesteps=1, n_features)\n",
        "        X3 = X.reshape((X.shape[0], 1, X.shape[1]))                             # NEW\n",
        "\n",
        "        # Construimos el RNN\n",
        "        self.model_ = Sequential()\n",
        "        self.model_.add(SimpleRNN(self.units, activation='tanh', input_shape=(1, X.shape[1])))\n",
        "        self.model_.add(Dropout(self.dropout_rate))\n",
        "        self.model_.add(Dense(1))\n",
        "        self.model_.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "\n",
        "        # Entrenamiento\n",
        "        self.model_.fit(X3, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Si vienen DataFrames o similares, extrae el ndarray:\n",
        "        X_arr = X.values if hasattr(X, \"values\") else X\n",
        "        # Ahora reshape en el ndarray:\n",
        "        X3 = X_arr.reshape((X_arr.shape[0], 1, X_arr.shape[1]))\n",
        "        y_scaled = self.model_.predict(X3, verbose=self.verbose).ravel()\n",
        "        if self.sy is not None:\n",
        "            return self.sy.inverse_transform(y_scaled.reshape(-1,1)).ravel()\n",
        "        return y_scaled\n",
        "\n",
        "# \n",
        "#  INICIO DE LA FUNCIN DE PERSISTENCIA  (RNN)\n",
        "#     Guarda: modelo Keras (.keras)  +  metadatos (.pkl)\n",
        "#     Si el motor es Optuna  guarda tambin el objeto study\n",
        "# \n",
        "import pathlib, pickle, datetime\n",
        "\n",
        "def guardar_rnn(best_estimator, score, metodo, motor,\n",
        "                selector_scoring, X_cols, Y_ref,\n",
        "                sx, sy,                         # --- ADDED scaler args ---\n",
        "                study=None, traza_out=None):\n",
        "\n",
        "    \"\"\"\n",
        "    best_estimator  -> instancia RNNRegressor entrenada\n",
        "    score           -> mtrica devuelta por el CV\n",
        "    metodo          -> mtodo de seleccin (Pearson, )\n",
        "    motor           -> motor de optimizacin (RandomSearch, )\n",
        "    selector_scoring-> widget con la mtrica elegida\n",
        "    X_cols          -> lista de columnas usadas en X\n",
        "    Y_ref           -> Y_train (Series o DataFrame) para extraer nombre\n",
        "    study           -> objeto optuna.study.Study   (solo motor Optuna)\n",
        "    traza_out       -> panel Output donde imprimir mensajes (opcional)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1) carpeta destino\n",
        "        pathlib.Path(\"modelos_opt\").mkdir(exist_ok=True)\n",
        "\n",
        "        # 2) nombre robusto\n",
        "        ts  = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        tag = f\"rnn_{metodo.lower()}_{motor.lower()}_opt_{ts}\"\n",
        "\n",
        "        # 3) guardar sub-modelo Keras (*.keras)\n",
        "        keras_f = f\"modelos_opt/{tag}.keras\"\n",
        "        best_estimator.model_.save(keras_f, include_optimizer=True)\n",
        "\n",
        "        # 4) metadatos\n",
        "        y_name = getattr(Y_ref, \"name\", None) \\\n",
        "                 or (Y_ref.columns[0] if hasattr(Y_ref, \"columns\") else \"Y\")\n",
        "\n",
        "        meta = dict(\n",
        "            params       = best_estimator.get_params(),\n",
        "            score        = float(score),\n",
        "            func_obj     = selector_scoring.value,\n",
        "            motor        = motor,\n",
        "            metodo_x     = metodo,\n",
        "            cols         = X_cols,\n",
        "            yname        = y_name,\n",
        "            fecha        = ts,\n",
        "            sx           = sx,                 # --- ADDED ---\n",
        "            sy           = sy                  # --- ADDED ---\n",
        "        )\n",
        "        with open(f\"modelos_opt/{tag}_meta.pkl\", \"wb\") as f:\n",
        "            pickle.dump(meta, f)\n",
        "\n",
        "        # 5) study Optuna (si procede)\n",
        "        if motor.lower() == \"optuna\" and study is not None:\n",
        "            with open(f\"modelos_opt/{tag}_study.pkl\", \"wb\") as f:\n",
        "                pickle.dump(study, f)\n",
        "\n",
        "        # 6) log\n",
        "        if traza_out is not None:\n",
        "            with traza_out:\n",
        "                print(f\" Modelo guardado      {keras_f}\")\n",
        "                print(f\" Metadatos guardados  {tag}_meta.pkl\")\n",
        "                if motor.lower() == \"optuna\" and study is not None:\n",
        "                    print(f\" Study Optuna        {tag}_study.pkl\")\n",
        "\n",
        "    except Exception as e:\n",
        "        txt = f\" No se pudo guardar modelo/estudio: {e}\"\n",
        "        if traza_out is not None:\n",
        "            with traza_out: print(txt)\n",
        "        else:\n",
        "            print(txt)\n",
        "# \n",
        "#  FIN DE LA FUNCIN DE PERSISTENCIA  (RNN)\n",
        "# \n",
        "\n",
        "# Funcin principal de la celda\n",
        "def mostrar_optimizacion_rnn():\n",
        "    \"\"\"\n",
        "    Carga las variables X, Y y FECHAS de los conjuntos Train/Test\n",
        "    segn el mtodo de seleccin escogido.\n",
        "    Permite preview, seleccin de motores, y ejecuta RandomSearchCV\n",
        "    sobre el wrapper RNNRegressor.\n",
        "    \"\"\"\n",
        "    global X_train_sel, Y_train_sel, FECHAS_train_sel\n",
        "    global X_test_sel, Y_test_sel, FECHAS_test_sel\n",
        "    global X_train, Y_train, FECHAS_train, X_test, Y_test, FECHAS_test\n",
        "    global RESUMEN_METODOS\n",
        "\n",
        "    global OPT_MODELS\n",
        "    if 'OPT_MODELS' not in globals() or not isinstance(OPT_MODELS, dict):\n",
        "        OPT_MODELS = {}\n",
        "    #  ESCALERS GLOBALES PARA PASAR A guardar_rnn \n",
        "    global sx, sy\n",
        "    sx, sy = None, None\n",
        "\n",
        "    with out_opt_rnn:\n",
        "        clear_output(wait=True)\n",
        "        # Verificar que los conjuntos existan\n",
        "        required = ['X_train','Y_train','FECHAS_train','X_test','Y_test','FECHAS_test']\n",
        "        missing = [v for v in required if v not in globals()]\n",
        "        if missing:\n",
        "            print(f\" Faltan datos: {', '.join(missing)}. Ejecuta la segmentacin antes.\")\n",
        "            return\n",
        "\n",
        "        # 2) Ahora s puedo sanear columnas\n",
        "        # Usar sanitize_name para limpiar columnas en el payload\n",
        "        def clean_cols(col_list):\n",
        "            return [sanitize_name(c) for c in col_list]\n",
        "        # Ejemplo de sanitizacin de X_train antes de fit\n",
        "        X_train.columns = [sanitize_name(col) for col in X_train.columns]\n",
        "        X_test.columns = [sanitize_name(col) for col in X_test.columns]\n",
        "\n",
        "        # Dropdown de mtodos de seleccin\n",
        "        metodos = ['Pearson','Spearman','MutualInfo','Boruta','UMAP']\n",
        "        selector_metodo = widgets.Dropdown(\n",
        "            options=['Todos'] + metodos,\n",
        "            description='Mtodo X:',\n",
        "            layout=widgets.Layout(width='300px')\n",
        "        )\n",
        "        # Botn de carga de variables y preview\n",
        "        btn_cargar = widgets.Button(description=' Cargar Variables', button_style='primary')\n",
        "        salida_carga = widgets.Output()\n",
        "\n",
        "        # Selector de motores (incluye Todos)\n",
        "        motores = ['RandomSearch','Bayesian','Hyperband','Optuna']\n",
        "        selector_motor = widgets.SelectMultiple(\n",
        "            options=['Todos'] + motores,\n",
        "            description='Motores:',\n",
        "            layout=widgets.Layout(width='300px', height='100px')\n",
        "        )\n",
        "        # Scoring, CV y N iteraciones\n",
        "        selector_scoring = widgets.Dropdown(\n",
        "            options=['r2','neg_mean_squared_error','neg_mean_absolute_error'],\n",
        "            value='r2',\n",
        "            description='Scoring:',\n",
        "            layout=widgets.Layout(width='300px')\n",
        "        )\n",
        "        cv = widgets.IntSlider(value=5, min=2, max=10, step=1, description='cv:')\n",
        "        n_iter = widgets.IntSlider(value=10, min=5, max=100, step=1, description='N iter:')\n",
        "        progreso_reloj = widgets.HTML(' 00:00:00')\n",
        "        # Botn de ejecucin\n",
        "        btn_ejecutar = widgets.Button(description=' Ejecutar Optimizacin', button_style='success')\n",
        "        salida_logs = widgets.Output()\n",
        "\n",
        "        # === cargar_variables: filtra y muestra preview ===\n",
        "        def cargar_variables(_):\n",
        "            global X_train_sel, Y_train_sel, FECHAS_train_sel\n",
        "            global X_test_sel, Y_test_sel, FECHAS_test_sel\n",
        "            salida_carga.clear_output(wait=True)\n",
        "\n",
        "            metodo = selector_metodo.value\n",
        "            with salida_carga:\n",
        "                # Determinar columnas X\n",
        "                if metodo == 'Todos':\n",
        "                    cols = sorted({c for m in metodos for c in RESUMEN_METODOS.get(m, [])})\n",
        "                else:\n",
        "                    cols = RESUMEN_METODOS.get(metodo, [])\n",
        "                if not cols:\n",
        "                    print(f\" No hay variables para '{metodo}'.\")\n",
        "                    return\n",
        "\n",
        "                X_train_sel = X_train[cols].copy()\n",
        "                Y_train_sel = Y_train.copy()\n",
        "                FECHAS_train_sel = FECHAS_train.copy()\n",
        "                X_test_sel  = X_test[cols].copy()\n",
        "                Y_test_sel  = Y_test.copy()\n",
        "                FECHAS_test_sel  = FECHAS_test.copy()\n",
        "\n",
        "                # Preview robusto: concat por posicin (reset_index) para evitar NaN\n",
        "                df_train = pd.concat([\n",
        "                    X_train_sel.head(5).reset_index(drop=True),\n",
        "                    Y_train_sel.head(5).reset_index(drop=True),\n",
        "                    FECHAS_train_sel.head(5).reset_index(drop=True).rename('Fecha')\n",
        "                ], axis=1)\n",
        "                df_train['Tipo'] = 'Train'\n",
        "\n",
        "                df_test = pd.concat([\n",
        "                    X_test_sel.head(5).reset_index(drop=True),\n",
        "                    Y_test_sel.head(5).reset_index(drop=True),\n",
        "                    FECHAS_test_sel.head(5).reset_index(drop=True).rename('Fecha')\n",
        "                ], axis=1)\n",
        "                df_test['Tipo'] = 'Test'\n",
        "\n",
        "                # --- ADDED: crear y guardar scalers ---\n",
        "                global sx, sy\n",
        "                sx = StandardScaler().fit(X_train_sel.values)\n",
        "                sy = StandardScaler().fit(Y_train_sel.values.reshape(-1,1))\n",
        "                with salida_carga:\n",
        "                    print(f\" Cargadas {len(cols)} variables y escalers preparados para '{metodo}'\")\n",
        "\n",
        "                df_preview = pd.concat([df_train, df_test], ignore_index=True)\n",
        "                display(HTML(\"<h4> Preview Train/Test (5 filas cada uno)</h4>\"))\n",
        "                display(df_preview)\n",
        "\n",
        "        # === ejecutar_optimizacion: RandomSearchCV con RNNRegressor ===\n",
        "        def ejecutar_optimizacion(_):\n",
        "            # Iniciar reloj\n",
        "            stop_event = threading.Event()\n",
        "            start_time = time.time()\n",
        "            def run_clock():\n",
        "                while not stop_event.is_set():\n",
        "                    elapsed = int(time.time() - start_time)\n",
        "                    h, rem = divmod(elapsed, 3600)\n",
        "                    m, s   = divmod(rem, 60)\n",
        "                    progreso_reloj.value = f\" {h:02d}:{m:02d}:{s:02d}\"\n",
        "                    time.sleep(1)\n",
        "            t = threading.Thread(target=run_clock, daemon=True)\n",
        "            t.start()\n",
        "\n",
        "            salida_logs.clear_output(wait=True)\n",
        "            #start_time = time.time()  # inicio temporizador\n",
        "\n",
        "            with salida_logs:\n",
        "                resultados = []\n",
        "                # determinar mtodos\n",
        "                sel_met = selector_metodo.value\n",
        "                methods = metodos.copy() if sel_met=='Todos' else [sel_met]\n",
        "                # determinar motores\n",
        "                sel_mot = list(selector_motor.value)\n",
        "                mot_sel = motores if 'Todos' in sel_mot else sel_mot\n",
        "\n",
        "                for metodo in methods:\n",
        "\n",
        "                    # (Aqu va la parte para obtener `cols` desde RESUMEN_METODOS)\n",
        "                    if metodo == 'Todos':\n",
        "                        cols = []\n",
        "                        for m_aux in metodos:\n",
        "                            cols += RESUMEN_METODOS.get(m_aux, [])\n",
        "                        cols = sorted(set(cols))\n",
        "                    else:\n",
        "                        cols = RESUMEN_METODOS.get(metodo, [])\n",
        "\n",
        "                    for motor in mot_sel:\n",
        "                        #  AQUI VA el bloque que te he entregado\n",
        "                        print(f\" {motor} en '{metodo}'\")\n",
        "\n",
        "                        # Validar columnas existentes\n",
        "                        effective_cols = [c for c in cols if c in X_test.columns]\n",
        "                        print(f\"[DEBUG] RNN mtodo '{metodo}' - columnas esperadas: {len(cols)}, vlidas: {len(effective_cols)}  {effective_cols}\")\n",
        "\n",
        "                        if len(effective_cols) < 2:\n",
        "                            print(f\" Se omite '{metodo}' con motor '{motor}' porque solo seleccion {len(effective_cols)} columnas vlidas: {effective_cols}\")\n",
        "                            continue\n",
        "\n",
        "                        # Preparar subconjuntos\n",
        "                        X_train_sel = X_train[effective_cols].copy()\n",
        "                        Y_train_sel = Y_train.copy()\n",
        "                        FECHAS_train_sel = FECHAS_train.copy()\n",
        "\n",
        "                        X_test_sel  = X_test[effective_cols].copy()\n",
        "                        Y_test_sel  = Y_test.copy()\n",
        "                        FECHAS_test_sel  = FECHAS_test.copy()\n",
        "\n",
        "                        X_test_vals = X_test_sel.values\n",
        "                        y_test_vals = Y_test.values.ravel()\n",
        "\n",
        "                        kr = RNNRegressor()\n",
        "                        # ============================================\n",
        "                        # Motor de optimizacin RandomSearch\n",
        "                        # ============================================\n",
        "                        if motor == 'RandomSearch':\n",
        "                            #  AADIDO: re-ajustar escaladores para el filtrado actual \n",
        "                            sx = StandardScaler().fit(X_train_sel.values)\n",
        "                            sy = StandardScaler().fit(Y_train_sel.values.reshape(-1,1))\n",
        "                            # preparar datos (ahora con el scaler correcto)\n",
        "                            Xv = sx.transform(X_train_sel.values)\n",
        "                            yv = sy.transform(Y_train_sel.values.reshape(-1,1)).ravel()\n",
        "                            # Distribuciones de scipy.stats\n",
        "                            from scipy.stats import randint, uniform, loguniform\n",
        "                            # Usar nuestro wrapper\n",
        "                            kr = RNNRegressor(\n",
        "                                units=64, dropout_rate=0.2,\n",
        "                                learning_rate=1e-3, epochs=50,\n",
        "                                batch_size=32, verbose=3\n",
        "                            )  # NEW\n",
        "                            rs_params = {\n",
        "                                'units': randint(10, 61),\n",
        "                                'dropout_rate': uniform(0.0, 0.5),\n",
        "                                'learning_rate': loguniform(1e-4, 1e-2),\n",
        "                                'epochs': randint(50, 401),\n",
        "                                'batch_size': randint(16, 257)\n",
        "                            }\n",
        "                            rs = RandomizedSearchCV(\n",
        "                                estimator=kr,\n",
        "                                param_distributions=rs_params,\n",
        "                                n_iter=n_iter.value,\n",
        "                                scoring=selector_scoring.value,\n",
        "                                cv=cv.value,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1,\n",
        "                                verbose = 3\n",
        "                            )\n",
        "                            print(\" Ejecutando RandomizedSearchCV\")\n",
        "                            rs.fit(Xv, yv)\n",
        "                            best = rs.best_estimator_\n",
        "                            score = rs.best_score_\n",
        "                            params = rs.best_params_\n",
        "\n",
        "                        # ============================================\n",
        "                        # Motor de optimizacin Bayesian\n",
        "                        # ============================================\n",
        "                        elif motor=='Bayesian':\n",
        "                            # preparar datos\n",
        "                            #Xv = X_train_sel.values\n",
        "                            #yv = Y_train_sel.values.ravel()\n",
        "                            sx = StandardScaler().fit(X_train_sel.values)\n",
        "                            sy = StandardScaler().fit(Y_train_sel.values.reshape(-1,1))\n",
        "                            Xv = sx.transform(X_train_sel.values)\n",
        "                            yv = sy.transform(Y_train_sel.values.reshape(-1,1)).ravel()\n",
        "                            # Usar nuestro wrapper\n",
        "                            kr = RNNRegressor()\n",
        "                            #param_space = {\n",
        "                            bay_params = {\n",
        "                                'units': Integer(10,60),\n",
        "                                'dropout_rate': Real(0.0,0.5),\n",
        "                                'learning_rate': Real(1e-4,1e-2,'log-uniform'),\n",
        "                                'epochs': Integer(50,400),\n",
        "                                'batch_size': Integer(16,256)\n",
        "                            }\n",
        "                            bs = BayesSearchCV(\n",
        "                                kr,\n",
        "                                #param_space,\n",
        "                                bay_params,\n",
        "                                n_iter=n_iter.value,\n",
        "                                scoring=selector_scoring.value,\n",
        "                                cv=cv.value,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1,\n",
        "                                verbose=3\n",
        "                            )\n",
        "                            print(\" Ejecutando Bayesian\")\n",
        "                            bs.fit(Xv,yv)\n",
        "                            best, score, params = bs.best_estimator_, bs.best_score_, bs.best_params_\n",
        "\n",
        "                        # ============================================\n",
        "                        # Motor de optimizacin Hyperband\n",
        "                        # ============================================\n",
        "                        elif motor=='Hyperband':\n",
        "                            # preparar datos\n",
        "                            #Xv = X_train_sel.values\n",
        "                            #yv = Y_train_sel.values.ravel()\n",
        "                            sx = StandardScaler().fit(X_train_sel.values)\n",
        "                            sy = StandardScaler().fit(Y_train_sel.values.reshape(-1,1))\n",
        "                            Xv = sx.transform(X_train_sel.values)\n",
        "                            yv = sy.transform(Y_train_sel.values.reshape(-1,1)).ravel()\n",
        "                            # Usar nuestro wrapper\n",
        "                            kr = RNNRegressor()\n",
        "                            # Distribuciones scipy.stats para Hyperband\n",
        "                            from scipy.stats import randint, uniform, loguniform\n",
        "                            hb_params = {\n",
        "                                'units': randint(10, 61),              # enteros 1060\n",
        "                                'dropout_rate': uniform(0.0, 0.5),      # continuo 00.5\n",
        "                                'learning_rate': loguniform(1e-4, 1e-2),# log-uniform 1e-41e-2\n",
        "                                'batch_size': randint(16, 257)         # enteros 16256\n",
        "                            }\n",
        "                            hb = HalvingRandomSearchCV(\n",
        "                                estimator=kr,\n",
        "                                param_distributions=hb_params,\n",
        "                                factor=3,\n",
        "                                resource='epochs',\n",
        "                                #max_resources=400,\n",
        "                                max_resources=50,\n",
        "                                scoring=selector_scoring.value,\n",
        "                                cv=cv.value,\n",
        "                                #cv=2,\n",
        "                                random_state=42,\n",
        "                                #n_jobs=-1,\n",
        "                                n_jobs=1,\n",
        "                                verbose=3,\n",
        "                                error_score='raise'\n",
        "                            )\n",
        "                            print(\" Ejecutando Hyperband\")\n",
        "                            hb.fit(Xv,yv)\n",
        "                            best, score, params = hb.best_estimator_, hb.best_score_, hb.best_params_\n",
        "\n",
        "                        # ============================================\n",
        "                        # Motor de optimizacin Optuna\n",
        "                        # ============================================\n",
        "                        elif motor=='Optuna':\n",
        "                            # Preparamos los datos\n",
        "                            #Xv = X_train_sel.values\n",
        "                            #yv = Y_train_sel.values.ravel()\n",
        "                            sx = StandardScaler().fit(X_train_sel.values)\n",
        "                            sy = StandardScaler().fit(Y_train_sel.values.reshape(-1,1))\n",
        "                            Xv = sx.transform(X_train_sel.values)\n",
        "                            yv = sy.transform(Y_train_sel.values.reshape(-1,1)).ravel()\n",
        "                            kr = RNNRegressor()  # wrapper limpio\n",
        "                            import optuna\n",
        "                            def objective(trial):\n",
        "                                # Sugerencia de hiperparmetros\n",
        "                                units = trial.suggest_int('units', 10, 20)\n",
        "                                dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
        "                                learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "                                epochs = trial.suggest_int('epochs', 50, 100)\n",
        "                                batch_size = trial.suggest_int('batch_size', 16, 256)\n",
        "                                # Instanciamos y entrenamos el modelo\n",
        "                                model = RNNRegressor(\n",
        "                                    units=units,\n",
        "                                    dropout_rate=dropout_rate,\n",
        "                                    learning_rate=learning_rate,\n",
        "                                    epochs=epochs,\n",
        "                                    batch_size=batch_size,\n",
        "                                    verbose=3\n",
        "                                )\n",
        "                                model.fit(Xv, yv)\n",
        "                                preds = model.predict(Xv)\n",
        "                                # Devolvemos la mtrica a optimizar\n",
        "                                if selector_scoring.value == 'r2':\n",
        "                                    return r2_score(yv, preds)\n",
        "                                elif selector_scoring.value == 'neg_mean_squared_error':\n",
        "                                    return -mean_squared_error(yv, preds)\n",
        "                                else:  # neg_mean_absolute_error\n",
        "                                    return -mean_absolute_error(yv, preds)\n",
        "                            # Creamos y ejecutamos el estudio\n",
        "                            study = optuna.create_study(\n",
        "                                direction='maximize'\n",
        "                                if selector_scoring.value == 'r2'\n",
        "                                else 'minimize'\n",
        "                            )\n",
        "                            study.optimize(objective, n_trials=n_iter.value)\n",
        "                            # Recuperamos los mejores parmetros y entrenamos el modelo final\n",
        "                            best_params = study.best_params\n",
        "                            best = RNNRegressor(**best_params)\n",
        "                            best.fit(Xv, yv)\n",
        "                            # Interpretamos el score devuelto\n",
        "                            if selector_scoring.value == 'r2':\n",
        "                                score = study.best_value\n",
        "                            else:\n",
        "                                score = -study.best_value\n",
        "                            params = best_params\n",
        "\n",
        "                        else:\n",
        "                            best=None\n",
        "                            score = None\n",
        "                            params = {}\n",
        "\n",
        "                        #  GUARDAR MODELO / METADATOS  \n",
        "                        guardar_rnn(best, score, metodo, motor,\n",
        "                                    selector_scoring, cols, Y_train,\n",
        "                                    sx, sy,                    # <-- ahora son obligatorios\n",
        "                                    study if motor=='Optuna' else None,\n",
        "                                    traza_out=salida_logs)\n",
        "                        # \n",
        "\n",
        "                        #  AADIR REGISTRO EN OPT_MODELS AQU \n",
        "                        # construimos el tag y las rutas (deberan coincidir con las usadas en guardar_rnn)\n",
        "                        ts       = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                        tag      = f\"rnn_{metodo.lower()}_{motor.lower()}_opt_{ts}\"\n",
        "                        model_f  = f\"modelos_opt/{tag}.keras\"\n",
        "                        meta_f   = f\"modelos_opt/{tag}_meta.pkl\"\n",
        "\n",
        "                        OPT_MODELS[('rnn', metodo, motor)] = {\n",
        "                            'model_path': model_f,\n",
        "                            'meta_path' : meta_f,\n",
        "                            'score'     : float(score),\n",
        "                            'metric'    : selector_scoring.value,   #  aqu la mtrica\n",
        "                            'params'    : params,\n",
        "                            'model'     : best,\n",
        "                            'cols'      : cols,\n",
        "                            'sx'        : sx,\n",
        "                            'sy'        : sy\n",
        "                        }\n",
        "                        # Si es Optuna, guarda tambin la ruta al study\n",
        "                        if motor.lower() == 'optuna' and study is not None:\n",
        "                            OPT_MODELS[('rnn', metodo, motor)]['study_path'] = f\"modelos_opt/{tag}_study.pkl\"\n",
        "\n",
        "                        # \n",
        "\n",
        "                        resultados.append({\n",
        "                            'metodo':metodo,\n",
        "                            'motor':motor,\n",
        "                            'score':score,\n",
        "                            'params':params,\n",
        "                            'best':best\n",
        "                        })\n",
        "                # Detener reloj\n",
        "                stop_event.set()\n",
        "                t.join()\n",
        "                # mostrar mejor\n",
        "                valid = [r for r in resultados if r['score'] is not None]\n",
        "                if valid:\n",
        "                    best_r = max(valid, key=lambda x: x['score'])\n",
        "                    print(\"\\n Mejor optimizacin:\")\n",
        "                    print(f\"  Mtodo: {best_r['metodo']}\")\n",
        "                    print(f\"  Motor: {best_r['motor']}\")\n",
        "                    print(f\"  Score: {best_r['score']:.4f}\")\n",
        "                    print(f\"  Params: {best_r['params']}\")\n",
        "\n",
        "                    # === Grficas del mejor modelo ===\n",
        "                    modelo_best = best_r['best']\n",
        "                    metodo_best = best_r['metodo']\n",
        "                    cols_best   = [sanitize_name(c) for c in RESUMEN_METODOS.get(metodo_best, [])]\n",
        "\n",
        "                    # Filtramos columnas vlidas que estn presentes\n",
        "                    effective_cols = [c for c in cols_best if c in X_test.columns]\n",
        "                    if len(effective_cols) < 2:\n",
        "                        print(f\" El mtodo '{metodo_best}' no tiene columnas vlidas para el Test. Se omite prediccin.\")\n",
        "                    else:\n",
        "                        X_test_vals = X_test[effective_cols].copy().values\n",
        "                        y_test_vals = Y_test.values.ravel()\n",
        "                        fechas      = FECHAS_test.values\n",
        "\n",
        "                        # 1) calcula la prediccin escalada\n",
        "                        y_pred_scaled = modelo_best.predict(X_test_vals)\n",
        "                        # 2) inversa de sy para volver a la escala original\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                        residuos = y_test_vals - y_pred\n",
        "\n",
        "                        # Grfico de puntos Y real. vs. Y predicho por fecha\n",
        "                        plt.figure(figsize=(8,3))\n",
        "                        plt.scatter(fechas, y_test_vals, label='Y real')\n",
        "                        plt.scatter(fechas, y_pred, label='Y predicha')\n",
        "                        plt.xlabel('Fecha')\n",
        "                        plt.ylabel('Y')\n",
        "                        plt.title('Y real vs predicha por Fecha')\n",
        "                        plt.legend()\n",
        "                        plt.tight_layout()\n",
        "                        plt.show()\n",
        "\n",
        "                        # Grfico scatter Y real vs Y predicha\n",
        "                        plt.figure(figsize=(5,5))\n",
        "                        plt.scatter(y_test_vals, y_pred, alpha=0.6)\n",
        "                        plt.plot([y_test_vals.min(), y_test_vals.max()], [y_test_vals.min(), y_test_vals.max()], 'r--', label='Ideal')\n",
        "                        plt.xlabel('Y real'); plt.ylabel('Y predicha'); plt.title('Y real vs Y predicha')\n",
        "                        plt.tight_layout(); plt.legend(); plt.show()\n",
        "\n",
        "                        # Histograma residuos con Normal y KDE\n",
        "                        plt.figure(figsize=(6,4))\n",
        "                        from scipy.stats import norm, gaussian_kde, shapiro\n",
        "                        n,bins,_ = plt.hist(residuos, bins=20, density=True, alpha=0.6, label='Residuos')\n",
        "                        mu, sd = norm.fit(residuos)\n",
        "                        x = np.linspace(bins.min(), bins.max(), 100)\n",
        "                        plt.plot(x, norm.pdf(x, mu, sd), 'r--', label='Normal')\n",
        "                        kde = gaussian_kde(residuos)\n",
        "                        plt.plot(x, kde(x), 'g-', label='KDE')\n",
        "                        plt.xlabel('Residuo'); plt.ylabel('Densidad'); plt.title('Histograma de residuos')\n",
        "                        plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "                        # Test de normalidad Shapiro-Wilk\n",
        "                        stat, p_value = shapiro(residuos)\n",
        "                        print(f\" Shapiro-Wilk: estadstico={stat:.4f}, p-valor={p_value:.4f} -> {'Normal' if p_value > 0.05 else 'No normal'}\")\n",
        "\n",
        "                else:\n",
        "                    print(\" No hay resultados con score definido.\")\n",
        "                # Tabla completa de optimizaciones\n",
        "                df_all=pd.DataFrame(resultados)\n",
        "                display(HTML(\"<h4> Todos los resultados de optimizacin</h4>\"))\n",
        "                display(df_all)\n",
        "\n",
        "        # enlazar callbacks\n",
        "        btn_cargar.on_click(cargar_variables)\n",
        "        btn_ejecutar.on_click(ejecutar_optimizacion)\n",
        "\n",
        "        # mostrar interfaz\n",
        "        display(HTML(\"<h3> Optimizacin RNN - Carga y Motores</h3>\"))\n",
        "        display(widgets.VBox([\n",
        "            selector_metodo,\n",
        "            btn_cargar,\n",
        "            salida_carga,\n",
        "            selector_motor,\n",
        "            selector_scoring,\n",
        "            cv,\n",
        "            n_iter,\n",
        "            progreso_reloj,\n",
        "            btn_ejecutar,\n",
        "            salida_logs\n",
        "        ]))\n",
        "\n",
        "# Mostrar contenedor al cargar la celda\n",
        "display(out_opt_rnn)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 10. INTERPRETACION xIA - FUNCIONA CORRECTAMENTE - REQUIERE AJUSTES\n",
        "# Este mdulo es el responsable de usar los modelos SHAP, LIME y KernelExplainer para explicar el comportamiento de los modelos de Modelado usados (SVR, NN y XGBoost)\n",
        "# ===============================================================\n",
        "import glob, io, base64, pickle, random, pprint\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "import os, re\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "# SHAP & LIME\n",
        "import shap\n",
        "from shap import TreeExplainer, KernelExplainer, GradientExplainer, DeepExplainer\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "# Scikitlearn\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "try:\n",
        "    from interpret.glassbox import ExplainableBoostingRegressor\n",
        "except ImportError:\n",
        "    ExplainableBoostingRegressor = None  # placeholder\n",
        "\n",
        "if 'xai_results' not in globals():                        # Nuevo para celda 12\n",
        "    xai_results = {}                                      # Nuevo para celda 12\n",
        "\n",
        "# Mapeo de nombres para bsqueda de archivos\n",
        "MODEL_KEYS = {\n",
        "    'SVR': 'svr',\n",
        "    'NN': 'nn',\n",
        "    'XGBoost': 'xgb',\n",
        "    'Random Forest': 'rf',\n",
        "    'RNN': 'rnn'\n",
        "}\n",
        "\n",
        "# Lista maestra de motores que tienes disponibles\n",
        "ALL_MOTORES = [\n",
        "    \"SHAP\",\n",
        "    \"LIME\",\n",
        "    \"KernelExplainer\",\n",
        "    \"Integrated Gradients\",\n",
        "    \"DeepLIFT / LRP\",\n",
        "    \"Permutation Feature Importance\",\n",
        "    \"Partial Dependence Plots (PDP)\",\n",
        "    \"Accumulated Local Effects (ALE)\",\n",
        "    \"Individual Conditional Expectation (ICE) Plots\",\n",
        "    \"Counterfactual Explanations\",\n",
        "    \"Anchors\",\n",
        "    \"Surrogate Models (Global/Local)\",\n",
        "    \"Explainable Boosting Machine (EBM)\",\n",
        "    \"Optuna Hyperparameter Importance\"\n",
        "]\n",
        "\n",
        "#  0) Inyectamos CSS para todos los Dropdowns y SelectMultiple \n",
        "display(HTML(\"\"\"\n",
        "<style>\n",
        "/* Ancho y tipografa de los widgets */\n",
        ".widget-dropdown, .widget-select-multiple, .widget-button {\n",
        "  width: 400px !important;       /* cajas ms anchas */\n",
        "  font-size: 14px !important;    /* texto de 14px */\n",
        "}\n",
        ".widget-dropdown > label, .widget-select-multiple > label {\n",
        "  font-size: 14px !important;    /* etiquetas tambin grandes */\n",
        "}\n",
        "</style>\n",
        "\"\"\"))\n",
        "\n",
        "# Listas de modelos entrenados y ptimos\n",
        "TRAINED_MODELS = [f\"{m}\" for m in MODEL_KEYS]\n",
        "OPTIMIZED_MODELS = [f\"{m}\" for m in MODEL_KEYS]\n",
        "# Mtodos de seleccin\n",
        "SELECT_METHODS = ['Pearson', 'Spearman', 'Mutualinfo', 'Boruta', 'UMAP']\n",
        "\n",
        "# xIA methods\n",
        "XAI_METHODS = [\n",
        "    'SHAP', 'LIME', 'KernelExplainer', 'Integrated Gradients',\n",
        "    'DeepLIFT / LRP', 'Permutation Feature Importance',\n",
        "    'Partial Dependence Plots (PDP)', 'Accumulated Local Effects (ALE)',\n",
        "    'Individual Conditional Expectation (ICE) Plots',\n",
        "    'Counterfactual Explanations', 'Anchors',\n",
        "    'Surrogate Models (Global/Local)', 'Explainable Boosting Machine (EBM)',\n",
        "    'Optuna Hyperparameter Importance', 'Todos'\n",
        "]\n",
        "\n",
        "XAI_HELP = {\n",
        "    'SHAP': (\n",
        "        '<h4>SHAP</h4>'\n",
        "        '<p><b>SHAP</b> (SHapley Additive exPlanations) utiliza teora de juegos para descomponer '\n",
        "        'la prediccin de un modelo en aportes aditivos de cada caracterstica.</p>'\n",
        "        '<p><b>Valores SHAP:</b> representan la contribucin de cada variable a la prediccin final.</p>'\n",
        "        '<ul>'\n",
        "        '<li><b>Empuje hacia arriba (positivo):</b> indica que la variable incrementa la prediccin respecto al valor base.</li>'\n",
        "        '<li><b>Empuje hacia abajo (negativo):</b> indica que la variable decrementa la prediccin respecto al valor base.</li>'\n",
        "        '</ul>'\n",
        "        '<p>El <i>valor base</i> es la prediccin promedio del modelo sin conocer ninguna caracterstica.</p>'\n",
        "        '<p>Para un punto de datos, la suma de los valores SHAP ms el valor base equivale a la prediccin del modelo.</p>'\n",
        "    ),\n",
        "    'LIME': (\n",
        "        '<h4>LIME</h4>'\n",
        "        '<p><b>LIME</b> (Local Interpretable Model-agnostic Explanations) explica la prediccin '\n",
        "        'de cualquier modelo construyendo, en la vecindad de la instancia, un modelo lineal simple.</p>'\n",
        "        '<ul>'\n",
        "          '<li>Se perturban aleatoriamente las caractersticas de la muestra.</li>'\n",
        "          '<li>Se calcula la prediccin del modelo negro.</li>'\n",
        "          '<li>Se ajusta una funcin lineal ponderada por proximidad al punto original.</li>'\n",
        "          '<li>Los coeficientes resultantes indican direccin y magnitud de influencia local.</li>'\n",
        "        '</ul>'\n",
        "        '<p><b>Interpretacin:</b> coeficiente positivo  la caracterstica empuja la prediccin hacia arriba; '\n",
        "        'coeficiente negativo  la empuja hacia abajo.</p>'\n",
        "    ),\n",
        "    'KernelExplainer': (\n",
        "        '<h4>KernelExplainer</h4>'\n",
        "        '<p><b>KernelExplainer</b> es una extensin de SHAP para modelos de caja negra, '\n",
        "        'usando un ncleo de similitud para aproximar valores SHAP sin requerir acceso a gradientes.</p>'\n",
        "        '<ul>'\n",
        "        '<li><b>Fondo (background):</b> subconjunto de datos para estimar valor base.</li>'\n",
        "        '<li><b>Valor base:</b> prediccin promedio del fondo.</li>'\n",
        "        '<li><b>Valores Kernel SHAP:</b> contribuciones de cada caracterstica calculadas mediante ponderaciones del ncleo.</li>'\n",
        "        '<li><b>Empuje hacia arriba (positivo):</b> la caracterstica aumenta la prediccin con respecto al valor base.</li>'\n",
        "        '<li><b>Empuje hacia abajo (negativo):</b> la caracterstica disminuye la prediccin.</li>'\n",
        "        '</ul>'\n",
        "        '<p>El mtodo pesa cada combinacin de caractersticas segn su similitud al punto de inters, '\n",
        "        'ofreciendo explicaciones globales y locales.</p>'\n",
        "        '<p><i>Interpretacin de tablas:</i> cada fila es una muestra, columnas son caractersticas; '\n",
        "        'valores muestran su empuje.</p>'\n",
        "        '<p><i>Interpretacin del grfico summary:</i> distribucin de valores Kernel SHAP, '\n",
        "        'el color indica magnitud de la caracterstica.</p>'\n",
        "    ),\n",
        "    'Integrated Gradients': (\n",
        "        '<h4>Integrated Gradients</h4>'\n",
        "        '<p><b>Integrated Gradients</b> es un mtodo de atribucin para modelos diferenciables, '\n",
        "        'que integra gradientes desde una referencia (p.ej. vector cero) hasta la instancia objetivo.</p>'\n",
        "        '<ul>'\n",
        "        '<li><b>Ruta de integracin:</b> lnea recta desde referencia hasta punto de inters en el espacio de caractersticas.</li>'\n",
        "        '<li><b>Atributos IG:</b> promedio de gradientes a lo largo de la ruta, ponderando contribuciones.</li>'\n",
        "        '<li><b>Interpretacin:</b> valores positivos  aumento de prediccin; valores negativos  disminucin.</li>'\n",
        "        '</ul>'\n",
        "        '<p><i>Interpretacin de tablas:</i> cada fila es una muestra, columnas son caractersticas; '\n",
        "        'valores muestran la contribucin integrada.</p>'\n",
        "        '<p><i>Interpretacin de grfico:</i> distribucin de valores IG, destacando variables con mayores efectos acumulados.</p>'\n",
        "    ),\n",
        "    'DeepLIFT / LRP': (\n",
        "        '<h4>DeepLIFT / LRP</h4>'\n",
        "        '<p><b>DeepLIFT</b> y <b>Layer-wise Relevance Propagation (LRP)</b> son mtodos de atribucin que '\n",
        "        'propagan la relevancia de la salida de la red hacia atrs a cada neurona de entrada.</p>'\n",
        "        '<ul>'\n",
        "        '<li><b>Relevancia positiva:</b> indica que la caracterstica contribuy a aumentar la salida.</li>'\n",
        "        '<li><b>Relevancia negativa:</b> indica que la caracterstica contribuy a disminuir la salida.</li>'\n",
        "        '</ul>'\n",
        "        '<p>La suma de las relevancias de entrada equivale a la activacin de salida menos la referencia.</p>'\n",
        "        '<p><b>Interpretacin de tabla:</b> cada fila es una muestra, columnas son caractersticas y valores de relevancia.</p>'\n",
        "        '<p><b>Interpretacin de grfico:</b> barras muestran relevancia media global.</p>'\n",
        "    ),\n",
        "    'Permutation Feature Importance': (\n",
        "        '<h4>Permutation Feature Importance</h4>'\n",
        "        '<p>Mide la importancia de cada caracterstica evaluando la cada en rendimiento '\n",
        "        'al permutar sus valores.</p>'\n",
        "        '<ul>'\n",
        "        '<li>Para cada variable, se permutan sus valores en el dataset de prueba.</li>'\n",
        "        '<li>Se mide la diferencia en la mtrica (p.ej. R).</li>'\n",
        "        '<li>Una cada mayor indica mayor importancia de esa variable.</li>'\n",
        "        '</ul>'\n",
        "        '<p>Interpretacin de la tabla:</p>'\n",
        "        '<ul>'\n",
        "        '<li><b>Mean Importance:</b> promedio de las cadas de rendimiento tras permutar.</li>'\n",
        "        '<li><b>Std Importance:</b> variabilidad en esos descensos.</li>'\n",
        "        '</ul>'\n",
        "        '<p>Interpretacin del grfico:</p>'\n",
        "        '<ul>'\n",
        "        '<li>Puntos representan importancia media; barras de error, desviacin estndar.</li>'\n",
        "        '</ul>'\n",
        "    ),\n",
        "    'Partial Dependence Plots (PDP)': (\n",
        "        '<h4>Partial Dependence Plots (PDP)</h4>'\n",
        "        '<p><b>Partial Dependence Plots</b> (PDP) permiten visualizar el efecto medio que tiene una variable (o par de variables) '\n",
        "        'sobre la prediccin de un modelo, manteniendo fijas todas las dems. Es un mtodo global y agnstico al modelo, muy til para '\n",
        "        'entender la direccin (positiva o negativa) y la magnitud del impacto de cada caracterstica.</p>'\n",
        "\n",
        "        '<h5>Cmo se calcula?</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Se selecciona una variable y se construye una rejilla de valores representativos en su rango.</li>'\n",
        "        '<li>Para cada valor de la rejilla, se reemplaza dicha variable en todas las observaciones del conjunto de datos con ese valor.</li>'\n",
        "        '<li>Se predice el valor objetivo para este nuevo dataset y se calcula la media de las predicciones.</li>'\n",
        "        '<li>Estos valores promedio constituyen la <b>curva PDP</b> de la variable.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Interpretacin:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>El <b>eje X</b> representa los valores posibles de la variable analizada.</li>'\n",
        "        '<li>El <b>eje Y</b> representa la prediccin promedio del modelo cuando la variable toma esos valores.</li>'\n",
        "        '<li>Una <b>pendiente positiva</b> indica que un aumento de la variable incrementa la prediccin media.</li>'\n",
        "        '<li>Una <b>pendiente negativa</b> indica que un aumento de la variable reduce la prediccin media.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Importancia global con PDP:</h5>'\n",
        "        '<p>En este motor se calcula como el <b>rango</b> (mximo menos mnimo) de la curva PDP. '\n",
        "        'Esto representa cunto puede cambiar la prediccin promedio si se modifica la variable a lo largo de todo su dominio. '\n",
        "        'Cuanto mayor el rango, mayor es la importancia de esa variable en el comportamiento del modelo.</p>'\n",
        "\n",
        "        '<h5>Importancia local:</h5>'\n",
        "        '<p>Tambin se calcula una tabla con los efectos PDP para las primeras muestras del conjunto de datos. '\n",
        "        'Para cada muestra y variable, se evala cunto cambia la prediccin promedio cuando se fija la variable al valor observado '\n",
        "        'y se mantiene el resto con su distribucin real. Esto permite interpretar el efecto individual de cada caracterstica '\n",
        "        'en una muestra especfica, de forma anloga a SHAP o LIME.</p>'\n",
        "\n",
        "        '<h5>Limitaciones:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>No tiene en cuenta interacciones entre variables (a menos que se usen PDP bivariados).</li>'\n",
        "        '<li>Puede ser engaoso si hay fuerte correlacin entre variables (reemplazar una variable puede generar combinaciones no plausibles).</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<p><i>En resumen:</i> PDP ayuda a responder: <b>Cmo vara la prediccin promedio del modelo cuando cambio una variable concreta?</b></p>'\n",
        "    ),\n",
        "    'Accumulated Local Effects (ALE)': (\n",
        "        '<h4>Accumulated Local Effects (ALE)</h4>'\n",
        "        '<p><b>ALE</b> (Efectos Locales Acumulados) es un mtodo de interpretabilidad global que muestra '\n",
        "        'cmo cambia la prediccin promedio del modelo cuando una caracterstica vara dentro de su dominio, '\n",
        "        'teniendo en cuenta las correlaciones entre variables.</p>'\n",
        "\n",
        "        '<h5>Cmo se calcula?</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Se divide el rango de una variable en <i>bins</i> (intervalos), usualmente basados en cuantiles para que contengan un nmero similar de observaciones.</li>'\n",
        "        '<li>En cada bin, se mide el efecto local de cambiar el valor de la variable desde el lmite inferior al superior, manteniendo fijas las dems variables.</li>'\n",
        "        '<li>Estos efectos se acumulan a lo largo de los bins, produciendo una <b>curva ALE</b> que muestra cmo influye la variable sobre la prediccin.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Ventajas frente a PDP:</h5>'\n",
        "        '<ul>'\n",
        "        '<li><b>Robusto ante correlaciones:</b> A diferencia de PDP, ALE no genera combinaciones irreales de variables, ya que respeta la distribucin original de los datos.</li>'\n",
        "        '<li><b>Computacionalmente eficiente:</b> Slo evala muestras dentro de cada bin, evitando duplicaciones masivas.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Interpretacin:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>El eje <b>X</b> representa el valor de la variable (centrado por defecto).</li>'\n",
        "        '<li>El eje <b>Y</b> representa el efecto acumulado sobre la prediccin del modelo.</li>'\n",
        "        '<li>Una pendiente positiva indica que aumentar esa variable tiende a aumentar la prediccin.</li>'\n",
        "        '<li>Una pendiente negativa indica lo contrario.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Importancia global:</h5>'\n",
        "        '<p>En este motor, la <b>importancia global</b> de una variable se calcula como el <b>rango</b> '\n",
        "        'de su curva ALE (es decir, la diferencia entre el valor mximo y mnimo del efecto acumulado). '\n",
        "        'Un mayor rango implica mayor impacto medio de esa caracterstica sobre la salida del modelo.</p>'\n",
        "\n",
        "        '<h5>Importancia local:</h5>'\n",
        "        '<p>Se muestran los valores ALE correspondientes a las primeras muestras del conjunto de datos. '\n",
        "        'Estos valores permiten entender el efecto individual de cada variable sobre la prediccin de cada observacin.</p>'\n",
        "\n",
        "        '<h5>Limitaciones:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Asume que los efectos son aditivos y no modela explcitamente interacciones (aunque puede ampliarse con ALE 2D).</li>'\n",
        "        '<li>Puede suavizar excesivamente relaciones no lineales muy complejas si se usan pocos bins.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<p><i>Resumen:</i> ALE es una tcnica moderna, confiable y robusta para evaluar la importancia de cada variable '\n",
        "        'respetando la estructura estadstica real del dataset.</p>'\n",
        "    ),\n",
        "    'Individual Conditional Expectation (ICE) Plots': (\n",
        "        '<h4>Individual Conditional Expectation (ICE) Plots</h4>'\n",
        "        '<p><b>ICE</b> es una tcnica de interpretabilidad local que representa cmo vara la prediccin de un modelo '\n",
        "        'para una observacin concreta cuando se modifica una de sus caractersticas, manteniendo fijas las dems. '\n",
        "        'Es una generalizacin del mtodo PDP (Partial Dependence Plots), pero a nivel individual.</p>'\n",
        "\n",
        "        '<h5>Cmo se calcula?</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Para una observacin concreta, se generan mltiples versiones de ella cambiando solo una variable (por ejemplo, X1) a lo largo de un rango de valores.</li>'\n",
        "        '<li>Se evala el modelo sobre estas versiones y se obtienen las predicciones correspondientes.</li>'\n",
        "        '<li>La curva resultante muestra cmo cambia la salida del modelo solo por esa variable en esa observacin concreta.</li>'\n",
        "        '<li>Repitiendo esto para varias muestras, se obtiene un conjunto de curvas ICE que capturan efectos individuales.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Interpretacin:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Las curvas muestran cmo cambia la prediccin de cada muestra cuando se vara una caracterstica.</li>'\n",
        "        '<li>Permiten detectar <b>interacciones no lineales</b>, <b>heterogeneidad de efectos</b> o <b>inestabilidad</b> en el modelo.</li>'\n",
        "        '<li>Cuando todas las curvas son paralelas, la relacin es globalmente estable (similar al PDP).</li>'\n",
        "        '<li>Cuando las curvas difieren fuertemente entre s, la variable tiene un efecto que depende del resto del contexto (otras variables).</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Importancia global:</h5>'\n",
        "        '<p>En este motor, la importancia global de una variable se calcula como la <b>media del rango</b> de las curvas ICE sobre un subconjunto de muestras. '\n",
        "        'Este valor representa cunto cambia, en promedio, la prediccin individual cuando se vara esa caracterstica. '\n",
        "        'Un mayor rango indica mayor influencia.</p>'\n",
        "\n",
        "        '<h5>Importancia local:</h5>'\n",
        "        '<p>Se muestra tambin una tabla con los <b>rangos individuales ICE</b> para las primeras observaciones del conjunto. '\n",
        "        'Esto permite ver cmo de sensible es cada muestra a cambios en una variable concreta.</p>'\n",
        "\n",
        "        '<h5>Ventajas:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Captura <b>efectos individuales</b>, no promedios, lo que permite diagnsticos precisos por observacin.</li>'\n",
        "        '<li>Permite detectar comportamientos atpicos, interacciones complejas y sesgos locales.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Limitaciones:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Puede ser ruidoso si se muestran demasiadas curvas simultneamente.</li>'\n",
        "        '<li>Como PDP, puede generar combinaciones irreales si las variables estn fuertemente correlacionadas.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<p><i>Resumen:</i> ICE muestra <b>cmo cambia la prediccin para cada observacin</b> al modificar una variable especfica, '\n",
        "        'ofreciendo una perspectiva individual que complementa la visin global de otros mtodos.</p>'\n",
        "    ),\n",
        "    'Counterfactual Explanations' : (\n",
        "        '<h4>Counterfactual Explanations</h4>'\n",
        "        '<p>Las <b>explicaciones contrafactuales</b> muestran cmo debe modificarse una observacin para que el modelo devuelva una prediccin significativamente distinta, con el menor cambio posible en sus variables.</p>'\n",
        "\n",
        "        '<h5>Cmo funciona?</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Se parte de una prediccin original y se busca un valor deseado que suponga un cambio relevante (por ejemplo, un +10%).</li>'\n",
        "        '<li>Se recorren los datos reales del conjunto de entrenamiento en busca de observaciones cuya prediccin cumpla ese cambio.</li>'\n",
        "        '<li>De entre estas, se selecciona la ms cercana (menor distancia) respecto a la observacin original.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Interpretacin:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>La <b>tabla local</b> muestra los contrafactuales ms cercanos para las primeras observaciones.</li>'\n",
        "        '<li>La <b>importancia global</b> indica el cambio medio absoluto necesario en cada variable para alcanzar el objetivo deseado.</li>'\n",
        "        '<li>El <b>grfico</b> ayuda a identificar qu variables son ms influyentes a la hora de cambiar el resultado del modelo.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Ventajas:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Interpretabilidad muy intuitiva: responde a la pregunta \"qu tendra que cambiar para obtener otro resultado?\"</li>'\n",
        "        '<li>Utiliza ejemplos reales del dataset, evitando combinaciones irreales.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Limitaciones:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Puede no encontrar contrafactuales viables si el cambio deseado es muy ambicioso o los datos estn muy restringidos.</li>'\n",
        "        '<li>Los resultados dependen de la densidad del dataset y de su cobertura del espacio de entrada.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<p><i>Resumen:</i> Las explicaciones contrafactuales ofrecen una herramienta poderosa y humana para entender qu modificaciones mnimas podran generar resultados ms deseables en un modelo de regresin.</p>'\n",
        "    ),\n",
        "    'Anchors' : (\n",
        "        '<h4>Anchors</h4>'\n",
        "        '<p><b>Anchors</b> es un mtodo de explicabilidad local que identifica reglas sencillas tipo \"SI/ENTONCES\" que justifican una prediccin concreta. Estas reglas actan como <b>anclas</b>, es decir, condiciones que al cumplirse aseguran con alta probabilidad que la prediccin del modelo se mantenga inalterada.</p>'\n",
        "\n",
        "        '<h5>Cmo se generan?</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Para cada muestra, se selecciona un conjunto de observaciones vecinas (por ejemplo, mediante muestreo aleatorio).</li>'\n",
        "        '<li>Se binariza la variable de salida en funcin del valor de la muestra objetivo.</li>'\n",
        "        '<li>Se entrena un rbol de decisin con profundidad limitada para detectar las reglas que mejor separan los datos segn esa binarizacin.</li>'\n",
        "        '<li>Se extrae la regla correspondiente a la muestra objetivo (el camino en el rbol).</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Interpretacin de resultados:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>La <b>tabla de reglas ancla</b> muestra, para cada muestra explicada, la regla encontrada, su <i>cobertura</i> (porcentaje de vecinos que la cumplen) y su <i>precisin</i> (porcentaje de vecinos cubiertos cuya prediccin coincide con la de la muestra).</li>'\n",
        "        '<li>La <b>tabla de importancia global</b> refleja la frecuencia con la que cada variable aparece en las reglas generadas para las distintas muestras.</li>'\n",
        "        '<li>El <b>grfico de dispersin</b> permite visualizar qu variables son ms recurrentes en las reglas ancla, ayudando a detectar aquellas ms influyentes en decisiones locales.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Ventajas:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Alta interpretabilidad, ya que genera explicaciones similares a reglas humanas simples.</li>'\n",
        "        '<li>Evala tanto precisin como cobertura, ofreciendo una visin balanceada de la fiabilidad de la explicacin.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Limitaciones:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>No siempre se pueden encontrar reglas con buena cobertura y precisin.</li>'\n",
        "        '<li>Las explicaciones pueden ser sensibles a la seleccin de vecinos y a la profundidad del rbol.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<p><i>Resumen:</i> Anchors permite entender las predicciones de un modelo mediante reglas simples que fijan su comportamiento local. Es especialmente til cuando se busca justificar decisiones modelo en trminos comprensibles y accionables.</p>'\n",
        "    ),\n",
        "    'Surrogate Models (Global/Local)' : (\n",
        "        '<h4>Surrogate Models (Global/Local)</h4>'\n",
        "        '<p>Los modelos sustitutos permiten interpretar el comportamiento de un modelo complejo mediante un segundo modelo interpretable que lo imita. En este motor se utilizan dos enfoques complementarios:</p>'\n",
        "        '<h5> Surrogate Global</h5>'\n",
        "        '<p>Se construye un rbol de decisin de profundidad limitada que se entrena sobre el dataset completo, utilizando como variable dependiente la salida del modelo original. El rbol acta como un \"modelo proxy\" que resume las reglas generales de decisin del modelo complejo. La tabla resultante muestra la importancia relativa de cada variable en el rbol (contribucin a la reduccin del error).</p>'\n",
        "        '<h5> Surrogate Local</h5>'\n",
        "        '<p>Para cada una de las primeras muestras se construye un modelo lineal ajustado en su vecindario ms cercano. Esto permite identificar qu variables tienen mayor influencia en cada caso individual. Se calcula el valor medio absoluto de los coeficientes para todas las variables y se presenta como una medida de importancia local.</p>'\n",
        "        '<h5> Comparacin grfica</h5>'\n",
        "        '<p>El grfico compara visualmente la importancia de cada variable en el modelo global frente a su influencia local promedio. Las variables con alta importancia en ambos enfoques son especialmente robustas. En cambio, si una variable tiene alta influencia local pero no global (o viceversa), puede indicar comportamientos especficos o inconsistencias locales.</p>'\n",
        "        '<h5> Utilidad</h5>'\n",
        "        '<p>Este enfoque resulta til cuando se desea contrastar patrones globales con explicaciones locales, evaluar consistencia en la importancia de las variables o detectar sesgos o excepciones locales en el modelo.</p>'\n",
        "        '<h4>Surrogate Models (Global/Local)</h4>'\n",
        "        '<p>Un <b>modelo sustituto</b> imita el comportamiento del modelo complejo con un algoritmo interpretable.</p>'\n",
        "        '<h5>Global</h5><p>rbol de decisin entrenado sobre todo el dataset.</p>'\n",
        "        '<h5>Local</h5><p>Regresiones lineales ajustadas en vecindarios de cada muestra.</p>'\n",
        "        '<p>Las tablas y el grfico resumen la importancia de variables a ambas escalas.</p>'\n",
        "    ),\n",
        "    'Explainable Boosting Machine (EBM)' : (\n",
        "        '<h4>Explainable Boosting Machine (EBM)</h4>'\n",
        "        '<p>EBM (Explainable Boosting Machine) es un modelo de aprendizaje automtico de tipo aditivo generalizado (GAM) que combina interpretabilidad total con capacidad predictiva competitiva.</p>'\n",
        "        '<p>EBM se basa en boosting de rboles muy pequeos (stumps) que se agregan para aprender funciones univariantes (una por variable) o bivariantes (combinaciones seleccionadas automticamente). Estas funciones se combinan de forma aditiva para producir la prediccin.</p>'\n",
        "        '<h5> Funcionamiento:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Para cada variable, se ajusta una funcin parcial que explica su contribucin a la prediccin.</li>'\n",
        "        '<li>Estas funciones se aprenden de forma secuencial y se corrigen entre s (boosting).</li>'\n",
        "        '<li>Al final, la prediccin total es la suma de todas las contribuciones univariantes + un sesgo.</li>'\n",
        "        '</ul>'\n",
        "        '<h5> Salidas del motor:</h5>'\n",
        "        '<ul>'\n",
        "        '<li><b>Importancia Global</b>: ganancia relativa de cada funcin parcial, ordenada de mayor a menor.</li>'\n",
        "        '<li><b>Tabla Local</b>: muestra para las primeras muestras cunto contribuye cada variable (positiva o negativamente) al valor final predicho.</li>'\n",
        "        '<li><b>Grfico de media absoluta</b>: la media de las contribuciones absolutas refleja la influencia promedio de cada variable.</li>'\n",
        "        '</ul>'\n",
        "        '<h5> Interpretabilidad:</h5>'\n",
        "        '<p>EBM permite visualizar cada funcin de forma directa: cmo cambia la prediccin segn los valores de una variable. Adems, se pueden explorar interacciones seleccionadas por el modelo.</p>'\n",
        "        '<h5> Utilidad:</h5>'\n",
        "        '<p>EBM es especialmente til cuando se requiere una explicacin precisa, reproducible y completamente interpretable del modelo, sin necesidad de tcnicas post-hoc.</p>'\n",
        "    ),\n",
        "    'Optuna Hyperparameter Importance' : (\n",
        "        '<h4>Optuna Hyperparameter Importance</h4>'\n",
        "        '<p>Este motor de interpretabilidad analiza el impacto de cada hiperparmetro en la mtrica objetivo utilizada durante la optimizacin automtica con Optuna.</p>'\n",
        "        '<h5> Cmo funciona?</h5>'\n",
        "        '<p>El motor se basa en el mdulo <code>optuna.importance</code>, que estima la importancia de cada hiperparmetro utilizando tcnicas basadas en permutaciones o regresin de sustitucin. Evala cmo vara la mtrica objetivo (por ejemplo, el error) cuando se altera un hiperparmetro en particular, manteniendo los dems fijos.</p>'\n",
        "        '<ul>'\n",
        "        '<li>Se utiliza un <code>study</code> previamente entrenado (en memoria o desde un archivo).</li>'\n",
        "        '<li>Se extraen los <code>trials</code> y se aplica el mtodo <code>get_param_importances()</code> para obtener las contribuciones relativas.</li>'\n",
        "        '</ul>'\n",
        "        '<h5> Salidas interpretables</h5>'\n",
        "        '<ul>'\n",
        "        '<li><b>Tabla de Importancia Global</b>: muestra el porcentaje de influencia de cada hiperparmetro en la variabilidad del resultado. Cuanto mayor sea la contribucin, ms crtico es ese parmetro para mejorar el rendimiento del modelo.</li>'\n",
        "        '<li><b>Top 10 Trials</b>: recoge los 10 mejores ensayos (trials) con sus hiperparmetros y resultados, lo que permite identificar configuraciones ptimas.</li>'\n",
        "        '<li><b>Grfico de barras</b>: visualiza la importancia relativa de los hiperparmetros, facilitando su comparacin directa.</li>'\n",
        "        '</ul>'\n",
        "        '<h5> Utilidad prctica</h5>'\n",
        "        '<p>Esta herramienta es especialmente til para:</p>'\n",
        "        '<ul>'\n",
        "        '<li>Identificar qu hiperparmetros son verdaderamente influyentes y cules se pueden fijar o descartar.</li>'\n",
        "        '<li>Reducir el espacio de bsqueda para futuras optimizaciones.</li>'\n",
        "        '<li>Justificar decisiones sobre tuning del modelo de forma objetiva y visual.</li>'\n",
        "        '</ul>'\n",
        "    ),\n",
        "    'Todos': '<b>Todos</b>: Mostrar todas las explicaciones anteriores.'\n",
        "}\n",
        "\n",
        "# ---------------- Ajustes visuales ------------------------------\n",
        "N_SHAP_SAMPLES      = 50   # muestras para SHAP\n",
        "N_SHAP_BACKGROUND   = 50   # cuntas filas usar como fondo para KernelExplainer\n",
        "N_LIME_SAMPLES      = 50   # muestras para LIME\n",
        "N_KERNEL_SAMPLES    = 50   # muestras para KERNEL EXPLAINER\n",
        "N_KERNEL_BACKGROUND = 50   # cuntas filas usar como fondo para KernelExplainer\n",
        "N_DEEP_SAMPLES      = 50   # muestras para DeepLIFT\n",
        "N_PERM_SAMPLES      = 50   # muestras para Permutation\n",
        "N_PDP_SAMPLES       = 50   # muestras para PDP\n",
        "N_ALE_SAMPLES       = 50   # muestras para ALE\n",
        "N_ICE_SAMPLES       = 50   # muestras para ICE\n",
        "N_CF_SAMPLES        = 50   # muestras para Counterfactuales\n",
        "N_ANCHOR_SAMPLES    = 50   # muestras para Anchors\n",
        "N_SURR_SAMPLES      = 100  # muestras para Surrogate\n",
        "N_EBM_SAMPLES       = 200  # muestras para EBM\n",
        "GRID_RES            = 20   # puntos para PDP/ICE\n",
        "FIRST_SAMPLES       = 10   # filas tablas locales\n",
        "ALE_BINS            = 20   # bins para ALE\n",
        "ICE_SAMPLES         = 50   # n mximo de curvas ICE para importancia global\n",
        "CF_SAMPLES          = 20   # n muestras para contrafactuales\n",
        "CF_TARGET_DELTA     = 0.1  # cambio relativo % deseado en salida (10%)\n",
        "ANC_NEIGHBORS       = 200  # vecinos para ajustar rbol local Anchors\n",
        "SURR_TREE_DEPTH     = 3    # profundidad del rbol sustituto global\n",
        "SURR_LOCAL_K        = 50   # n de vecinos en cada regresin local\n",
        "SURR_COLOR_GLOBAL   = \"#1f77b4\"   # color puntos globales\n",
        "SURR_COLOR_LOCAL    = \"#d62728\"   # color aspas locales\n",
        "EBM_MAX_ITERS       = 500  # nmero de iteraciones boosting\n",
        "OPTUNA_STUDY_FILE = \"optuna_study.pkl\"  # ruta por defecto (puede cambiarse)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Diagramas de ejemplo para la ayuda dinmica\n",
        "# ----------------------------------------------------------------\n",
        "def _load_optuna_study(path: str):\n",
        "    if optuna is None:\n",
        "        return None\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "\n",
        "def _fig_to_base64(fig):\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\"); plt.close(fig)\n",
        "    buf.seek(0)\n",
        "    return base64.b64encode(buf.read()).decode()\n",
        "\n",
        "def _generate_simple_bar(title, feats, vals, ylabel):\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.bar(feats, vals, color=[\"green\" if v>0 else \"red\" for v in vals]); ax.axhline(0,c=\"k\")\n",
        "    ax.set_title(title); ax.set_ylabel(ylabel); plt.tight_layout();\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "\n",
        "# --- SHAP ------------------------------------------------------\n",
        "_generate_shap_diagram = lambda : _generate_simple_bar(\"Ejemplo Valores SHAP\", [\"X1\",\"X2\",\"X3\",\"X4\"], [0.4,-0.3,0.2,-0.1], \"SHAP value\")\n",
        "# --- LIME ------------------------------------------------------\n",
        "_generate_lime_diagram = lambda : _generate_simple_bar(\"Ejemplo Pesos LIME\", [\"X1\",\"X2\",\"X3\",\"X4\"], [0.7,-0.5,0.25,-0.15], \"Peso LIME\")\n",
        "# --- Integrated Gradients --------------------------------------\n",
        "_generate_ig_diagram   = lambda : _generate_simple_bar(\"Ejemplo Integrated Gradients\", [\"X1\",\"X2\",\"X3\",\"X4\"], [0.3,-0.1,0.4,-0.2], \"IG value\")\n",
        "# --- DeepLIFT/LRP ----------------------------------------------\n",
        "_generate_dl_diagram   = lambda : _generate_simple_bar(\"Ejemplo DeepLIFT / LRP\", [\"X1\",\"X2\",\"X3\",\"X4\"], [0.2,-0.05,0.1,-0.15], \"Relevancia\")\n",
        "# --- Permutation Importance ------------------------------------\n",
        "def _generate_perm_diagram():\n",
        "    feats = [\"X1\", \"X2\", \"X3\", \"X4\"]; means = [0.18, 0.07, 0.12, 0.25]; stds = [0.02, 0.01, 0.015, 0.03]\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.errorbar(range(len(feats)), means, yerr=stds, fmt='o', capsize=5)\n",
        "    ax.set_xticks(range(len(feats))); ax.set_xticklabels(feats)\n",
        "    ax.set_title(\"Ejemplo Permutation Importance\"); ax.set_ylabel(\"Importancia media\"); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "# --- PDP --------------------------------------------------------\n",
        "def _generate_pdp_diagram():\n",
        "    feats = [\"X1\", \"X2\", \"X3\", \"X4\"]; rng = [0.8, 0.3, 0.5, 1.0]\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.scatter(range(4), rng); ax.set_xticks(range(4)); ax.set_xticklabels(feats)\n",
        "    ax.set_ylabel(\"Rango PDP\"); ax.set_title(\"Ejemplo Importancia PDP\"); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "# --- ALE --------------------------------------------------------\n",
        "def _generate_ale_diagram():\n",
        "    feats = [\"X1\", \"X2\", \"X3\", \"X4\"]; rng = [1.1, 0.2, 0.6, 0.9]\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.scatter(range(4), rng); ax.set_xticks(range(4)); ax.set_xticklabels(feats)\n",
        "    ax.set_ylabel(\"Rango ALE\"); ax.set_title(\"Ejemplo Importancia ALE\"); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "# --- ICE --------------------------------------------------------\n",
        "def _generate_ice_diagram():\n",
        "    feats=[\"X1\",\"X2\",\"X3\",\"X4\"]; rng=[0.6,0.15,0.35,0.75]\n",
        "    fig,ax=plt.subplots(figsize=(4,3)); ax.scatter(range(4),rng); ax.set_xticks(range(4)); ax.set_xticklabels(feats)\n",
        "    ax.set_ylabel(\"Rango ICE\"); ax.set_title(\"Ejemplo Importancia ICE\"); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "# --- Counterfactual ----------------------------------------------\n",
        "def _generate_cf_diagram():\n",
        "    feats=[\"X1\",\"X2\",\"X3\",\"X4\"]; costs=[0.2,0.05,0.12,0.3]\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.bar(feats, costs, color=\"steelblue\"); ax.set_title(\"Ejemplo Coste Counterfactual\"); ax.set_ylabel(\"|feature| medio\"); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "# --- Anchors ----------------------------------------------\n",
        "def _generate_anchor_diagram():\n",
        "    feats=[\"X1\",\"X2\",\"X3\",\"X4\"]; cover=[0.45,0.15,0.05,0.35]\n",
        "    fig,ax=plt.subplots(figsize=(4,3)); ax.bar(feats,cover,color=\"skyblue\"); ax.set_title(\"Cobertura Anchors ej.\"); ax.set_ylabel(\"Cobertura\"); plt.tight_layout();\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_b64(fig)}' width='300px'>\"\n",
        "# --- Surrogate Models (Global/Local) ----------------------------------------------\n",
        "def _generate_surr_diagram():\n",
        "    feats = [\"X1\", \"X2\", \"X3\", \"X4\"]; coef = [0.5, 0.1, 0.3, 0.2]\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.bar(feats, coef, color='goldenrod'); ax.set_title('Coeficientes Surrogate ej.'); ax.set_ylabel('|Coef|'); plt.tight_layout()\n",
        "    b64 = _fig_to_b64(fig)\n",
        "    return f\"<img src='data:image/png;base64,{b64}' width='300px'>\"\n",
        "# --- Explainable Boosting Machine (EBM) ----------------------------------------------\n",
        "def _generate_ebm_diagram():\n",
        "    feats=[\"X1\",\"X2\",\"X3\",\"X4\"]; gains=[0.25,0.1,0.15,0.35]\n",
        "    fig,ax=plt.subplots(figsize=(4,3)); ax.bar(feats,gains,color='seagreen');\n",
        "    ax.set_title('Importancia EBM ej.'); ax.set_ylabel('Ganancia relativa'); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_b64(fig)}' width='300px'>\"\n",
        "# --- Optuna Hyperparameter Importance ----------------------------------------------\n",
        "def _generate_optuna_diagram():\n",
        "    params=[\"lr\",\"depth\",\"n_estim\",\"subsample\"]; imp=[0.4,0.2,0.25,0.15]\n",
        "    fig,ax=plt.subplots(figsize=(4,3)); ax.bar(params,imp,color='mediumpurple'); ax.set_title('Importancia Optuna ej.'); ax.set_ylabel('Contribucin'); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_b64(fig)}' width='300px'>\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. Motor SHAP\n",
        "# ============================================================\n",
        "explainer = None\n",
        "def _motor_shap(key, model_obj, X, predict_fn):\n",
        "    \"\"\"SHAP completo: valores, tabla y grfico.\"\"\"\n",
        "    if key in ['xgb','rf']:\n",
        "        explainer = TreeExplainer(model_obj)\n",
        "        print(\"Verbose: Usando TreeExplainer\")\n",
        "    elif key=='svr':\n",
        "        background = shap.sample(X, min(N_SHAP_BACKGROUND, len(X)))\n",
        "        print(f\"Verbose: Background SVR sample shape: {background.shape}\")\n",
        "        explainer = KernelExplainer(predict_fn, background)\n",
        "        print(\"Verbose: Usando KernelExplainer para SVR\")\n",
        "    else:\n",
        "        background = shap.sample(X, min(N_SHAP_BACKGROUND, len(X)))\n",
        "        print(f\"Verbose: Background kernel sample shape: {background.shape}\")\n",
        "        explainer = KernelExplainer(predict_fn, background)\n",
        "        print(\"Verbose: Usando KernelExplainer\")\n",
        "\n",
        "    muestra = shap.sample(X, min(N_SHAP_SAMPLES, len(X)))\n",
        "    print(f\"Verbose: Muestra SHAP shape: {muestra.shape}\")\n",
        "    shap_vals = explainer.shap_values(muestra)\n",
        "    print(f\"Verbose: shap_vals shape: {np.array(shap_vals).shape}\")\n",
        "\n",
        "    # grfica SHAP\n",
        "    print(\"Verbose: Generando summary_plot...\")\n",
        "    shap.summary_plot(shap_vals, muestra, plot_type=\"dot\", show=False)\n",
        "    fig = plt.gcf()      # Nuevo para la celda 12\n",
        "    plt.show()\n",
        "    # === Explicacin de la grfica SHAP ===\n",
        "    display(HTML(\"\"\"\n",
        "    <h4>Interpretacin del grfico SHAP</h4>\n",
        "    <p>En el grfico summary_plot:<ul>\n",
        "    <li>Cada punto representa el efecto de una caracterstica en una muestra.</li>\n",
        "    <li>El eje X muestra el valor SHAP (positivo empuja la prediccin hacia arriba, negativo hacia abajo).</li>\n",
        "    <li>Los colores indican el valor de la caracterstica (rojo = alto, azul = bajo).</li>\n",
        "    </ul></p>\n",
        "    \"\"\"))\n",
        "\n",
        "    # tabla de valores SHAP\n",
        "    print(\"Verbose: Creando DataFrame de valores SHAP...\")\n",
        "    shap_df = pd.DataFrame(shap_vals, columns=X.columns)\n",
        "    display(HTML(\"<h4>Valores SHAP (primeras 10 muestras)</h4>\"))\n",
        "    display(shap_df.head(10))\n",
        "\n",
        "    # === Explicacin de la tabla de valores SHAP ===\n",
        "    display(HTML(\"\"\"\n",
        "    <h4>Interpretacin de la tabla de valores SHAP</h4>\n",
        "    <ul>\n",
        "    <li>Cada fila corresponde a una muestra (observacin).</li>\n",
        "    <li>Cada columna muestra el valor SHAP de esa caracterstica.</li>\n",
        "    <li>Valores positivos empujan la prediccin hacia arriba; negativos, hacia abajo.</li>\n",
        "    </ul>\n",
        "    \"\"\"))\n",
        "\n",
        "    # importancias globales\n",
        "    mean_abs = np.abs(shap_vals).mean(axis=0)\n",
        "    imp_df = pd.DataFrame({'feature':X.columns, 'mean_abs_shap':mean_abs})\n",
        "    imp_df = imp_df.sort_values('mean_abs_shap', ascending=False).reset_index(drop=True)\n",
        "    #imp_df = imp_df.sort_values('mean_abs_shap', ascending=False)\n",
        "    print(\"Verbose: Importancia global calculada\")\n",
        "    display(HTML(\"<h4>Importancia global (valor absoluto medio)</h4>\"))\n",
        "    display(imp_df)\n",
        "    # === Explicacin de la importancia global ===\n",
        "    display(HTML(\"\"\"\n",
        "    <h4>Interpretacin de la importancia global</h4>\n",
        "    <p>La importancia global ordena caractersticas por su valor absoluto medio de SHAP.</p>\n",
        "    <p>Valores ms altos indican mayor contribucin promedio al modelo.</p>\n",
        "    <p>Ejemplo: Una caracterstica con <i>mean_abs_shap</i>=0.5 contribuye en promedio 0.5 unidades a la prediccin.</p>\n",
        "    \"\"\"))\n",
        "    # Estadsticas adicionales  ---- Nuevo para la celda 12\n",
        "    stats = {\n",
        "        'shap_mean_abs_overall': float(mean_abs.mean()),\n",
        "        'shap_std_abs_overall': float(np.abs(shap_vals).std()),\n",
        "        'shap_imp_percentiles': imp_df['mean_abs_shap'].quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "    # Resultado  ---- Nuevo para la celda 12\n",
        "    resultado = {\n",
        "        'imp_df': imp_df.rename(columns={'mean_abs_shap':'value'}),\n",
        "        'df_local': shap_df,\n",
        "        'fig_summary': fig,\n",
        "        'stats': stats\n",
        "    }\n",
        "    return resultado\n",
        "pass\n",
        "\n",
        "# ============================================================\n",
        "# 2. Motor LIME\n",
        "# ============================================================\n",
        "explainer = None\n",
        "def _motor_lime(X, predict_fn):\n",
        "    \"\"\"LIME completo.\"\"\"\n",
        "    print(\"Verbose: Generando explicacin con LIME para\", N_LIME_SAMPLES, \"muestras...\")\n",
        "    explainer = LimeTabularExplainer(\n",
        "        training_data=X.values,\n",
        "        feature_names=X.columns.tolist(),\n",
        "        mode='regression'\n",
        "    )\n",
        "    # Muestreamos slo las primeras N_LIME_SAMPLES instancias\n",
        "    X_sample = X.iloc[:N_LIME_SAMPLES]\n",
        "\n",
        "    # calcular explicaciones para todas las muestras\n",
        "    local_weights = np.zeros((X_sample.shape[0], X_sample.shape[1]))\n",
        "    for i in range(X_sample.shape[0]):\n",
        "        exp = explainer.explain_instance(X_sample.values[i], predict_fn)\n",
        "        # usar ndice_feat de exp.as_map()[0]\n",
        "        for feat_idx, weight in exp.as_map()[0]:\n",
        "            local_weights[i, feat_idx] = weight\n",
        "\n",
        "    # tabla primeras 10 muestras\n",
        "    lime_df = pd.DataFrame(local_weights, columns=X.columns)\n",
        "    display(HTML(\"<h4>Valores LIME (primeras 10 muestras)</h4>\"))\n",
        "    display(lime_df.head(10))\n",
        "    # Explicacin tabla LIME\n",
        "    display(HTML(\n",
        "        '<p>En la tabla de LIME local, cada fila es una muestra y cada columna el peso asignado por LIME. '\n",
        "        'Valores positivos indican que la caracterstica aumenta la prediccin localmente, negativos la disminuyen.</p>'\n",
        "    ))\n",
        "\n",
        "    # importancia global media\n",
        "    mean_w = np.abs(local_weights).mean(axis=0)\n",
        "    #imp_df = pd.DataFrame({'feature':X.columns,'mean_abs_weight':mean_w}).sort_values('mean_abs_weight',ascending=False)\n",
        "    imp_df = pd.DataFrame({'feature': X.columns, 'mean_abs_weight': mean_w})\n",
        "    imp_df = imp_df.sort_values('mean_abs_weight', ascending=False).reset_index(drop=True)\n",
        "    display(HTML(\"<h4>Importancia global LIME</h4>\"))\n",
        "    display(imp_df)\n",
        "    # Explicacin importancia global LIME\n",
        "    display(HTML(\n",
        "        '<p>La importancia global de LIME se calcula como el valor absoluto medio de los pesos '\n",
        "        'a travs de todas las muestras. Caracterstica con mayor valor afecta ms la prediccin de manera local.</p>'\n",
        "    ))\n",
        "\n",
        "    # grfico LIME Value vs Feature Value\n",
        "    plt.figure(figsize=(8,6))\n",
        "    for idx, feat in enumerate(X.columns):\n",
        "        plt.scatter(X_sample[feat], local_weights[:,idx], label=feat, alpha=0.6)\n",
        "    plt.axhline(0,color='black',linewidth=0.8)\n",
        "    plt.xlabel('Valor de la caracterstica')\n",
        "    plt.ylabel('Peso LIME')\n",
        "    plt.title('LIME: Peso vs Valor de la caracterstica')\n",
        "    plt.legend(bbox_to_anchor=(1.05,1),loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()           # Nuevo para la celda 12\n",
        "    plt.show()\n",
        "    display(HTML(\n",
        "        '<p>En el grfico LIME Value vs Feature Value, cada punto representa una muestra. '\n",
        "        'La posicin vertical es el peso LIME, horizontal el valor original de la caracterstica. '\n",
        "        'Permite ver cmo cambia la influencia de la variable segn su valor.</p>'\n",
        "    ))\n",
        "\n",
        "    # Estadsticas adicionales  --- Nuevo para la celda 12\n",
        "    stats = {\n",
        "        'lime_imp_percentiles': imp_df['mean_abs_weight'].quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "\n",
        "    resultado = {\n",
        "        'imp_df': imp_df.rename(columns={'mean_abs_weight':'value'}),  # ['feature','value']\n",
        "        'df_local': lime_df,  # DataFrame de pesos locales\n",
        "        'fig_summary': fig,\n",
        "        'stats': stats\n",
        "    }\n",
        "    return resultado\n",
        "pass\n",
        "\n",
        "# ============================================================\n",
        "# 3. Motor KERNEL EXPLAINER\n",
        "# ============================================================\n",
        "#if 'KernelExplainer' in xai.value:\n",
        "def _motor_kernel(X, predict_fn):\n",
        "    \"\"\"KernelExplainer (SHAP).\"\"\"\n",
        "    print(\"Verbose: Calculando valores Kernel SHAP para\", N_KERNEL_SAMPLES, \" muestras...\")\n",
        "\n",
        "    # 1) Tomamos la muestra y el background de X\n",
        "    muestra     = shap.sample(X, min(N_KERNEL_SAMPLES, len(X)))\n",
        "    background  = shap.sample(X, min(N_KERNEL_BACKGROUND, len(X)))\n",
        "\n",
        "    # 2) Creamos el explainer y calculamos SHAPvalues solo para 'muestra'\n",
        "    ke_expl = KernelExplainer(predict_fn, background)\n",
        "    print(f\"[DEBUG] Calculando SHAP values para {len(muestra)} instancias\")\n",
        "    ke_vals = ke_expl.shap_values(muestra.values)  # matriz (M, p)\n",
        "\n",
        "    # 3) Importancia local y global\n",
        "    ke_df = pd.DataFrame(ke_vals, columns=X.columns)\n",
        "    mean_ke = np.abs(ke_vals).mean(axis=0)\n",
        "    imp_df = pd.DataFrame({'feature': X.columns, 'mean_abs_kernel': mean_ke})\n",
        "    imp_df = imp_df.sort_values('mean_abs_kernel', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # 4) Summary plot  **usar 'muestra'**, no 'X' completo\n",
        "    shap.summary_plot(ke_vals, muestra, show=False)   # <<< aqu estaba el error\n",
        "    fig = plt.gcf()   # recupera la figura actual\n",
        "\n",
        "    # 5) Estadsticas adicionales\n",
        "    stats = {\n",
        "        'kernel_shap_imp_percentiles': imp_df['mean_abs_kernel'].quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "\n",
        "    display(HTML(\"<h4>Valores Kernel SHAP (primeras 10 muestras)</h4>\"))\n",
        "    display(ke_df.head(10))\n",
        "    display(HTML(\n",
        "        '<p>En la tabla anterior, cada fila corresponde a una muestra y cada columna al valor Kernel SHAP de esa caracterstica. '\n",
        "        'Valores positivos indican empuje hacia arriba, negativos empuje hacia abajo.</p>'\n",
        "    ))\n",
        "    display(HTML(\"<h4>Importancia global Kernel SHAP</h4>\"))\n",
        "    display(imp_df)\n",
        "    display(HTML(\n",
        "        '<p>La tabla de importancia global muestra el valor absoluto medio del Kernel SHAP para cada caracterstica. '\n",
        "        'Caracterstica con valor ms alto tiene mayor impacto medio en las predicciones.</p>'\n",
        "    ))\n",
        "    plt.show()\n",
        "    display(HTML(\n",
        "        '<p>El grfico summary para Kernel SHAP funciona igual que SHAP: muestra distribucin de valores, mostrando heterogeneidad e influencia de cada variable.</p>'\n",
        "    ))\n",
        "\n",
        "    # 6) Construir resultado\n",
        "    resultado = {\n",
        "        'imp_df': imp_df.rename(columns={'mean_abs_kernel':'value'}),\n",
        "        'df_local': ke_df,\n",
        "        'fig_summary': fig,\n",
        "        'stats': stats\n",
        "    }\n",
        "    return resultado\n",
        "pass\n",
        "\n",
        "# ============================================================\n",
        "# 4. Motor INTEGRATED GRADIENTS\n",
        "# ============================================================\n",
        "def _motor_ig(key, model_obj, X, cols, sx, predict_fn):\n",
        "    \"\"\"\n",
        "    Integrated Gradients:\n",
        "     - Para SVR/NN/XGBoost/RF: usamos SHAP GradientExplainer.\n",
        "     - Para RNN: implementamos IG manual sobre la secuencia.\n",
        "    \"\"\"\n",
        "    import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "    from IPython.display import display, HTML\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # parmetros IG\n",
        "    STEPS   = 50     # pasos de interpolacin\n",
        "    TOP_N   = 5      # features globales a mostrar\n",
        "    N_LOCAL = 3      # muestras locales a mostrar\n",
        "\n",
        "    if key != 'rnn':\n",
        "        #  Camino original con SHAP GradientExplainer \n",
        "        from shap import GradientExplainer\n",
        "\n",
        "        background = sx.transform(\n",
        "            X.sample(min(200, len(X)), random_state=0)\n",
        "        )\n",
        "        ig_expl = GradientExplainer(model_obj, background)\n",
        "        vals = ig_expl.shap_values(sx.transform(X))\n",
        "\n",
        "        # si devuelve shape (n, p, 1): comprmelo a 2D\n",
        "        if isinstance(vals, np.ndarray) and vals.ndim == 3 and vals.shape[2] == 1:\n",
        "            vals = np.squeeze(vals, -1)\n",
        "\n",
        "        ig_vals = vals  # (n, p)\n",
        "        # DataFrame de valores locales\n",
        "        ig_df = pd.DataFrame(ig_vals, columns=cols)\n",
        "        # importancia global\n",
        "        mean_abs = np.abs(ig_vals).mean(axis=0)\n",
        "        imp_df = (\n",
        "            pd.DataFrame({'feature': cols, 'value': mean_abs})\n",
        "              .sort_values('value', ascending=False)\n",
        "              .reset_index(drop=True)\n",
        "        )\n",
        "        # grfico global\n",
        "        fig, ax = plt.subplots(figsize=(6,4))\n",
        "        ax.barh(imp_df['feature'].head(TOP_N)[::-1],\n",
        "                imp_df['value'].head(TOP_N)[::-1])\n",
        "        ax.set_title('IG: Top Global')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        display(HTML(\"<h4>IG: Importancia global (top features)</h4>\"))\n",
        "        display(imp_df.head(TOP_N))\n",
        "        display(fig)\n",
        "        display(HTML(\"<h4>IG: Valores locales (primeras muestras)</h4>\"))\n",
        "        display(ig_df.head(N_LOCAL))\n",
        "\n",
        "        return {'imp_df': imp_df, 'df_local': ig_df, 'fig_summary': fig}\n",
        "\n",
        "    else:\n",
        "        #  IG manual para RNN \n",
        "        # necesitamos X_seq en globals(): array NumPy 3D (n_samples, timesteps, features)\n",
        "        X_seq = globals().get('X_seq')\n",
        "        if X_seq is None:\n",
        "            raise RuntimeError(\"Para RNN necesitas tener `X_seq` en globals().\")\n",
        "\n",
        "        # submuestra\n",
        "        n_sub = min(20, X_seq.shape[0])\n",
        "        idxs  = np.random.RandomState(0).choice(X_seq.shape[0], n_sub, replace=False)\n",
        "        seqs_np     = X_seq[idxs]                              # NumPy array (n_sub, t, f)\n",
        "        baseline_np = np.zeros_like(seqs_np[0:1])              # (1, t, f)\n",
        "\n",
        "        # convertimos a tensores solo para el tape\n",
        "        seqs_t     = tf.convert_to_tensor(seqs_np,     dtype=tf.float32)\n",
        "        baseline_t = tf.convert_to_tensor(baseline_np, dtype=tf.float32)\n",
        "\n",
        "        # acumulador en NumPy\n",
        "        all_grads = np.zeros_like(seqs_np, dtype=float)       # (n_sub, t, f)\n",
        "\n",
        "        # bucle de interpolacin\n",
        "        for alpha in np.linspace(0, 1, STEPS):\n",
        "            interp = baseline_t + alpha * (seqs_t - baseline_t)\n",
        "            with tf.GradientTape() as tape:\n",
        "                tape.watch(interp)\n",
        "                preds = model_obj(interp)                      # (n_sub, 1)\n",
        "            grads_t = tape.gradient(preds, interp)             # tf.Tensor (n_sub, t, f)\n",
        "            grads_np = grads_t.numpy()                         # convertimos a NumPy\n",
        "            all_grads += grads_np\n",
        "\n",
        "        avg_grads = all_grads / STEPS                         # (n_sub, t, f)\n",
        "        ig_attribs = (seqs_np - baseline_np) * avg_grads      # (n_sub, t, f)\n",
        "\n",
        "        # importancia global: promedio absoluto sobre muestras y timesteps\n",
        "        global_imp = np.mean(np.abs(ig_attribs), axis=(0,1))  # (f,)\n",
        "        imp_df = (\n",
        "            pd.DataFrame({'feature': cols, 'value': global_imp})\n",
        "              .sort_values('value', ascending=False)\n",
        "              .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        timesteps = seqs_np.shape[1]\n",
        "\n",
        "        #  aqu vamos a dividir los locales por variable \n",
        "        display(HTML(\"<h4>IG RNN: Valores locales por variable</h4>\"))\n",
        "        for feat_idx, feat_name in enumerate(cols):\n",
        "            # extraemos la matriz (N_LOCAL, timesteps) para esta variable\n",
        "            local_mat = ig_attribs[:N_LOCAL, :, feat_idx]\n",
        "            local_df   = pd.DataFrame(\n",
        "                local_mat,\n",
        "                index=[f\"muestra {i+1}\" for i in range(local_mat.shape[0])],\n",
        "                columns=[f\"timestep {t}\" for t in range(timesteps)]\n",
        "            )\n",
        "            display(HTML(f\"<h5>Variable: {feat_name}</h5>\"))\n",
        "            display(local_df)\n",
        "\n",
        "        # grfico global RNN\n",
        "        fig, ax = plt.subplots(figsize=(6,4))\n",
        "        ax.barh(imp_df['feature'].head(TOP_N)[::-1],\n",
        "                imp_df['value'].head(TOP_N)[::-1])\n",
        "        ax.set_title('IG RNN: Top Global')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        display(HTML(\"<h4>IG RNN: Importancia global</h4>\"))\n",
        "        display(imp_df.head(TOP_N))\n",
        "        display(fig)\n",
        "\n",
        "        return {'imp_df': imp_df, 'df_local': local_df, 'fig_summary': fig}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. Motor DeepLIFT / LRP\n",
        "# ============================================================\n",
        "#if 'DeepLIFT / LRP' in xai.value:\n",
        "def _motor_dl(model_obj, key, X, cols, sx, sy):\n",
        "    \"\"\"DeepLIFT / LRP.\"\"\"\n",
        "    print(\" Calculando DeepLIFT/LRP para todas las muestras...\")\n",
        "    from shap import GradientExplainer\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # 1) Crear la submuestra y escalarla \n",
        "    X_sample = X.sample(min(N_DEEP_SAMPLES, len(X)), random_state=0)\n",
        "    X_scaled = sx.transform(X_sample)\n",
        "\n",
        "    #  2) Si es RNN, reshape 2D3D segn la forma de entrada del modelo \n",
        "    if key == 'rnn':\n",
        "        # model_obj.input_shape suele ser (None, timesteps, features)\n",
        "        _, timesteps, feat_dim = model_obj.input_shape\n",
        "        try:\n",
        "            X_scaled = X_scaled.reshape(-1, timesteps, feat_dim)\n",
        "            print(f\"[DEBUG] DeepLIFT RNN: reshaped a {X_scaled.shape}\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(\n",
        "                f\"No pude reshapear para RNN: esperaba (_, {timesteps}, {feat_dim}), \"\n",
        "                f\"pero sx.transform devolvi {sx.transform(X_sample).shape}. \"\n",
        "                f\"Error: {e}\"\n",
        "            )\n",
        "\n",
        "    #  3) Crear explainer y calcular valores \n",
        "    explainer_dl = GradientExplainer(model_obj, X_scaled)\n",
        "    dl_vals = explainer_dl.shap_values(X_scaled)   # ya 3D  funciona OK\n",
        "\n",
        "    # 1.1) Squeeze si viene con dimensin extra\n",
        "    if isinstance(dl_vals, np.ndarray):\n",
        "        # eliminar ejes de longitud 1\n",
        "        dl_vals = np.squeeze(dl_vals)\n",
        "        print(f\"[DEBUG] tras squeeze: {dl_vals.shape}\")\n",
        "        # si sigue siendo 3D, asumimos (n_samples, time_steps, n_features)\n",
        "        if dl_vals.ndim == 3:\n",
        "            # colapsamos time_steps tomando la media\n",
        "            dl_vals = dl_vals.mean(axis=1)\n",
        "            print(f\"[DEBUG] tras mean over time axis: {dl_vals.shape}\")\n",
        "\n",
        "    #  5) DataFrame de relevancias locales \n",
        "    dl_df = pd.DataFrame(dl_vals, columns=cols)\n",
        "    display(HTML('<h4>DeepLIFT / LRP: Primeras 10 muestras</h4>'))\n",
        "    display(dl_df.head(10))\n",
        "    display(HTML(\"\"\"\n",
        "    <h4> Cmo leer la tabla de las primeras 10 muestras</h4>\n",
        "    <ul>\n",
        "      <li>Cada fila corresponde a una de las primeras 10 observaciones de tu conjunto de datos.</li>\n",
        "      <li>Cada columna muestra la relevancia asignada por DeepLIFT/LRP a esa caracterstica en esa muestra.</li>\n",
        "      <li>Un valor positivo indica que la caracterstica empuj la prediccin <b>hacia arriba</b> respecto al valor de referencia.</li>\n",
        "      <li>Un valor negativo indica que la caracterstica empuj la prediccin <b>hacia abajo</b>.</li>\n",
        "      <li>Por ejemplo, si para la muestra #3 el valor en la columna X2 es 0.15, quiere decir que X2 aument la salida del modelo en 0.15 unidades.</li>\n",
        "    </ul>\n",
        "    \"\"\"))\n",
        "\n",
        "    #  6) Importancia global (media absoluta) \n",
        "    mean_abs = np.abs(dl_vals).mean(axis=0)\n",
        "    #imp_df = pd.DataFrame({'feature':cols,'mean_abs_dl':mean_abs}).sort_values('mean_abs_dl',ascending=False)\n",
        "    imp_df = pd.DataFrame({'feature': cols, 'mean_abs_dl': mean_abs})\n",
        "    imp_df = imp_df.sort_values('mean_abs_dl', ascending=False).reset_index(drop=True)\n",
        "    display(HTML('<h4>DeepLIFT / LRP: Importancia global</h4>'))\n",
        "    display(imp_df)\n",
        "    display(HTML(\"\"\"\n",
        "    <h4> Cmo leer la tabla de importancia global</h4>\n",
        "    <ul>\n",
        "      <li>La importancia global es la media del valor absoluto de las relevancias en <b>todas</b> las muestras.</li>\n",
        "      <li>Se ordena de mayor a menor: las variables que aparecen arriba son las que, en promedio, ms afectan la prediccin.</li>\n",
        "      <li>Por ejemplo, si la media absoluta de X4 es 0.35, significa que X4 desvi la prediccin en 0.35 de media.</li>\n",
        "    </ul>\n",
        "    \"\"\"))\n",
        "\n",
        "    #  7) Grfico de importancia global \n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.scatter(range(len(imp_df)), imp_df['mean_abs_dl'], s=80)\n",
        "    plt.xticks(range(len(imp_df)), imp_df['feature'], rotation=45)\n",
        "    plt.title('Importancia DeepLIFT / LRP')\n",
        "    display(HTML(\"\"\"\n",
        "    <h4> Cmo interpretar el grfico de relevancias</h4>\n",
        "    <ul>\n",
        "      <li>Cada barra representa la relevancia media absoluta de una caracterstica (la misma que en la tabla).</li>\n",
        "      <li>La altura de la barra indica cun importante es esa variable en el conjunto completo.</li>\n",
        "      <li>Las barras verdes (si tuvieras colores) son relevancias positivas medias y las rojas negativas medias.</li>\n",
        "      <li>Una barra alta significa que, variando esa caracterstica, la prediccin del modelo cambia sustancialmente.</li>\n",
        "      <li>Este grfico te ayuda a ver de un vistazo qu variables mueven ms la prediccin.</li>\n",
        "    </ul>\n",
        "    \"\"\"))\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    #  8) Estadsticas adicionales \n",
        "    stats = {'dlrp_imp_percentiles': imp_df['mean_abs_dl'].quantile([0.25, 0.5, 0.75]).to_dict()}\n",
        "\n",
        "    #  9) Devolver en el formato esperado \n",
        "    resultado = {\n",
        "        'imp_df': imp_df.rename(columns={'mean_abs_dl':'value'}),\n",
        "        'df_local': dl_df,\n",
        "        'fig_summary': fig,\n",
        "        'stats': stats\n",
        "    }\n",
        "    return resultado\n",
        "pass\n",
        "\n",
        "# ============================================================\n",
        "# 6. Motor Permutation Feature Importance\n",
        "# ============================================================\n",
        "def _motor_perm(model_obj, X, cols, sx, sy):\n",
        "    \"\"\"Permutation Feature Importance.\"\"\"\n",
        "    from sklearn.inspection import permutation_importance\n",
        "    import numpy as np, pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    print(\" Calculando Permutation Importance para un subconjunto de muestras...\")\n",
        "\n",
        "    # 1) Seleccionamos X_test/Y_test o fallback a X/Y\n",
        "    X_target = globals().get('X_test', X)\n",
        "    y_target = globals().get('Y_test', None)\n",
        "    if y_target is None:\n",
        "        raise ValueError(\"Y_test no definido para Permutation Importance\")\n",
        "\n",
        "    # \n",
        "    # 1.1) HARDCODE: nmero de muestras para la permutacin\n",
        "    #N_PERM_SAMPLES = 50\n",
        "    # 1.2) Muestreamos esas instancias\n",
        "    idxs = X_target.sample(\n",
        "        n=min(N_PERM_SAMPLES, len(X_target)),\n",
        "        random_state=0\n",
        "    ).index\n",
        "    X_target = X_target.loc[idxs, cols]\n",
        "    y_target = y_target.loc[idxs]\n",
        "    print(f\"[DEBUG] Usando {len(X_target)} muestras para Permutation Importance\")\n",
        "    # \n",
        "\n",
        "    # 2) Escalamos nuestro subset\n",
        "    X_scaled = sx.transform(X_target)\n",
        "\n",
        "    # 3) Calculamos la importancia por permutacin\n",
        "    if hasattr(model_obj, 'predict'):\n",
        "        perm = permutation_importance(\n",
        "            model_obj,\n",
        "            X_scaled,\n",
        "            y_target.values.ravel(),\n",
        "            n_repeats=10,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    else:\n",
        "        raise TypeError(\"Modelo no soporta permutation_importance\")\n",
        "\n",
        "    # 4) Creamos el DataFrame ordenado\n",
        "    imp_df = pd.DataFrame({\n",
        "        'feature':         cols,\n",
        "        'mean_importance': perm.importances_mean,\n",
        "        'std_importance':  perm.importances_std\n",
        "    }).sort_values('mean_importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # 5) Mostramos la tabla global\n",
        "    display(HTML('<h4>Permutation Importance (subconjunto)</h4>'))\n",
        "    display(imp_df)\n",
        "\n",
        "    # 6) Grfico con barras de error\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.errorbar(\n",
        "        range(len(imp_df)),\n",
        "        imp_df['mean_importance'],\n",
        "        yerr=imp_df['std_importance'],\n",
        "        fmt='o', capsize=5\n",
        "    )\n",
        "    plt.xticks(range(len(imp_df)), imp_df['feature'], rotation=45)\n",
        "    plt.title('Permutation Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    # 7) Estadsticas adicionales\n",
        "    stats = {\n",
        "        'perm_imp_percentiles': imp_df['mean_importance']\n",
        "                                  .quantile([0.25,0.5,0.75])\n",
        "                                  .to_dict()\n",
        "    }\n",
        "\n",
        "    # 8) Devolvemos el resultado en el formato esperado\n",
        "    return {\n",
        "        'imp_df':      imp_df.rename(columns={'mean_importance':'value'}),\n",
        "        'df_local':    imp_df.head(N_PERM_SAMPLES),  # top-N features\n",
        "        'fig_summary': fig,\n",
        "        'stats':       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "# ============================================================\n",
        "# 7. Motor Partial Dependence Plots (PDP)\n",
        "# ============================================================\n",
        "def _motor_pdp(X: pd.DataFrame, cols: list[str], predict_fn):\n",
        "    import numpy as np, pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor PDP\")\n",
        "\n",
        "    # \n",
        "    # HARDCODE: nmero de muestras para la parte local de PDP\n",
        "    #N_PDP_SAMPLES = 50\n",
        "    print(f\"[DEBUG] Usando {N_PDP_SAMPLES} muestras aleatorias para PDP local\")\n",
        "    # \n",
        "\n",
        "    # ---------- PDP global -----------------------------------\n",
        "    pdp_ranges = []\n",
        "    pdp_curves = {}\n",
        "    for feat in cols:\n",
        "        print(f\"[DEBUG] Calculando PDP global para feature '{feat}'\")\n",
        "        grid = np.linspace(X[feat].min(), X[feat].max(), GRID_RES)\n",
        "        pd_vals = []\n",
        "        for g in grid:\n",
        "            X_temp = X.copy()\n",
        "            X_temp[feat] = g\n",
        "            preds = predict_fn(X_temp)\n",
        "            pd_vals.append(preds.mean())\n",
        "        pd_vals = np.array(pd_vals)\n",
        "        pdp_curves[feat] = pd_vals\n",
        "        pdp_ranges.append(pd_vals.max() - pd_vals.min())\n",
        "\n",
        "    imp_df = (\n",
        "        pd.DataFrame({\"feature\": cols, \"pdp_range\": pdp_ranges})\n",
        "          .sort_values(\"pdp_range\", ascending=False)\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "    print(\"[DEBUG] DataFrame de importancia global PDP creado\")\n",
        "\n",
        "    display(HTML(\"<h4>PDP: Importancia global (rango de la curva)</h4>\"))\n",
        "    display(imp_df)\n",
        "    display(HTML(\n",
        "        \"<p>La <b>importancia global</b> de cada caracterstica se mide \"\n",
        "        \"como el rango (mx  mn) de su curva PDP.</p>\"\n",
        "    ))\n",
        "\n",
        "    # ---------- PDP local (subconjunto aleatorio) -------------\n",
        "    X_sample = X.sample(n=min(N_PDP_SAMPLES, len(X)), random_state=0)\n",
        "    n_samples = len(X_sample)\n",
        "    print(f\"[DEBUG] Calculando PDP local para {n_samples} muestras\")\n",
        "\n",
        "    base_pred_mean = predict_fn(X).mean()\n",
        "    pdp_local = np.zeros((n_samples, len(cols)))\n",
        "    for i, (_, row) in enumerate(X_sample.iterrows()):\n",
        "        for j, feat in enumerate(cols):\n",
        "            X_temp = X.copy()\n",
        "            X_temp[feat] = row[feat]\n",
        "            pdp_local[i, j] = predict_fn(X_temp).mean() - base_pred_mean\n",
        "\n",
        "    pdp_df = pd.DataFrame(pdp_local, columns=cols)\n",
        "    print(\"[DEBUG] DataFrame de PDP local creado\")\n",
        "\n",
        "    display(HTML(f\"<h4>PDP: Valores para {n_samples} muestras seleccionadas</h4>\"))\n",
        "    display(pdp_df)\n",
        "    display(HTML(\n",
        "        \"<p>Cada celda muestra cunto vara la prediccin promedio cuando \"\n",
        "        \"fijamos la caracterstica al valor de la muestra.</p>\"\n",
        "    ))\n",
        "\n",
        "    # ---------- Grfico global --------------------------------\n",
        "    print(\"[DEBUG] Generando grfico de importancia global PDP\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.scatter(range(len(imp_df)), imp_df[\"pdp_range\"], s=80)\n",
        "    plt.xticks(range(len(imp_df)), imp_df[\"feature\"], rotation=45)\n",
        "    plt.ylabel(\"Rango PDP\")\n",
        "    plt.title(\"Importancia Partial Dependence (rango)\")\n",
        "    plt.axhline(0, color=\"black\", linewidth=0.8)\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    # ---------- Estadsticas adicionales ----------------------\n",
        "    stats = {\n",
        "        \"pdp_range_percentiles\": imp_df[\"pdp_range\"]\n",
        "                                   .quantile([0.25, 0.5, 0.75])\n",
        "                                   .to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadsticas adicionales calculadas: {stats}\")\n",
        "\n",
        "    # ---------- Resultado -------------------------------------\n",
        "    return {\n",
        "        \"imp_df\":      imp_df.rename(columns={\"pdp_range\": \"value\"}),\n",
        "        \"df_local\":    pdp_df,\n",
        "        \"fig_summary\": fig,\n",
        "        \"stats\":       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 8. Motor Accumulated Local Effects (ALE)\n",
        "# ============================================================\n",
        "def _motor_ale(X: pd.DataFrame, cols: list[str], predict_fn, n_bins: int = ALE_BINS):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor ALE\")\n",
        "\n",
        "    # \n",
        "    # HARDCODE: nmero de muestras para la parte local de ALE\n",
        "    print(f\"[DEBUG] Usando {N_ALE_SAMPLES} muestras aleatorias para ALE local\")\n",
        "    # \n",
        "\n",
        "    # ---------- ALE global ------------------------------------\n",
        "    ale_ranges = []\n",
        "    ale_curves = {}\n",
        "    for feat in cols:\n",
        "        print(f\"[DEBUG] Calculando ALE global para '{feat}'\")\n",
        "        # bordes de los bins\n",
        "        edges = np.quantile(X[feat], np.linspace(0, 1, n_bins + 1))\n",
        "        edges[0] -= 1e-9\n",
        "        edges[-1] += 1e-9\n",
        "\n",
        "        curve = np.zeros(n_bins)\n",
        "        cum = 0.0\n",
        "        for b in range(n_bins):\n",
        "            lo, hi = edges[b], edges[b+1]\n",
        "            mask = (X[feat] > lo) & (X[feat] <= hi)\n",
        "            if mask.any():\n",
        "                X_lo = X.loc[mask].copy(); X_hi = X.loc[mask].copy()\n",
        "                X_lo[feat] = lo; X_hi[feat] = hi\n",
        "                delta = predict_fn(X_hi) - predict_fn(X_lo)\n",
        "                cum += delta.mean()\n",
        "            curve[b] = cum\n",
        "        ale_curves[feat] = curve\n",
        "        ale_ranges.append(curve.max() - curve.min())\n",
        "\n",
        "    imp_df = (\n",
        "        pd.DataFrame({\"feature\": cols, \"ale_range\": ale_ranges})\n",
        "          .sort_values(\"ale_range\", ascending=False)\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "    print(\"[DEBUG] DataFrame de importancia global ALE creado\")\n",
        "\n",
        "    display(HTML(\"<h4>ALE: Importancia global (rango)</h4>\"))\n",
        "    display(imp_df)\n",
        "    display(HTML(\n",
        "        \"<p>El <b>rango ALE</b> mide cunto vara la curva acumulada \"\n",
        "        \"al recorrer toda la distribucin de la variable.</p>\"\n",
        "    ))\n",
        "\n",
        "    # ---------- ALE local (subconjunto aleatorio) -------------\n",
        "    X_sample = X.sample(n=min(N_ALE_SAMPLES, len(X)), random_state=0)\n",
        "    n_samples = len(X_sample)\n",
        "    print(f\"[DEBUG] Calculando ALE local para {n_samples} muestras seleccionadas\")\n",
        "\n",
        "    ale_local = np.zeros((n_samples, len(cols)))\n",
        "    for i, (_, row) in enumerate(X_sample.iterrows()):\n",
        "        for j, feat in enumerate(cols):\n",
        "            # identificar bin de la muestra\n",
        "            bin_idx = np.digitize(row[feat], edges[1:-1], right=True)\n",
        "            ale_local[i, j] = ale_curves[feat][bin_idx]\n",
        "\n",
        "    ale_df = pd.DataFrame(ale_local, columns=cols)\n",
        "    print(\"[DEBUG] DataFrame de ALE local creado\")\n",
        "\n",
        "    display(HTML(f\"<h4>ALE: Valores para {n_samples} muestras seleccionadas</h4>\"))\n",
        "    display(ale_df)\n",
        "    display(HTML(\n",
        "        \"<p>Cada celda muestra el valor ALE acumulado en el bin en que cae la muestra.</p>\"\n",
        "    ))\n",
        "\n",
        "    # ---------- Grfico global --------------------------------\n",
        "    print(\"[DEBUG] Generando grfico de importancia global ALE\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.scatter(range(len(imp_df)), imp_df[\"ale_range\"], s=80)\n",
        "    plt.xticks(range(len(imp_df)), imp_df[\"feature\"], rotation=45)\n",
        "    plt.ylabel(\"Rango ALE\")\n",
        "    plt.title(\"Importancia Accumulated Local Effects\")\n",
        "    plt.axhline(0, color=\"black\", linewidth=0.8)\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    # ---------- Estadsticas adicionales ----------------------\n",
        "    stats = {\n",
        "        \"ale_range_percentiles\": imp_df[\"ale_range\"]\n",
        "                                   .quantile([0.25, 0.5, 0.75])\n",
        "                                   .to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadsticas adicionales calculadas: {stats}\")\n",
        "\n",
        "    # ---------- Resultado -------------------------------------\n",
        "    return {\n",
        "        \"imp_df\":      imp_df.rename(columns={\"ale_range\": \"value\"}),\n",
        "        \"df_local\":    ale_df,\n",
        "        \"fig_summary\": fig,\n",
        "        \"stats\":       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "# ===================================================================\n",
        "# 9. Motor Individual Conditional Expectation (ICE)\n",
        "# ===================================================================\n",
        "def _motor_ice(\n",
        "    X: pd.DataFrame,\n",
        "    cols: list[str],\n",
        "    predict_fn,\n",
        "    *,\n",
        "    grid_res: int = GRID_RES\n",
        "):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor ICE\")\n",
        "\n",
        "    # \n",
        "    # HARDCODE: nmero de muestras para ICE (tanto global como local)\n",
        "    #N_ICE_SAMPLES = 50\n",
        "    print(f\"[DEBUG] Usando N_ICE_SAMPLES={N_ICE_SAMPLES} para ICE global y local\")\n",
        "    # \n",
        "\n",
        "    #  1) Tomar subconjunto aleatorio para ICE global y local \n",
        "    X_sub = X.sample(n=min(N_ICE_SAMPLES, len(X)), random_state=0)\n",
        "    n_total = len(X_sub)\n",
        "    n_local = min(FIRST_SAMPLES, n_total)\n",
        "    print(f\"[DEBUG] Submuestra ICE creada con {n_total} instancias (local={n_local})\")\n",
        "\n",
        "    # contenedores\n",
        "    ice_local = np.zeros((n_local, len(cols)))\n",
        "    ice_ranges = []\n",
        "\n",
        "    # \n",
        "    # 2) Calcular rangos ICE por caracterstica sobre X_sub\n",
        "    # \n",
        "    for j, feat in enumerate(cols):\n",
        "        print(f\"[DEBUG] Calculando curvas ICE para '{feat}'\")\n",
        "        grid = np.linspace(X[feat].min(), X[feat].max(), grid_res)\n",
        "        ranges_feat = []\n",
        "\n",
        "        for i, (_, row) in enumerate(X_sub.iterrows()):\n",
        "            # construir DataFrame repitiendo la fila\n",
        "            X_grid = pd.DataFrame(\n",
        "                np.repeat(row.values.reshape(1, -1), grid_res, axis=0),\n",
        "                columns=cols\n",
        "            )\n",
        "            X_grid[feat] = grid\n",
        "            preds = predict_fn(X_grid)\n",
        "            r = preds.max() - preds.min()\n",
        "            ranges_feat.append(r)\n",
        "\n",
        "            if i < n_local:\n",
        "                ice_local[i, j] = r\n",
        "\n",
        "        mean_range = float(np.mean(ranges_feat))\n",
        "        ice_ranges.append(mean_range)\n",
        "        print(f\"[DEBUG] Rango medio ICE para '{feat}': {mean_range:.4f}\")\n",
        "\n",
        "    # \n",
        "    # 3) Importancia global (media de rangos)\n",
        "    # \n",
        "    imp_df = (\n",
        "        pd.DataFrame({\n",
        "            \"feature\": cols,\n",
        "            \"ice_range_mean\": ice_ranges\n",
        "        })\n",
        "        .sort_values(\"ice_range_mean\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    print(\"[DEBUG] DataFrame de importancia global ICE creado\")\n",
        "\n",
        "    display(HTML(\"<h4>ICE: Importancia global (media de rangos)</h4>\"))\n",
        "    display(imp_df)\n",
        "    display(HTML(\n",
        "        \"<p>Cada punto muestra la media del rango ICE de la caracterstica. \"\n",
        "        \"Un valor mayor indica mayor sensibilidad de la prediccin a esa variable.</p>\"\n",
        "    ))\n",
        "\n",
        "    # \n",
        "    # 4) Tabla local (primeras n_local muestras de X_sub)\n",
        "    # \n",
        "    ice_df = pd.DataFrame(ice_local, columns=cols)\n",
        "    print(\"[DEBUG] DataFrame de ICE local creado\")\n",
        "    display(HTML(f\"<h4>ICE: Rangos para las primeras {n_local} muestras</h4>\"))\n",
        "    display(ice_df)\n",
        "\n",
        "    # \n",
        "    # 5) Grfico global\n",
        "    # \n",
        "    print(\"[DEBUG] Generando grfico de importancia ICE\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.scatter(range(len(imp_df)), imp_df[\"ice_range_mean\"], s=80)\n",
        "    plt.xticks(range(len(imp_df)), imp_df[\"feature\"], rotation=45)\n",
        "    plt.ylabel(\"Rango medio ICE\")\n",
        "    plt.title(\"Importancia Individual Conditional Expectation\")\n",
        "    plt.axhline(0, color=\"black\", lw=0.8)\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    display(HTML(\n",
        "        \"<p>La dispersin de estos puntos indica qu variables tienen \"\n",
        "        \"mayor efecto condicional individual sobre la prediccin.</p>\"\n",
        "    ))\n",
        "\n",
        "    # \n",
        "    # 6) Estadsticas adicionales\n",
        "    # \n",
        "    stats = {\n",
        "        \"ice_range_percentiles\": imp_df[\"ice_range_mean\"]\n",
        "                                    .quantile([0.25, 0.5, 0.75])\n",
        "                                    .to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadsticas ICE calculadas: {stats}\")\n",
        "\n",
        "    # \n",
        "    # 7) Resultado\n",
        "    # \n",
        "    return {\n",
        "        \"imp_df\":      imp_df.rename(columns={\"ice_range_mean\": \"value\"}),\n",
        "        \"df_local\":    ice_df,\n",
        "        \"fig_summary\": fig,\n",
        "        \"stats\":       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "# ===================================================================\n",
        "# 10. Motor Counterfactual Explanations   **todas las muestras**\n",
        "# ===================================================================\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def _motor_counterfactual(\n",
        "    X: pd.DataFrame,\n",
        "    predict_fn,\n",
        "    *,\n",
        "    rel_delta: float = CF_TARGET_DELTA,    # +10% por defecto\n",
        "    show_first: int = FIRST_SAMPLES\n",
        "):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor Counterfactual\")\n",
        "\n",
        "    # \n",
        "    # HARDCODE: nmero de muestras para buscar contrafactuales\n",
        "    #N_CF_SAMPLES = 50\n",
        "    print(f\"[DEBUG] Usando N_CF_SAMPLES={N_CF_SAMPLES} para submuestreo\")\n",
        "    # \n",
        "\n",
        "    # 1) Submuestra aleatoria de X para buscar contrafactuales\n",
        "    X_sub = X.sample(n=min(N_CF_SAMPLES, len(X)), random_state=0)\n",
        "    print(f\"[DEBUG] Submuestra creada con {len(X_sub)} instancias\")\n",
        "\n",
        "    # 2) Predicciones de la submuestra\n",
        "    preds_sub = predict_fn(X_sub)\n",
        "    print(\"[DEBUG] Predicciones calculadas para submuestra\")\n",
        "\n",
        "    cf_rows = []\n",
        "    deltas = []\n",
        "\n",
        "    # 3) Para cada muestra en X_sub, buscar el vecino ms cercano que supere el delta\n",
        "    for i, idx in enumerate(X_sub.index):\n",
        "        x0 = X_sub.loc[idx].values.reshape(1, -1)\n",
        "        y0 = preds_sub[i]\n",
        "        target = y0 * (1 + rel_delta)\n",
        "        print(f\"[DEBUG] Muestra idx={idx}, y0={y0:.4f}, target>={target:.4f}\")\n",
        "\n",
        "        # candidatos de X (pueden ser toda X o X_sub, aqu usamos X para ms posibilidades)\n",
        "        all_preds = predict_fn(X)\n",
        "        mask = all_preds >= target\n",
        "        X_cand = X.loc[mask]\n",
        "        print(f\"[DEBUG] Encontrados {len(X_cand)} candidatos que cumplen delta\")\n",
        "\n",
        "        if X_cand.empty:\n",
        "            cf_rows.append({\"ndice\": idx, **{c: None for c in X.columns}, \"Distancia\": None})\n",
        "            deltas.append(np.nan)\n",
        "            continue\n",
        "\n",
        "        nbrs = NearestNeighbors(n_neighbors=1, metric=\"euclidean\")\n",
        "        nbrs.fit(X_cand.values)\n",
        "        dist, ind = nbrs.kneighbors(x0, return_distance=True)\n",
        "        cf = X_cand.iloc[ind[0][0]]\n",
        "        delta_feat = np.abs(cf.values - x0.ravel())\n",
        "        mean_delta = float(delta_feat.mean())\n",
        "\n",
        "        cf_rows.append({\n",
        "            \"ndice\":        idx,\n",
        "            **{c: float(v) for c, v in zip(X.columns, cf.values)},\n",
        "            \"Distancia\":    float(dist[0][0])\n",
        "        })\n",
        "        deltas.append(mean_delta)\n",
        "        print(f\"[DEBUG] Contrafactual idx={idx}: distancia={dist[0][0]:.4f}, medio_feat={mean_delta:.4f}\")\n",
        "\n",
        "    # 4) Construir DataFrame local\n",
        "    df_local = pd.DataFrame(cf_rows).set_index(\"ndice\")\n",
        "    n_show = min(show_first, len(df_local))\n",
        "    display(HTML(f\"<h4>Contrafactuales (primeras {n_show} muestras)</h4>\"))\n",
        "    display(df_local.head(n_show))\n",
        "\n",
        "    # 5) Importancia global: |feature| medio\n",
        "    imp_series = pd.Series(0.0, index=X.columns)\n",
        "    valid = df_local[\"Distancia\"].notna()\n",
        "    for idx in df_local[valid].index:\n",
        "        diff = np.abs(df_local.loc[idx, X.columns].values - X.loc[idx].values)\n",
        "        imp_series += diff\n",
        "    imp_series /= valid.sum()\n",
        "    imp_series = imp_series.sort_values(ascending=False)\n",
        "    imp_df = imp_series.rename(\"value\").reset_index().rename(columns={\"index\":\"feature\"})\n",
        "\n",
        "    display(HTML(\"<h4>Importancia global por contrafactuales</h4>\"))\n",
        "    display(imp_df)\n",
        "\n",
        "    # 6) Grfico de barras\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.bar(imp_series.index, imp_series.values)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"|feature| medio\")\n",
        "    plt.title(\"Importancia global  Counterfactual\")\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    # 7) Estadsticas adicionales\n",
        "    stats = {\n",
        "        \"cf_imp_percentiles\": imp_series.quantile([0.25, 0.5, 0.75]).to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadsticas contrafactuales: {stats}\")\n",
        "\n",
        "    # 8) Resultado\n",
        "    return {\n",
        "        \"imp_df\":      imp_df,\n",
        "        \"df_local\":    df_local,\n",
        "        \"fig_summary\": fig,\n",
        "        \"stats\":       stats\n",
        "    }\n",
        "\n",
        "# ===================================================================\n",
        "# 11. Motor Anchors  Reglas locales con DecisionTree como proxy\n",
        "# ===================================================================\n",
        "def _motor_anchors(X: pd.DataFrame, cols: list[str], predict_fn):\n",
        "    \"\"\"\n",
        "    Genera reglas-Anchor (rbol surrogate poco profundo) para una SUBmuestra de X.\n",
        "    Importancia global = frecuencia (relativa) de aparicin de cada variable\n",
        "    en todas las reglas obtenidas.\n",
        "    \"\"\"\n",
        "    import re, random\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor Anchors\")\n",
        "\n",
        "    # \n",
        "    # Hardcode: nmero de muestras a analizar\n",
        "    #N_ANCHOR_SAMPLES = 50\n",
        "    print(f\"[DEBUG] Usando N_ANCHOR_SAMPLES={N_ANCHOR_SAMPLES}\")\n",
        "    # \n",
        "\n",
        "    # 1) Submuestra de X para acelerar el proceso\n",
        "    X_sub = X.sample(n=min(N_ANCHOR_SAMPLES, len(X)), random_state=0)\n",
        "    n_sub = len(X_sub)\n",
        "    print(f\"[DEBUG] Submuestra creada con {n_sub} instancias\")\n",
        "\n",
        "    # Preparar contadores\n",
        "    reglas, coberturas, precisiones = [], [], []\n",
        "    var_freq = {c: 0 for c in cols}\n",
        "\n",
        "    # 2) Iterar solo sobre la submuestra\n",
        "    for i, idx in enumerate(X_sub.index):\n",
        "        x0 = X_sub.loc[idx]\n",
        "        y0 = predict_fn(x0.values.reshape(1, -1))[0]\n",
        "        print(f\"[DEBUG] Muestra {i+1}/{n_sub} (idx={idx}), pred={y0:.4f}\")\n",
        "\n",
        "        # 3) Vecinos aleatorios de la submuestra\n",
        "        neigh_idx = random.sample(\n",
        "            list(X_sub.index),\n",
        "            k=min(ANC_NEIGHBORS, n_sub)\n",
        "        )\n",
        "        X_nei = X_sub.loc[neigh_idx]\n",
        "        y_nei = predict_fn(X_nei.values)\n",
        "        print(f\"[DEBUG]  Vecinos seleccionados: {len(X_nei)}\")\n",
        "\n",
        "        # 4) Binarizar segn exceder o no la prediccin base\n",
        "        y_bin = (y_nei >= y0).astype(int)\n",
        "\n",
        "        # 5) Ajustar rbol surrogate\n",
        "        tree = DecisionTreeRegressor(\n",
        "            max_depth=3, min_samples_leaf=5, random_state=0\n",
        "        )\n",
        "        tree.fit(X_nei, y_bin)\n",
        "\n",
        "        # 6) Extraer las condiciones del path de x0\n",
        "        node_indicator = tree.decision_path(x0.values.reshape(1,-1))\n",
        "        features      = tree.tree_.feature\n",
        "        thresholds    = tree.tree_.threshold\n",
        "\n",
        "        anchor_conds = []\n",
        "        for node_id in node_indicator.indices:\n",
        "            if features[node_id] == -2:\n",
        "                continue  # hoja\n",
        "            f_idx = features[node_id]\n",
        "            feat = cols[f_idx]\n",
        "            thr  = thresholds[node_id]\n",
        "            op   = \"\" if x0[feat] <= thr else \">\"\n",
        "            cond = f\"{feat} {op} {thr:.3g}\"\n",
        "            anchor_conds.append(cond)\n",
        "            var_freq[feat] += 1\n",
        "\n",
        "        # 7) Calcular cobertura y precisin\n",
        "        cover = np.ones(len(X_nei), dtype=bool)\n",
        "        for cond in anchor_conds:\n",
        "            m = re.match(r'\\s*(.+?)\\s*(|>=|>|<)\\s*([-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?)', cond)\n",
        "            if not m:\n",
        "                continue\n",
        "            f, op_sym, val = m.groups()\n",
        "            val = float(val)\n",
        "            if op_sym in (\"\", \"<=\"):\n",
        "                cover &= (X_nei[f] <= val)\n",
        "            elif op_sym in (\">\", \"\"):\n",
        "                cover &= (X_nei[f] >= val)\n",
        "        coverage  = cover.mean()\n",
        "        precision = (y_nei[cover] >= y0).mean() if coverage > 0 else 0.0\n",
        "\n",
        "        reglas.append(\"  \".join(anchor_conds))\n",
        "        coberturas.append(coverage)\n",
        "        precisiones.append(precision)\n",
        "        print(f\"[DEBUG]  Regla: {'  '.join(anchor_conds)}\")\n",
        "        print(f\"[DEBUG]  Cobertura={coverage:.2%}, Precisin={precision:.2%}\")\n",
        "\n",
        "    #  Tablas y grficas \n",
        "    df_local = pd.DataFrame({\n",
        "        \"Regla\":     reglas,\n",
        "        \"Cobertura\": coberturas,\n",
        "        \"Precisin\": precisiones\n",
        "    })\n",
        "\n",
        "    display(HTML(\"<h4> Reglas Anchor (submuestra)</h4>\"))\n",
        "    display(df_local.head(10).style.format({\"Cobertura\":\"{:.2%}\",\"Precisin\":\"{:.2%}\"}))\n",
        "\n",
        "    # Importancia global\n",
        "    imp = (pd.Series(var_freq) / n_sub).sort_values(ascending=False)\n",
        "    imp_df = imp.reset_index().rename(columns={\"index\":\"feature\", 0:\"value\"})\n",
        "\n",
        "    display(HTML(\"<h4> Importancia global (frecuencia en submuestra)</h4>\"))\n",
        "    display(imp_df)\n",
        "\n",
        "    # Grfico de frecuencias\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.bar(imp.index, imp.values)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"Frecuencia\")\n",
        "    plt.title(\"Anchors  Importancia global\")\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    # Estadsticas adicionales\n",
        "    stats = {\n",
        "        \"anchors_freq_percentiles\": imp.quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadsticas Anchors: {stats}\")\n",
        "\n",
        "    return {\n",
        "        \"imp_df\":      imp_df,\n",
        "        \"df_local\":    df_local,\n",
        "        \"fig_summary\": fig,\n",
        "        \"stats\":       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "# ===================================================================\n",
        "# 12. Motor Surrogate Models (Global/Local)\n",
        "# ===================================================================\n",
        "def _motor_surrogate(X: pd.DataFrame, cols: list[str], predict_fn):\n",
        "    \"\"\"\n",
        "    Calcula rbol sustituto global + regresiones locales usando solo una\n",
        "    submuestra de X para acelerar el clculo.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor Surrogate\")\n",
        "\n",
        "    # \n",
        "    # Hardcode: nmero de muestras para el surrogate global y local\n",
        "    #N_SURR_SAMPLES = 100\n",
        "    print(f\"[DEBUG] Usando N_SURR_SAMPLES={N_SURR_SAMPLES}\")\n",
        "    # \n",
        "\n",
        "    # 1) Submuestra para surrogate global\n",
        "    n_sub = min(N_SURR_SAMPLES, len(X))\n",
        "    X_sub = X.sample(n=n_sub, random_state=0)\n",
        "    print(f\"[DEBUG] Submuestra global creada con {n_sub} instancias\")\n",
        "\n",
        "    # ---------- Global surrogate ----------\n",
        "    y_sub = predict_fn(X_sub)\n",
        "    print(\"[DEBUG] Entrenando rbol surrogate global\")\n",
        "    tree = DecisionTreeRegressor(max_depth=SURR_TREE_DEPTH, random_state=0)\n",
        "    tree.fit(X_sub, y_sub)\n",
        "    imp_global = pd.Series(tree.feature_importances_, index=cols).sort_values(ascending=False)\n",
        "    print(\"[DEBUG] Importancias globales calculadas\")\n",
        "\n",
        "    display(HTML('<h4> Importancia Global (Surrogate rbol)</h4>'))\n",
        "    display(imp_global.to_frame('Importancia').T.style.format('{:.3f}'))\n",
        "    display(HTML(\n",
        "        '<p>Cada celda muestra la contribucin de la variable a la reduccin de MSE en el rbol '\n",
        "        'sustituto entrenado sobre la submuestra.</p>'\n",
        "    ))\n",
        "\n",
        "    # ---------- Local surrogates ----------\n",
        "    n_local = min(FIRST_SAMPLES, n_sub)\n",
        "    print(f\"[DEBUG] Generando {n_local} surrogates locales sobre la submuestra\")\n",
        "    local_abs_coef = np.zeros((n_local, len(cols)))\n",
        "\n",
        "    # usamos X_sub para vecinos locales\n",
        "    for i, idx in enumerate(X_sub.index[:n_local]):\n",
        "        x0 = X_sub.loc[idx]\n",
        "        # distancias sobre la submuestra\n",
        "        dists = np.linalg.norm(X_sub.values - x0.values, axis=1)\n",
        "        neigh_idx = dists.argsort()[1:SURR_LOCAL_K+1]\n",
        "        X_nei = X_sub.iloc[neigh_idx]\n",
        "        y_nei = predict_fn(X_nei)\n",
        "        lin = LinearRegression().fit(X_nei, y_nei)\n",
        "        local_abs_coef[i] = np.abs(lin.coef_)\n",
        "        print(f\"[DEBUG] Local surrogate {i+1}: coef abs media calculada\")\n",
        "\n",
        "    imp_local = pd.Series(local_abs_coef.mean(axis=0), index=cols).sort_values(ascending=False)\n",
        "    imp_df_local = imp_local.reset_index().rename(columns={'index':'feature', 0:'value'})\n",
        "    print(\"[DEBUG] Importancias locales medias calculadas\")\n",
        "\n",
        "    display(HTML('<h4> Importancia Local media (|coef|)</h4>'))\n",
        "    display(imp_df_local)\n",
        "    display(HTML(\n",
        "        '<p>Promedio del valor absoluto de los coeficientes de las regresiones locales '\n",
        "        'sobre la submuestra.</p>'\n",
        "    ))\n",
        "\n",
        "    # ---------- Grfico comparativo ----------\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.scatter(range(len(imp_global)), imp_global.values,\n",
        "                label='Global (rbol)', color=SURR_COLOR_GLOBAL)\n",
        "    plt.scatter(range(len(imp_local)),  imp_local.values,\n",
        "                label='Local (media coef)', marker='x', color=SURR_COLOR_LOCAL)\n",
        "    plt.xticks(range(len(cols)), imp_global.index, rotation=45, ha='right')\n",
        "    plt.ylabel('Importancia / |Coef|')\n",
        "    plt.title('Comparativa Surrogate Global vs Local')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "    print(\"[DEBUG] Grfico comparativo generado\")\n",
        "\n",
        "    # Estadsticas adicionales\n",
        "    diff = (imp_global - imp_local).abs()\n",
        "    stats = {\n",
        "        'surrogate_global_percentiles': imp_global.quantile([0.25,0.5,0.75]).to_dict(),\n",
        "        'surrogate_local_percentiles':  imp_local.quantile([0.25,0.5,0.75]).to_dict(),\n",
        "        'surrogate_diff_percentiles':   diff.quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadsticas Surrogate: {stats}\")\n",
        "\n",
        "    # Resultado\n",
        "    return {\n",
        "        'imp_df':      imp_df_local,\n",
        "        'df_local':    pd.DataFrame(local_abs_coef, columns=cols),\n",
        "        'fig_summary': fig,\n",
        "        'stats':       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "# ===================================================================\n",
        "# 13. Motor Explainable Boosting Machine (EBM)\n",
        "# ===================================================================\n",
        "def _motor_ebm(\n",
        "    X: pd.DataFrame,\n",
        "    cols: list[str],\n",
        "    predict_fn,\n",
        "    *,\n",
        "    max_rounds: int = EBM_MAX_ITERS,\n",
        "    n_local: int = FIRST_SAMPLES,\n",
        "):\n",
        "    \"\"\"\n",
        "     Entrena EBM sobre una submuestra de X para acelerar el clculo.\n",
        "     Muestra importancia global y contribuciones locales (n_local filas).\n",
        "    \"\"\"\n",
        "    from interpret.glassbox import ExplainableBoostingRegressor\n",
        "    import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # 1) Comprobacin de disponibilidad de interpret\n",
        "    if ExplainableBoostingRegressor is None:\n",
        "        display(HTML(\n",
        "            \"<p style='color:red'>  Falta el paquete <code>interpret</code>. \"\n",
        "            \"Instlalo con <code>pip install interpret</code>.</p>\"\n",
        "        ))\n",
        "        return\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor EBM\")\n",
        "\n",
        "    # \n",
        "    # Hardcode: tamao de la submuestra para EBM\n",
        "    #N_EBM_SAMPLES = 200\n",
        "    print(f\"[DEBUG] Usando N_EBM_SAMPLES={N_EBM_SAMPLES}\")\n",
        "    # \n",
        "\n",
        "    # 2) Crear submuestra para entrenamiento global\n",
        "    n_sub = min(N_EBM_SAMPLES, len(X))\n",
        "    X_sub = X.sample(n=n_sub, random_state=0)\n",
        "    print(f\"[DEBUG] Submuestra para EBM creada: {n_sub} instancias\")\n",
        "\n",
        "    # 3) Entrenar EBM sobre la submuestra\n",
        "    y_sub = predict_fn(X_sub)\n",
        "    print(\"[DEBUG] Entrenando Explainable Boosting Machine (EBM) sobre submuestra\")\n",
        "    ebm = ExplainableBoostingRegressor(\n",
        "        max_rounds=max_rounds,\n",
        "        random_state=0,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    ebm.fit(X_sub, y_sub)\n",
        "\n",
        "    # 4) Importancia global\n",
        "    g_info = ebm.explain_global().data()\n",
        "    gains = (pd.Series(g_info[\"scores\"], index=g_info[\"names\"])\n",
        "               .reindex(cols, fill_value=0.0)\n",
        "               .sort_values(ascending=False))\n",
        "    imp_df = gains.reset_index().rename(columns={'index':'feature', 0:'value'})\n",
        "    print(\"[DEBUG] Importancia global EBM calculada\")\n",
        "\n",
        "    display(HTML(\"<h4> Importancia global EBM</h4>\"))\n",
        "    display(gains.to_frame(\"Ganancia\").style.format(\"{:.3f}\"))\n",
        "    display(imp_df)\n",
        "\n",
        "    # 5) Contribuciones locales (hasta n_local o tamao de submuestra)\n",
        "    n_loc = min(n_local, n_sub)\n",
        "    print(f\"[DEBUG] Calculando contribuciones locales para las primeras {n_loc} instancias de la submuestra\")\n",
        "    contrib = None\n",
        "\n",
        "    try:  # interpret  0.26\n",
        "        _, contrib = ebm.predict(X_sub.iloc[:n_loc], output_contrib=True)\n",
        "    except TypeError:\n",
        "        try:  # interpret 0.24  0.25\n",
        "            _, contrib = ebm.predict_and_contrib(X_sub.iloc[:n_loc])\n",
        "        except (AttributeError, TypeError):\n",
        "            contrib = None\n",
        "\n",
        "    # fallback manual si API oficial no disponible\n",
        "    if contrib is None:\n",
        "        print(\"[DEBUG] API local no disponible, calculando manualmente\")\n",
        "        term_scores = ebm.term_scores_\n",
        "        term_feats  = getattr(ebm, \"feature_groups_\", getattr(ebm, \"term_features_\", None))\n",
        "        if term_feats is None:\n",
        "            term_feats = [[i] for i in range(len(cols))]\n",
        "        bins_attr = \"bin_edges_\" if hasattr(ebm, \"bin_edges_\") else \"bins_\"\n",
        "        bin_struct = getattr(ebm, bins_attr)\n",
        "\n",
        "        contrib = np.zeros((n_loc, len(cols)))\n",
        "        for t, feats in enumerate(term_feats):\n",
        "            if len(feats) != 1:\n",
        "                continue\n",
        "            feat_idx = feats[0]\n",
        "            # obtener cortes\n",
        "            cuts = np.asarray(bin_struct[t].get(\"cuts\", []) if isinstance(bin_struct[t], dict) else bin_struct[t])\n",
        "            if cuts.size == 0:\n",
        "                continue\n",
        "            scores = term_scores[t]\n",
        "            vals = X_sub.iloc[:n_loc, feat_idx].values\n",
        "            bin_idx = np.searchsorted(cuts, vals, side=\"right\")\n",
        "            contrib[:, feat_idx] = scores[bin_idx]\n",
        "\n",
        "    contrib_df = pd.DataFrame(contrib, columns=cols, index=X_sub.index[:n_loc])\n",
        "    print(\"[DEBUG] Contribuciones locales calculadas\")\n",
        "\n",
        "    display(HTML(f\"<h4> Contribuciones locales (primeras {n_loc})</h4>\"))\n",
        "    display(contrib_df)\n",
        "\n",
        "    # 6) Grfico de importancia local media\n",
        "    mean_abs = contrib_df.abs().mean().reindex(gains.index)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.scatter(range(len(mean_abs)), mean_abs.values, color=\"seagreen\")\n",
        "    plt.xticks(range(len(mean_abs)), mean_abs.index, rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"|Contribucin| media\")\n",
        "    plt.title(\"Importancia EBM (media |contribucin|)\")\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "    print(\"[DEBUG] Grfico local EBM generado\")\n",
        "\n",
        "    display(HTML(\n",
        "        \"<p>Cada punto muestra la media del valor absoluto de la contribucin de la variable \"\n",
        "        f\"en las primeras {n_loc} muestras de la submuestra.</p>\"\n",
        "    ))\n",
        "\n",
        "    # 7) Estadsticas adicionales\n",
        "    stats = {\n",
        "        'ebm_global_percentiles': gains.quantile([0.25,0.5,0.75]).to_dict(),\n",
        "        'ebm_local_percentiles':  mean_abs.quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadsticas EBM: {stats}\")\n",
        "\n",
        "    # 8) Resultado\n",
        "    return {\n",
        "        'imp_df':      imp_df,\n",
        "        'df_local':    contrib_df,\n",
        "        'fig_summary': fig,\n",
        "        'stats':       stats\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 14  Motor Optuna  Importancia de hiperparmetros\n",
        "# ============================================================\n",
        "def _motor_optuna(\n",
        "    X: pd.DataFrame,            # (no se usa pero se mantiene la firma)\n",
        "    cols: list[str],            # (no se usa)\n",
        "    _,                          # predict_fn (sin uso)\n",
        "    default_file: str = \"optuna_study.pkl\",\n",
        "    min_trials: int = 10        # n mnimo aconsejable de trials\n",
        "):\n",
        "    \"\"\"\n",
        "    Muestra la importancia de hiperparmetros de un `optuna.study.Study`.\n",
        "    1) Busca un objeto `study` en memoria.\n",
        "    2) Si no lo encuentra, intenta cargar `default_file`.\n",
        "    3) Si tampoco est, escanea el directorio en busca de `*.pkl`\n",
        "       con un objeto Study dentro.\n",
        "    4) Si sigue sin hallarlo, muestra un mensaje muy explcito con\n",
        "       los pasos para generarlo.\n",
        "    \"\"\"\n",
        "\n",
        "    #  0. Comprobar dependencias\n",
        "    if optuna is None:\n",
        "        display(HTML(\n",
        "            \"<p style='color:red'> <code>optuna</code> no est instalado. \"\n",
        "            \"Ejecuta <code>pip install optuna</code> e intntalo de nuevo.</p>\"\n",
        "        ))\n",
        "        return\n",
        "    try:\n",
        "        from optuna.importance import get_param_importances\n",
        "    except Exception as e:\n",
        "        display(HTML(\n",
        "            f\"<p style='color:red'> No se pudo importar \"\n",
        "            f\"<code>optuna.importance</code>: {e}</p>\"\n",
        "        ))\n",
        "        return\n",
        "\n",
        "    #  1. Intentar encontrar el Study en memoria -------------\n",
        "    study = globals().get(\"study\")\n",
        "    source = \"memoria\"\n",
        "\n",
        "    #  2. Intentar cargar el fichero por defecto -------------\n",
        "    if study is None and os.path.exists(default_file):\n",
        "        try:\n",
        "            with open(default_file, \"rb\") as f:\n",
        "                study = pickle.load(f)\n",
        "            source = f'archivo {default_file}'\n",
        "        except Exception as e:\n",
        "            display(HTML(\n",
        "                f\"<p style='color:red'> No se pudo cargar \"\n",
        "                f\"<code>{default_file}</code>: {e}</p>\"\n",
        "            ))\n",
        "\n",
        "    #  3. Buscar cualquier *.pkl si an no hay Study ---------\n",
        "    if study is None:\n",
        "        for pkl in glob.glob(\"*.pkl\"):\n",
        "            try:\n",
        "                with open(pkl, \"rb\") as f:\n",
        "                    obj = pickle.load(f)\n",
        "                if isinstance(obj, optuna.study.Study):\n",
        "                    study = obj\n",
        "                    source = f'archivo {pkl}'\n",
        "                    break\n",
        "            except Exception:\n",
        "                continue   # ignorar .pkl que no sean Study\n",
        "\n",
        "    #  4. Si sigue sin Study  gua al usuario ---------------\n",
        "    if study is None:\n",
        "        display(HTML(\n",
        "            f\"\"\"\n",
        "            <div style='border:1px solid #e57373;padding:10px;border-radius:6px'>\n",
        "              <h4 style='margin-top:0;color:#c62828'>  No se encontr ningn estudio Optuna</h4>\n",
        "              <p>\n",
        "                Para utilizar este panel primero necesitas <b>crear y guardar</b> un estudio\n",
        "                Optuna. Tienes dos formas:\n",
        "              </p>\n",
        "              <ol>\n",
        "                <li>Ejecuta una optimizacin desde el <i>Bloque&nbsp;3  Optimizacin</i>\n",
        "                    (elige motor <code>Optuna</code>). Al finalizar se guardar\n",
        "                    automticamente <code>{default_file}</code>.</li>\n",
        "                <li>Si ya tienes un objeto <code>study</code>, gurdalo manualmente:<br>\n",
        "                   <code>import pickle<br>\n",
        "                   with open(\"{default_file}\", \"wb\") as f:<br>&nbsp;&nbsp;&nbsp;&nbsp;pickle.dump(study, f)</code>\n",
        "                </li>\n",
        "              </ol>\n",
        "              <p>Vuelve a lanzar la explicacin cuando dispongas del archivo.</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        ))\n",
        "        return\n",
        "\n",
        "    #  5. El Study se ha encontrado --------------------------\n",
        "    n_trials = len(study.trials)\n",
        "    display(HTML(\n",
        "        f\"<p> <i>Study</i> localizado desde <b>{source}</b> \"\n",
        "        f\"con <b>{n_trials}</b> trials.</p>\"\n",
        "    ))\n",
        "    if n_trials < min_trials:\n",
        "        display(HTML(\n",
        "            f\"<p style='color:#c57f17'> El estudio contiene menos de \"\n",
        "            f\"{min_trials} trials; la estimacin de importancia puede ser inestable.</p>\"\n",
        "        ))\n",
        "\n",
        "    #  6. Calcular importancia de hiperparmetros ------------\n",
        "    display(HTML(\"<h4> Importancia global de hiperparmetros</h4>\"))\n",
        "    try:\n",
        "        importances = get_param_importances(study)\n",
        "    except Exception as e:\n",
        "        display(HTML(\n",
        "            f\"<p style='color:red'> Fall el clculo de importancia: {e}</p>\"\n",
        "        ))\n",
        "        return\n",
        "\n",
        "    imp_series = pd.Series(importances).sort_values(ascending=False)\n",
        "    imp_df = imp_series.reset_index().rename(columns={'index':'feature', 0:'value'})\n",
        "    display(imp_df)\n",
        "    display(imp_series.to_frame('Contribucin').style.format('{:.2%}'))\n",
        "    display(HTML(\n",
        "        '<p><b>Cmo leerla?</b> El porcentaje indica cunto explica cada '\n",
        "        'hiperparmetro la variacin de la mtrica objetivo. '\n",
        "        '<br> <b>> 25 %</b>  parmetro crtico.<br>'\n",
        "        ' <b>< 5 %</b>  parmetro con poca influencia.</p>'\n",
        "    ))\n",
        "\n",
        "    #  7. Mostrar Top-10 trials ------------------------------\n",
        "    best_trials = sorted(study.trials, key=lambda t: t.value)[:10]\n",
        "    df_trials   = pd.DataFrame(\n",
        "        [{\"value\": t.value, **t.params} for t in best_trials]\n",
        "    )\n",
        "    display(HTML(\"<h4> Top 10 trials</h4>\")); display(df_trials)\n",
        "\n",
        "    #  8. Grfico de barras ---------------------------------\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar(imp_series.index, imp_series.values, color='mediumpurple')\n",
        "    plt.ylabel('Contribucin (%)'); plt.title('Importancia hiperparmetros Optuna')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()               # Nuevo para la celda 12\n",
        "    plt.show()\n",
        "\n",
        "    display(HTML(\n",
        "        '<p>La altura de cada barra muestra la influencia relativa. '\n",
        "        'salo para priorizar en futuras bsquedas.</p>'\n",
        "    ))\n",
        "\n",
        "    stats = {\n",
        "        'optuna_imp_percentiles': imp_series.quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "    resultado = {\n",
        "        'imp_df': imp_df,\n",
        "        'df_local': df_trials,  # top trials\n",
        "        'fig_summary': fig,\n",
        "        'stats': stats\n",
        "    }\n",
        "    return resultado\n",
        "\n",
        "\n",
        "pass\n",
        "\n",
        "out_rec = widgets.Output()\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Funcin para mostrar UI xIA\n",
        "# ----------------------------------------------------------------\n",
        "def mostrar_xai():\n",
        "    import pandas as pd\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "\n",
        "    out_xai = widgets.Output()\n",
        "\n",
        "    # 1) Output para pintar la tabla de recomendaciones\n",
        "    out_rec = widgets.Output()\n",
        "\n",
        "    # Aqu tu lista de dicts con todas las recomendaciones\n",
        "    recomendaciones = [\n",
        "        {\"Modelo\":\"SVR\",          \" Motor\":\"SHAP\",                     \" Rendimiento\":\"Alto\",    \" Rapidez\":\"Lento\",   \" Justificacin\":\"Ideal para explicaciones globales en SVR.\"},\n",
        "        {\"Modelo\":\"SVR\",          \" Motor\":\"LIME\",                     \" Rendimiento\":\"Medio\",   \" Rapidez\":\"Medio\",   \" Justificacin\":\"Explicaciones locales muy intuitivas.\"},\n",
        "        {\"Modelo\":\"NN\",           \" Motor\":\"Integrated Gradients\",     \" Rendimiento\":\"Alto\",    \" Rapidez\":\"Medio\",   \" Justificacin\":\"ptimo para redes diferenciables.\"},\n",
        "        {\"Modelo\":\"NN\",           \" Motor\":\"DeepLIFT / LRP\",           \" Rendimiento\":\"Alto\",    \" Rapidez\":\"Rpido\",  \" Justificacin\":\"Muy eficiente en Atribuciones acumuladas.\"},\n",
        "        {\"Modelo\":\"NN\",           \" Motor\":\"SHAP\",                     \" Rendimiento\":\"Alto\",    \" Rapidez\":\"Lento\",   \" Justificacin\":\"Model-agnstico, capta complejidades no lineales.\"},\n",
        "        {\"Modelo\":\"XGBoost\",      \" Motor\":\"SHAP (TreeExplainer)\",     \" Rendimiento\":\"Alto\",    \" Rapidez\":\"Rpido\",  \" Justificacin\":\"Explainer nativo y ultra-rpido para rboles.\"},\n",
        "        {\"Modelo\":\"XGBoost\",      \" Motor\":\"Partial Dependence Plot\",  \" Rendimiento\":\"Medio\",   \" Rapidez\":\"Medio\",   \" Justificacin\":\"Visualiza efectos marginales.\"},\n",
        "        {\"Modelo\":\"RandomForest\", \" Motor\":\"SHAP (TreeExplainer)\",     \" Rendimiento\":\"Alto\",    \" Rapidez\":\"Rpido\",  \" Justificacin\":\"Exacto global para bosques.\"},\n",
        "        {\"Modelo\":\"RandomForest\", \" Motor\":\"Permutation Importance\",   \" Rendimiento\":\"Medio\",   \" Rapidez\":\"Medio\",   \" Justificacin\":\"Fcil de comparar importancias.\"},\n",
        "        {\"Modelo\":\"RNN\",          \" Motor\":\"Integrated Gradients\",     \" Rendimiento\":\"Alto\",    \" Rapidez\":\"Medio\",   \" Justificacin\":\"Captura efectos temporales.\"},\n",
        "        {\"Modelo\":\"RNN\",          \" Motor\":\"DeepLIFT / LRP\",           \" Rendimiento\":\"Alto\",    \" Rapidez\":\"Rpido\",  \" Justificacin\":\"Eficiente en series temporales.\"},\n",
        "    ]\n",
        "\n",
        "    # 2) Dropdown para seleccionar modelo\n",
        "    model_dd = widgets.Dropdown(\n",
        "        options=['SVR','NN','XGBoost','RandomForest','RNN'],\n",
        "        description='Modelo:',\n",
        "        layout=widgets.Layout(width='50%')\n",
        "    )\n",
        "\n",
        "    # 3) Callback que construye y muestra la tabla\n",
        "    def _on_model_change(change):\n",
        "        if change['type']=='change' and change['name']=='value':\n",
        "            key = change['new']\n",
        "            # Filtrar slo las filas que correspondan al modelo seleccionado\n",
        "            df = pd.DataFrame([r for r in recomendaciones if r['Modelo']==key])\n",
        "            # Estilizar\n",
        "            styled = (df.style\n",
        "                .set_table_styles([\n",
        "                    {'selector':'th', 'props':[('background-color','#2E3B4E'),\n",
        "                                              ('color','white'),\n",
        "                                              ('font-size','14px'),\n",
        "                                              ('padding','3px'),\n",
        "                                              ('text-align','center')]},\n",
        "                    {'selector':'td', 'props':[('font-size','12px'),\n",
        "                                              ('padding','3px'),\n",
        "                                              ('text-align','left')]},\n",
        "                ])\n",
        "                .hide(axis='index')\n",
        "                .set_caption(f\" Recomendaciones xIA para {key}\")\n",
        "            )\n",
        "            # Pintar\n",
        "            with out_rec:\n",
        "                out_rec.clear_output()\n",
        "                display(styled)\n",
        "\n",
        "    # 4) Registrar el observer **despus** de haber definido la funcin**\n",
        "    model_dd.observe(_on_model_change, names='value')\n",
        "\n",
        "    # 1) Tipo\n",
        "    tipo = widgets.Dropdown(options=[('Entrenado','entrenado'),('Optimizado','optimo')], description='Tipo:', layout=widgets.Layout(width='400px'), style={'description_width': '100px'})\n",
        "    # 2) Modelo\n",
        "    modelo = widgets.Dropdown(options=[], description='Modelo:', layout=widgets.Layout(width='400px'), style={'description_width': '100px'})\n",
        "    # 3) Mtodo seleccin\n",
        "    metodo_sel = widgets.Dropdown(options=SELECT_METHODS, description='Mtodo X:', layout=widgets.Layout(width='400px'), style={'description_width': '100px'})\n",
        "    # xIA methods\n",
        "    xai = widgets.SelectMultiple(options=XAI_METHODS, description='xIA:', layout=widgets.Layout(width='400px', height='150px'), style={'description_width': '100px'})\n",
        "\n",
        "    # Widget de ayuda para describir el mtodo xIA seleccionado\n",
        "    help_html = widgets.HTML(value='Seleccione un mtodo xIA para ver su descripcin aqu.')  # <-- Lnea nueva\n",
        "\n",
        "    # Callback para actualizar lista de modelos segn tipo\n",
        "    def _on_tipo(change):\n",
        "        modelo.options = TRAINED_MODELS if change['new']=='entrenado' else OPTIMIZED_MODELS\n",
        "    tipo.observe(_on_tipo, names='value')\n",
        "    modelo.options = TRAINED_MODELS  # por defecto\n",
        "\n",
        "    # Callback para mostrar ayuda dinmica segn seleccin de xIA\n",
        "    def _on_xai(change):\n",
        "        selected = change['new']\n",
        "        if not selected:\n",
        "            help_html.value = 'Seleccione un mtodo xIA para ver su descripcin aqu.'  # <-- Lnea nueva\n",
        "        else:\n",
        "            parts = []\n",
        "            for m in selected:\n",
        "                desc = XAI_HELP.get(m, '')\n",
        "                if m == 'SHAP': parts.append(desc + _generate_shap_diagram())\n",
        "                elif m == 'LIME': parts.append(desc + _generate_lime_diagram())\n",
        "                elif m == 'KernelExplainer': parts.append(desc + _generate_shap_diagram())\n",
        "                elif m == 'Integrated Gradients': parts.append(desc + _generate_ig_diagram())\n",
        "                elif m == 'DeepLIFT / LRP': parts.append(desc + _generate_dl_diagram())\n",
        "                elif m == 'Permutation Feature Importance': parts.append(desc + _generate_perm_diagram())\n",
        "                elif m == 'Partial Dependence Plots (PDP)': parts.append(desc + _generate_pdp_diagram())\n",
        "                elif m == 'Accumulated Local Effects (ALE)': parts.append(desc+_generate_ale_diagram())\n",
        "                elif m == \"Individual Conditional Expectation (ICE) Plots\": parts.append(desc+_generate_ice_diagram())\n",
        "                elif m == \"Counterfactual Explanations\": parts.append(desc+_generate_cf_diagram())\n",
        "                elif m == \"Anchors\": parts.append(desc + _generate_anchor_diagram())\n",
        "                elif m == \"Surrogate Models (Global/Local)\": parts.append(desc + _generate_surr_diagram())\n",
        "                elif m == \"Explainable Boosting Machine (EBM)\": parts.append(desc + _generate_ebm_diagram())\n",
        "                elif m == \"Optuna Hyperparameter Importance\": parts.append(desc + _generate_optuna_diagram())\n",
        "                else:\n",
        "                    parts.append(desc)\n",
        "            help_html.value = ''.join(parts)\n",
        "    xai.observe(_on_xai, names='value')\n",
        "\n",
        "    # Botn de explicacin\n",
        "    btn = widgets.Button(description=' Explicar', button_style='info')\n",
        "\n",
        "    def _on_explain(b):\n",
        "        import ipywidgets as widgets\n",
        "        from IPython.display import display, clear_output\n",
        "        global xai_results\n",
        "        with out_xai:\n",
        "            clear_output()\n",
        "\n",
        "            t = tipo.value\n",
        "            m_disp = modelo.value\n",
        "            print(f\"-> Tipo: {t}\")\n",
        "            print(f\"-> Modelo: {m_disp}\")\n",
        "            print(f\"-> Mtodo seleccin: {metodo_sel.value}\")\n",
        "            print(f\"-> xIA seleccionadas: {', '.join(xai.value)}\")\n",
        "\n",
        "            raw = modelo.value  # p.ej. \"RNN\"\n",
        "            key = None\n",
        "            model_display = None\n",
        "\n",
        "            # 1) Match exacto\n",
        "            if raw in MODEL_KEYS:\n",
        "                key = MODEL_KEYS[raw]\n",
        "                model_display = raw\n",
        "            else:\n",
        "                # 2) Fallback sufijo (muy raro que empiece a usarse)\n",
        "                for display_name, short_key in MODEL_KEYS.items():\n",
        "                    if raw.endswith(display_name):\n",
        "                        key = short_key\n",
        "                        model_display = display_name\n",
        "                        break\n",
        "\n",
        "            if key is None:\n",
        "                raise ValueError(f\"No puedo mapear {raw} a clave interna de modelo.\")\n",
        "\n",
        "            # Asegurarnos de tener el dict inicializado\n",
        "            if 'xai_results' not in globals():\n",
        "                xai_results = {}\n",
        "            if model_display not in xai_results:\n",
        "                xai_results[model_display] = {}\n",
        "            if tipo.value == \"entrenado\":\n",
        "                patrones = [\n",
        "                    #  EN EL DIRECTORIO ACTUAL \n",
        "                    f\"modelo_{key}_{metodo_sel.value.lower()}.pkl\",\n",
        "                    f\"{key}_{metodo_sel.value.lower()}.pkl\",\n",
        "                    f\"modelo_{key}_{metodo_sel.value.lower()}.h5\",\n",
        "                    f\"{key}_{metodo_sel.value.lower()}.h5\",\n",
        "                    #  EN SUBCARPETAS (recursivo) \n",
        "                    f\"**/modelo_{key}_{metodo_sel.value.lower()}.pkl\",\n",
        "                    f\"**/{key}_{metodo_sel.value.lower()}.pkl\",\n",
        "                    f\"**/modelo_{key}_{metodo_sel.value.lower()}.h5\",\n",
        "                    f\"**/{key}_{metodo_sel.value.lower()}.h5\",\n",
        "                ]\n",
        "            else:  # 'optimo'\n",
        "                patrones = [\n",
        "                    # bsqueda de modelo serializado\n",
        "                    f\"modelos_opt/modelo_{key}_{metodo_sel.value.lower()}*_opt*.pkl\",\n",
        "                    # en caso de que guardes metadata por separado\n",
        "                    f\"modelos_opt/meta_{key}_{metodo_sel.value.lower()}*_opt*.pkl\",\n",
        "                    # si tambin guardas .h5\n",
        "                    f\"modelos_opt/modelo_{key}_{metodo_sel.value.lower()}*_opt*.h5\",\n",
        "                ]\n",
        "            print(\"[DEBUG] patrones =\", patrones)\n",
        "            # 2) Bsqueda recursiva\n",
        "            files = []\n",
        "            for pat in patrones:\n",
        "                files.extend(glob.glob(pat, recursive=True))\n",
        "\n",
        "            # 3) Depuracin opcional (puedes comentar la siguiente lnea cuando compruebes que funciona)\n",
        "            pprint.pprint(files)\n",
        "\n",
        "            # 4) Seleccionar la primera coincidencia\n",
        "            if not files:\n",
        "                print(f\"  No se encontr ningn archivo que coincida con los patrones:\\n    {patrones}\")\n",
        "                return\n",
        "            ruta_modelo = files[0]\n",
        "            print(f\"  Modelo encontrado en: {ruta_modelo}\")\n",
        "\n",
        "            print(f\"Verbose: Archivos encontrados: {files}\")\n",
        "            if not ruta_modelo:\n",
        "                print(f\" No se encontr ningn archivo para patrn(s): {patrones}\")\n",
        "                return\n",
        "            print(f\"Verbose: Cargando ruta_modelo: {ruta_modelo}\")\n",
        "\n",
        "            #  cargar modelo y escaladores \n",
        "            if ruta_modelo.endswith('.pkl'):\n",
        "                with open(ruta_modelo, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                # si es un fichero de metadata (no contiene 'model'), buscamos el .h5 asociado\n",
        "                if 'model' not in datos:\n",
        "                    print(\"Verbose: fichero metadata detectado, buscado .h5 correspondiente\")\n",
        "                    # Ejemplo: modelos_opt/meta_nn_pearson_opt_randomsearch.pkl\n",
        "                    modelo_h5 = ruta_modelo.replace('/meta_', '/modelo_').replace('meta_', 'modelo_').replace('.pkl', '.h5')\n",
        "                    model_obj = load_model(modelo_h5, compile=False)\n",
        "                    #  Elige una de las dos:\n",
        "                    # 1) Descompilar:\n",
        "                    model_obj = load_model(modelo_h5, compile=False)\n",
        "                else:\n",
        "                    model_obj = datos['model']\n",
        "                # en ambos casos sacamos scalers y cols de este .pkl\n",
        "                sx   = datos.get('sx', datos.get('scaler_X'))\n",
        "                sy   = datos.get('sy', datos.get('scaler_Y'))\n",
        "                cols = datos['cols']\n",
        "                print(f\"Verbose: Escaladores y cols cargados de {ruta_modelo}: {list(datos.keys())}\")\n",
        "            elif ruta_modelo.endswith('.h5'):\n",
        "                model_obj = load_model(ruta_modelo)\n",
        "                # cargamos slo el scaler de X si existe en tu metadata\n",
        "                meta_file = ruta_modelo.replace('modelo_','escaladores_').replace('.h5','.pkl')\n",
        "                with open(meta_file,'rb') as f:\n",
        "                    datos_meta = pickle.load(f)\n",
        "                sx = datos_meta.get('sx', datos_meta.get('scaler_X'))\n",
        "                sy = None\n",
        "                cols = datos_meta['cols']\n",
        "                print(\"Verbose: NN .h5 cargado. Slo sx:\", sx, \"sy ser None\")\n",
        "            else:\n",
        "                raise ValueError(f\"Formato de fichero no soportado: {ruta_modelo}\")\n",
        "\n",
        "            #  preparar X antes de llamar a los motores \n",
        "            X = X_data[cols].copy()\n",
        "            print(f\"Verbose: Columnas seleccionadas: {cols}\")\n",
        "            print(f\"Verbose: X_data shape: {X.shape}\")\n",
        "\n",
        "            #  construir funcin de prediccin \n",
        "            if sy is not None:\n",
        "                predict_fn = lambda X_in: sy.inverse_transform(\n",
        "                    model_obj.predict(sx.transform(X_in)).reshape(-1,1)\n",
        "                ).ravel()\n",
        "            else:\n",
        "                if sx is not None:\n",
        "                    predict_fn = lambda X_in: model_obj.predict(sx.transform(X_in)).ravel()\n",
        "                else:\n",
        "                    predict_fn = lambda X_in: model_obj.predict(X_in).ravel()\n",
        "\n",
        "            # 0) Preparar lista de motores a ejecutar:\n",
        "            seleccion = list(xai.value)\n",
        "            if \"Todos\" in seleccion:\n",
        "                seleccion = ALL_MOTORES\n",
        "\n",
        "            # ----------------------------------------------------------\n",
        "            #   Motores de explicacin (13 Motores:\n",
        "            # SHAP, LIME, KernelExplainer, Integrated Gradients, DeepLIFT/LRP, Permutation Importance, Partial Dependence Plots (PDP),\n",
        "            # Accumulated Local Effects (ALE), Individual Conditional Expectation (ICE) Plots, Counterfactual Explanations, Anchors, Surrogate Models (Global/Local),\n",
        "            # Explainable Boosting Machine (EBM) y Optuna Hyperparameter Importance.\n",
        "            # ----------------------------------------------------------\n",
        "            # ------------- 1. SHAP --------------------------------\n",
        "            if \"SHAP\" in seleccion:\n",
        "                res = _motor_shap(key, model_obj, X, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['SHAP'] = res\n",
        "\n",
        "            # ------------- 2. LIME --------------------------------\n",
        "            if \"LIME\" in seleccion:\n",
        "                res = _motor_lime(X, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['LIME'] = res\n",
        "\n",
        "            # ------------- 3. KernelExplainer ---------------------\n",
        "            if \"KernelExplainer\" in seleccion:\n",
        "                res = _motor_kernel(X, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['KernelExplainer'] = res\n",
        "\n",
        "            # ------------- 4. IntegratedGradients ----------------\n",
        "            if \"Integrated Gradients\" in seleccion:\n",
        "                res = _motor_ig(key, model_obj, X, cols, sx, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Integrated Gradients'] = res\n",
        "\n",
        "            # ------------- 5. DeepLIFT / LRP ----------------------\n",
        "            if \"DeepLIFT / LRP\" in seleccion:\n",
        "                res = _motor_dl(model_obj, key, X, cols, sx, sy)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['DeepLIFT / LRP'] = res\n",
        "\n",
        "            # ------------- 6. Permutation Importance --------------\n",
        "            if \"Permutation Feature Importance\" in seleccion:\n",
        "                res = _motor_perm(model_obj, X, cols, sx, sy)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Permutation Feature Importance'] = res\n",
        "\n",
        "            # ------------- 7. Partial Dependence Plots (PDP) ------\n",
        "            if \"Partial Dependence Plots (PDP)\" in seleccion:\n",
        "                res = _motor_pdp(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Partial Dependence Plots (PDP)'] = res\n",
        "\n",
        "            # ------------- 8. Accumulated Local Effects (ALE) ------\n",
        "            if \"Accumulated Local Effects (ALE)\" in seleccion:\n",
        "                res = _motor_ale(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Accumulated Local Effects (ALE)'] = res\n",
        "\n",
        "            # ------------- 9. Individual Conditional Expectation (ICE) Plots ------\n",
        "            if \"Individual Conditional Expectation (ICE) Plots\" in seleccion:\n",
        "                res = _motor_ice(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Individual Conditional Expectation (ICE) Plots'] = res\n",
        "\n",
        "            # ------------- 10. Counterfactual Explanations ------\n",
        "            if \"Counterfactual Explanations\" in seleccion:\n",
        "                res = _motor_counterfactual(X, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Counterfactual Explanations'] = res\n",
        "\n",
        "            # ------------- 11. Anchors ------\n",
        "            if \"Anchors\" in seleccion:\n",
        "                res = _motor_anchors(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Anchors'] = res\n",
        "\n",
        "            # ------------- 12. Surrogate Models (Global/Local) ------\n",
        "            if \"Surrogate Models (Global/Local)\" in seleccion:\n",
        "                res = _motor_surrogate(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Surrogate Models (Global/Local)'] = res\n",
        "\n",
        "            # ------------- 13. Explainable Boosting Machine (EBM) ------\n",
        "            if \"Explainable Boosting Machine (EBM)\" in seleccion:\n",
        "                res = _motor_ebm(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Explainable Boosting Machine (EBM)'] = res\n",
        "\n",
        "            # ------------- 14. Optuna Hyperparameter Importance ------\n",
        "            if \"Optuna Hyperparameter Importance\" in seleccion:\n",
        "                res = _motor_optuna(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Optuna Hyperparameter Importance'] = res\n",
        "\n",
        "    # enlazar callbacks\n",
        "    btn.on_click(_on_explain)\n",
        "\n",
        "    display(\n",
        "        widgets.VBox([\n",
        "            tipo, modelo, metodo_sel, xai, help_html, model_dd, out_rec, btn, out_xai\n",
        "        ])\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 12. GENERAR INFORME  TABLAS FORMATEADAS + EXPLICACIN IA\n",
        "# ===============================================================\n",
        "from IPython.display import clear_output, Markdown, display\n",
        "from IPython.display import HTML\n",
        "import pandas as pd\n",
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import ipywidgets as widgets\n",
        "import scipy.stats\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "from matplotlib.figure import Figure\n",
        "import numpy as np\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "import re\n",
        "\n",
        "def sanitize_name(s):\n",
        "    \"\"\"\n",
        "    Unifica la sanitizacin de cualquier string de columna:\n",
        "    - Reemplaza todo carcter no alfanumrico o guin bajo por '_'\n",
        "    - Colapsa mltiples '_' consecutivos\n",
        "    - Elimina '_' al inicio y final\n",
        "    \"\"\"\n",
        "    t = re.sub(r\"[^\\w]\", \"_\", str(s))\n",
        "    t = re.sub(r\"_+\", \"_\", t)\n",
        "    return t.strip(\"_\")\n",
        "\n",
        "\n",
        "#  0) Inicializamos el cliente OpenAI (asegrate de exportar OPENAI_API_KEY)\n",
        "_api_key = os.getenv(\"OPENAI_API_KEY\") or \"\"\n",
        "_client = OpenAI(api_key=_api_key, timeout=30)\n",
        "\n",
        "# Nmero mximo de tokens y Temperatura para explicacin de la IA\n",
        "MAX_EXPLANATION_TOKENS = 800     # Valor recomendado por la IA - Para equilibrar coste, nivel de detalle y consistencia en los anlisis IA de PMS IA recomienda con GPT-4 (lmite 8 192), usar 1 500 tokens - para reducir coste usamos 1.000\n",
        "#MAX_EXPLANATION_TOKENS = 10      # Limitado para que se reduzca consumo de API y se reduzca tiempo de ejecucin en pruebas\n",
        "TEMPERATURE_VAL = 0.2             # Valor recomendado por la IA - Se fija en 0.2 para dar un poco de espontaneidad a la IA de modo que se generen explicaciones ms naturales o ejemplos. No subir de este nivel.\n",
        "\n",
        "class ReportBuilder:\n",
        "    \"\"\"Orquesta la creacin del informe a partir de globals().\"\"\"\n",
        "    def __init__(self, global_ns):\n",
        "        self.g = global_ns\n",
        "        #self.sections = []\n",
        "\n",
        "        #  AADIDO: sanitizar slo una vez que X_train y X_test existen \n",
        "        #import re\n",
        "        #def clean_name(s):\n",
        "        #    t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))\n",
        "        #    t = re.sub(r'_+', '_', t)\n",
        "        #    return t.strip('_')\n",
        "\n",
        "        # 1) Limpieza de X_train y X_test\n",
        "        #for df_name in ('X_train','X_test'):\n",
        "        #    if df_name in self.g:\n",
        "        #        df = self.g[df_name]\n",
        "        #        df.columns = [clean_name(c) for c in df.columns]\n",
        "\n",
        "        # 2) Limpieza de RESUMEN_METODOS\n",
        "        #if 'RESUMEN_METODOS' in self.g:\n",
        "        #    for m, lst in self.g['RESUMEN_METODOS'].items():\n",
        "        #        if isinstance(lst, list):\n",
        "        #            self.g['RESUMEN_METODOS'][m] = [clean_name(c) for c in lst]\n",
        "        #        elif isinstance(lst, pd.DataFrame) and not lst.empty:\n",
        "        #            # si fuese DataFrame, saneamos su columna de Variable\n",
        "        #            col = 'Variable' if 'Variable' in lst.columns else lst.columns[0]\n",
        "        #            self.g['RESUMEN_METODOS'][m][col] = lst[col].astype(str).map(clean_name)\n",
        "\n",
        "        #  **AADIDO** SANITIZACIN DE LOS payload[\"cols\"] EN OPT_MODELS \n",
        "        #if 'OPT_MODELS' in self.g:\n",
        "        #    for key, payload in self.g['OPT_MODELS'].items():\n",
        "        #        if isinstance(payload, dict) and 'cols' in payload:\n",
        "        #            payload['cols'] = [clean_name(c) for c in payload['cols']]\n",
        "        #  FIN AADIDO \n",
        "\n",
        "        self.sections = []\n",
        "\n",
        "        # NUEVO: atributos para el mejor modelo (se rellenarn en seccin de seleccin integral)\n",
        "        self.best_model_info = {}\n",
        "\n",
        "        self.figures = {}   # Guardaremos aqu las figuras matplotlib\n",
        "        print(\"[DEBUG] 1.1. ReportBuilder.__init__\")\n",
        "\n",
        "    def build_sections(self):\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "        import matplotlib.pyplot as plt\n",
        "        from scipy import stats\n",
        "        print(\"[DEBUG] 1.2. ReportBuilder.build_sections start\")\n",
        "\n",
        "        # --- Inicio de Bloque para normalizar el payload de OPT_MODELS ---\n",
        "        def _normalize_payload(raw):\n",
        "            \"\"\"\n",
        "            Toma un payload arbitrario de OPT_MODELS y devuelve siempre\n",
        "            un dict con las mismas claves: model, sx, sy, cols, best_params, score, metric.\n",
        "            \"\"\"\n",
        "            norm = {}\n",
        "            # 1) Modelo\n",
        "            if 'model' in raw:\n",
        "                norm['model'] = raw['model']\n",
        "            elif raw.get('model_path'):\n",
        "                try:\n",
        "                    import joblib, tensorflow as tf\n",
        "                    if raw['model_path'].endswith(('.h5','.tf')):\n",
        "                        from tensorflow.keras.models import load_model\n",
        "                        norm['model'] = load_model(raw['model_path'], compile=False)\n",
        "                    else:\n",
        "                        norm['model'] = joblib.load(raw['model_path'])\n",
        "                except:\n",
        "                    norm['model'] = None\n",
        "            else:\n",
        "                norm['model'] = None\n",
        "\n",
        "            # 2) Mtadatos: sx, sy, cols, score, metric, best_params\n",
        "            for k in ('sx','sy','cols','score','metric','best_params'):\n",
        "                if k in raw:\n",
        "                    norm[k] = raw[k]\n",
        "                elif raw.get('meta_path'):\n",
        "                    if '_meta' not in raw:\n",
        "                        import pickle\n",
        "                        raw['_meta'] = pickle.load(open(raw['meta_path'],'rb'))\n",
        "                    norm[k] = raw['_meta'].get(k) if k!='best_params' else raw['_meta'].get('best_params', {})\n",
        "                else:\n",
        "                    norm[k] = None\n",
        "\n",
        "            #  AADIDO: asegurar que los cols del payload estn saneados \n",
        "            #if norm.get('cols') is not None:\n",
        "            #    import re\n",
        "            #    def clean_name(s):\n",
        "            #        t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))\n",
        "            #        t = re.sub(r'_+', '_', t).strip('_')\n",
        "            #        return t\n",
        "            #    norm['cols'] = [clean_name(c) for c in norm['cols']]\n",
        "            # \n",
        "\n",
        "            return norm\n",
        "\n",
        "        # ahora, al empezar cada bloque de optimizacin, en lugar de:\n",
        "        #    payload = OPT_MODELS[('svr', sel_method, engine)]\n",
        "        #    model  = payload['model']; sx = payload['sx']; ...\n",
        "        # haras:\n",
        "        #    raw = OPT_MODELS[('svr', sel_method, engine)]\n",
        "        #    p   = _normalize_payload(raw)\n",
        "        #    model, sx, sy, cols, score, metric, best_params = (\n",
        "        #        p['model'], p['sx'], p['sy'], p['cols'], p['score'], p['metric'], p['best_params']\n",
        "        #    )\n",
        "        # --- Fin de Bloque para normalizar el payload de OPT_MODELS ---\n",
        "\n",
        "        self.sections.clear()\n",
        "        #all_metrics = []\n",
        "        # =============================================================\n",
        "        # 1. Carga de datos\n",
        "        # =============================================================\n",
        "        try:\n",
        "            if all(k in self.g for k in (\"X_data\", \"Y_data\", \"FECHAS\")):\n",
        "                X_data = self.g[\"X_data\"]\n",
        "                Y_data = self.g[\"Y_data\"]\n",
        "                FECHAS = self.g[\"FECHAS\"]\n",
        "                n_rows = len(X_data)\n",
        "                n_cols = X_data.shape[1] if hasattr(X_data, \"shape\") else None\n",
        "                cols = list(X_data.columns) if hasattr(X_data, \"columns\") else None\n",
        "                n_nulls = X_data.isna().sum().sum() if hasattr(X_data, \"isna\") else None\n",
        "\n",
        "                # Tomar solo primeras 5 filas en DataFrame de muestra:\n",
        "                df_sample = pd.concat([\n",
        "                    X_data.head(5).reset_index(drop=True),\n",
        "                    (Y_data.head(5).reset_index(drop=True)\n",
        "                        .rename(columns=lambda c: f\"Y_{c}\" if isinstance(Y_data, pd.DataFrame) else \"Y\")\n",
        "                        if isinstance(Y_data, pd.DataFrame) else Y_data.head(5).rename(\"Y\")\n",
        "                    ),\n",
        "                    FECHAS.head(5).reset_index(drop=True).rename(\"Fecha\")\n",
        "                ], axis=1)\n",
        "                self.sections.append((\"### Muestra de Datos Cargados (primeras 5 filas)\", df_sample))\n",
        "                print(\"[DEBUG] 1.3. Seccin muestra de datos cargados aadida\")\n",
        "\n",
        "                prompt_carga = (\n",
        "                    \"Por favor, explica de forma profesional y detallada cmo se ha realizado \"\n",
        "                    \"la carga de datos, basndote en la siguiente informacin de contexto:\\n\\n\"\n",
        "                    f\"- Nmero total de filas originales: {n_rows}\\n\"\n",
        "                    f\"- Nmero de variables (columnas) cargadas: {n_cols}\\n\"\n",
        "                    f\"- Nombres de columnas (muestra): {cols[:5] if cols else 'N/A'}{'...' if cols and len(cols)>5 else ''}\\n\"\n",
        "                    f\"- Total de valores nulos en X_data: {n_nulls}\\n\\n\"\n",
        "                    \"Explica por qu es importante revisar estos aspectos antes de entrenar modelos, \"\n",
        "                    \"qu implicaciones tienen (por ejemplo, manejo de nulos, tipos de datos, fechas, etc.), \"\n",
        "                    \"y menciona buenas prcticas en esta fase de carga/preprocesado inicial.\"\n",
        "                )\n",
        "                print(\"[DEBUG]1.4. Iniciando llamada a OpenAI para explicacin de carga...\")\n",
        "                t0 = time.time()\n",
        "                stream_resp = _client.chat.completions.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"Eres un experto en ingeniera de datos y preprocesado para ML.\"},\n",
        "                        {\"role\": \"user\",   \"content\": prompt_carga}\n",
        "                    ],\n",
        "                    temperature=TEMPERATURE_VAL,\n",
        "                    max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    stream=True\n",
        "                )\n",
        "                ai_answer_carga = \"\"\n",
        "                for chunk in stream_resp:\n",
        "                    choice = chunk.choices[0]\n",
        "                    if hasattr(choice, \"delta\") and hasattr(choice.delta, \"content\"):\n",
        "                        delta = choice.delta.content\n",
        "                        if delta:\n",
        "                            ai_answer_carga += delta\n",
        "                ai_answer_carga = ai_answer_carga.strip()\n",
        "                if ai_answer_carga:\n",
        "                    self.sections.append((\n",
        "                        \"###  Explicacin IA de la Carga de Datos\",\n",
        "                        ai_answer_carga\n",
        "                    ))\n",
        "                    print(\"[DEBUG] 1.5. Seccin explicacin IA de carga aadida\")\n",
        "                else:\n",
        "                    print(\"[DEBUG] No se recibi contenido IA para carga\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No estn X_data/Y_data/FECHAS en globals(), omito muestra y explicacin de carga\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar seccin o explicacin IA de carga: {e}\")\n",
        "        # =============================================================\n",
        "        # 2. Segmentacin train/test\n",
        "        # =============================================================\n",
        "        try:\n",
        "            if all(k in self.g for k in (\"X_train\", \"Y_train\")):\n",
        "                df_tr = pd.concat([\n",
        "                    self.g[\"X_train\"].head(5).reset_index(drop=True),\n",
        "                    self.g[\"Y_train\"].head(5).reset_index(drop=True)\n",
        "                ], axis=1)\n",
        "                self.sections.append((\n",
        "                    \"### Tabla 1: Conjunto de Entrenamiento  Primeras 5 Muestras\",\n",
        "                    df_tr\n",
        "                ))\n",
        "                print(\"[DEBUG] 2.1. Seccin entrenamiento aadida\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay X_train/Y_train en globals()\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al crear seccin entrenamiento: {e}\")\n",
        "\n",
        "        # 3) Tabla de validacin (5 filas)\n",
        "        try:\n",
        "            if all(k in self.g for k in (\"X_test\", \"Y_test\")):\n",
        "                df_te = pd.concat([\n",
        "                    self.g[\"X_test\"].head(5).reset_index(drop=True),\n",
        "                    self.g[\"Y_test\"].head(5).reset_index(drop=True)\n",
        "                ], axis=1)\n",
        "                self.sections.append((\n",
        "                    \"### Tabla 2: Conjunto de Validacin  Primeras 5 Muestras\",\n",
        "                    df_te\n",
        "                ))\n",
        "                print(\"[DEBUG] 2.2. Seccin test aadida\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay X_test/Y_test en globals()\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al crear seccin test: {e}\")\n",
        "        # ... fin de la seccin de carga de datos ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 3. Resumen estadstico de X_train y Y_train\n",
        "        # =============================================================\n",
        "        try:\n",
        "            if \"X_train\" in self.g:\n",
        "                Xtr = self.g[\"X_train\"]\n",
        "                desc_X = Xtr.describe().T.reset_index().rename(columns={\"index\":\"Variable\"})\n",
        "                desc_X_sample = desc_X.head(10)\n",
        "                self.sections.append((\n",
        "                    \"### Estadsticos de X_train (primeras 10 variables)\", desc_X_sample\n",
        "                ))\n",
        "                print(\"[DEBUG] 3.1. Seccin estadsticos X_train aadida\")\n",
        "\n",
        "                prompt_stats = (\n",
        "                    \"Interpreta profesionalmente estos estadsticos de entrenamiento (primeras 10 variables):\\n\\n\"\n",
        "                    f\"{desc_X_sample.to_dict(orient='list')}\\n\\n\"\n",
        "                    \"Comenta posibles implicaciones (por ejemplo: presencia de outliers, escalas muy distintas entre variables, necesidad de normalizacin, sesgos en la distribucin) \"\n",
        "                    \"y cules podran ser buenas prcticas antes de entrenar modelos.\"\n",
        "                )\n",
        "                print(\"[DEBUG] 3.2. Iniciando llamada a OpenAI para explicacin IA de estadsticos X_train...\")\n",
        "                stream_resp = _client.chat.completions.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"Eres un experto en anlisis de datos para Machine Learning.\"},\n",
        "                        {\"role\": \"user\",   \"content\": prompt_stats}\n",
        "                    ],\n",
        "                    temperature=TEMPERATURE_VAL,\n",
        "                    max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    stream=True\n",
        "                )\n",
        "                ai_stats = \"\"\n",
        "                for chunk in stream_resp:\n",
        "                    choice = chunk.choices[0]\n",
        "                    if hasattr(choice, \"delta\") and hasattr(choice.delta, \"content\"):\n",
        "                        delta = choice.delta.content\n",
        "                        if delta:\n",
        "                            ai_stats += delta\n",
        "                ai_stats = ai_stats.strip()\n",
        "                if ai_stats:\n",
        "                    self.sections.append((\n",
        "                        \"###  Explicacin IA de los Estadsticos de X_train\", ai_stats\n",
        "                    ))\n",
        "                    print(\"[DEBUG] 3.3. Seccin IA estadsticos X_train aadida\")\n",
        "                else:\n",
        "                    print(\"[DEBUG] No se recibi contenido IA para estadsticos X_train\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay X_train en globals(), omito seccin estadsticos X_train\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar seccin estadsticos X_train: {e}\")\n",
        "\n",
        "        try:\n",
        "            if \"Y_train\" in self.g:\n",
        "                Ytr = self.g[\"Y_train\"]\n",
        "                if isinstance(Ytr, pd.DataFrame):\n",
        "                    serie = Ytr.iloc[:, 0]\n",
        "                else:\n",
        "                    serie = pd.Series(Ytr)\n",
        "                desc_Ys = serie.describe()  # Series.describe() -> Series\n",
        "                desc_Y = desc_Ys.to_frame().T.reset_index().rename(columns={\"index\":\"Estadstico\"})\n",
        "                self.sections.append((\"### Estadsticos de Y_train\", desc_Y))\n",
        "                print(\"[DEBUG] 3.4. Seccin estadsticos Y_train aadida\")\n",
        "\n",
        "                prompt_Y = (\n",
        "                    \"Interpreta profesionalmente estos estadsticos de la variable objetivo Y:\\n\\n\"\n",
        "                    f\"{desc_Y.to_dict(orient='list')}\\n\\n\"\n",
        "                    \"Comenta posibles implicaciones (asimetra, outliers, necesidad de transformaciones como log, etc.) \"\n",
        "                    \"y su efecto posible en el modelado.\"\n",
        "                )\n",
        "                print(\"[DEBUG] 3.5. Iniciando llamada a OpenAI para explicacin IA de Y_train...\")\n",
        "                stream_resp = _client.chat.completions.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"Eres un experto en anlisis de datos para Machine Learning.\"},\n",
        "                        {\"role\": \"user\",   \"content\": prompt_Y}\n",
        "                    ],\n",
        "                    temperature=TEMPERATURE_VAL,\n",
        "                    max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    stream=True\n",
        "                )\n",
        "                ai_Y = \"\"\n",
        "                for chunk in stream_resp:\n",
        "                    choice = chunk.choices[0]\n",
        "                    if hasattr(choice, \"delta\") and hasattr(choice.delta, \"content\"):\n",
        "                        delta = choice.delta.content\n",
        "                        if delta:\n",
        "                            ai_Y += delta\n",
        "                ai_Y = ai_Y.strip()\n",
        "                if ai_Y:\n",
        "                    self.sections.append((\n",
        "                        \"###  Explicacin IA de los Estadsticos de Y_train\", ai_Y\n",
        "                    ))\n",
        "                    print(\"[DEBUG] 3.6. Seccin IA estadsticos Y_train aadida\")\n",
        "                else:\n",
        "                    print(\"[DEBUG] No se recibi contenido IA para Y_train\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay Y_train en globals(), omito seccin estadsticos Y_train\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar seccin estadsticos Y_train: {e}\")\n",
        "\n",
        "        # Versin y entorno\n",
        "        try:\n",
        "            import sys, sklearn\n",
        "            #import pandas as _pd\n",
        "            info = {\n",
        "                \"python_version\": sys.version.split()[0],\n",
        "                \"pandas_version\": pd.__version__,\n",
        "                \"sklearn_version\": sklearn.__version__,\n",
        "            }\n",
        "            #import pandas as _pd\n",
        "            df_env = pd.DataFrame(list(info.items()), columns=[\"Paquete\",\"Versin\"])\n",
        "            self.sections.append((\"### Entorno y Versiones de Libreras\", df_env))\n",
        "            print(\"[DEBUG] 3.7. Seccin entorno/versiones aadida\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar seccin entorno/versiones: {e}\")\n",
        "\n",
        "        # 4) Explicacin IA del split al final\n",
        "        try:\n",
        "            if all(k in self.g for k in (\"X_train\", \"X_test\", \"Y_train\", \"Y_test\")):\n",
        "                sp = self.g.get(\"SPLIT_PARAMS\", {})\n",
        "                # Construir prompt solo si SPLIT_PARAMS tiene las claves esperadas\n",
        "                if sp:\n",
        "                    prompt_split = (\n",
        "                        \"Por favor, explica cmo se ha realizado la segmentacin de los datos. \"\n",
        "                        \"Usa la siguiente informacin de contexto:\\n\\n\"\n",
        "                        f\"- Parmetros de segmentacin: {sp}\\n\"\n",
        "                        f\"- Number de muestras train: {len(self.g['X_train'])}\\n\"\n",
        "                        f\"- Nmero de muestras test: {len(self.g['X_test'])}\\n\\n\"\n",
        "                        f\"- test_size: {sp.get('test_size')}\\n\"\n",
        "                        f\"- random_state: {sp.get('random_state')}\\n\"\n",
        "                        f\"- estratificar: {sp.get('stratify')}\\n\"\n",
        "                        f\"- mtodo de bins: {sp.get('bin_method')}\\n\"\n",
        "                        f\"- nmero de bins: {sp.get('q_bins')}\\n\\n\"\n",
        "                        \"Quiero un texto profesional, bien estructurado y suficientemente detallado, \"\n",
        "                        \"que tambin comente brevemente por qu estos valores de parmetros pueden afectar al rendimiento del modelo.\"\n",
        "                    )\n",
        "                    print(\"[DEBUG] 3.8. Iniciando llamada a OpenAI para explicacin del split...\")\n",
        "                    stream_resp = _client.chat.completions.create(\n",
        "                        model=\"gpt-4\",\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"Eres un experto en preprocesado de datos.\"},\n",
        "                            {\"role\": \"user\",   \"content\": prompt_split}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                        stream=True\n",
        "                    )\n",
        "                    ai_answer_split = \"\"\n",
        "                    for chunk in stream_resp:\n",
        "                        choice = chunk.choices[0]\n",
        "                        if hasattr(choice, \"delta\") and hasattr(choice.delta, \"content\"):\n",
        "                            delta = choice.delta.content\n",
        "                            if delta:\n",
        "                                ai_answer_split += delta\n",
        "                    ai_answer_split = ai_answer_split.strip()\n",
        "                    if ai_answer_split:\n",
        "                        self.sections.append((\n",
        "                            \"###  Explicacin IA del Preprocesado y del Split\",\n",
        "                            ai_answer_split\n",
        "                        ))\n",
        "                        print(\"[DEBUG] 3.9. Seccin explicacin IA del split aadida\")\n",
        "                    else:\n",
        "                        print(\"[DEBUG] No se recibi contenido IA para split\")\n",
        "                else:\n",
        "                    print(\"[DEBUG] No hay SPLIT_PARAMS definidos, omito explicacin IA del split\")\n",
        "            else:\n",
        "                print(\"[DEBUG] Faltan datos de entrenamiento/test, omito explicacin IA del split\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar explicacin IA del split: {e}\")\n",
        "        # ... fin de la seccin de Split de los Datos Cargados ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 4. Visualizaciones de Y y de las correlaciones X vs Y\n",
        "        # =============================================================\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # 2.1 Histograma de Y_train\n",
        "        try:\n",
        "            if \"Y_train\" in self.g:\n",
        "                y_train = self.g[\"Y_train\"]\n",
        "                # Si Y_train es DataFrame con varias columnas, tomamos la primera:\n",
        "                if isinstance(y_train, pd.DataFrame) and y_train.shape[1] > 1:\n",
        "                    y_ser = y_train.iloc[:, 0]\n",
        "                else:\n",
        "                    # si es DataFrame de 1 columna o Serie:\n",
        "                    y_ser = y_train.iloc[:, 0] if isinstance(y_train, pd.DataFrame) else pd.Series(y_train)\n",
        "                fig_hist, ax = plt.subplots()\n",
        "                ax.hist(y_ser.dropna(), bins=30, edgecolor='black')\n",
        "                ax.set_title(\"Distribucin de Y_train\")\n",
        "                ax.set_xlabel(\"Y\")\n",
        "                ax.set_ylabel(\"Frecuencia\")\n",
        "                plt.tight_layout()\n",
        "                # Guardar figura en self.figures y en sections para render\n",
        "                self.figures[\"hist_Y_train\"] = fig_hist\n",
        "                self.sections.append((\n",
        "                    \"### Grfico: Histograma de Y (Train)\",\n",
        "                    fig_hist\n",
        "                ))\n",
        "                print(\"[DEBUG] 4.1. Seccin histograma Y_train aadida\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay Y_train en globals(), omito histograma\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar histograma Y_train: {e}\")\n",
        "\n",
        "        # 2.2 Correlacin X_train vs Y_train\n",
        "        # 2.2 Correlacin X_train vs Y_train (filtrada por umbral)\n",
        "        try:\n",
        "            if \"X_train\" in self.g and \"Y_train\" in self.g:\n",
        "                X_train = self.g[\"X_train\"]\n",
        "                y_train = self.g[\"Y_train\"]\n",
        "\n",
        "                if isinstance(y_train, pd.DataFrame) and y_train.shape[1] > 1:\n",
        "                    y_ser = y_train.iloc[:, 0]\n",
        "                else:\n",
        "                    y_ser = y_train.iloc[:, 0] if isinstance(y_train, pd.DataFrame) else pd.Series(y_train)\n",
        "\n",
        "                df_corr = X_train.copy().reset_index(drop=True)\n",
        "                df_corr[\"_Y_target\"] = y_ser.reset_index(drop=True)\n",
        "\n",
        "                corr_matrix = df_corr.corr(numeric_only=True)\n",
        "\n",
        "                # ===  FILTRO: seleccionar solo columnas con correlacin > umbral con _Y_target ===\n",
        "                umbral_corr = 0.3  # Se puede ajustar este valor\n",
        "                correlaciones_con_y = corr_matrix[\"_Y_target\"].abs()\n",
        "                variables_filtradas = correlaciones_con_y[correlaciones_con_y > umbral_corr].index.tolist()\n",
        "\n",
        "                # Mantener solo las columnas con correlacin alta\n",
        "                corr_matrix_filtrada = corr_matrix.loc[variables_filtradas, variables_filtradas]\n",
        "\n",
        "                fig_corr, ax = plt.subplots(figsize=(max(10, len(variables_filtradas)*0.6), max(8, len(variables_filtradas)*0.5)))\n",
        "                cax = ax.matshow(corr_matrix_filtrada, cmap='viridis')\n",
        "                fig_corr.colorbar(cax, ax=ax, shrink=0.8)\n",
        "\n",
        "                labels = list(corr_matrix_filtrada.columns)\n",
        "                ax.set_xticks(range(len(labels)))\n",
        "                ax.set_yticks(range(len(labels)))\n",
        "                ax.set_xticklabels(labels, rotation=90, fontsize=8, ha='left')\n",
        "                ax.set_yticklabels(labels, fontsize=8)\n",
        "\n",
        "                ax.tick_params(axis='x', which='both', labelsize=7, pad=1)\n",
        "                ax.tick_params(axis='y', which='both', labelsize=7, pad=1)\n",
        "\n",
        "                ax.set_title(f\"Matriz de correlacin (|r| > {umbral_corr})\", pad=30, fontsize=12)\n",
        "                plt.tight_layout()\n",
        "\n",
        "                self.figures[\"corr_XY_train\"] = fig_corr\n",
        "                self.sections.append((\n",
        "                    f\"### Grfico: Matriz de Correlacin X vs Y (|r| > {umbral_corr})\",\n",
        "                    fig_corr\n",
        "                ))\n",
        "                print(\"[DEBUG] 4.2. Seccin matriz de correlacin aadida con filtro\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay X_train/Y_train en globals(), omito correlacin\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar matriz de correlacin: {e}\")\n",
        "\n",
        "#        try:\n",
        "#            if \"X_train\" in self.g and \"Y_train\" in self.g:\n",
        "#                X_train = self.g[\"X_train\"]\n",
        "#                y_train = self.g[\"Y_train\"]\n",
        "#                # Sacar Serie de Y como antes\n",
        "#                if isinstance(y_train, pd.DataFrame) and y_train.shape[1] > 1:\n",
        "#                    y_ser = y_train.iloc[:, 0]\n",
        "#                else:\n",
        "#                    y_ser = y_train.iloc[:, 0] if isinstance(y_train, pd.DataFrame) else pd.Series(y_train)\n",
        "#                # Concatenar para clculo de correlacin:\n",
        "#                df_corr = X_train.copy().reset_index(drop=True)\n",
        "#                df_corr[\"_Y_target\"] = y_ser.reset_index(drop=True)\n",
        "#                # Calculamos matriz de correlaciones:\n",
        "#                corr_matrix = df_corr.corr(numeric_only=True)  # pandas 1.5\n",
        "#                # Creamos heatmap con matplotlib puro:\n",
        "#                fig_corr, ax = plt.subplots(figsize=(6, 6))\n",
        "#                cax = ax.matshow(corr_matrix, cmap='viridis')\n",
        "#                fig_corr.colorbar(cax, ax=ax)\n",
        "#                # Etiquetas:\n",
        "#                labels = list(corr_matrix.columns)\n",
        "#                ax.set_xticks(range(len(labels)))\n",
        "#                ax.set_yticks(range(len(labels)))\n",
        "#                ax.set_xticklabels(labels, rotation=90, fontsize=8)\n",
        "#                ax.set_yticklabels(labels, fontsize=8)\n",
        "#                ax.set_title(\"Matriz de correlacin (X_train vs Y_train incluida)\", pad=20)\n",
        "#                plt.tight_layout()\n",
        "#                self.figures[\"corr_XY_train\"] = fig_corr\n",
        "#                self.sections.append((\n",
        "#                    \"### Grfico: Matriz de Correlacin X vs Y (Train)\",\n",
        "#                    fig_corr\n",
        "#                ))\n",
        "#                print(\"[DEBUG] 4.2. Seccin matriz de correlacin aadida\")\n",
        "#            else:\n",
        "#                print(\"[DEBUG] No hay X_train/Y_train en globals(), omito correlacin\")\n",
        "#        except Exception as e:\n",
        "#            print(f\"[ERROR] al generar matriz de correlacin: {e}\")\n",
        "\n",
        "        # 2.3 Comentario IA sobre distribucin y correlaciones\n",
        "        try:\n",
        "            # Solo si disponemos de histogram y/o correlaciones:\n",
        "            if \"hist_Y_train\" in self.figures:\n",
        "                # Preparamos prompt para la IA\n",
        "                # Ejemplo de contexto: medias, sesgo, kurtosis, correlaciones mximas\n",
        "                import numpy as np\n",
        "                # Estadsticos de Y:\n",
        "                y_arr = y_ser.dropna().values\n",
        "                media = float(np.mean(y_arr))\n",
        "                mediana = float(np.median(y_arr))\n",
        "                std = float(np.std(y_arr, ddof=1))\n",
        "                # Sesgo y curtosis opcionales si numpy/scipy disponibles:\n",
        "                try:\n",
        "                    from scipy.stats import skew, kurtosis\n",
        "                    sesgo = float(skew(y_arr))\n",
        "                    kurt = float(kurtosis(y_arr))\n",
        "                except Exception:\n",
        "                    sesgo = None\n",
        "                    kurt = None\n",
        "                # Estadsticos de correlacin: extraer de corr_matrix\n",
        "                if \"corr_XY_train\" in self.figures:\n",
        "                    # Obtenemos correlaciones de X con Y:\n",
        "                    # La columna _Y_target\n",
        "                    corrs = corr_matrix[\"_Y_target\"].drop(\"_Y_target\", errors='ignore')\n",
        "                    # Tomamos los pares con mayor valor absoluto:\n",
        "                    if not corrs.empty:\n",
        "                        top = corrs.abs().sort_values(ascending=False).head(3)\n",
        "                        # Formatear para prompt\n",
        "                        top_info = {col: float(corrs[col]) for col in top.index}\n",
        "                    else:\n",
        "                        top_info = {}\n",
        "                else:\n",
        "                    top_info = {}\n",
        "                # Construir prompt:\n",
        "                prompt_vis = (\n",
        "                    \"Eres un experto en anlisis exploratorio de datos.\\n\"\n",
        "                    \"Analiza la distribucin de la variable objetivo Y y las correlaciones \"\n",
        "                    \"entre las variables X y Y basndote en estos estadsticos:\\n\\n\"\n",
        "                    f\"- Media de Y_train: {media:.4f}\\n\"\n",
        "                    f\"- Mediana de Y_train: {mediana:.4f}\\n\"\n",
        "                    f\"- Desviacin estndar de Y_train: {std:.4f}\\n\"\n",
        "                )\n",
        "                if sesgo is not None:\n",
        "                    prompt_vis += f\"- Sesgo (skewness) de Y_train: {sesgo:.4f}\\n\"\n",
        "                if kurt is not None:\n",
        "                    prompt_vis += f\"- Curtosis de Y_train: {kurt:.4f}\\n\"\n",
        "                if top_info:\n",
        "                    prompt_vis += \"- Correlaciones ms relevantes X vs Y:\\n\"\n",
        "                    for feat, corrv in top_info.items():\n",
        "                        prompt_vis += f\"     {feat}: {corrv:.4f}\\n\"\n",
        "                prompt_vis += (\n",
        "                    \"\\nPor favor, genera un texto profesional y detallado que comente:\\n\"\n",
        "                    \"  * Si la distribucin de Y parece simtrica o sesgada, posibles implicaciones.\\n\"\n",
        "                    \"  * Si hay variables con fuerte correlacin absoluta con Y (positiva o negativa) y qu puede indicar.\\n\"\n",
        "                    \"  * Buenas prcticas o precauciones al modelar con base en dicha distribucin/correlaciones.\\n\"\n",
        "                )\n",
        "                print(\"[DEBUG] 4.3. Iniciando llamada a OpenAI para comentario visualizaciones...\")\n",
        "                # Llamada a OpenAI (sin stream, con lmite de tokens razonable):\n",
        "                resp = _client.chat.completions.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"Eres un experto en anlisis exploratorio de datos para ML.\"},\n",
        "                        {\"role\": \"user\", \"content\": prompt_vis}\n",
        "                    ],\n",
        "                    temperature=TEMPERATURE_VAL,\n",
        "                    max_tokens=MAX_EXPLANATION_TOKENS  # ajustable\n",
        "                )\n",
        "                comentario_vis = resp.choices[0].message.content.strip()\n",
        "                if comentario_vis:\n",
        "                    self.sections.append((\n",
        "                        \"###  Comentario IA: Distribucin de Y y Correlaciones\",\n",
        "                        comentario_vis\n",
        "                    ))\n",
        "                    print(\"[DEBUG] 4.4. Seccin comentario IA visualizaciones aadida\")\n",
        "                else:\n",
        "                    print(\"[DEBUG] No se recibi contenido IA para visualizaciones\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay histograma de Y para IA, omito comentario visualizaciones\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar comentario IA de visualizaciones: {e}\")\n",
        "        # ... fin de la seccin de visualizacin de anlisis de datos cargados ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 5. Seleccin de variables independientes X\n",
        "        # =============================================================\n",
        "        try:\n",
        "            # Caso 1: RESUMEN_METODOS (varios mtodos acumulativos)\n",
        "            if \"RESUMEN_METODOS\" in self.g and isinstance(self.g[\"RESUMEN_METODOS\"], dict):\n",
        "                resumen = self.g[\"RESUMEN_METODOS\"]\n",
        "            else:\n",
        "                resumen = None\n",
        "\n",
        "            # Caso 2: Seleccin ms reciente puntual\n",
        "            tiene_reciente = all(k in self.g for k in (\"VARIABLES_SELECCIONADAS\", \"METODO_SELECCION\"))\n",
        "            # Preparar datos para correlaciones si estn en globals\n",
        "            tiene_corr = \"VALORES_CORRELACION\" in self.g\n",
        "            corr_global = self.g.get(\"VALORES_CORRELACION\", None)\n",
        "\n",
        "            # Tambin comprobamos si disponemos de X_train/Y_train para reclculo de correl si se prefiere\n",
        "            have_xy = all(k in self.g for k in (\"X_train\", \"Y_train\"))\n",
        "            if have_xy:\n",
        "                X_train = self.g[\"X_train\"]\n",
        "                Y_train = self.g[\"Y_train\"]\n",
        "                import pandas as _pd  # asegurar pandas disponible\n",
        "                if isinstance(Y_train, _pd.DataFrame):\n",
        "                    y_ser = Y_train.iloc[:, 0]\n",
        "                else:\n",
        "                    y_ser = Y_train\n",
        "\n",
        "            # Primero, si RESUMEN_METODOS existe, iteramos cada mtodo\n",
        "            if resumen:\n",
        "                # Si adems quieres usar VALORES_CORRELACION puntual, podras ignorarlo aqu\n",
        "                for metodo, cols in resumen.items():\n",
        "                    # Construccin de DataFrame con correlaciones\n",
        "                    df_sel = None\n",
        "                    if have_xy:\n",
        "                        corr_vals = []\n",
        "                        for col in cols:\n",
        "                            if col in X_train.columns:\n",
        "                                try:\n",
        "                                    corr = X_train[col].corr(y_ser)\n",
        "                                except Exception:\n",
        "                                    corr = None\n",
        "                            else:\n",
        "                                corr = None\n",
        "                            corr_vals.append(corr)\n",
        "                        import pandas as _pd\n",
        "                        df_sel = _pd.DataFrame({\n",
        "                            \"Variable\": cols,\n",
        "                            \"Correlacin con Y\": corr_vals\n",
        "                        })\n",
        "                    else:\n",
        "                        import pandas as _pd\n",
        "                        df_sel = _pd.DataFrame({\"Variable\": cols})\n",
        "                    titulo_tab = f\"### Tabla: Seleccin de Variables ({metodo})\"\n",
        "                    self.sections.append((titulo_tab, df_sel))\n",
        "                    print(f\"[DEBUG] 5.1. Seccin seleccin de variables aadida para mtodo: {metodo}\")\n",
        "\n",
        "                    # Preparar prompt IA\n",
        "                    # Si existe un dict global con parmetros, usalo; si no, omtelo:\n",
        "                    params = self.g.get(\"SELECTION_PARAMS\", {}).get(metodo, {})\n",
        "                    prompt_sel = (\n",
        "                        f\"Has aplicado un mtodo de seleccin de variables llamado '{metodo}'.\\n\"\n",
        "                        f\"Parmetros del mtodo: {params}\\n\"\n",
        "                        f\"Variables seleccionadas ({len(cols)}): {cols}\\n\"\n",
        "                    )\n",
        "                    if have_xy:\n",
        "                        prompt_sel += \"Correlaciones con la variable objetivo:\\n\"\n",
        "                        for var, corr in zip(cols, corr_vals):\n",
        "                            prompt_sel += f\"  - {var}: {corr}\\n\"\n",
        "                        prompt_sel += \"\\n\"\n",
        "                    prompt_sel += (\n",
        "                        \"Por favor, genera un texto profesional y detallado que explique:\\n\"\n",
        "                        \"- En qu consiste este mtodo de seleccin de variables y significado de sus parmetros.\\n\"\n",
        "                        \"- Cmo influyen dichos parmetros en la seleccin.\\n\"\n",
        "                        \"- Interpretacin de los valores de correlacin obtenidos.\\n\"\n",
        "                        \"- Buenas prcticas al usar este mtodo en preprocesado de datos para ML.\\n\"\n",
        "                    )\n",
        "                    try:\n",
        "                        print(f\"[DEBUG] 5.2. Iniciando llamada a OpenAI para explicacin seleccin ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en seleccin de variables para ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_sel}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_sel = resp.choices[0].message.content.strip()\n",
        "                        if explanation_sel:\n",
        "                            titulo_exp = f\"###  Explicacin IA Seleccin de Variables ({metodo})\"\n",
        "                            self.sections.append((titulo_exp, explanation_sel))\n",
        "                            print(f\"[DEBUG] 5.3. Seccin explicacin IA seleccin aadida para mtodo: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibi contenido IA para seleccin {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA seleccin {metodo}: {e}\")\n",
        "\n",
        "            # Luego, si existe una seleccin puntual reciente\n",
        "            if not resumen and tiene_reciente:\n",
        "                metodo = self.g[\"METODO_SELECCION\"]\n",
        "                cols = self.g[\"VARIABLES_SELECCIONADAS\"]\n",
        "                # Construir DataFrame\n",
        "                df_sel = None\n",
        "                import pandas as _pd\n",
        "                if tiene_corr:\n",
        "                    corr_vals = None\n",
        "                    # VALORES_CORRELACION puede ser dict o lista\n",
        "                    vc = corr_global\n",
        "                    if isinstance(vc, dict):\n",
        "                        # Aseguramos la lista del mismo orden\n",
        "                        corr_vals = [vc.get(var, None) for var in cols]\n",
        "                    elif isinstance(vc, (list, tuple)):\n",
        "                        # Si la longitud coincide con cols\n",
        "                        if len(vc) == len(cols):\n",
        "                            corr_vals = list(vc)\n",
        "                        else:\n",
        "                            corr_vals = [None]*len(cols)\n",
        "                    else:\n",
        "                        corr_vals = [None]*len(cols)\n",
        "                    df_sel = _pd.DataFrame({\n",
        "                        \"Variable\": cols,\n",
        "                        \"Correlacin con Y\": corr_vals\n",
        "                    })\n",
        "                else:\n",
        "                    # Si no hay VALORES_CORRELACION, pero se dispone de X_train/Y_train, recalculamos\n",
        "                    if have_xy:\n",
        "                        corr_vals = []\n",
        "                        for col in cols:\n",
        "                            if col in X_train.columns:\n",
        "                                try:\n",
        "                                    corr = X_train[col].corr(y_ser)\n",
        "                                except Exception:\n",
        "                                    corr = None\n",
        "                            else:\n",
        "                                corr = None\n",
        "                            corr_vals.append(corr)\n",
        "                        df_sel = _pd.DataFrame({\n",
        "                            \"Variable\": cols,\n",
        "                            \"Correlacin con Y\": corr_vals\n",
        "                        })\n",
        "                    else:\n",
        "                        df_sel = _pd.DataFrame({\"Variable\": cols})\n",
        "                titulo_tab = f\"### Tabla: Seleccin de Variables ({metodo})\"\n",
        "                self.sections.append((titulo_tab, df_sel))\n",
        "                print(f\"[DEBUG] 5.4. Seccin seleccin de variables puntual aadida para mtodo: {metodo}\")\n",
        "\n",
        "                # Prompt IA\n",
        "                prompt_sel = (\n",
        "                    f\"Has aplicado un mtodo de seleccin de variables llamado '{metodo}'.\\n\"\n",
        "                    f\"Variables seleccionadas ({len(cols)}): {cols}\\n\"\n",
        "                )\n",
        "                if tiene_corr or have_xy:\n",
        "                    prompt_sel += \"Correlaciones con la variable objetivo:\\n\"\n",
        "                    if 'corr_vals' in locals():\n",
        "                        for var, corr in zip(cols, corr_vals):\n",
        "                            prompt_sel += f\"  - {var}: {corr}\\n\"\n",
        "                    prompt_sel += \"\\n\"\n",
        "                prompt_sel += (\n",
        "                    \"Por favor, genera un texto profesional y detallado que explique:\\n\"\n",
        "                    \"- En qu consiste este mtodo de seleccin de variables (breve descripcin basada en su nombre) y significado de sus parmetros si los conoces.\\n\"\n",
        "                    \"- Interpretacin de los valores de correlacin obtenidos.\\n\"\n",
        "                    \"- Buenas prcticas al usar este mtodo en preprocesado de datos para ML.\\n\"\n",
        "                )\n",
        "                try:\n",
        "                    print(f\"[DEBUG] 5.5. Iniciando llamada a OpenAI para explicacin seleccin puntual ({metodo})...\")\n",
        "                    resp = _client.chat.completions.create(\n",
        "                        model=\"gpt-4\",\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"Eres un experto en seleccin de variables para ML.\"},\n",
        "                            {\"role\": \"user\", \"content\": prompt_sel}\n",
        "                        ],\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                        temperature=TEMPERATURE_VAL\n",
        "                    )\n",
        "                    explanation_sel = resp.choices[0].message.content.strip()\n",
        "                    if explanation_sel:\n",
        "                        titulo_exp = f\"###  Explicacin IA Seleccin de Variables ({metodo})\"\n",
        "                        self.sections.append((titulo_exp, explanation_sel))\n",
        "                        print(f\"[DEBUG] 5.6. Seccin explicacin IA seleccin puntual aadida para mtodo: {metodo}\")\n",
        "                    else:\n",
        "                        print(\"[DEBUG] No se recibi contenido IA para seleccin puntual\")\n",
        "                except Exception as e:\n",
        "                    print(f\"[ERROR] al generar explicacin IA seleccin puntual {metodo}: {e}\")\n",
        "\n",
        "            if not resumen and not tiene_reciente:\n",
        "                print(\"[DEBUG] No hay RESUMEN_METODOS ni seleccin puntual en globals(), omito seccin de seleccin de variables\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar seccin o explicacin IA de seleccin de variables: {e}\")\n",
        "\n",
        "        # ... fin de la seccin de seleccin de variables ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 6. Entrenamiento Modelo SVR\n",
        "        # =============================================================\n",
        "        try:\n",
        "            # <<< Aqu inserta las inicializaciones >>>\n",
        "            rmse = mae = r2 = None\n",
        "            residuos = None\n",
        "            # Comprobamos RESUMEN_METODOS y existencia de X_test/Y_test\n",
        "            if \"RESUMEN_METODOS\" in self.g and \"X_test\" in self.g and \"Y_test\" in self.g:\n",
        "                import os, pickle\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "                import pandas as _pd\n",
        "\n",
        "                X_test_full = self.g[\"X_test\"]\n",
        "                Y_test = self.g[\"Y_test\"]\n",
        "                # Extraer y_test_arr como 1D array\n",
        "                arr = None\n",
        "                import numpy as _np\n",
        "                y_test_arr = None\n",
        "                if hasattr(Y_test, \"values\"):\n",
        "                    arr = Y_test.values\n",
        "                    if arr.ndim == 2:\n",
        "                        if arr.shape[1] == 1:\n",
        "                            y_test_arr = arr[:, 0]\n",
        "                        else:\n",
        "                            y_test_arr = arr[:, 0]\n",
        "                    else:\n",
        "                        y_test_arr = arr\n",
        "                else:\n",
        "                    y_test_arr = _np.array(Y_test)\n",
        "                    if y_test_arr.ndim == 2 and y_test_arr.shape[1] == 1:\n",
        "                        y_test_arr = y_test_arr[:, 0]\n",
        "                if y_test_arr.ndim > 1:\n",
        "                    y_test_arr = y_test_arr.ravel()\n",
        "\n",
        "                metrics_summary = []\n",
        "                resumen_modelos = []\n",
        "                # Iteramos sobre cada mtodo declarado en RESUMEN_METODOS\n",
        "                for metodo, vars_sel in self.g[\"RESUMEN_METODOS\"].items():\n",
        "                    metodo_low = metodo.lower()\n",
        "                    # Nombre de fichero pickle segn celda 7.1\n",
        "                    fname = f\"modelo_svr_{metodo_low}.pkl\"\n",
        "                    if not os.path.exists(fname):\n",
        "                        print(f\"[DEBUG] 6.1. Fichero de modelo SVR no encontrado para mtodo '{metodo}': {fname}, omito este mtodo\")\n",
        "                        continue\n",
        "                    # Cargar pickle\n",
        "                    try:\n",
        "                        with open(fname, \"rb\") as f:\n",
        "                            data = pickle.load(f)\n",
        "                        model = data.get(\"model\", None)\n",
        "                        sx = data.get(\"sx\", None)\n",
        "                        sy = data.get(\"sy\", None)\n",
        "                        cols = data.get(\"cols\", None)\n",
        "                        # yname = data.get(\"yname\", None)  # si quieres mostrar el nombre de la variable\n",
        "                        if model is None or sx is None or sy is None or cols is None:\n",
        "                            print(f\"[DEBUG] Falta alguna clave en pickle SVR para mtodo '{metodo}', omito\")\n",
        "                            continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al cargar pickle SVR para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Verificar columnas en X_test_full\n",
        "                    missing = [c for c in cols if c not in X_test_full.columns]\n",
        "                    if missing:\n",
        "                        print(f\"[DEBUG] Para mtodo '{metodo}', faltan columnas en X_test: {missing}, omito\")\n",
        "                        continue\n",
        "                    # Subconjunto X_test\n",
        "                    X_test_sel = X_test_full[cols].copy()\n",
        "                    # Escalar, predecir y desescalar\n",
        "                    try:\n",
        "                        X_test_scaled = sx.transform(X_test_sel)\n",
        "                        y_pred_scaled = model.predict(X_test_scaled)\n",
        "                        # sy estuvo ajustado sobre y entrenado; para inverse_transform debe recibir 2D:\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al predecir/desescalar SVR para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Calcular mtricas\n",
        "                    try:\n",
        "                        # Rangos Real vs Predicha\n",
        "                        y_real_min, y_real_max = float(_np.min(y_test_arr)), float(_np.max(y_test_arr))\n",
        "                        y_pred_min, y_pred_max = float(_np.min(y_pred)), float(_np.max(y_pred))\n",
        "                        # 3) Estadsticos de residuos:\n",
        "                        residuals = y_test_arr - y_pred\n",
        "\n",
        "                        res_mean = float(np.mean(residuals))            # Media\n",
        "                        res_std  = float(np.std(residuals))             # Desviacin estndar:\n",
        "                        res_series = pd.Series(residuals)               # Asimetria\n",
        "                        res_skew = float(res_series.skew())             # Asimetria\n",
        "                        res_kurt = float(res_series.kurtosis())         # Curtosis\n",
        "                        q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]    # Cuantiles:\n",
        "                        # Mtricas\n",
        "                        mse = mean_squared_error(y_test_arr, y_pred)\n",
        "                        rmse = float(_np.sqrt(mse))\n",
        "                        mae = float(mean_absolute_error(y_test_arr, y_pred))\n",
        "                        r2 = float(r2_score(y_test_arr, y_pred))\n",
        "                        resumen_modelos.append({\n",
        "                            'Mtodo': metodo,\n",
        "                            'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae\n",
        "                        })\n",
        "                        # Correlacin Real vs Predicha\n",
        "                        try:\n",
        "                            corr = float(_np.corrcoef(y_test_arr, y_pred)[0, 1])\n",
        "                        except:\n",
        "                            corr = None\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al calcular mtricas SVR para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    metrics_summary.append({\n",
        "                        \"metodo\": metodo,\n",
        "                        \"rmse\": rmse,\n",
        "                        \"mae\": mae,\n",
        "                        \"r2\": r2\n",
        "                    })\n",
        "\n",
        "                    # 1) Parmetros de entrenamiento obtenidos desde el modelo\n",
        "                    try:\n",
        "                        # SVR tiene atributos: C, epsilon, kernel, gamma\n",
        "                        C_val = getattr(model, \"C\", None)\n",
        "                        epsilon_val = getattr(model, \"epsilon\", None)\n",
        "                        kernel_val = getattr(model, \"kernel\", None)\n",
        "                        gamma_val = getattr(model, \"gamma\", None)\n",
        "                        params = {\n",
        "                            \"C\": C_val,\n",
        "                            \"epsilon\": epsilon_val,\n",
        "                            \"kernel\": kernel_val,\n",
        "                            \"gamma\": gamma_val\n",
        "                        }\n",
        "                        df_params = _pd.DataFrame({\n",
        "                            \"Hiperparmetro\": list(params.keys()),\n",
        "                            \"Valor\": [str(v) for v in params.values()]\n",
        "                        })\n",
        "                        titulo_p = f\"### Parmetros de Entrenamiento SVR ({metodo})\"\n",
        "                        self.sections.append((titulo_p, df_params))\n",
        "                        print(f\"[DEBUG] 6.2. Seccin parmetros SVR aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al extraer/parmetros SVR para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA de los hiperparmetros\n",
        "                    try:\n",
        "                        prompt_params = (\n",
        "                            f\"Has entrenado un modelo SVR con seleccin de variables '{metodo}'.\\n\"\n",
        "                            f\"Estos fueron los hiperparmetros utilizados:\\n\"\n",
        "                        )\n",
        "                        for k, v in params.items():\n",
        "                            prompt_params += f\"- {k}: {v}\\n\"\n",
        "                        prompt_params += (\n",
        "                            \"\\nPor favor, explica de forma profesional y detallada cmo estos hiperparmetros \"\n",
        "                            \"pueden influir en el entrenamiento del modelo SVR, su impacto en ajuste, \"\n",
        "                            \"y buenas prcticas para seleccionarlos o afinarlos.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 6.3. Iniciando llamada a OpenAI para explicacin hiperparmetros SVR ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en entrenamiento de modelos SVR.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_params}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_params = resp.choices[0].message.content.strip()\n",
        "                        if explanation_params:\n",
        "                            titulo_exp_p = f\"###  Explicacin IA Hiperparmetros SVR ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_p, explanation_params))\n",
        "                            print(f\"[DEBUG] 6.4. Seccin explicacin IA hiperparmetros aadida para mtodo: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibi IA para hiperparmetros SVR ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA hiperparmetros SVR para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 2) Grfica Predicho vs Real\n",
        "                    try:\n",
        "                        fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "                        ax1.scatter(y_test_arr, y_pred, alpha=0.6)\n",
        "                        ax1.plot([y_test_arr.min(), y_test_arr.max()],\n",
        "                                 [y_test_arr.min(), y_test_arr.max()],\n",
        "                                 'r--', lw=2)\n",
        "                        ax1.set_xlabel(\"Y real\")\n",
        "                        ax1.set_ylabel(\"Y predicho\")\n",
        "                        ax1.set_title(f\"SVR Predicho vs Real ({metodo})\")\n",
        "                        titulo_fig1 = f\"### Grfica SVR Predicho vs Real ({metodo})\"\n",
        "                        self.sections.append((titulo_fig1, fig1))\n",
        "                        print(f\"[DEBUG] 6.5. Seccin grfica Pred vs Real aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear grfica Pred vs Real para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA Pred vs Real\n",
        "                    try:\n",
        "                        prompt_pred_real = (\n",
        "                        f\"A continuacin tienes datos de la grfica de comparacin Real vs Prediccin para el modelo SVR con mtodo '{metodo}':\\n\"\n",
        "                        #f\"- R2: {r2:.4f}\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- R: {r2}\\n\"\n",
        "                        f\"- Correlacin entre Y real y predicha: {corr:.4f}\\n\"\n",
        "                        f\"- Rango Y real: [{y_real_min:.4f}, {y_real_max:.4f}]\\n\"\n",
        "                        f\"- Rango Y predicha: [{y_pred_min:.4f}, {y_pred_max:.4f}]\\n\"\n",
        "                        \"\\n\"\n",
        "                        \"Tambin tienes datos de la grfica de residuos:\\n\"\n",
        "                        f\"- Media de residuos (Real - Predicha): {res_mean:.4f}\\n\"\n",
        "                        f\"- Desviacin estndar de residuos: {res_std:.4f}\\n\"\n",
        "                        f\"- Asimetra de residuos: {res_skew:.4f}\\n\"\n",
        "                        f\"- Curtosis de residuos: {res_kurt:.4f}\\n\"\n",
        "                        f\"- Cuantiles de residuos: 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}\\n\"\n",
        "                        \"\\n\"\n",
        "                        \"Basndote en estos valores y en las grficas generadas (Real vs Prediccin y Residuos), \"\n",
        "                        \"proporciona un anlisis detallado, sealando si hay sesgos sistemticos (por ejemplo, subestimacin o sobrestimacin en ciertos rangos), \"\n",
        "                        \"si la dispersin es mayor en algn rango de prediccin, si los residuos muestran patrones (p. ej. forma de embudo), \"\n",
        "                        \"y qu implicaciones tiene para la calidad del modelo. \"\n",
        "                        \"Usa un texto profesional y bien estructurado, y menciona qu indicios de la grfica respaldan tus conclusiones.\"\n",
        "                    )\n",
        "                        print(f\"[DEBUG] 6.6. Llamada IA Pred vs Real ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_pred_real}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_pr = resp.choices[0].message.content.strip()\n",
        "                        if explanation_pr:\n",
        "                            titulo_exp_pr = f\"###  Explicacin IA Predicho vs Real ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_pr, explanation_pr))\n",
        "                            print(f\"[DEBUG] 6.7. Seccin explicacin IA Pred vs Real aadida para mtodo: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibi IA Pred vs Real ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA Pred vs Real para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 3) Grfica de residuos\n",
        "                    try:\n",
        "                        # 4) Rango de Y real y predicha:\n",
        "                        y_real_min, y_real_max = float(np.min(y_test_arr)), float(np.max(y_test_arr))\n",
        "                        y_pred_min, y_pred_max = float(np.min(y_pred)), float(np.max(y_pred))\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "                        ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "                        ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "                        ax2.set_xlabel(\"Y predicho\")\n",
        "                        ax2.set_ylabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "                        ax2.set_title(f\"SVR Residuos ({metodo})\")\n",
        "                        titulo_fig2 = f\"### Grfica SVR Residuos ({metodo})\"\n",
        "                        self.sections.append((titulo_fig2, fig2))\n",
        "                        print(f\"[DEBUG] 6.8. Seccin grfica residuos aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear grfica residuos para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA Residuos\n",
        "                    try:\n",
        "                        prompt_residuos = (\n",
        "                            f\"A continuacin tienes estadsticas de los residuos (Real - Predicha) del modelo SVR con mtodo '{metodo}':\\n\"\n",
        "                            f\"- Media: {res_mean:.4f}\\n\"\n",
        "                            f\"- Desviacin estndar: {res_std:.4f}\\n\"\n",
        "                            f\"- Asimetra: {res_skew:.4f}\\n\"\n",
        "                            f\"- Curtosis: {res_kurt:.4f}\\n\"\n",
        "                            f\"- Cuantiles: 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}\\n\"\n",
        "                            \"\\n\"\n",
        "                            \"Basndote en estos valores y en la grfica de residuos, analiza si hay patrones (por ejemplo, heterocedasticidad, outliers, sesgos en rangos), \"\n",
        "                            \"y comenta qu implicaciones tiene para la robustez y generalizacin del modelo. \"\n",
        "                            \"Usa un texto profesional y bien estructurado.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 6.9. Llamada IA Residuos ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_residuos}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_res = resp.choices[0].message.content.strip()\n",
        "                        if explanation_res:\n",
        "                            titulo_exp_res = f\"###  Explicacin IA Residuos ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_res, explanation_res))\n",
        "                            print(f\"[DEBUG] 6.10. Seccin explicacin IA residuos aadida para mtodo: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibi IA Residuos ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA residuos para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 4) Tabla Mtricas y explicacin IA\n",
        "                    try:\n",
        "                        df_met = _pd.DataFrame([{\"Mtrica\": \"RMSE\", \"Valor\": rmse},\n",
        "                                                {\"Mtrica\": \"MAE\", \"Valor\": mae},\n",
        "                                                {\"Mtrica\": \"R2\",  \"Valor\": r2}])\n",
        "                        titulo_met = f\"### Mtricas SVR ({metodo})\"\n",
        "                        self.sections.append((titulo_met, df_met))\n",
        "                        print(f\"[DEBUG] 6.11. Seccin mtricas aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame mtricas SVR para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_metrics = (\n",
        "                            f\"Estas son las mtricas del modelo SVR con mtodo '{metodo}':\\n\"\n",
        "                            f\"- R2: {r2:.4f}\\n\"\n",
        "                            f\"- MSE: {mse:.4f}\\n\"\n",
        "                            f\"- RMSE: {rmse:.4f}\\n\"\n",
        "                            f\"- MAE: {mae:.4f}\\n\"\n",
        "                            f\"- Correlacin Real vs Predicha: {corr:.4f}\\n\"\n",
        "                            \"\\n\"\n",
        "                            \"Analiza estos valores en contexto: son adecuados? qu sugieren respecto al rendimiento del modelo? \"\n",
        "                            \"Menciona referencias a la grfica Real vs Prediccin y a los residuos si procede.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 6.12. Llamada IA Mtricas SVR ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_metrics}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_met = resp.choices[0].message.content.strip()\n",
        "                        if explanation_met:\n",
        "                            titulo_exp_met = f\"###  Explicacin IA Mtricas SVR ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_met, explanation_met))\n",
        "                            print(f\"[DEBUG] 6.13. Seccin explicacin IA mtricas aadida para mtodo: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibi IA Mtricas SVR ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA mtricas para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                # 5) Comparativa global de mtricas SVR\n",
        "                if metrics_summary:\n",
        "                    try:\n",
        "                        df_comp = _pd.DataFrame(metrics_summary)\n",
        "                        df_comp_sorted = df_comp.sort_values(\"rmse\")\n",
        "                        titulo_comp = \"### Comparativa Mtricas SVR entre Mtodos\"\n",
        "                        self.sections.append((titulo_comp, df_comp_sorted))\n",
        "                        print(\"[DEBUG] 6.14. Seccin comparativa mtricas SVR aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame comparativo mtricas SVR: {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_conc = (\n",
        "                            \"Se han entrenado varios modelos SVR con diferentes mtodos de seleccin de variables.\\n\"\n",
        "                            \"Mtricas obtenidas en test:\\n\"\n",
        "                        )\n",
        "                        for entry in metrics_summary:\n",
        "                            prompt_conc += f\"- Mtodo '{entry['metodo']}': RMSE={entry['rmse']}, MAE={entry['mae']}, R2={entry['r2']}\\n\"\n",
        "                        prompt_conc += (\n",
        "                            \"\\nPor favor, proporciona conclusiones profesionales comparando estos mtodos: \"\n",
        "                            \"indica cul se comporta mejor, posibles razones y recomendaciones sobre seleccin de variables o ajustes para mejorar SVR.\"\n",
        "                        )\n",
        "                        print(\"[DEBUG] 6.15. Llamada IA Conclusiones SVR...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_conc}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_conc = resp.choices[0].message.content.strip()\n",
        "                        if explanation_conc:\n",
        "                            titulo_exp_conc = \"###  Conclusiones IA Entrenamiento SVR\"\n",
        "                            self.sections.append((titulo_exp_conc, explanation_conc))\n",
        "                            print(\"[DEBUG] 6.16. Seccin explicacin IA conclusiones SVR aadida\")\n",
        "                        else:\n",
        "                            print(\"[DEBUG] No se recibi IA Conclusiones SVR\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA conclusiones SVR: {e}\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No estn RESUMEN_METODOS o X_test/Y_test en globals(), omito seccin SVR\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar seccin SVR en informe: {e}\")\n",
        "        # ... fin de la seccin de entrenamiento SVR ...\n",
        "\n",
        "        # ==============================\n",
        "        # 6.1. Interpretacin xIA para modelo entrenado SVR\n",
        "        # ==============================\n",
        "        import openai\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # Verificacin previa\n",
        "        # ----------------------------------------------------------------\n",
        "        try:\n",
        "            print(\"[DEBUG] 6.17. Iniciando seccin xIA para SVR\")\n",
        "            if 'xai_results' not in globals() or 'SVR' not in xai_results:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontr `xai_results['SVR']`. \"\n",
        "                    \"Asegrate de haber ejecutado la Celda 10 y almacenado los resultados xIA de SVR en `xai_results['SVR']`.\"\n",
        "                )\n",
        "\n",
        "                # Cabecera\n",
        "                self.sections.append((\n",
        "                    \"##  Anlisis xIA de SVR: Resultados concretos y explicaciones Generativas\",\n",
        "                    \"\"  # contenido vaco, la cabecera se mostrar como Markdown\n",
        "                ))\n",
        "\n",
        "\n",
        "            # Funcin para llamar a OpenAI con un prompt especfico\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                \"\"\"\n",
        "                Llama a OpenAI ChatCompletion con un sistema experto en ML/XAI,\n",
        "                devuelve la respuesta de la IA en texto.\n",
        "                \"\"\"\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un asistente experto en Machine Learning e interpretabilidad de modelos. \"\n",
        "                                \"Proporciona explicaciones detalladas y basadas en los datos concretos proporcionados.\"\n",
        "                            )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    texto = response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    texto = f\"[Error llamando a OpenAI: {e}]\"\n",
        "                return texto\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Configuracin: top N variables y primeras M muestras locales\n",
        "            # ----------------------------------------------------------------\n",
        "            TOP_N = 5      # cuntas caractersticas top incluir en el prompt\n",
        "            N_LOCAL = 3    # cuntas muestras locales incluir en prompt\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Lista de mtodos xIA y claves en xai_results['SVR']\n",
        "            # ----------------------------------------------------------------\n",
        "            # Las claves deben coincidir exactamente con las usadas en Celda 10 al almacenar resultados.\n",
        "            metodos_claves = [\n",
        "                ('SHAP', 'SHAP'),\n",
        "                ('LIME', 'LIME'),\n",
        "                ('KernelExplainer', 'KernelExplainer'),\n",
        "                ('Integrated Gradients', 'Integrated Gradients'),\n",
        "                ('DeepLIFT / LRP', 'DeepLIFT / LRP'),\n",
        "                ('Permutation Feature Importance', 'Permutation Feature Importance'),\n",
        "                ('Partial Dependence Plots (PDP)', 'Partial Dependence Plots (PDP)'),\n",
        "                ('Accumulated Local Effects (ALE)', 'Accumulated Local Effects (ALE)'),\n",
        "                ('Individual Conditional Expectation (ICE) Plots', 'Individual Conditional Expectation (ICE) Plots'),\n",
        "                ('Counterfactual Explanations', 'Counterfactual Explanations'),\n",
        "                ('Anchors', 'Anchors'),\n",
        "                ('Surrogate Models (Global/Local)', 'Surrogate Models (Global/Local)'),\n",
        "                ('Explainable Boosting Machine (EBM)', 'Explainable Boosting Machine (EBM)'),\n",
        "                ('Optuna Hyperparameter Importance', 'Optuna Hyperparameter Importance'),\n",
        "            ]\n",
        "\n",
        "            for titulo, clave in metodos_claves:\n",
        "                print(f\"[DEBUG] 6.18. Procesando seccin xIA: {titulo}\")\n",
        "                datos = xai_results['SVR'].get(clave)\n",
        "                if datos is None:\n",
        "                    print(f\"[DEBUG] No hay resultados xIA para {titulo}, se omite\")\n",
        "                    continue\n",
        "#                    self.sections.append((\n",
        "#                        f\"###  No hay resultados para {titulo}\",\n",
        "#                        f\"No se hallaron resultados para la clave '{clave}'.\"\n",
        "#                    ))\n",
        "#                    continue\n",
        "\n",
        "                # ---------------- Mostrar figura guardada ----------------\n",
        "                print(f\"[DEBUG] 6.19. Mostrando figura para {titulo}\")\n",
        "                fig = datos.get('fig_summary') or datos.get('fig')\n",
        "                if fig is not None:\n",
        "                    self.sections.append((f\"### {titulo}: Grfico\", fig))\n",
        "\n",
        "                # --------------- Mostrar DataFrames de importancia y local ---------------\n",
        "                print(f\"[DEBUG] 6.20. Mostrando DataFrames para {titulo}\")\n",
        "                imp_df = datos.get('imp_df')\n",
        "                df_local = datos.get('df_local')\n",
        "                stats_extra = datos.get('stats', None)  # opcional: estadsticas adicionales, p.ej. percentiles, pos_pct SHAP, etc.\n",
        "\n",
        "                if isinstance(imp_df, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Importancia global\", imp_df.reset_index(drop=True)))\n",
        "\n",
        "                if isinstance(df_local, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Valores locales\", df_local.head(N_LOCAL)))\n",
        "\n",
        "                # --------------------------------- Extraer datos del grfico (suponiendo que sea un barh con 'feature' y 'importance') ---------------------------\n",
        "                features = [text.get_text() for text in ax.get_yticklabels()]\n",
        "                importances = [bar.get_width() for bar in ax.patches]\n",
        "\n",
        "                # --------------- Construir prompt con valores numricos concretos ---------------\n",
        "                print(f\"[DEBUG] 6.21. Construyendo prompt para {titulo}\")\n",
        "                prompt = f\"He aplicado el mtodo xIA '{titulo}' al modelo SVR entrenado y he obtenido estos resultados concretos:\\n\\n\"\n",
        "\n",
        "                prompt += \"- Datos del grfico (feature vs importancia):\\n\"\n",
        "                for f, imp in zip(features, importances):\n",
        "                    prompt += f\"     {f}: {imp:.4f}\\n\"\n",
        "\n",
        "                # Ahora s le pides que interprete el grfico:\n",
        "                prompt += (\n",
        "                    \"- Interpreta el grfico anterior: \"\n",
        "                    \"describe qu patrones o relaciones visuales revela cmo se distribuye la importancia.\\n\"\n",
        "                )\n",
        "\n",
        "                # 1) Extraer importancia global: top N\n",
        "                if isinstance(imp_df, pd.DataFrame) and not imp_df.empty:\n",
        "                    imp_df = imp_df.reset_index()\n",
        "                    imp_df = imp_df.rename(columns={ imp_df.columns[0]: \"feature\" })\n",
        "                    cols = imp_df.columns.tolist()\n",
        "                    if len(cols) >= 2:\n",
        "                        feat_col = \"feature\"\n",
        "                        val_col  = cols[1]\n",
        "                        try:\n",
        "                            imp_df_sorted = imp_df.sort_values(val_col, ascending=False)\n",
        "                        except Exception:\n",
        "                            imp_df_sorted = imp_df\n",
        "\n",
        "                        top_n      = min(TOP_N, len(imp_df_sorted))\n",
        "                        top_imp_df = imp_df_sorted.iloc[:top_n][[feat_col, val_col]]\n",
        "                        top_imp_list = []\n",
        "                        for _, row in top_imp_df.iterrows():\n",
        "                            raw = row[val_col]\n",
        "                            try:\n",
        "                                v = float(raw)\n",
        "                            except Exception:\n",
        "                                v = raw  # si no se puede convertir, lo dejo tal cual\n",
        "                            top_imp_list.append({feat_col: row[feat_col], val_col: v})\n",
        "\n",
        "                        prompt += f\"- Top {top_n} caractersticas por importancia global ({feat_col}, {val_col}):\\n  {top_imp_list}\\n\"\n",
        "\n",
        "                        extra_cols = cols[2:]\n",
        "                        if extra_cols:\n",
        "                            prompt += f\"  (El DataFrame de importancia global incluye tambin columnas: {extra_cols}.)\\n\"\n",
        "                    else:\n",
        "                        snippet = imp_df.head(TOP_N).to_dict(orient='records')\n",
        "                        prompt += f\"- Importancia global (primeras filas, formato genrico):\\n  {snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de importancia global.\\n\"\n",
        "\n",
        "#                    #  Siempre sacamos el ndice como columna llamada \"feature\"\n",
        "                    imp_df = imp_df.rename(columns={ idx_col: \"feature\" })\n",
        "\n",
        "                # 2) Extraer valores locales: primeras M muestras\n",
        "                if isinstance(df_local, pd.DataFrame) and not df_local.empty:\n",
        "                    n_loc = min(N_LOCAL, len(df_local))\n",
        "                    loc_snippet = df_local.head(n_loc).to_dict(orient='records')\n",
        "                    prompt += f\"- Valores locales para las primeras {n_loc} muestras (cada dict mapea feature a valor/atribucin):\\n  {loc_snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de valores locales.\\n\"\n",
        "\n",
        "                # 3) Incluir estadsticas extra si existen\n",
        "                if isinstance(stats_extra, dict):\n",
        "                    prompt += \"- Estadsticas adicionales:\\n\"\n",
        "                    for k, v in stats_extra.items():\n",
        "                        prompt += f\"  * {k}: {v}\\n\"\n",
        "\n",
        "                # 4) Contexto general del modelo SVR\n",
        "                prompt += (\n",
        "                    \"\\nContexto: El modelo SVR fue entrenado con variables seleccionadas y StandardScaler, \"\n",
        "                    \"con predicciones desescaladas. Ahora interpretamos los resultados xIA para este SVR.\\n\"\n",
        "                )\n",
        "\n",
        "                # 5) Preguntas/pautas especficas segn el mtodo\n",
        "                if clave == 'SHAP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores concretos de SHAP para:\\n\"\n",
        "                        \"1. Explicar la importancia global de cada variable en base a los valores de SHAP mostrados.\\n\"\n",
        "                        \"2. Analizar los valores SHAP de las primeras muestras: si una variable muestra SHAP positivo alto, qu implica sobre la prediccin en ese caso? Y si es negativo, qu implica?\\n\"\n",
        "                        \"3. Identificar patrones en SHAP (por ejemplo, variables que consistentemente empujan en una direccin) y cmo afecta al comportamiento general del SVR.\\n\"\n",
        "                        \"4. Sugerir posibles transformaciones de variables o validaciones adicionales basadas en estos resultados SHAP.\\n\"\n",
        "                    )\n",
        "                elif clave == 'LIME':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos pesos LIME concretos para:\\n\"\n",
        "                        \"1. Explicar el significado de pesos positivos y negativos en LIME para SVR.\\n\"\n",
        "                        \"2. Analizar casos de las primeras muestras: si una variable tiene peso LIME fuerte (positivo/negativo), qu implica para la prediccin local?\\n\"\n",
        "                        \"3. Comentar si la variabilidad de los pesos sugiere relaciones no lineales o interacciones no capturadas.\\n\"\n",
        "                        \"4. Recomendar acciones: agrupacin de variables, deteccin de outliers, etc., basadas en la interpretacin LIME.\\n\"\n",
        "                    )\n",
        "                elif clave == 'KernelExplainer':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de KernelExplainer (SHAP caja negra) para:\\n\"\n",
        "                        \"1. Explicar la importancia global segn los valores medios absolutos Kernel SHAP.\\n\"\n",
        "                        \"2. Analizar los valores locales para las primeras muestras: cmo cada caracterstica empuja la prediccin en cada caso.\\n\"\n",
        "                        \"3. Comparar con SHAP (si ya lo hiciste con TreeExplainer para otro modelo o Kernel SHAP para SVR), si aplica.\\n\"\n",
        "                        \"4. Sugerir consideraciones sobre fondo (background) usado y posibles ajustes si las explicaciones muestran comportamiento inesperado.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Integrated Gradients':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Integrated Gradients para:\\n\"\n",
        "                        \"1. Explicar la contribucin integrada de cada variable: interpretacin de importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: qu implicaciones tiene un valor IG alto o bajo en cada variable.\\n\"\n",
        "                        \"3. Sealar limitaciones: compatibilidad con SVR no diferenciable; si estos valores provienen de un modelo aproximado, comentar fiabilidad.\\n\"\n",
        "                        \"4. Sugerir pasos adicionales o comparaciones con otros mtodos (SHAP/LIME) para validar interpretaciones.\\n\"\n",
        "                    )\n",
        "                elif clave == 'DeepLIFT / LRP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de DeepLIFT / LRP para:\\n\"\n",
        "                        \"1. Explicar la relevancia asignada a cada variable en importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: si una variable tiene relevancia positiva o negativa, qu significa para la prediccin.\\n\"\n",
        "                        \"3. Comparar con IG o SHAP si se dispone: consistencia de atribuciones.\\n\"\n",
        "                        \"4. Recomendar verificaciones o transformaciones en caso de interpretaciones inesperadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Permutation Feature Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Permutation Feature Importance para:\\n\"\n",
        "                        \"1. Explicar la cada en la mtrica al permutar cada variable: por qu ciertas variables son crticas.\\n\"\n",
        "                        \"2. Comentar la desviacin estndar: indica inestabilidad en la importancia? Dnde conviene reforzar validaciones?\\n\"\n",
        "                        \"3. Comparar con importancias de SHAP/LIME: similitudes o diferencias.\\n\"\n",
        "                        \"4. Sugerir prioridades para ajuste de modelo o seleccin de variables basadas en esta mtrica.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Partial Dependence Plots (PDP)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos PDP para:\\n\"\n",
        "                        \"1. Explicar el efecto promedio de cada variable sobre la prediccin segn el rango PDP obtenido.\\n\"\n",
        "                        \"2. Sealar si los rangos sugieren relaciones montonas o no lineales.\\n\"\n",
        "                        \"3. Advertir sobre correlaciones fuertes que puedan afectar la interpretacin.\\n\"\n",
        "                        \"4. Recomendar posibles exploraciones adicionales (PDP bivariados, transformaciones) segn los resultados.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Accumulated Local Effects (ALE)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ALE y valores locales ALE para:\\n\"\n",
        "                        \"1. Explicar cmo ALE corrige artefactos de correlacin y qu nos dicen los valores concretos.\\n\"\n",
        "                        \"2. Interpretar importancia global ALE: variables con mayor efecto acumulado.\\n\"\n",
        "                        \"3. Analizar heterogeneidad local a partir de valores ALE de primeras muestras.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales (ALE 2D, inspeccin de distribucin) segn hallazgos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Individual Conditional Expectation (ICE) Plots':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ICE medios para:\\n\"\n",
        "                        \"1. Explicar qu mide el rango ICE y su diferencia respecto a PDP.\\n\"\n",
        "                        \"2. Analizar heterogeneidad: variables con alta variabilidad en rangos ICE indican interacciones o comportamiento inestable.\\n\"\n",
        "                        \"3. Sealar implicaciones para el modelo y posibles ajustes si hay alto efecto local variable.\\n\"\n",
        "                        \"4. Recomendar exploraciones adicionales para entender la variabilidad local.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Counterfactual Explanations':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Contrafactuales para:\\n\"\n",
        "                        \"1. Explicar cmo interpretar los contrafactuales: cambios en variables que generan aumento en prediccin.\\n\"\n",
        "                        \"2. Analizar variables con mayor || medio: implicaciones sobre sensibilidad del modelo.\\n\"\n",
        "                        \"3. Sealar si faltan contrafactuales para algunas muestras: qu puede indicar (limites del modelo o datos).\\n\"\n",
        "                        \"4. Sugerir cmo usar estos insights para ajuste de modelo o recoleccin de datos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Anchors':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Anchors para:\\n\"\n",
        "                        \"1. Explicar cmo leer las reglas ancla de las primeras muestras: condiciones que aseguran la prediccin.\\n\"\n",
        "                        \"2. Analizar frecuencia global de aparicin de variables en reglas: implicaciones sobre estabilidad y sesgos.\\n\"\n",
        "                        \"3. Sealar regiones de bajo coverage o baja precisin: dnde el modelo es menos fiable.\\n\"\n",
        "                        \"4. Recomendar acciones: recoleccin de datos, refinamiento de variables o validaciones dirigidas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Surrogate Models (Global/Local)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Surrogate Models para:\\n\"\n",
        "                        \"1. Explicar la importancia global del surrogate (rbol) y la importancia local media (coeficientes regresiones locales).\\n\"\n",
        "                        \"2. Comparar global vs local: variables con alta importancia local pero baja global, o viceversa, y qu sugiere.\\n\"\n",
        "                        \"3. Concluir sobre consistencia de comportamiento del SVR en diferentes regiones del espacio.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales o ajustes de modelo segn discrepancias detectadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Explainable Boosting Machine (EBM)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de EBM para:\\n\"\n",
        "                        \"1. Explicar la importancia global segn EBM: cmo se comparan con otros mtodos.\\n\"\n",
        "                        \"2. Analizar contribuciones locales de las primeras muestras: qu patrones se observan.\\n\"\n",
        "                        \"3. Sealar si EBM revela interacciones no consideradas en SVR.\\n\"\n",
        "                        \"4. Recomendar posibles ajustes en caractersticas o validaciones segn insights de EBM.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Optuna Hyperparameter Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Optuna para:\\n\"\n",
        "                        \"1. Explicar la importancia global de hiperparmetros en la optimizacin del SVR.\\n\"\n",
        "                        \"2. Analizar top trials si estn disponibles: qu combinaciones de hiperparmetros funcionaron mejor.\\n\"\n",
        "                        \"3. Sealar limitaciones de la muestra de trials (nmero de pruebas) y posibles riesgos de sobreajuste en la bsqueda.\\n\"\n",
        "                        \"4. Recomendar prximas acciones para tuning basadas en estas importancias.\\n\"\n",
        "                    )\n",
        "                else:\n",
        "                    prompt += \"\\nPor favor, explica estos resultados numricos y qu implicaciones tienen para el modelo SVR.\\n\"\n",
        "\n",
        "                # --------------- Llamada a OpenAI ---------------\n",
        "                print(f\"[DEBUG] 6.22. Llamando a OpenAI para {titulo}\")\n",
        "                explicacion = call_openai_explanation(prompt)\n",
        "                self.sections.append((f\"### {titulo}: Explicacin Generativa\", explicacion))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"###  Error en seccin xIA SVR\",\n",
        "                f\"Se produjo un error al generar la seccin xIA de SVR: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =============================================================\n",
        "        # 7. Entrenamiento Modelo NN\n",
        "        # =============================================================\n",
        "        try:\n",
        "            # Comprobamos RESUMEN_METODOS y existencia de X_test/Y_test\n",
        "            if \"RESUMEN_METODOS\" in self.g and \"X_test\" in self.g and \"Y_test\" in self.g:\n",
        "                import os, pickle\n",
        "                import numpy as np\n",
        "                import pandas as _pd\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "                from tensorflow.keras.models import load_model\n",
        "\n",
        "                X_test_full = self.g[\"X_test\"]\n",
        "                Y_test = self.g[\"Y_test\"]\n",
        "                # Preparamos array 1D de y_test\n",
        "                if hasattr(Y_test, \"values\"):\n",
        "                    arr = Y_test.values\n",
        "                    if arr.ndim == 2 and arr.shape[1] == 1:\n",
        "                        y_test_arr = arr[:, 0]\n",
        "                    elif arr.ndim == 2 and arr.shape[1] > 1:\n",
        "                        y_test_arr = arr[:, 0]\n",
        "                    else:\n",
        "                        y_test_arr = arr.ravel()\n",
        "                else:\n",
        "                    y_test_arr = np.array(Y_test).ravel()\n",
        "                # Aseguramos 1D\n",
        "                y_test_arr = y_test_arr.ravel()\n",
        "\n",
        "                metrics_summary_nn = []\n",
        "                # Iteramos sobre cada mtodo declarado en RESUMEN_METODOS\n",
        "                for metodo, vars_sel in self.g[\"RESUMEN_METODOS\"].items():\n",
        "                    metodo_low = metodo.lower()\n",
        "                    # Rutas a los archivos NN guardados en celda 7.2\n",
        "                    model_fname = f\"modelo_nn_{metodo_low}.h5\"\n",
        "                    scaler_fname = f\"escaladores_nn_{metodo_low}.pkl\"\n",
        "                    hp_fname = f\"hyperparams_nn_{metodo_low}.pkl\"\n",
        "                    #hyper_fname = f\"hyperparams_nn_{metodo_low}.pkl\"\n",
        "                    if not os.path.exists(model_fname) or not os.path.exists(scaler_fname):\n",
        "                        print(f\"[DEBUG] Fichero de modelo NN o escaladores no encontrado para mtodo '{metodo}': omito este mtodo\")\n",
        "                        continue\n",
        "                    # Cargar modelo y escaladores\n",
        "                    try:\n",
        "                        model = load_model(model_fname)\n",
        "                        with open(scaler_fname, \"rb\") as f:\n",
        "                            data_s = pickle.load(f)\n",
        "                        sx = data_s.get(\"scaler_X\", None)\n",
        "                        sy = data_s.get(\"scaler_Y\", None)\n",
        "                        cols = data_s.get(\"cols\", None)\n",
        "                        # y_variable_name = data_s.get(\"yname\", None)\n",
        "                        if model is None or sx is None or sy is None or cols is None:\n",
        "                            print(f\"[DEBUG] Falta clave en escaladores NN para mtodo '{metodo}', omito\")\n",
        "                            continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al cargar modelo/escaladores NN para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Verificar columnas en X_test\n",
        "                    missing = [c for c in cols if c not in X_test_full.columns]\n",
        "                    if missing:\n",
        "                        print(f\"[DEBUG] Para mtodo '{metodo}', faltan columnas en X_test: {missing}, omito\")\n",
        "                        continue\n",
        "\n",
        "                    # Subconjunto X_test\n",
        "                    X_test_sel = X_test_full[cols].copy()\n",
        "                    # Escalar, predecir y desescalar\n",
        "                    try:\n",
        "                        X_test_scaled = sx.transform(X_test_sel)\n",
        "                        #y_pred_scaled = model.predict(X_test_scaled).ravel()\n",
        "                        y_pred_scaled = model.predict(X_test_scaled, verbose=0).ravel()\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al predecir/desescalar NN para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # 1) Cargar hiperparmetros\n",
        "                    try:\n",
        "                        hp_fname = f\"hyperparams_nn_{metodo.lower()}.pkl\"\n",
        "                        if os.path.exists(hp_fname):\n",
        "                            with open(hp_fname, \"rb\") as f:\n",
        "                                hp = pickle.load(f)\n",
        "                        else:\n",
        "                            hp = None\n",
        "                        # Representar en DataFrame\n",
        "                        if hp:\n",
        "                            import pandas as _pd\n",
        "                            df_hp = _pd.DataFrame({\n",
        "                                \"Hiperparmetro\": list(hp.keys()),\n",
        "                                \"Valor\": [str(v) for v in hp.values()]\n",
        "                            })\n",
        "                            titulo_hp = f\"### Parmetros de Entrenamiento NN ({metodo})\"\n",
        "                            self.sections.append((titulo_hp, df_hp))\n",
        "                            print(f\"[DEBUG] 7.1. Seccin parmetros NN aadida para mtodo: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No hay hyperparams guardados para NN mtodo '{metodo}'\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al extraer/parmetros NN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA hiperparmetros NN\n",
        "                    try:\n",
        "                        if hp:\n",
        "                            prompt_params = (\n",
        "                                f\"Has entrenado un modelo de Red Neuronal con seleccin de variables '{metodo}'.\\n\"\n",
        "                                \"Estos fueron los hiperparmetros utilizados:\\n\"\n",
        "                            )\n",
        "                            for k, v in hp.items():\n",
        "                                prompt_params += f\"- {k}: {v}\\n\"\n",
        "                            prompt_params += (\n",
        "                                \"\\nPor favor, explica de forma profesional y detallada cmo estos hiperparmetros \"\n",
        "                                \"pueden influir en el entrenamiento de la red neuronal, su impacto en ajuste, \"\n",
        "                                \"y buenas prcticas para seleccionarlos o afinarlos.\"\n",
        "                            )\n",
        "                            print(f\"[DEBUG] 7.2. Iniciando llamada a OpenAI para explicacin hiperparmetros NN ({metodo})...\")\n",
        "                            resp = _client.chat.completions.create(\n",
        "                                model=\"gpt-4\",\n",
        "                                messages=[\n",
        "                                    {\"role\": \"system\", \"content\": \"Eres un experto en entrenamiento de redes neuronales para regresin.\"},\n",
        "                                    {\"role\": \"user\", \"content\": prompt_params}\n",
        "                                ],\n",
        "                                max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                                temperature=TEMPERATURE_VAL\n",
        "                            )\n",
        "                            explanation_hp = resp.choices[0].message.content.strip()\n",
        "                            if explanation_hp:\n",
        "                                titulo_exp_hp = f\"###  Explicacin IA Hiperparmetros NN ({metodo})\"\n",
        "                                self.sections.append((titulo_exp_hp, explanation_hp))\n",
        "                                print(f\"[DEBUG] 7.3. Seccin explicacin IA hiperparmetros NN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA hiperparmetros NN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 2) Grfica Predicho vs Real\n",
        "                    try:\n",
        "                        import matplotlib.pyplot as plt\n",
        "                        fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "                        ax1.scatter(y_test_arr, y_pred, alpha=0.6)\n",
        "                        ax1.plot([y_test_arr.min(), y_test_arr.max()],\n",
        "                                 [y_test_arr.min(), y_test_arr.max()],\n",
        "                                 'r--', lw=2)\n",
        "                        ax1.set_xlabel(\"Y real\")\n",
        "                        ax1.set_ylabel(\"Y predicho\")\n",
        "                        ax1.set_title(f\"NN Predicho vs Real ({metodo})\")\n",
        "                        titulo_fig1 = f\"### Grfica NN Predicho vs Real ({metodo})\"\n",
        "                        self.sections.append((titulo_fig1, fig1))\n",
        "                        print(f\"[DEBUG] 7.4. Seccin grfica Pred vs Real NN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear grfica Pred vs Real NN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Estadsticas para contexto IA Pred vs Real\n",
        "                    try:\n",
        "                        mse = mean_squared_error(y_test_arr, y_pred)\n",
        "                        rmse = np.sqrt(mse)\n",
        "                        mae = mean_absolute_error(y_test_arr, y_pred)\n",
        "                        r2 = r2_score(y_test_arr, y_pred)\n",
        "                        corr = np.corrcoef(y_test_arr, y_pred)[0,1] if len(y_test_arr)>1 else None\n",
        "                        y_real_min, y_real_max = float(np.min(y_test_arr)), float(np.max(y_test_arr))\n",
        "                        y_pred_min, y_pred_max = float(np.min(y_pred)), float(np.max(y_pred))\n",
        "                    except Exception:\n",
        "                        mse = rmse = mae = r2 = corr = None\n",
        "                        y_real_min = y_real_max = y_pred_min = y_pred_max = None\n",
        "\n",
        "                    # Explicacin IA Pred vs Real NN con contexto numrico\n",
        "                    try:\n",
        "                        prompt_pr = (\n",
        "                            f\"A continuacin tienes datos de la grfica de comparacin Real vs Prediccin para el modelo NN con mtodo '{metodo}':\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- R: {r2}\\n\"\n",
        "                            f\"- Correlacin entre Y real y predicha: {corr}\\n\"\n",
        "                            f\"- Rango Y real: [{y_real_min}, {y_real_max}]\\n\"\n",
        "                            f\"- Rango Y predicha: [{y_pred_min}, {y_pred_max}]\\n\\n\"\n",
        "                            \"Basndote en estos valores y en la grfica generada (Real vs Prediccin), \"\n",
        "                            \"proporciona un anlisis detallado, sealando si hay sesgos sistemticos (por ejemplo, subestimacin o sobreestimacin en ciertos rangos), \"\n",
        "                            \"si la dispersin es mayor en algn rango de prediccin, y qu implicaciones tiene para la calidad del modelo. \"\n",
        "                            \"Usa un texto profesional y bien estructurado, y menciona qu indicios de la grfica respaldan tus conclusiones.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 7.5. Llamada IA Pred vs Real NN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_pr}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_pr = resp.choices[0].message.content.strip()\n",
        "                        if explanation_pr:\n",
        "                            titulo_exp_pr = f\"###  Explicacin IA Predicho vs Real NN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_pr, explanation_pr))\n",
        "                            print(f\"[DEBUG] 7.6. Seccin explicacin IA Pred vs Real NN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA Pred vs Real NN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 3) Grfica de residuos NN\n",
        "                    try:\n",
        "                        residuals = y_test_arr - y_pred\n",
        "                        res_mean = float(np.mean(residuals))\n",
        "                        res_std  = float(np.std(residuals))\n",
        "                        # Estadsticos de residuos con pandas\n",
        "                        res_series = _pd.Series(residuals)\n",
        "                        res_skew = float(res_series.skew())\n",
        "                        res_kurt = float(res_series.kurtosis())\n",
        "                        q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]\n",
        "                        # Rango de Y real y predicha\n",
        "                        # (ya lo tenemos en y_real_min, etc.)\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "                        ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "                        ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "                        ax2.set_xlabel(\"Y predicho\")\n",
        "                        ax2.set_ylabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "                        ax2.set_title(f\"NN Residuos ({metodo})\")\n",
        "                        titulo_fig2 = f\"### Grfica NN Residuos ({metodo})\"\n",
        "                        self.sections.append((titulo_fig2, fig2))\n",
        "                        print(f\"[DEBUG] 7.7. Seccin grfica residuos NN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear grfica residuos NN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA Residuos NN con contexto numrico\n",
        "                    try:\n",
        "                        prompt_res = (\n",
        "                            f\"A continuacin tienes estadsticas de los residuos (Real - Predicha) del modelo NN con mtodo '{metodo}':\\n\"\n",
        "                            f\"- Media: {res_mean}\\n\"\n",
        "                            f\"- Desviacin estndar: {res_std}\\n\"\n",
        "                            f\"- Asimetra: {res_skew}\\n\"\n",
        "                            f\"- Curtosis: {res_kurt}\\n\"\n",
        "                            f\"- Cuantiles: 25%={q25}, 50%={q50}, 75%={q75}\\n\\n\"\n",
        "                            \"Basndote en estos valores y en la grfica de residuos, analiza si hay patrones (por ejemplo, heterocedasticidad, outliers, sesgos en rangos), \"\n",
        "                            \"y comenta qu implicaciones tiene para la robustez y generalizacin del modelo. \"\n",
        "                            \"Usa un texto profesional y bien estructurado.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 7.8. Llamada IA Residuos NN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_res}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_res = resp.choices[0].message.content.strip()\n",
        "                        if explanation_res:\n",
        "                            titulo_exp_res = f\"###  Explicacin IA Residuos NN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_res, explanation_res))\n",
        "                            print(f\"[DEBUG] 7.9. Seccin explicacin IA residuos NN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA residuos NN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 4) Tabla Mtricas NN y explicacin IA\n",
        "                    try:\n",
        "                        df_met = _pd.DataFrame([\n",
        "                            {\"Mtrica\": \"RMSE\", \"Valor\": rmse},\n",
        "                            {\"Mtrica\": \"MAE\", \"Valor\": mae},\n",
        "                            {\"Mtrica\": \"R2\",  \"Valor\": r2}\n",
        "                        ])\n",
        "                        titulo_met = f\"### Mtricas NN ({metodo})\"\n",
        "                        self.sections.append((titulo_met, df_met))\n",
        "                        print(f\"[DEBUG] 7.10. Seccin mtricas NN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame mtricas NN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_met = (\n",
        "                            f\"Estas son las mtricas del modelo NN con mtodo '{metodo}':\\n\"\n",
        "                            f\"- R2: {r2}\\n\"\n",
        "                            f\"- MSE: {mse}\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- Correlacin Real vs Predicha: {corr}\\n\\n\"\n",
        "                            \"Analiza estos valores en contexto: son adecuados? qu sugieren respecto al rendimiento del modelo? \"\n",
        "                            \"Menciona referencias a la grfica Real vs Prediccin y a los residuos si procede.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 7.11. Llamada IA Mtricas NN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_met}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_met = resp.choices[0].message.content.strip()\n",
        "                        if explanation_met:\n",
        "                            titulo_exp_met = f\"###  Explicacin IA Mtricas NN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_met, explanation_met))\n",
        "                            print(f\"[DEBUG] 7.12. Seccin explicacin IA mtricas NN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA mtricas NN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Acumular para comparativa\n",
        "                    metrics_summary_nn.append({\n",
        "                        \"metodo\": metodo,\n",
        "                        \"rmse\": rmse,\n",
        "                        \"mae\": mae,\n",
        "                        \"r2\": r2\n",
        "                    })\n",
        "\n",
        "                # 5) Comparativa global de mtricas NN\n",
        "                if metrics_summary_nn:\n",
        "                    try:\n",
        "                        df_comp_nn = _pd.DataFrame(metrics_summary_nn)\n",
        "                        df_comp_nn_sorted = df_comp_nn.sort_values(\"rmse\")\n",
        "                        titulo_comp_nn = \"### Comparativa Mtricas NN entre Mtodos\"\n",
        "                        self.sections.append((titulo_comp_nn, df_comp_nn_sorted))\n",
        "                        print(\"[DEBUG] 7.13. Seccin comparativa mtricas NN aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame comparativo mtricas NN: {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_conc_nn = (\n",
        "                            \"Se han entrenado varios modelos de Red Neuronal con diferentes mtodos de seleccin de variables.\\n\"\n",
        "                            \"Mtricas obtenidas en test:\\n\"\n",
        "                        )\n",
        "                        for entry in metrics_summary_nn:\n",
        "                            prompt_conc_nn += f\"- Mtodo '{entry['metodo']}': RMSE={entry['rmse']}, MAE={entry['mae']}, R2={entry['r2']}\\n\"\n",
        "                        prompt_conc_nn += (\n",
        "                            \"\\nPor favor, proporciona conclusiones profesionales comparando estos mtodos: \"\n",
        "                            \"indica cul se comporta mejor, posibles razones y recomendaciones sobre seleccin de variables o ajustes para mejorar la red neuronal.\"\n",
        "                        )\n",
        "                        print(\"[DEBUG] 7.14. Llamada IA Conclusiones NN...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_conc_nn}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_conc_nn = resp.choices[0].message.content.strip()\n",
        "                        if explanation_conc_nn:\n",
        "                            titulo_exp_conc_nn = \"###  Conclusiones IA Entrenamiento NN\"\n",
        "                            self.sections.append((titulo_exp_conc_nn, explanation_conc_nn))\n",
        "                            print(\"[DEBUG] 7.15. Seccin explicacin IA conclusiones NN aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA conclusiones NN: {e}\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No estn RESUMEN_METODOS o X_test/Y_test en globals(), omito seccin NN\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar seccin NN en informe: {e}\")\n",
        "        # ... fin de la seccin de entrenamiento NN ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 7.1. Interpretacin xIA para modelo entrenado NN\n",
        "        # =============================================================\n",
        "        import openai\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # Verificacin previa\n",
        "        # ----------------------------------------------------------------\n",
        "        try:\n",
        "            print(\"[DEBUG] 7.16. Iniciando seccin xIA para NN\")\n",
        "            if 'xai_results' not in globals() or 'NN' not in xai_results:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontr `xai_results['NN']`. \"\n",
        "                    \"Asegrate de haber ejecutado la Celda 10 y almacenado los resultados xIA de NN en `xai_results['NN']`.\"\n",
        "                )\n",
        "\n",
        "                # Cabecera\n",
        "                self.sections.append((\n",
        "                    \"##  Anlisis xIA de NN: Resultados concretos y explicaciones Generativas\",\n",
        "                    \"\"  # contenido vaco, la cabecera se mostrar como Markdown\n",
        "                ))\n",
        "\n",
        "\n",
        "            # Funcin para llamar a OpenAI con un prompt especfico\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                \"\"\"\n",
        "                Llama a OpenAI ChatCompletion con un sistema experto en ML/XAI,\n",
        "                devuelve la respuesta de la IA en texto.\n",
        "                \"\"\"\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un asistente experto en Machine Learning e interpretabilidad de modelos. \"\n",
        "                                \"Proporciona explicaciones detalladas y basadas en los datos concretos proporcionados.\"\n",
        "                            )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    texto = response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    texto = f\"[Error llamando a OpenAI: {e}]\"\n",
        "                return texto\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Configuracin: top N variables y primeras M muestras locales\n",
        "            # ----------------------------------------------------------------\n",
        "            TOP_N = 5      # cuntas caractersticas top incluir en el prompt\n",
        "            N_LOCAL = 3    # cuntas muestras locales incluir en prompt\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Lista de mtodos xIA y claves en xai_results['NN']\n",
        "            # ----------------------------------------------------------------\n",
        "            # Las claves deben coincidir exactamente con las usadas en Celda 10 al almacenar resultados.\n",
        "            metodos_claves = [\n",
        "                ('SHAP', 'SHAP'),\n",
        "                ('LIME', 'LIME'),\n",
        "                ('KernelExplainer', 'KernelExplainer'),\n",
        "                ('Integrated Gradients', 'Integrated Gradients'),\n",
        "                ('DeepLIFT / LRP', 'DeepLIFT / LRP'),\n",
        "                ('Permutation Feature Importance', 'Permutation Feature Importance'),\n",
        "                ('Partial Dependence Plots (PDP)', 'Partial Dependence Plots (PDP)'),\n",
        "                ('Accumulated Local Effects (ALE)', 'Accumulated Local Effects (ALE)'),\n",
        "                ('Individual Conditional Expectation (ICE) Plots', 'Individual Conditional Expectation (ICE) Plots'),\n",
        "                ('Counterfactual Explanations', 'Counterfactual Explanations'),\n",
        "                ('Anchors', 'Anchors'),\n",
        "                ('Surrogate Models (Global/Local)', 'Surrogate Models (Global/Local)'),\n",
        "                ('Explainable Boosting Machine (EBM)', 'Explainable Boosting Machine (EBM)'),\n",
        "                ('Optuna Hyperparameter Importance', 'Optuna Hyperparameter Importance'),\n",
        "            ]\n",
        "\n",
        "            for titulo, clave in metodos_claves:\n",
        "                print(f\"[DEBUG] 7.17. Procesando seccin xIA: {titulo}\")\n",
        "                datos = xai_results['NN'].get(clave)\n",
        "                if datos is None:\n",
        "                    print(f\"[DEBUG] No hay resultados xIA para {titulo}, se omite\")\n",
        "                    continue\n",
        "#                    self.sections.append((\n",
        "#                        f\"###  No hay resultados para {titulo}\",\n",
        "#                        f\"No se hallaron resultados para la clave '{clave}'.\"\n",
        "#                    ))\n",
        "#                    continue\n",
        "\n",
        "                # ---------------- Mostrar figura guardada ----------------\n",
        "                print(f\"[DEBUG] 7.18. Mostrando figura para {titulo}\")\n",
        "                fig = datos.get('fig_summary') or datos.get('fig')\n",
        "                if fig is not None:\n",
        "                    self.sections.append((f\"### {titulo}: Grfico\", fig))\n",
        "\n",
        "                # --------------- Mostrar DataFrames de importancia y local ---------------\n",
        "                print(f\"[DEBUG] 7.19. Mostrando DataFrames para {titulo}\")\n",
        "                imp_df = datos.get('imp_df')\n",
        "                df_local = datos.get('df_local')\n",
        "                stats_extra = datos.get('stats', None)  # opcional: estadsticas adicionales, p.ej. percentiles, pos_pct SHAP, etc.\n",
        "\n",
        "                if isinstance(imp_df, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Importancia global\", imp_df.reset_index(drop=True)))\n",
        "\n",
        "                if isinstance(df_local, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Valores locales\", df_local.head(N_LOCAL)))\n",
        "\n",
        "                # --------------------------------- Extraer datos del grfico (suponiendo que sea un barh con 'feature' y 'importance') ---------------------------\n",
        "                features = [text.get_text() for text in ax.get_yticklabels()]\n",
        "                importances = [bar.get_width() for bar in ax.patches]\n",
        "\n",
        "                # --------------- Construir prompt con valores numricos concretos ---------------\n",
        "                print(f\"[DEBUG] 7.20. Construyendo prompt para {titulo}\")\n",
        "                prompt = f\"He aplicado el mtodo xIA '{titulo}' al modelo NN entrenado y he obtenido estos resultados concretos:\\n\\n\"\n",
        "\n",
        "                prompt += \"- Datos del grfico (feature vs importancia):\\n\"\n",
        "                for f, imp in zip(features, importances):\n",
        "                    prompt += f\"     {f}: {imp:.4f}\\n\"\n",
        "\n",
        "                # Ahora s le pides que interprete el grfico:\n",
        "                prompt += (\n",
        "                    \"- Interpreta el grfico anterior: \"\n",
        "                    \"describe qu patrones o relaciones visuales revela cmo se distribuye la importancia.\\n\"\n",
        "                )\n",
        "\n",
        "                # 1) Extraer importancia global: top N\n",
        "                if isinstance(imp_df, pd.DataFrame) and not imp_df.empty:\n",
        "                    imp_df = imp_df.reset_index()\n",
        "                    imp_df = imp_df.rename(columns={ imp_df.columns[0]: \"feature\" })\n",
        "                    cols = imp_df.columns.tolist()\n",
        "                    if len(cols) >= 2:\n",
        "                        feat_col = \"feature\"\n",
        "                        val_col  = cols[1]\n",
        "                        try:\n",
        "                            imp_df_sorted = imp_df.sort_values(val_col, ascending=False)\n",
        "                        except Exception:\n",
        "                            imp_df_sorted = imp_df\n",
        "\n",
        "                        top_n      = min(TOP_N, len(imp_df_sorted))\n",
        "                        top_imp_df = imp_df_sorted.iloc[:top_n][[feat_col, val_col]]\n",
        "                        top_imp_list = []\n",
        "                        for _, row in top_imp_df.iterrows():\n",
        "                            raw = row[val_col]\n",
        "                            try:\n",
        "                                v = float(raw)\n",
        "                            except Exception:\n",
        "                                v = raw  # si no se puede convertir, lo dejo tal cual\n",
        "                            top_imp_list.append({feat_col: row[feat_col], val_col: v})\n",
        "\n",
        "                        prompt += f\"- Top {top_n} caractersticas por importancia global ({feat_col}, {val_col}):\\n  {top_imp_list}\\n\"\n",
        "\n",
        "                        extra_cols = cols[2:]\n",
        "                        if extra_cols:\n",
        "                            prompt += f\"  (El DataFrame de importancia global incluye tambin columnas: {extra_cols}.)\\n\"\n",
        "                    else:\n",
        "                        snippet = imp_df.head(TOP_N).to_dict(orient='records')\n",
        "                        prompt += f\"- Importancia global (primeras filas, formato genrico):\\n  {snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de importancia global.\\n\"\n",
        "\n",
        "#                    #  Siempre sacamos el ndice como columna llamada \"feature\"\n",
        "                    imp_df = imp_df.rename(columns={ idx_col: \"feature\" })\n",
        "\n",
        "                # 2) Extraer valores locales: primeras M muestras\n",
        "                if isinstance(df_local, pd.DataFrame) and not df_local.empty:\n",
        "                    n_loc = min(N_LOCAL, len(df_local))\n",
        "                    loc_snippet = df_local.head(n_loc).to_dict(orient='records')\n",
        "                    prompt += f\"- Valores locales para las primeras {n_loc} muestras (cada dict mapea feature a valor/atribucin):\\n  {loc_snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de valores locales.\\n\"\n",
        "\n",
        "                # 3) Incluir estadsticas extra si existen\n",
        "                if isinstance(stats_extra, dict):\n",
        "                    prompt += \"- Estadsticas adicionales:\\n\"\n",
        "                    for k, v in stats_extra.items():\n",
        "                        prompt += f\"  * {k}: {v}\\n\"\n",
        "\n",
        "                # 4) Contexto general del modelo NN\n",
        "                prompt += (\n",
        "                    \"\\nContexto: El modelo NN fue entrenado con variables seleccionadas y StandardScaler, \"\n",
        "                    \"con predicciones desescaladas. Ahora interpretamos los resultados xIA para este NN.\\n\"\n",
        "                )\n",
        "\n",
        "                # 5) Preguntas/pautas especficas segn el mtodo\n",
        "                if clave == 'SHAP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores concretos de SHAP para:\\n\"\n",
        "                        \"1. Explicar la importancia global de cada variable en base a los valores de SHAP mostrados.\\n\"\n",
        "                        \"2. Analizar los valores SHAP de las primeras muestras: si una variable muestra SHAP positivo alto, qu implica sobre la prediccin en ese caso? Y si es negativo, qu implica?\\n\"\n",
        "                        \"3. Identificar patrones en SHAP (por ejemplo, variables que consistentemente empujan en una direccin) y cmo afecta al comportamiento general del NN.\\n\"\n",
        "                        \"4. Sugerir posibles transformaciones de variables o validaciones adicionales basadas en estos resultados SHAP.\\n\"\n",
        "                    )\n",
        "                elif clave == 'LIME':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos pesos LIME concretos para:\\n\"\n",
        "                        \"1. Explicar el significado de pesos positivos y negativos en LIME para NN.\\n\"\n",
        "                        \"2. Analizar casos de las primeras muestras: si una variable tiene peso LIME fuerte (positivo/negativo), qu implica para la prediccin local?\\n\"\n",
        "                        \"3. Comentar si la variabilidad de los pesos sugiere relaciones no lineales o interacciones no capturadas.\\n\"\n",
        "                        \"4. Recomendar acciones: agrupacin de variables, deteccin de outliers, etc., basadas en la interpretacin LIME.\\n\"\n",
        "                    )\n",
        "                elif clave == 'KernelExplainer':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de KernelExplainer (SHAP caja negra) para:\\n\"\n",
        "                        \"1. Explicar la importancia global segn los valores medios absolutos Kernel SHAP.\\n\"\n",
        "                        \"2. Analizar los valores locales para las primeras muestras: cmo cada caracterstica empuja la prediccin en cada caso.\\n\"\n",
        "                        \"3. Comparar con SHAP (si ya lo hiciste con TreeExplainer para otro modelo o Kernel SHAP para NN), si aplica.\\n\"\n",
        "                        \"4. Sugerir consideraciones sobre fondo (background) usado y posibles ajustes si las explicaciones muestran comportamiento inesperado.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Integrated Gradients':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Integrated Gradients para:\\n\"\n",
        "                        \"1. Explicar la contribucin integrada de cada variable: interpretacin de importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: qu implicaciones tiene un valor IG alto o bajo en cada variable.\\n\"\n",
        "                        \"3. Sealar limitaciones: compatibilidad con NN no diferenciable; si estos valores provienen de un modelo aproximado, comentar fiabilidad.\\n\"\n",
        "                        \"4. Sugerir pasos adicionales o comparaciones con otros mtodos (SHAP/LIME) para validar interpretaciones.\\n\"\n",
        "                    )\n",
        "                elif clave == 'DeepLIFT / LRP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de DeepLIFT / LRP para:\\n\"\n",
        "                        \"1. Explicar la relevancia asignada a cada variable en importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: si una variable tiene relevancia positiva o negativa, qu significa para la prediccin.\\n\"\n",
        "                        \"3. Comparar con IG o SHAP si se dispone: consistencia de atribuciones.\\n\"\n",
        "                        \"4. Recomendar verificaciones o transformaciones en caso de interpretaciones inesperadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Permutation Feature Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Permutation Feature Importance para:\\n\"\n",
        "                        \"1. Explicar la cada en la mtrica al permutar cada variable: por qu ciertas variables son crticas.\\n\"\n",
        "                        \"2. Comentar la desviacin estndar: indica inestabilidad en la importancia? Dnde conviene reforzar validaciones?\\n\"\n",
        "                        \"3. Comparar con importancias de SHAP/LIME: similitudes o diferencias.\\n\"\n",
        "                        \"4. Sugerir prioridades para ajuste de modelo o seleccin de variables basadas en esta mtrica.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Partial Dependence Plots (PDP)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos PDP para:\\n\"\n",
        "                        \"1. Explicar el efecto promedio de cada variable sobre la prediccin segn el rango PDP obtenido.\\n\"\n",
        "                        \"2. Sealar si los rangos sugieren relaciones montonas o no lineales.\\n\"\n",
        "                        \"3. Advertir sobre correlaciones fuertes que puedan afectar la interpretacin.\\n\"\n",
        "                        \"4. Recomendar posibles exploraciones adicionales (PDP bivariados, transformaciones) segn los resultados.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Accumulated Local Effects (ALE)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ALE y valores locales ALE para:\\n\"\n",
        "                        \"1. Explicar cmo ALE corrige artefactos de correlacin y qu nos dicen los valores concretos.\\n\"\n",
        "                        \"2. Interpretar importancia global ALE: variables con mayor efecto acumulado.\\n\"\n",
        "                        \"3. Analizar heterogeneidad local a partir de valores ALE de primeras muestras.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales (ALE 2D, inspeccin de distribucin) segn hallazgos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Individual Conditional Expectation (ICE) Plots':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ICE medios para:\\n\"\n",
        "                        \"1. Explicar qu mide el rango ICE y su diferencia respecto a PDP.\\n\"\n",
        "                        \"2. Analizar heterogeneidad: variables con alta variabilidad en rangos ICE indican interacciones o comportamiento inestable.\\n\"\n",
        "                        \"3. Sealar implicaciones para el modelo y posibles ajustes si hay alto efecto local variable.\\n\"\n",
        "                        \"4. Recomendar exploraciones adicionales para entender la variabilidad local.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Counterfactual Explanations':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Contrafactuales para:\\n\"\n",
        "                        \"1. Explicar cmo interpretar los contrafactuales: cambios en variables que generan aumento en prediccin.\\n\"\n",
        "                        \"2. Analizar variables con mayor || medio: implicaciones sobre sensibilidad del modelo.\\n\"\n",
        "                        \"3. Sealar si faltan contrafactuales para algunas muestras: qu puede indicar (limites del modelo o datos).\\n\"\n",
        "                        \"4. Sugerir cmo usar estos insights para ajuste de modelo o recoleccin de datos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Anchors':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Anchors para:\\n\"\n",
        "                        \"1. Explicar cmo leer las reglas ancla de las primeras muestras: condiciones que aseguran la prediccin.\\n\"\n",
        "                        \"2. Analizar frecuencia global de aparicin de variables en reglas: implicaciones sobre estabilidad y sesgos.\\n\"\n",
        "                        \"3. Sealar regiones de bajo coverage o baja precisin: dnde el modelo es menos fiable.\\n\"\n",
        "                        \"4. Recomendar acciones: recoleccin de datos, refinamiento de variables o validaciones dirigidas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Surrogate Models (Global/Local)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Surrogate Models para:\\n\"\n",
        "                        \"1. Explicar la importancia global del surrogate (rbol) y la importancia local media (coeficientes regresiones locales).\\n\"\n",
        "                        \"2. Comparar global vs local: variables con alta importancia local pero baja global, o viceversa, y qu sugiere.\\n\"\n",
        "                        \"3. Concluir sobre consistencia de comportamiento del NN en diferentes regiones del espacio.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales o ajustes de modelo segn discrepancias detectadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Explainable Boosting Machine (EBM)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de EBM para:\\n\"\n",
        "                        \"1. Explicar la importancia global segn EBM: cmo se comparan con otros mtodos.\\n\"\n",
        "                        \"2. Analizar contribuciones locales de las primeras muestras: qu patrones se observan.\\n\"\n",
        "                        \"3. Sealar si EBM revela interacciones no consideradas en NN.\\n\"\n",
        "                        \"4. Recomendar posibles ajustes en caractersticas o validaciones segn insights de EBM.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Optuna Hyperparameter Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Optuna para:\\n\"\n",
        "                        \"1. Explicar la importancia global de hiperparmetros en la optimizacin del NN.\\n\"\n",
        "                        \"2. Analizar top trials si estn disponibles: qu combinaciones de hiperparmetros funcionaron mejor.\\n\"\n",
        "                        \"3. Sealar limitaciones de la muestra de trials (nmero de pruebas) y posibles riesgos de sobreajuste en la bsqueda.\\n\"\n",
        "                        \"4. Recomendar prximas acciones para tuning basadas en estas importancias.\\n\"\n",
        "                    )\n",
        "                else:\n",
        "                    prompt += \"\\nPor favor, explica estos resultados numricos y qu implicaciones tienen para el modelo NN.\\n\"\n",
        "\n",
        "                # --------------- Llamada a OpenAI ---------------\n",
        "                print(f\"[DEBUG] 7.21. Llamando a OpenAI para {titulo}\")\n",
        "                explicacion = call_openai_explanation(prompt)\n",
        "                self.sections.append((f\"### {titulo}: Explicacin Generativa\", explicacion))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"###  Error en seccin xIA NN\",\n",
        "                f\"Se produjo un error al generar la seccin xIA de NN: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =====================================================================\n",
        "        # 8. Entrenamiento Modelo XGBoost\n",
        "        # =====================================================================\n",
        "        try:\n",
        "            if \"RESUMEN_METODOS\" in self.g and \"X_test\" in self.g and \"Y_test\" in self.g:\n",
        "                import os, pickle\n",
        "                import numpy as np\n",
        "                import pandas as _pd\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "                X_test_full = self.g[\"X_test\"]\n",
        "                Y_test = self.g[\"Y_test\"]\n",
        "                # Extraer array 1D de Y_test\n",
        "                if hasattr(Y_test, \"values\"):\n",
        "                    arr = Y_test.values\n",
        "                    if arr.ndim == 2 and arr.shape[1] >= 1:\n",
        "                        y_test_arr = arr[:, 0].ravel()\n",
        "                    else:\n",
        "                        y_test_arr = arr.ravel()\n",
        "                else:\n",
        "                    y_test_arr = np.array(Y_test).ravel()\n",
        "                y_test_arr = y_test_arr.ravel()\n",
        "\n",
        "                metrics_summary_xgb = []\n",
        "                # Iterar sobre cada mtodo\n",
        "                for metodo, vars_sel in self.g[\"RESUMEN_METODOS\"].items():\n",
        "                    metodo_low = metodo.lower()\n",
        "                    pickle_fname = f\"modelo_xgb_{metodo_low}.pkl\"\n",
        "                    if not os.path.exists(pickle_fname):\n",
        "                        print(f\"[DEBUG] Fichero de modelo XGBoost no encontrado para mtodo '{metodo}': {pickle_fname}, omito este mtodo\")\n",
        "                        continue\n",
        "                    # Cargar modelo y escaladores\n",
        "                    try:\n",
        "                        with open(pickle_fname, \"rb\") as f:\n",
        "                            data = pickle.load(f)\n",
        "                        model = data.get(\"model\", None)\n",
        "                        sx = data.get(\"sx\", None)\n",
        "                        sy = data.get(\"sy\", None)\n",
        "                        cols = data.get(\"cols\", None)\n",
        "                        if model is None or sx is None or sy is None or cols is None:\n",
        "                            print(f\"[DEBUG] Faltan claves en pickle XGBoost para mtodo '{metodo}', omito\")\n",
        "                            continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al cargar pickle XGBoost para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Verificar columnas\n",
        "                    missing = [c for c in cols if c not in X_test_full.columns]\n",
        "                    if missing:\n",
        "                        print(f\"[DEBUG] Para mtodo '{metodo}', faltan columnas en X_test: {missing}, omito\")\n",
        "                        continue\n",
        "                    # Subconjunto X_test\n",
        "                    X_test_sel = X_test_full[cols].copy()\n",
        "                    # Escalar, predecir, desescalar\n",
        "                    try:\n",
        "                        X_test_scaled = sx.transform(X_test_sel)\n",
        "                        y_pred_scaled = model.predict(X_test_scaled)\n",
        "                        # Desescalar: sy.inverse_transform espera 2D\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al predecir/desescalar XGBoost para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Clculo de mtricas y estadsticos\n",
        "                    try:\n",
        "                        # Rangos Y real y predicha\n",
        "                        y_real_min, y_real_max = float(np.min(y_test_arr)), float(np.max(y_test_arr))\n",
        "                        y_pred_min, y_pred_max = float(np.min(y_pred)), float(np.max(y_pred))\n",
        "                        # Residuos\n",
        "                        residuals = y_test_arr - y_pred\n",
        "                        res_mean = float(np.mean(residuals))\n",
        "                        res_std = float(np.std(residuals))\n",
        "                        # Estadsticos de residuos con pandas\n",
        "                        res_series = _pd.Series(residuals)\n",
        "                        res_skew = float(res_series.skew())\n",
        "                        res_kurt = float(res_series.kurtosis())\n",
        "                        q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]\n",
        "                        # Mtricas clsicas\n",
        "                        mse = mean_squared_error(y_test_arr, y_pred)\n",
        "                        rmse = float(np.sqrt(mse))\n",
        "                        mae = float(mean_absolute_error(y_test_arr, y_pred))\n",
        "                        r2 = float(r2_score(y_test_arr, y_pred))\n",
        "                        # Correlacin real vs predicha (si hay suficientes puntos)\n",
        "                        try:\n",
        "                            corr = float(np.corrcoef(y_test_arr, y_pred)[0,1]) if len(y_test_arr) > 1 else None\n",
        "                        except:\n",
        "                            corr = None\n",
        "                        # Acumular resumen\n",
        "                        metrics_summary_xgb.append({\n",
        "                            \"metodo\": metodo,\n",
        "                            \"rmse\": rmse,\n",
        "                            \"mae\": mae,\n",
        "                            \"r2\": r2\n",
        "                        })\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al calcular mtricas XGBoost para mtodo '{metodo}': {e}\")\n",
        "                        # si algo falla, saltar a next\n",
        "                        continue\n",
        "\n",
        "                    # 1) Parmetros de entrenamiento obtenidos desde el modelo XGBRegressor\n",
        "                    try:\n",
        "                        # get_params suele incluir: 'n_estimators', 'learning_rate', 'max_depth', 'subsample', etc.\n",
        "                        params_all = model.get_params()\n",
        "                        # Extraer los principales:\n",
        "                        params = {\n",
        "                            \"n_estimators\": params_all.get(\"n_estimators\", None),\n",
        "                            \"learning_rate\": params_all.get(\"learning_rate\", None),\n",
        "                            \"max_depth\": params_all.get(\"max_depth\", None),\n",
        "                            \"subsample\": params_all.get(\"subsample\", None)\n",
        "                        }\n",
        "                        df_params = _pd.DataFrame({\n",
        "                            \"Hiperparmetro\": list(params.keys()),\n",
        "                            \"Valor\": [str(v) for v in params.values()]\n",
        "                        })\n",
        "                        titulo_p = f\"### Parmetros de Entrenamiento XGBoost ({metodo})\"\n",
        "                        self.sections.append((titulo_p, df_params))\n",
        "                        print(f\"[DEBUG] 8.1. Seccin parmetros XGBoost aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al extraer parmetros XGBoost para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA de los hiperparmetros XGBoost\n",
        "                    try:\n",
        "                        prompt_params = (\n",
        "                            f\"Has entrenado un modelo XGBoost con seleccin de variables '{metodo}'.\\n\"\n",
        "                            \"Estos fueron los hiperparmetros utilizados:\\n\"\n",
        "                        )\n",
        "                        for k, v in params.items():\n",
        "                            prompt_params += f\"- {k}: {v}\\n\"\n",
        "                        prompt_params += (\n",
        "                            \"\\nPor favor, explica de forma profesional y detallada cmo estos hiperparmetros \"\n",
        "                            \"pueden influir en el entrenamiento del modelo XGBoost, su impacto en ajuste, \"\n",
        "                            \"regularizacin, sobreajuste o subajuste, y buenas prcticas para afinarlos.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 8.2. Iniciando llamada a OpenAI para explicacin hiperparmetros XGBoost ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en entrenamiento de modelos XGBoost para regresin.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_params}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_params = resp.choices[0].message.content.strip()\n",
        "                        if explanation_params:\n",
        "                            titulo_exp_p = f\"###  Explicacin IA Hiperparmetros XGBoost ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_p, explanation_params))\n",
        "                            print(f\"[DEBUG] 8.3. Seccin explicacin IA hiperparmetros XGBoost aadida para mtodo: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibi IA para hiperparmetros XGBoost ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA hiperparmetros XGBoost para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 2) Grfica Predicho vs Real\n",
        "                    try:\n",
        "                        fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "                        ax1.scatter(y_test_arr, y_pred, alpha=0.6)\n",
        "                        ax1.plot([y_real_min, y_real_max], [y_real_min, y_real_max], 'r--', lw=2)\n",
        "                        ax1.set_xlabel(\"Y real\")\n",
        "                        ax1.set_ylabel(\"Y predicho\")\n",
        "                        ax1.set_title(f\"XGBoost Predicho vs Real ({metodo})\")\n",
        "                        titulo_fig1 = f\"### Grfico XGBoost Predicho vs Real ({metodo})\"\n",
        "                        self.sections.append((titulo_fig1, fig1))\n",
        "                        print(f\"[DEBUG] 8.4. Seccin grfica Pred vs Real XGBoost aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear grfica Pred vs Real XGBoost para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA Pred vs Real XGBoost con contexto numrico\n",
        "                    try:\n",
        "                        prompt_pr = (\n",
        "                            f\"A continuacin tienes datos de la grfica de comparacin Real vs Prediccin para el modelo XGBoost con mtodo '{metodo}':\\n\"\n",
        "                            f\"- RMSE: {rmse:.4f}\\n\"\n",
        "                            f\"- MAE: {mae:.4f}\\n\"\n",
        "                            f\"- R: {r2:.4f}\\n\"\n",
        "                            f\"- Correlacin entre Y real y predicha: {corr:.4f}\\n\"\n",
        "                            f\"- Rango Y real: [{y_real_min:.4f}, {y_real_max:.4f}]\\n\"\n",
        "                            f\"- Rango Y predicha: [{y_pred_min:.4f}, {y_pred_max:.4f}]\\n\\n\"\n",
        "                            \"Basndote en estos valores y en la grfica generada (Real vs Prediccin), \"\n",
        "                            \"proporciona un anlisis detallado, sealando si hay sesgos sistemticos, dispersin en ciertos rangos, \"\n",
        "                            \"y qu implicaciones tiene para la calidad del modelo. \"\n",
        "                            \"Usa un texto profesional y bien estructurado, citando qu indicios de la grfica respaldan tus conclusiones.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 8.5. Llamada IA Pred vs Real XGBoost ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_pr}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_pr = resp.choices[0].message.content.strip()\n",
        "                        if explanation_pr:\n",
        "                            titulo_exp_pr = f\"###  Explicacin IA Predicho vs Real XGBoost ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_pr, explanation_pr))\n",
        "                            print(f\"[DEBUG] 8.6. Seccin explicacin IA Pred vs Real XGBoost aadida para mtodo: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibi IA Pred vs Real XGBoost ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA Pred vs Real XGBoost para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 3) Grfica de residuos XGBoost\n",
        "                    try:\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "                        ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "                        ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "                        ax2.set_xlabel(\"Y predicho\")\n",
        "                        ax2.set_ylabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "                        ax2.set_title(f\"XGBoost Residuos ({metodo})\")\n",
        "                        titulo_fig2 = f\"### Grfica XGBoost Residuos ({metodo})\"\n",
        "                        self.sections.append((titulo_fig2, fig2))\n",
        "                        print(f\"[DEBUG] 8.7. Seccin grfica residuos XGBoost aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear grfica residuos XGBoost para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA Residuos XGBoost con contexto numrico\n",
        "                    try:\n",
        "                        prompt_res = (\n",
        "                            f\"A continuacin tienes estadsticas de los residuos (Real - Predicha) del modelo XGBoost con mtodo '{metodo}':\\n\"\n",
        "                            f\"- Media: {res_mean:.4f}\\n\"\n",
        "                            f\"- Desviacin estndar: {res_std:.4f}\\n\"\n",
        "                            f\"- Asimetra: {res_skew:.4f}\\n\"\n",
        "                            f\"- Curtosis: {res_kurt:.4f}\\n\"\n",
        "                            f\"- Cuantiles: 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}\\n\\n\"\n",
        "                            \"Basndote en estos valores y en la grfica de residuos, analiza si hay patrones (heterocedasticidad, outliers, sesgos), \"\n",
        "                            \"y comenta qu implicaciones tiene para la robustez y generalizacin del modelo. \"\n",
        "                            \"Usa un texto profesional y bien estructurado.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 8.8. Llamada IA Residuos XGBoost ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_res}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_res = resp.choices[0].message.content.strip()\n",
        "                        if explanation_res:\n",
        "                            titulo_exp_res = f\"###  Explicacin IA Residuos XGBoost ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_res, explanation_res))\n",
        "                            print(f\"[DEBUG] 8.9. Seccin explicacin IA residuos XGBoost aadida para mtodo: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibi IA Residuos XGBoost ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA residuos XGBoost para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 4) Tabla Mtricas XGBoost y explicacin IA\n",
        "                    try:\n",
        "                        df_met = _pd.DataFrame([\n",
        "                            {\"Mtrica\": \"RMSE\", \"Valor\": rmse},\n",
        "                            {\"Mtrica\": \"MAE\", \"Valor\": mae},\n",
        "                            {\"Mtrica\": \"R2\",  \"Valor\": r2}\n",
        "                        ])\n",
        "                        titulo_met = f\"### Mtricas XGBoost ({metodo})\"\n",
        "                        self.sections.append((titulo_met, df_met))\n",
        "                        print(f\"[DEBUG] 8.10. Seccin mtricas XGBoost aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame mtricas XGBoost para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_met = (\n",
        "                            f\"Estas son las mtricas del modelo XGBoost con mtodo '{metodo}':\\n\"\n",
        "                            f\"- R2: {r2:.4f}\\n\"\n",
        "                            f\"- MSE: {mse:.4f}\\n\"\n",
        "                            f\"- RMSE: {rmse:.4f}\\n\"\n",
        "                            f\"- MAE: {mae:.4f}\\n\"\n",
        "                            f\"- Correlacin Real vs Predicha: {corr:.4f}\\n\\n\"\n",
        "                            \"Analiza estos valores en contexto: son adecuados? qu sugieren respecto al rendimiento del modelo? \"\n",
        "                            \"Menciona referencias a la grfica Real vs Prediccin y a los residuos si procede.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 8.11. Llamada IA Mtricas XGBoost ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_met}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_met = resp.choices[0].message.content.strip()\n",
        "                        if explanation_met:\n",
        "                            titulo_exp_met = f\"###  Explicacin IA Mtricas XGBoost ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_met, explanation_met))\n",
        "                            print(f\"[DEBUG] 8.12. Seccin explicacin IA mtricas XGBoost aadida para mtodo: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibi IA Mtricas XGBoost ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA mtricas XGBoost para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                # 5) Comparativa global de mtricas XGBoost\n",
        "                if metrics_summary_xgb:\n",
        "                    try:\n",
        "                        df_comp_xgb = _pd.DataFrame(metrics_summary_xgb)\n",
        "                        df_comp_xgb_sorted = df_comp_xgb.sort_values(\"rmse\")\n",
        "                        titulo_comp = \"### Comparativa Mtricas XGBoost entre Mtodos\"\n",
        "                        self.sections.append((titulo_comp, df_comp_xgb_sorted))\n",
        "                        print(\"[DEBUG] 8.13. Seccin comparativa mtricas XGBoost aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame comparativo mtricas XGBoost: {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_conc = (\n",
        "                            \"Se han entrenado varios modelos XGBoost con diferentes mtodos de seleccin de variables.\\n\"\n",
        "                            \"Mtricas obtenidas en test:\\n\"\n",
        "                        )\n",
        "                        for entry in metrics_summary_xgb:\n",
        "                            prompt_conc += f\"- Mtodo '{entry['metodo']}': RMSE={entry['rmse']}, MAE={entry['mae']}, R2={entry['r2']}\\n\"\n",
        "                        prompt_conc += (\n",
        "                            \"\\nPor favor, proporciona conclusiones profesionales comparando estos mtodos: \"\n",
        "                            \"indica cul se comporta mejor, posibles razones y recomendaciones sobre seleccin de variables o ajustes para mejorar XGBoost.\"\n",
        "                        )\n",
        "                        print(\"[DEBUG] 8.14. Llamada IA Conclusiones XGBoost...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_conc}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_conc = resp.choices[0].message.content.strip()\n",
        "                        if explanation_conc:\n",
        "                            titulo_exp_conc = \"###  Conclusiones IA Entrenamiento XGBoost\"\n",
        "                            self.sections.append((titulo_exp_conc, explanation_conc))\n",
        "                            print(\"[DEBUG] 8.15. Seccin explicacin IA conclusiones XGBoost aadida\")\n",
        "                        else:\n",
        "                            print(\"[DEBUG] No se recibi IA Conclusiones XGBoost\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA conclusiones XGBoost: {e}\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No estn RESUMEN_METODOS o X_test/Y_test en globals(), omito seccin XGBoost\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar seccin XGBoost en informe: {e}\")\n",
        "        # ... fin de la seccin de entrenamiento XGBoost ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 8.1. Interpretacin xIA para modelo entrenado XGBoost\n",
        "        # =============================================================\n",
        "        import openai\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # Verificacin previa\n",
        "        # ----------------------------------------------------------------\n",
        "        try:\n",
        "            print(\"[DEBUG] 8.16. Iniciando seccin xIA para XGBoost\")\n",
        "            if 'xai_results' not in globals() or 'XGBoost' not in xai_results:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontr `xai_results['XGBoost']`. \"\n",
        "                    \"Asegrate de haber ejecutado la Celda 10 y almacenado los resultados xIA de XGBoost en `xai_results['XGBoost']`.\"\n",
        "                )\n",
        "\n",
        "                # Cabecera\n",
        "                self.sections.append((\n",
        "                    \"##  Anlisis xIA de XGBoost: Resultados concretos y explicaciones Generativas\",\n",
        "                    \"\"  # contenido vaco, la cabecera se mostrar como Markdown\n",
        "                ))\n",
        "\n",
        "\n",
        "            # Funcin para llamar a OpenAI con un prompt especfico\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                \"\"\"\n",
        "                Llama a OpenAI ChatCompletion con un sistema experto en ML/XAI,\n",
        "                devuelve la respuesta de la IA en texto.\n",
        "                \"\"\"\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un asistente experto en Machine Learning e interpretabilidad de modelos. \"\n",
        "                                \"Proporciona explicaciones detalladas y basadas en los datos concretos proporcionados.\"\n",
        "                            )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    texto = response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    texto = f\"[Error llamando a OpenAI: {e}]\"\n",
        "                return texto\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Configuracin: top N variables y primeras M muestras locales\n",
        "            # ----------------------------------------------------------------\n",
        "            TOP_N = 5      # cuntas caractersticas top incluir en el prompt\n",
        "            N_LOCAL = 3    # cuntas muestras locales incluir en prompt\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Lista de mtodos xIA y claves en xai_results['XGBoost']\n",
        "            # ----------------------------------------------------------------\n",
        "            # Las claves deben coincidir exactamente con las usadas en Celda 10 al almacenar resultados.\n",
        "            metodos_claves = [\n",
        "                ('SHAP', 'SHAP'),\n",
        "                ('LIME', 'LIME'),\n",
        "                ('KernelExplainer', 'KernelExplainer'),\n",
        "                ('Integrated Gradients', 'Integrated Gradients'),\n",
        "                ('DeepLIFT / LRP', 'DeepLIFT / LRP'),\n",
        "                ('Permutation Feature Importance', 'Permutation Feature Importance'),\n",
        "                ('Partial Dependence Plots (PDP)', 'Partial Dependence Plots (PDP)'),\n",
        "                ('Accumulated Local Effects (ALE)', 'Accumulated Local Effects (ALE)'),\n",
        "                ('Individual Conditional Expectation (ICE) Plots', 'Individual Conditional Expectation (ICE) Plots'),\n",
        "                ('Counterfactual Explanations', 'Counterfactual Explanations'),\n",
        "                ('Anchors', 'Anchors'),\n",
        "                ('Surrogate Models (Global/Local)', 'Surrogate Models (Global/Local)'),\n",
        "                ('Explainable Boosting Machine (EBM)', 'Explainable Boosting Machine (EBM)'),\n",
        "                ('Optuna Hyperparameter Importance', 'Optuna Hyperparameter Importance'),\n",
        "            ]\n",
        "\n",
        "            for titulo, clave in metodos_claves:\n",
        "                print(f\"[DEBUG] 8.17. Procesando seccin xIA: {titulo}\")\n",
        "                datos = xai_results['XGBoost'].get(clave)\n",
        "                if datos is None:\n",
        "                    print(f\"[DEBUG] No hay resultados xIA para {titulo}, se omite\")\n",
        "                    continue\n",
        "#                    self.sections.append((\n",
        "#                        f\"###  No hay resultados para {titulo}\",\n",
        "#                        f\"No se hallaron resultados para la clave '{clave}'.\"\n",
        "#                    ))\n",
        "#                    continue\n",
        "\n",
        "                # ---------------- Mostrar figura guardada ----------------\n",
        "                print(f\"[DEBUG] 8.18. Mostrando figura para {titulo}\")\n",
        "                fig = datos.get('fig_summary') or datos.get('fig')\n",
        "                if fig is not None:\n",
        "                    self.sections.append((f\"### {titulo}: Grfico\", fig))\n",
        "\n",
        "                # --------------- Mostrar DataFrames de importancia y local ---------------\n",
        "                print(f\"[DEBUG] 8.19. Mostrando DataFrames para {titulo}\")\n",
        "                imp_df = datos.get('imp_df')\n",
        "                df_local = datos.get('df_local')\n",
        "                stats_extra = datos.get('stats', None)  # opcional: estadsticas adicionales, p.ej. percentiles, pos_pct SHAP, etc.\n",
        "\n",
        "                if isinstance(imp_df, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Importancia global\", imp_df.reset_index(drop=True)))\n",
        "\n",
        "                if isinstance(df_local, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Valores locales\", df_local.head(N_LOCAL)))\n",
        "\n",
        "                # --------------------------------- Extraer datos del grfico (suponiendo que sea un barh con 'feature' y 'importance') ---------------------------\n",
        "                features = [text.get_text() for text in ax.get_yticklabels()]\n",
        "                importances = [bar.get_width() for bar in ax.patches]\n",
        "\n",
        "                # --------------- Construir prompt con valores numricos concretos ---------------\n",
        "                print(f\"[DEBUG] 8.20. Construyendo prompt para {titulo}\")\n",
        "                prompt = f\"He aplicado el mtodo xIA '{titulo}' al modelo XGBoost entrenado y he obtenido estos resultados concretos:\\n\\n\"\n",
        "\n",
        "                prompt += \"- Datos del grfico (feature vs importancia):\\n\"\n",
        "                for f, imp in zip(features, importances):\n",
        "                    prompt += f\"     {f}: {imp:.4f}\\n\"\n",
        "\n",
        "                # Ahora s le pides que interprete el grfico:\n",
        "                prompt += (\n",
        "                    \"- Interpreta el grfico anterior: \"\n",
        "                    \"describe qu patrones o relaciones visuales revela cmo se distribuye la importancia.\\n\"\n",
        "                )\n",
        "\n",
        "                # 1) Extraer importancia global: top N\n",
        "                if isinstance(imp_df, pd.DataFrame) and not imp_df.empty:\n",
        "                    imp_df = imp_df.reset_index()\n",
        "                    imp_df = imp_df.rename(columns={ imp_df.columns[0]: \"feature\" })\n",
        "                    cols = imp_df.columns.tolist()\n",
        "                    if len(cols) >= 2:\n",
        "                        feat_col = \"feature\"\n",
        "                        val_col  = cols[1]\n",
        "                        try:\n",
        "                            imp_df_sorted = imp_df.sort_values(val_col, ascending=False)\n",
        "                        except Exception:\n",
        "                            imp_df_sorted = imp_df\n",
        "\n",
        "                        top_n      = min(TOP_N, len(imp_df_sorted))\n",
        "                        top_imp_df = imp_df_sorted.iloc[:top_n][[feat_col, val_col]]\n",
        "                        top_imp_list = []\n",
        "                        for _, row in top_imp_df.iterrows():\n",
        "                            raw = row[val_col]\n",
        "                            try:\n",
        "                                v = float(raw)\n",
        "                            except Exception:\n",
        "                                v = raw  # si no se puede convertir, lo dejo tal cual\n",
        "                            top_imp_list.append({feat_col: row[feat_col], val_col: v})\n",
        "\n",
        "                        prompt += f\"- Top {top_n} caractersticas por importancia global ({feat_col}, {val_col}):\\n  {top_imp_list}\\n\"\n",
        "\n",
        "                        extra_cols = cols[2:]\n",
        "                        if extra_cols:\n",
        "                            prompt += f\"  (El DataFrame de importancia global incluye tambin columnas: {extra_cols}.)\\n\"\n",
        "                    else:\n",
        "                        snippet = imp_df.head(TOP_N).to_dict(orient='records')\n",
        "                        prompt += f\"- Importancia global (primeras filas, formato genrico):\\n  {snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de importancia global.\\n\"\n",
        "\n",
        "#                    #  Siempre sacamos el ndice como columna llamada \"feature\"\n",
        "                    imp_df = imp_df.rename(columns={ idx_col: \"feature\" })\n",
        "\n",
        "                # 2) Extraer valores locales: primeras M muestras\n",
        "                if isinstance(df_local, pd.DataFrame) and not df_local.empty:\n",
        "                    n_loc = min(N_LOCAL, len(df_local))\n",
        "                    loc_snippet = df_local.head(n_loc).to_dict(orient='records')\n",
        "                    prompt += f\"- Valores locales para las primeras {n_loc} muestras (cada dict mapea feature a valor/atribucin):\\n  {loc_snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de valores locales.\\n\"\n",
        "\n",
        "                # 3) Incluir estadsticas extra si existen\n",
        "                if isinstance(stats_extra, dict):\n",
        "                    prompt += \"- Estadsticas adicionales:\\n\"\n",
        "                    for k, v in stats_extra.items():\n",
        "                        prompt += f\"  * {k}: {v}\\n\"\n",
        "\n",
        "                # 4) Contexto general del modelo XGBoost\n",
        "                prompt += (\n",
        "                    \"\\nContexto: El modelo XGBoost fue entrenado con variables seleccionadas y StandardScaler, \"\n",
        "                    \"con predicciones desescaladas. Ahora interpretamos los resultados xIA para este XGBoost.\\n\"\n",
        "                )\n",
        "\n",
        "                # 5) Preguntas/pautas especficas segn el mtodo\n",
        "                if clave == 'SHAP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores concretos de SHAP para:\\n\"\n",
        "                        \"1. Explicar la importancia global de cada variable en base a los valores de SHAP mostrados.\\n\"\n",
        "                        \"2. Analizar los valores SHAP de las primeras muestras: si una variable muestra SHAP positivo alto, qu implica sobre la prediccin en ese caso? Y si es negativo, qu implica?\\n\"\n",
        "                        \"3. Identificar patrones en SHAP (por ejemplo, variables que consistentemente empujan en una direccin) y cmo afecta al comportamiento general del XGBoost.\\n\"\n",
        "                        \"4. Sugerir posibles transformaciones de variables o validaciones adicionales basadas en estos resultados SHAP.\\n\"\n",
        "                    )\n",
        "                elif clave == 'LIME':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos pesos LIME concretos para:\\n\"\n",
        "                        \"1. Explicar el significado de pesos positivos y negativos en LIME para XGBoost.\\n\"\n",
        "                        \"2. Analizar casos de las primeras muestras: si una variable tiene peso LIME fuerte (positivo/negativo), qu implica para la prediccin local?\\n\"\n",
        "                        \"3. Comentar si la variabilidad de los pesos sugiere relaciones no lineales o interacciones no capturadas.\\n\"\n",
        "                        \"4. Recomendar acciones: agrupacin de variables, deteccin de outliers, etc., basadas en la interpretacin LIME.\\n\"\n",
        "                    )\n",
        "                elif clave == 'KernelExplainer':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de KernelExplainer (SHAP caja negra) para:\\n\"\n",
        "                        \"1. Explicar la importancia global segn los valores medios absolutos Kernel SHAP.\\n\"\n",
        "                        \"2. Analizar los valores locales para las primeras muestras: cmo cada caracterstica empuja la prediccin en cada caso.\\n\"\n",
        "                        \"3. Comparar con SHAP (si ya lo hiciste con TreeExplainer para otro modelo o Kernel SHAP para XGBoost), si aplica.\\n\"\n",
        "                        \"4. Sugerir consideraciones sobre fondo (background) usado y posibles ajustes si las explicaciones muestran comportamiento inesperado.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Integrated Gradients':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Integrated Gradients para:\\n\"\n",
        "                        \"1. Explicar la contribucin integrada de cada variable: interpretacin de importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: qu implicaciones tiene un valor IG alto o bajo en cada variable.\\n\"\n",
        "                        \"3. Sealar limitaciones: compatibilidad con XGBoost no diferenciable; si estos valores provienen de un modelo aproximado, comentar fiabilidad.\\n\"\n",
        "                        \"4. Sugerir pasos adicionales o comparaciones con otros mtodos (SHAP/LIME) para validar interpretaciones.\\n\"\n",
        "                    )\n",
        "                elif clave == 'DeepLIFT / LRP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de DeepLIFT / LRP para:\\n\"\n",
        "                        \"1. Explicar la relevancia asignada a cada variable en importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: si una variable tiene relevancia positiva o negativa, qu significa para la prediccin.\\n\"\n",
        "                        \"3. Comparar con IG o SHAP si se dispone: consistencia de atribuciones.\\n\"\n",
        "                        \"4. Recomendar verificaciones o transformaciones en caso de interpretaciones inesperadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Permutation Feature Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Permutation Feature Importance para:\\n\"\n",
        "                        \"1. Explicar la cada en la mtrica al permutar cada variable: por qu ciertas variables son crticas.\\n\"\n",
        "                        \"2. Comentar la desviacin estndar: indica inestabilidad en la importancia? Dnde conviene reforzar validaciones?\\n\"\n",
        "                        \"3. Comparar con importancias de SHAP/LIME: similitudes o diferencias.\\n\"\n",
        "                        \"4. Sugerir prioridades para ajuste de modelo o seleccin de variables basadas en esta mtrica.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Partial Dependence Plots (PDP)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos PDP para:\\n\"\n",
        "                        \"1. Explicar el efecto promedio de cada variable sobre la prediccin segn el rango PDP obtenido.\\n\"\n",
        "                        \"2. Sealar si los rangos sugieren relaciones montonas o no lineales.\\n\"\n",
        "                        \"3. Advertir sobre correlaciones fuertes que puedan afectar la interpretacin.\\n\"\n",
        "                        \"4. Recomendar posibles exploraciones adicionales (PDP bivariados, transformaciones) segn los resultados.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Accumulated Local Effects (ALE)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ALE y valores locales ALE para:\\n\"\n",
        "                        \"1. Explicar cmo ALE corrige artefactos de correlacin y qu nos dicen los valores concretos.\\n\"\n",
        "                        \"2. Interpretar importancia global ALE: variables con mayor efecto acumulado.\\n\"\n",
        "                        \"3. Analizar heterogeneidad local a partir de valores ALE de primeras muestras.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales (ALE 2D, inspeccin de distribucin) segn hallazgos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Individual Conditional Expectation (ICE) Plots':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ICE medios para:\\n\"\n",
        "                        \"1. Explicar qu mide el rango ICE y su diferencia respecto a PDP.\\n\"\n",
        "                        \"2. Analizar heterogeneidad: variables con alta variabilidad en rangos ICE indican interacciones o comportamiento inestable.\\n\"\n",
        "                        \"3. Sealar implicaciones para el modelo y posibles ajustes si hay alto efecto local variable.\\n\"\n",
        "                        \"4. Recomendar exploraciones adicionales para entender la variabilidad local.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Counterfactual Explanations':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Contrafactuales para:\\n\"\n",
        "                        \"1. Explicar cmo interpretar los contrafactuales: cambios en variables que generan aumento en prediccin.\\n\"\n",
        "                        \"2. Analizar variables con mayor || medio: implicaciones sobre sensibilidad del modelo.\\n\"\n",
        "                        \"3. Sealar si faltan contrafactuales para algunas muestras: qu puede indicar (limites del modelo o datos).\\n\"\n",
        "                        \"4. Sugerir cmo usar estos insights para ajuste de modelo o recoleccin de datos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Anchors':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Anchors para:\\n\"\n",
        "                        \"1. Explicar cmo leer las reglas ancla de las primeras muestras: condiciones que aseguran la prediccin.\\n\"\n",
        "                        \"2. Analizar frecuencia global de aparicin de variables en reglas: implicaciones sobre estabilidad y sesgos.\\n\"\n",
        "                        \"3. Sealar regiones de bajo coverage o baja precisin: dnde el modelo es menos fiable.\\n\"\n",
        "                        \"4. Recomendar acciones: recoleccin de datos, refinamiento de variables o validaciones dirigidas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Surrogate Models (Global/Local)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Surrogate Models para:\\n\"\n",
        "                        \"1. Explicar la importancia global del surrogate (rbol) y la importancia local media (coeficientes regresiones locales).\\n\"\n",
        "                        \"2. Comparar global vs local: variables con alta importancia local pero baja global, o viceversa, y qu sugiere.\\n\"\n",
        "                        \"3. Concluir sobre consistencia de comportamiento del XGBoost en diferentes regiones del espacio.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales o ajustes de modelo segn discrepancias detectadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Explainable Boosting Machine (EBM)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de EBM para:\\n\"\n",
        "                        \"1. Explicar la importancia global segn EBM: cmo se comparan con otros mtodos.\\n\"\n",
        "                        \"2. Analizar contribuciones locales de las primeras muestras: qu patrones se observan.\\n\"\n",
        "                        \"3. Sealar si EBM revela interacciones no consideradas en XGBoost.\\n\"\n",
        "                        \"4. Recomendar posibles ajustes en caractersticas o validaciones segn insights de EBM.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Optuna Hyperparameter Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Optuna para:\\n\"\n",
        "                        \"1. Explicar la importancia global de hiperparmetros en la optimizacin del XGBoost.\\n\"\n",
        "                        \"2. Analizar top trials si estn disponibles: qu combinaciones de hiperparmetros funcionaron mejor.\\n\"\n",
        "                        \"3. Sealar limitaciones de la muestra de trials (nmero de pruebas) y posibles riesgos de sobreajuste en la bsqueda.\\n\"\n",
        "                        \"4. Recomendar prximas acciones para tuning basadas en estas importancias.\\n\"\n",
        "                    )\n",
        "                else:\n",
        "                    prompt += \"\\nPor favor, explica estos resultados numricos y qu implicaciones tienen para el modelo XGBoost.\\n\"\n",
        "\n",
        "                # --------------- Llamada a OpenAI ---------------\n",
        "                print(f\"[DEBUG] 8.21. Llamando a OpenAI para {titulo}\")\n",
        "                explicacion = call_openai_explanation(prompt)\n",
        "                self.sections.append((f\"### {titulo}: Explicacin Generativa\", explicacion))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"###  Error en seccin xIA XGBoost\",\n",
        "                f\"Se produjo un error al generar la seccin xIA de XGBoost: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =====================================================================\n",
        "        # 9. Entrenamiento Modelo Random Forest\n",
        "        # =====================================================================\n",
        "        try:\n",
        "            # Comprobamos RESUMEN_METODOS y existencia de X_test/Y_test\n",
        "            if \"RESUMEN_METODOS\" in self.g and \"X_test\" in self.g and \"Y_test\" in self.g:\n",
        "                import os, pickle\n",
        "                import numpy as _np\n",
        "                import pandas as _pd\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "                X_test_full = self.g[\"X_test\"]\n",
        "                Y_test = self.g[\"Y_test\"]\n",
        "                # Extraer array 1D de y_test\n",
        "                if hasattr(Y_test, \"values\"):\n",
        "                    arr = Y_test.values\n",
        "                    if arr.ndim == 2 and arr.shape[1] >= 1:\n",
        "                        # tomamos la primera columna si hay ms\n",
        "                        y_test_arr = arr[:, 0]\n",
        "                    else:\n",
        "                        y_test_arr = arr.ravel()\n",
        "                else:\n",
        "                    y_test_arr = _np.array(Y_test).ravel()\n",
        "                y_test_arr = y_test_arr.ravel()\n",
        "\n",
        "                metrics_summary_rf = []\n",
        "                # Iteramos sobre cada mtodo en RESUMEN_METODOS\n",
        "                for metodo, vars_sel in self.g[\"RESUMEN_METODOS\"].items():\n",
        "                    metodo_low = metodo.lower()\n",
        "                    fname = f\"modelo_rf_{metodo_low}.pkl\"\n",
        "                    if not os.path.exists(fname):\n",
        "                        print(f\"[DEBUG] Fichero de modelo RF no encontrado para mtodo '{metodo}': {fname}, omito este mtodo\")\n",
        "                        continue\n",
        "                    # Cargar pickle\n",
        "                    try:\n",
        "                        with open(fname, \"rb\") as f:\n",
        "                            data = pickle.load(f)\n",
        "                        model = data.get(\"model\", None)\n",
        "                        sx = data.get(\"sx\", None)\n",
        "                        sy = data.get(\"sy\", None)\n",
        "                        cols = data.get(\"cols\", None)\n",
        "                        if model is None or sx is None or sy is None or cols is None:\n",
        "                            print(f\"[DEBUG] Falta alguna clave en pickle RF para mtodo '{metodo}', omito\")\n",
        "                            continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al cargar pickle RF para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Verificar columnas en X_test\n",
        "                    missing = [c for c in cols if c not in X_test_full.columns]\n",
        "                    if missing:\n",
        "                        print(f\"[DEBUG] Para mtodo '{metodo}', faltan columnas en X_test: {missing}, omito\")\n",
        "                        continue\n",
        "\n",
        "                    # Subconjunto X_test\n",
        "                    X_test_sel = X_test_full[cols].copy()\n",
        "                    # Escalar, predecir y desescalar\n",
        "                    try:\n",
        "                        X_test_scaled = sx.transform(X_test_sel)\n",
        "                        y_pred_scaled = model.predict(X_test_scaled)\n",
        "                        # inverse_transform espera 2D\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al predecir/desescalar RF para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Estadsticos y mtricas\n",
        "                    try:\n",
        "                        # Rangos Real vs Predicha\n",
        "                        y_real_min, y_real_max = float(_np.min(y_test_arr)), float(_np.max(y_test_arr))\n",
        "                        y_pred_min, y_pred_max = float(_np.min(y_pred)), float(_np.max(y_pred))\n",
        "                        # Residuos\n",
        "                        residuals = y_test_arr - y_pred\n",
        "                        res_mean = float(_np.mean(residuals))\n",
        "                        res_std  = float(_np.std(residuals))\n",
        "                        res_series = _pd.Series(residuals)\n",
        "                        res_skew = float(res_series.skew())\n",
        "                        res_kurt = float(res_series.kurtosis())\n",
        "                        q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]\n",
        "                        # Mtricas\n",
        "                        mse = mean_squared_error(y_test_arr, y_pred)\n",
        "                        rmse = float(_np.sqrt(mse))\n",
        "                        mae = float(mean_absolute_error(y_test_arr, y_pred))\n",
        "                        r2 = float(r2_score(y_test_arr, y_pred))\n",
        "                        # Correlacin Real vs Predicha\n",
        "                        try:\n",
        "                            corr = float(_np.corrcoef(y_test_arr, y_pred)[0, 1])\n",
        "                        except:\n",
        "                            corr = None\n",
        "                        # Guardar resumen para comparativa\n",
        "                        metrics_summary_rf.append({\n",
        "                            \"metodo\": metodo,\n",
        "                            \"rmse\": rmse,\n",
        "                            \"mae\": mae,\n",
        "                            \"r2\": r2\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al calcular mtricas RF para mtodo '{metodo}': {e}\")\n",
        "                        # saltamos, aunque idealmente definimos rmse,etc = None\n",
        "\n",
        "                    # 1) Parmetros de entrenamiento obtenidos desde el modelo\n",
        "                    try:\n",
        "                        # RandomForestRegressor atributos: n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, bootstrap\n",
        "                        params = {\n",
        "                            \"n_estimators\": getattr(model, \"n_estimators\", None),\n",
        "                            \"max_depth\": getattr(model, \"max_depth\", None),\n",
        "                            \"min_samples_split\": getattr(model, \"min_samples_split\", None),\n",
        "                            \"min_samples_leaf\": getattr(model, \"min_samples_leaf\", None),\n",
        "                            \"max_features\": getattr(model, \"max_features\", None),\n",
        "                            \"bootstrap\": getattr(model, \"bootstrap\", None)\n",
        "                        }\n",
        "                        df_params = _pd.DataFrame({\n",
        "                            \"Hiperparmetro\": list(params.keys()),\n",
        "                            \"Valor\": [str(v) for v in params.values()]\n",
        "                        })\n",
        "                        titulo_p = f\"### Parmetros de Entrenamiento Random Forest ({metodo})\"\n",
        "                        self.sections.append((titulo_p, df_params))\n",
        "                        print(f\"[DEBUG] 9.1. Seccin parmetros RF aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al extraer parmetros RF para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA de los hiperparmetros RF\n",
        "                    try:\n",
        "                        prompt_params = (\n",
        "                            f\"Has entrenado un modelo Random Forest con seleccin de variables '{metodo}'.\\n\"\n",
        "                            \"Estos fueron los hiperparmetros utilizados:\\n\"\n",
        "                        )\n",
        "                        for k, v in params.items():\n",
        "                            prompt_params += f\"- {k}: {v}\\n\"\n",
        "                        prompt_params += (\n",
        "                            \"\\nPor favor, explica de forma profesional y detallada cmo estos hiperparmetros \"\n",
        "                            \"pueden influir en el entrenamiento del Random Forest, su impacto en ajuste/sobreajuste, \"\n",
        "                            \"y buenas prcticas para seleccionarlos o afinarlos.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 9.2. Iniciando llamada a OpenAI para explicacin hiperparmetros RF ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en entrenamiento de Random Forest para regresin.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_params}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_params = resp.choices[0].message.content.strip()\n",
        "                        if explanation_params:\n",
        "                            titulo_exp_p = f\"###  Explicacin IA Hiperparmetros RF ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_p, explanation_params))\n",
        "                            print(f\"[DEBUG] 9.3. Seccin explicacin IA hiperparmetros RF aadida para mtodo: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibi IA para hiperparmetros RF ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA hiperparmetros RF para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 2) Grfica Predicho vs Real\n",
        "                    try:\n",
        "                        fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "                        ax1.scatter(y_test_arr, y_pred, alpha=0.6)\n",
        "                        ax1.plot([y_test_arr.min(), y_test_arr.max()],\n",
        "                                 [y_test_arr.min(), y_test_arr.max()],\n",
        "                                 'r--', lw=2)\n",
        "                        ax1.set_xlabel(\"Y real\")\n",
        "                        ax1.set_ylabel(\"Y predicho\")\n",
        "                        ax1.set_title(f\"Random Forest Predicho vs Real ({metodo})\")\n",
        "                        titulo_fig1 = f\"### Grfico RF Predicho vs Real ({metodo})\"\n",
        "                        self.sections.append((titulo_fig1, fig1))\n",
        "                        print(f\"[DEBUG] 9.4. Seccin grfica Pred vs Real RF aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear grfica Pred vs Real RF para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA Pred vs Real RF con contexto numrico\n",
        "                    try:\n",
        "                        prompt_pr = (\n",
        "                            f\"A continuacin tienes datos de la grfica de comparacin Real vs Prediccin para el Random Forest con mtodo '{metodo}':\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- R: {r2}\\n\"\n",
        "                            f\"- Correlacin entre Y real y predicha: {corr}\\n\"\n",
        "                            f\"- Rango Y real: [{y_real_min}, {y_real_max}]\\n\"\n",
        "                            f\"- Rango Y predicha: [{y_pred_min}, {y_pred_max}]\\n\\n\"\n",
        "                            \"Basndote en estos valores y en la grfica generada (Real vs Prediccin), \"\n",
        "                            \"proporciona un anlisis detallado: sesgos sistemticos, dispersin en rangos, posibles problemas y recomendaciones.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 9.5. Llamada IA Pred vs Real RF ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_pr}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_pr = resp.choices[0].message.content.strip()\n",
        "                        if explanation_pr:\n",
        "                            titulo_exp_pr = f\"###  Explicacin IA Predicho vs Real RF ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_pr, explanation_pr))\n",
        "                            print(f\"[DEBUG] 9.6. Seccin explicacin IA Pred vs Real RF aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA Pred vs Real RF para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 3) Grfica de residuos RF\n",
        "                    try:\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "                        ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "                        ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "                        ax2.set_xlabel(\"Y predicho\")\n",
        "                        ax2.set_ylabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "                        ax2.set_title(f\"Random Forest Residuos ({metodo})\")\n",
        "                        titulo_fig2 = f\"### Grfico RF Residuos ({metodo})\"\n",
        "                        self.sections.append((titulo_fig2, fig2))\n",
        "                        print(f\"[DEBUG] 9.7. Seccin grfica residuos RF aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear grfica residuos RF para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA Residuos RF con contexto numrico\n",
        "                    try:\n",
        "                        prompt_res = (\n",
        "                            f\"A continuacin tienes estadsticas de los residuos (Real - Predicha) del Random Forest con mtodo '{metodo}':\\n\"\n",
        "                            f\"- Media: {res_mean}\\n\"\n",
        "                            f\"- Desviacin estndar: {res_std}\\n\"\n",
        "                            f\"- Asimetra: {res_skew}\\n\"\n",
        "                            f\"- Curtosis: {res_kurt}\\n\"\n",
        "                            f\"- Cuantiles: 25%={q25}, 50%={q50}, 75%={q75}\\n\\n\"\n",
        "                            \"Basndote en estos valores y en la grfica de residuos, analiza patrones (heterocedasticidad, outliers, sesgos) y qu implicaciones tiene para generalizacin.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 9.8. Llamada IA Residuos RF ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_res}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_res = resp.choices[0].message.content.strip()\n",
        "                        if explanation_res:\n",
        "                            titulo_exp_res = f\"###  Explicacin IA Residuos RF ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_res, explanation_res))\n",
        "                            print(f\"[DEBUG] 9.9. Seccin explicacin IA residuos RF aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA residuos RF para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 4) Tabla Mtricas y explicacin IA\n",
        "                    try:\n",
        "                        df_met = _pd.DataFrame([\n",
        "                            {\"Mtrica\": \"RMSE\", \"Valor\": rmse},\n",
        "                            {\"Mtrica\": \"MAE\", \"Valor\": mae},\n",
        "                            {\"Mtrica\": \"R2\",  \"Valor\": r2}\n",
        "                        ])\n",
        "                        titulo_met = f\"### Mtricas Random Forest ({metodo})\"\n",
        "                        self.sections.append((titulo_met, df_met))\n",
        "                        print(f\"[DEBUG] 9.10 Seccin mtricas RF aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame mtricas RF para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_met = (\n",
        "                            f\"Estas son las mtricas del Random Forest con mtodo '{metodo}':\\n\"\n",
        "                            f\"- R2: {r2}\\n\"\n",
        "                            f\"- MSE: {mse}\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- Correlacin Real vs Predicha: {corr}\\n\\n\"\n",
        "                            \"Analiza estos valores en contexto: son adecuados? qu sugieren respecto al rendimiento? \"\n",
        "                            \"Menciona referencias a la grfica Real vs Prediccin y residuos si procede.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 9.11. Llamada IA Mtricas RF ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_met}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_met = resp.choices[0].message.content.strip()\n",
        "                        if explanation_met:\n",
        "                            titulo_exp_met = f\"###  Explicacin IA Mtricas RF ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_met, explanation_met))\n",
        "                            print(f\"[DEBUG] 9.12. Seccin explicacin IA mtricas RF aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA mtricas RF para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                # 5) Comparativa global de mtricas RF\n",
        "                if metrics_summary_rf:\n",
        "                    try:\n",
        "                        df_comp_rf = _pd.DataFrame(metrics_summary_rf)\n",
        "                        df_comp_rf_sorted = df_comp_rf.sort_values(\"rmse\")\n",
        "                        titulo_comp_rf = \"### Comparativa Mtricas Random Forest entre Mtodos\"\n",
        "                        self.sections.append((titulo_comp_rf, df_comp_rf_sorted))\n",
        "                        print(\"[DEBUG] 9.13. Seccin comparativa mtricas RF aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame comparativo mtricas RF: {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_conc = (\n",
        "                            \"Se han entrenado varios Random Forest con diferentes mtodos de seleccin de variables.\\n\"\n",
        "                            \"Mtricas obtenidas en test:\\n\"\n",
        "                        )\n",
        "                        for entry in metrics_summary_rf:\n",
        "                            prompt_conc += f\"- Mtodo '{entry['metodo']}': RMSE={entry['rmse']}, MAE={entry['mae']}, R2={entry['r2']}\\n\"\n",
        "                        prompt_conc += (\n",
        "                            \"\\nPor favor, proporciona conclusiones profesionales comparando estos mtodos: \"\n",
        "                            \"indica cul se comporta mejor, posibles razones y recomendaciones sobre seleccin de variables o ajustes para mejorar Random Forest.\"\n",
        "                        )\n",
        "                        print(\"[DEBUG] 9.14. Llamada IA Conclusiones RF...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_conc}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_conc = resp.choices[0].message.content.strip()\n",
        "                        if explanation_conc:\n",
        "                            titulo_exp_conc = \"###  Conclusiones IA Entrenamiento Random Forest\"\n",
        "                            self.sections.append((titulo_exp_conc, explanation_conc))\n",
        "                            print(\"[DEBUG] 9.15. Seccin explicacin IA conclusiones RF aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA conclusiones RF: {e}\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No estn RESUMEN_METODOS o X_test/Y_test en globals(), omito seccin RF\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar seccin RF en informe: {e}\")\n",
        "        # ... fin de la seccin de entrenamiento Random Forest ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 9.1. Interpretacin xIA para modelo entrenado Random Forest\n",
        "        # =============================================================\n",
        "        import openai\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # Verificacin previa\n",
        "        # ----------------------------------------------------------------\n",
        "        try:\n",
        "            print(\"[DEBUG] 9.16. Iniciando seccin xIA para Random Forest\")\n",
        "            if 'xai_results' not in globals() or 'Random Forest' not in xai_results:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontr `xai_results['Random Forest']`. \"\n",
        "                    \"Asegrate de haber ejecutado la Celda 10 y almacenado los resultados xIA de Random Forest en `xai_results['Random Forest']`.\"\n",
        "                )\n",
        "\n",
        "                # Cabecera\n",
        "                self.sections.append((\n",
        "                    \"##  Anlisis xIA de Random Forest: Resultados concretos y explicaciones Generativas\",\n",
        "                    \"\"  # contenido vaco, la cabecera se mostrar como Markdown\n",
        "                ))\n",
        "\n",
        "\n",
        "            # Funcin para llamar a OpenAI con un prompt especfico\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                \"\"\"\n",
        "                Llama a OpenAI ChatCompletion con un sistema experto en ML/XAI,\n",
        "                devuelve la respuesta de la IA en texto.\n",
        "                \"\"\"\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un asistente experto en Machine Learning e interpretabilidad de modelos. \"\n",
        "                                \"Proporciona explicaciones detalladas y basadas en los datos concretos proporcionados.\"\n",
        "                            )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    texto = response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    texto = f\"[Error llamando a OpenAI: {e}]\"\n",
        "                return texto\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Configuracin: top N variables y primeras M muestras locales\n",
        "            # ----------------------------------------------------------------\n",
        "            TOP_N = 5      # cuntas caractersticas top incluir en el prompt\n",
        "            N_LOCAL = 3    # cuntas muestras locales incluir en prompt\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Lista de mtodos xIA y claves en xai_results['Random Forest']\n",
        "            # ----------------------------------------------------------------\n",
        "            # Las claves deben coincidir exactamente con las usadas en Celda 10 al almacenar resultados.\n",
        "            metodos_claves = [\n",
        "                ('SHAP', 'SHAP'),\n",
        "                ('LIME', 'LIME'),\n",
        "                ('KernelExplainer', 'KernelExplainer'),\n",
        "                ('Integrated Gradients', 'Integrated Gradients'),\n",
        "                ('DeepLIFT / LRP', 'DeepLIFT / LRP'),\n",
        "                ('Permutation Feature Importance', 'Permutation Feature Importance'),\n",
        "                ('Partial Dependence Plots (PDP)', 'Partial Dependence Plots (PDP)'),\n",
        "                ('Accumulated Local Effects (ALE)', 'Accumulated Local Effects (ALE)'),\n",
        "                ('Individual Conditional Expectation (ICE) Plots', 'Individual Conditional Expectation (ICE) Plots'),\n",
        "                ('Counterfactual Explanations', 'Counterfactual Explanations'),\n",
        "                ('Anchors', 'Anchors'),\n",
        "                ('Surrogate Models (Global/Local)', 'Surrogate Models (Global/Local)'),\n",
        "                ('Explainable Boosting Machine (EBM)', 'Explainable Boosting Machine (EBM)'),\n",
        "                ('Optuna Hyperparameter Importance', 'Optuna Hyperparameter Importance'),\n",
        "            ]\n",
        "\n",
        "            for titulo, clave in metodos_claves:\n",
        "                print(f\"[DEBUG] 9.17. Procesando seccin xIA: {titulo}\")\n",
        "                datos = xai_results['Random Forest'].get(clave)\n",
        "                if datos is None:\n",
        "                    print(f\"[DEBUG] No hay resultados xIA para {titulo}, se omite\")\n",
        "                    continue\n",
        "#                    self.sections.append((\n",
        "#                        f\"###  No hay resultados para {titulo}\",\n",
        "#                        f\"No se hallaron resultados para la clave '{clave}'.\"\n",
        "#                    ))\n",
        "#                    continue\n",
        "\n",
        "                # ---------------- Mostrar figura guardada ----------------\n",
        "                print(f\"[DEBUG] 9.18. Mostrando figura para {titulo}\")\n",
        "                fig = datos.get('fig_summary') or datos.get('fig')\n",
        "                if fig is not None:\n",
        "                    self.sections.append((f\"### {titulo}: Grfico\", fig))\n",
        "\n",
        "                # --------------- Mostrar DataFrames de importancia y local ---------------\n",
        "                print(f\"[DEBUG] 9.19. Mostrando DataFrames para {titulo}\")\n",
        "                imp_df = datos.get('imp_df')\n",
        "                df_local = datos.get('df_local')\n",
        "                stats_extra = datos.get('stats', None)  # opcional: estadsticas adicionales, p.ej. percentiles, pos_pct SHAP, etc.\n",
        "\n",
        "                if isinstance(imp_df, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Importancia global\", imp_df.reset_index(drop=True)))\n",
        "\n",
        "                if isinstance(df_local, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Valores locales\", df_local.head(N_LOCAL)))\n",
        "\n",
        "                # --------------------------------- Extraer datos del grfico (suponiendo que sea un barh con 'feature' y 'importance') ---------------------------\n",
        "                features = [text.get_text() for text in ax.get_yticklabels()]\n",
        "                importances = [bar.get_width() for bar in ax.patches]\n",
        "\n",
        "                # --------------- Construir prompt con valores numricos concretos ---------------\n",
        "                print(f\"[DEBUG] 9.20. Construyendo prompt para {titulo}\")\n",
        "                prompt = f\"He aplicado el mtodo xIA '{titulo}' al modelo Random Forest entrenado y he obtenido estos resultados concretos:\\n\\n\"\n",
        "\n",
        "                prompt += \"- Datos del grfico (feature vs importancia):\\n\"\n",
        "                for f, imp in zip(features, importances):\n",
        "                    prompt += f\"     {f}: {imp:.4f}\\n\"\n",
        "\n",
        "                # Ahora s le pides que interprete el grfico:\n",
        "                prompt += (\n",
        "                    \"- Interpreta el grfico anterior: \"\n",
        "                    \"describe qu patrones o relaciones visuales revela cmo se distribuye la importancia.\\n\"\n",
        "                )\n",
        "\n",
        "                # 1) Extraer importancia global: top N\n",
        "                if isinstance(imp_df, pd.DataFrame) and not imp_df.empty:\n",
        "                    imp_df = imp_df.reset_index()\n",
        "                    imp_df = imp_df.rename(columns={ imp_df.columns[0]: \"feature\" })\n",
        "                    cols = imp_df.columns.tolist()\n",
        "                    if len(cols) >= 2:\n",
        "                        feat_col = \"feature\"\n",
        "                        val_col  = cols[1]\n",
        "                        try:\n",
        "                            imp_df_sorted = imp_df.sort_values(val_col, ascending=False)\n",
        "                        except Exception:\n",
        "                            imp_df_sorted = imp_df\n",
        "\n",
        "                        top_n      = min(TOP_N, len(imp_df_sorted))\n",
        "                        top_imp_df = imp_df_sorted.iloc[:top_n][[feat_col, val_col]]\n",
        "                        top_imp_list = []\n",
        "                        for _, row in top_imp_df.iterrows():\n",
        "                            raw = row[val_col]\n",
        "                            try:\n",
        "                                v = float(raw)\n",
        "                            except Exception:\n",
        "                                v = raw  # si no se puede convertir, lo dejo tal cual\n",
        "                            top_imp_list.append({feat_col: row[feat_col], val_col: v})\n",
        "\n",
        "                        prompt += f\"- Top {top_n} caractersticas por importancia global ({feat_col}, {val_col}):\\n  {top_imp_list}\\n\"\n",
        "\n",
        "                        extra_cols = cols[2:]\n",
        "                        if extra_cols:\n",
        "                            prompt += f\"  (El DataFrame de importancia global incluye tambin columnas: {extra_cols}.)\\n\"\n",
        "                    else:\n",
        "                        snippet = imp_df.head(TOP_N).to_dict(orient='records')\n",
        "                        prompt += f\"- Importancia global (primeras filas, formato genrico):\\n  {snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de importancia global.\\n\"\n",
        "\n",
        "#                    #  Siempre sacamos el ndice como columna llamada \"feature\"\n",
        "                    imp_df = imp_df.rename(columns={ idx_col: \"feature\" })\n",
        "\n",
        "                # 2) Extraer valores locales: primeras M muestras\n",
        "                if isinstance(df_local, pd.DataFrame) and not df_local.empty:\n",
        "                    n_loc = min(N_LOCAL, len(df_local))\n",
        "                    loc_snippet = df_local.head(n_loc).to_dict(orient='records')\n",
        "                    prompt += f\"- Valores locales para las primeras {n_loc} muestras (cada dict mapea feature a valor/atribucin):\\n  {loc_snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de valores locales.\\n\"\n",
        "\n",
        "                # 3) Incluir estadsticas extra si existen\n",
        "                if isinstance(stats_extra, dict):\n",
        "                    prompt += \"- Estadsticas adicionales:\\n\"\n",
        "                    for k, v in stats_extra.items():\n",
        "                        prompt += f\"  * {k}: {v}\\n\"\n",
        "\n",
        "                # 4) Contexto general del modelo Random Forest\n",
        "                prompt += (\n",
        "                    \"\\nContexto: El modelo Random Forest fue entrenado con variables seleccionadas y StandardScaler, \"\n",
        "                    \"con predicciones desescaladas. Ahora interpretamos los resultados xIA para este Random Forest.\\n\"\n",
        "                )\n",
        "\n",
        "                # 5) Preguntas/pautas especficas segn el mtodo\n",
        "                if clave == 'SHAP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores concretos de SHAP para:\\n\"\n",
        "                        \"1. Explicar la importancia global de cada variable en base a los valores de SHAP mostrados.\\n\"\n",
        "                        \"2. Analizar los valores SHAP de las primeras muestras: si una variable muestra SHAP positivo alto, qu implica sobre la prediccin en ese caso? Y si es negativo, qu implica?\\n\"\n",
        "                        \"3. Identificar patrones en SHAP (por ejemplo, variables que consistentemente empujan en una direccin) y cmo afecta al comportamiento general del Random Forest.\\n\"\n",
        "                        \"4. Sugerir posibles transformaciones de variables o validaciones adicionales basadas en estos resultados SHAP.\\n\"\n",
        "                    )\n",
        "                elif clave == 'LIME':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos pesos LIME concretos para:\\n\"\n",
        "                        \"1. Explicar el significado de pesos positivos y negativos en LIME para Random Forest.\\n\"\n",
        "                        \"2. Analizar casos de las primeras muestras: si una variable tiene peso LIME fuerte (positivo/negativo), qu implica para la prediccin local?\\n\"\n",
        "                        \"3. Comentar si la variabilidad de los pesos sugiere relaciones no lineales o interacciones no capturadas.\\n\"\n",
        "                        \"4. Recomendar acciones: agrupacin de variables, deteccin de outliers, etc., basadas en la interpretacin LIME.\\n\"\n",
        "                    )\n",
        "                elif clave == 'KernelExplainer':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de KernelExplainer (SHAP caja negra) para:\\n\"\n",
        "                        \"1. Explicar la importancia global segn los valores medios absolutos Kernel SHAP.\\n\"\n",
        "                        \"2. Analizar los valores locales para las primeras muestras: cmo cada caracterstica empuja la prediccin en cada caso.\\n\"\n",
        "                        \"3. Comparar con SHAP (si ya lo hiciste con TreeExplainer para otro modelo o Kernel SHAP para Random Forest), si aplica.\\n\"\n",
        "                        \"4. Sugerir consideraciones sobre fondo (background) usado y posibles ajustes si las explicaciones muestran comportamiento inesperado.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Integrated Gradients':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Integrated Gradients para:\\n\"\n",
        "                        \"1. Explicar la contribucin integrada de cada variable: interpretacin de importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: qu implicaciones tiene un valor IG alto o bajo en cada variable.\\n\"\n",
        "                        \"3. Sealar limitaciones: compatibilidad con Random Forest no diferenciable; si estos valores provienen de un modelo aproximado, comentar fiabilidad.\\n\"\n",
        "                        \"4. Sugerir pasos adicionales o comparaciones con otros mtodos (SHAP/LIME) para validar interpretaciones.\\n\"\n",
        "                    )\n",
        "                elif clave == 'DeepLIFT / LRP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de DeepLIFT / LRP para:\\n\"\n",
        "                        \"1. Explicar la relevancia asignada a cada variable en importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: si una variable tiene relevancia positiva o negativa, qu significa para la prediccin.\\n\"\n",
        "                        \"3. Comparar con IG o SHAP si se dispone: consistencia de atribuciones.\\n\"\n",
        "                        \"4. Recomendar verificaciones o transformaciones en caso de interpretaciones inesperadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Permutation Feature Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Permutation Feature Importance para:\\n\"\n",
        "                        \"1. Explicar la cada en la mtrica al permutar cada variable: por qu ciertas variables son crticas.\\n\"\n",
        "                        \"2. Comentar la desviacin estndar: indica inestabilidad en la importancia? Dnde conviene reforzar validaciones?\\n\"\n",
        "                        \"3. Comparar con importancias de SHAP/LIME: similitudes o diferencias.\\n\"\n",
        "                        \"4. Sugerir prioridades para ajuste de modelo o seleccin de variables basadas en esta mtrica.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Partial Dependence Plots (PDP)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos PDP para:\\n\"\n",
        "                        \"1. Explicar el efecto promedio de cada variable sobre la prediccin segn el rango PDP obtenido.\\n\"\n",
        "                        \"2. Sealar si los rangos sugieren relaciones montonas o no lineales.\\n\"\n",
        "                        \"3. Advertir sobre correlaciones fuertes que puedan afectar la interpretacin.\\n\"\n",
        "                        \"4. Recomendar posibles exploraciones adicionales (PDP bivariados, transformaciones) segn los resultados.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Accumulated Local Effects (ALE)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ALE y valores locales ALE para:\\n\"\n",
        "                        \"1. Explicar cmo ALE corrige artefactos de correlacin y qu nos dicen los valores concretos.\\n\"\n",
        "                        \"2. Interpretar importancia global ALE: variables con mayor efecto acumulado.\\n\"\n",
        "                        \"3. Analizar heterogeneidad local a partir de valores ALE de primeras muestras.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales (ALE 2D, inspeccin de distribucin) segn hallazgos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Individual Conditional Expectation (ICE) Plots':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ICE medios para:\\n\"\n",
        "                        \"1. Explicar qu mide el rango ICE y su diferencia respecto a PDP.\\n\"\n",
        "                        \"2. Analizar heterogeneidad: variables con alta variabilidad en rangos ICE indican interacciones o comportamiento inestable.\\n\"\n",
        "                        \"3. Sealar implicaciones para el modelo y posibles ajustes si hay alto efecto local variable.\\n\"\n",
        "                        \"4. Recomendar exploraciones adicionales para entender la variabilidad local.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Counterfactual Explanations':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Contrafactuales para:\\n\"\n",
        "                        \"1. Explicar cmo interpretar los contrafactuales: cambios en variables que generan aumento en prediccin.\\n\"\n",
        "                        \"2. Analizar variables con mayor || medio: implicaciones sobre sensibilidad del modelo.\\n\"\n",
        "                        \"3. Sealar si faltan contrafactuales para algunas muestras: qu puede indicar (limites del modelo o datos).\\n\"\n",
        "                        \"4. Sugerir cmo usar estos insights para ajuste de modelo o recoleccin de datos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Anchors':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Anchors para:\\n\"\n",
        "                        \"1. Explicar cmo leer las reglas ancla de las primeras muestras: condiciones que aseguran la prediccin.\\n\"\n",
        "                        \"2. Analizar frecuencia global de aparicin de variables en reglas: implicaciones sobre estabilidad y sesgos.\\n\"\n",
        "                        \"3. Sealar regiones de bajo coverage o baja precisin: dnde el modelo es menos fiable.\\n\"\n",
        "                        \"4. Recomendar acciones: recoleccin de datos, refinamiento de variables o validaciones dirigidas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Surrogate Models (Global/Local)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Surrogate Models para:\\n\"\n",
        "                        \"1. Explicar la importancia global del surrogate (rbol) y la importancia local media (coeficientes regresiones locales).\\n\"\n",
        "                        \"2. Comparar global vs local: variables con alta importancia local pero baja global, o viceversa, y qu sugiere.\\n\"\n",
        "                        \"3. Concluir sobre consistencia de comportamiento del Random Forest en diferentes regiones del espacio.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales o ajustes de modelo segn discrepancias detectadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Explainable Boosting Machine (EBM)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de EBM para:\\n\"\n",
        "                        \"1. Explicar la importancia global segn EBM: cmo se comparan con otros mtodos.\\n\"\n",
        "                        \"2. Analizar contribuciones locales de las primeras muestras: qu patrones se observan.\\n\"\n",
        "                        \"3. Sealar si EBM revela interacciones no consideradas en Random Forest.\\n\"\n",
        "                        \"4. Recomendar posibles ajustes en caractersticas o validaciones segn insights de EBM.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Optuna Hyperparameter Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Optuna para:\\n\"\n",
        "                        \"1. Explicar la importancia global de hiperparmetros en la optimizacin del Random Forest.\\n\"\n",
        "                        \"2. Analizar top trials si estn disponibles: qu combinaciones de hiperparmetros funcionaron mejor.\\n\"\n",
        "                        \"3. Sealar limitaciones de la muestra de trials (nmero de pruebas) y posibles riesgos de sobreajuste en la bsqueda.\\n\"\n",
        "                        \"4. Recomendar prximas acciones para tuning basadas en estas importancias.\\n\"\n",
        "                    )\n",
        "                else:\n",
        "                    prompt += \"\\nPor favor, explica estos resultados numricos y qu implicaciones tienen para el modelo Random Forest.\\n\"\n",
        "\n",
        "                # --------------- Llamada a OpenAI ---------------\n",
        "                print(f\"[DEBUG] 9.21. Llamando a OpenAI para {titulo}\")\n",
        "                explicacion = call_openai_explanation(prompt)\n",
        "                self.sections.append((f\"### {titulo}: Explicacin Generativa\", explicacion))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"###  Error en seccin xIA Random Forest\",\n",
        "                f\"Se produjo un error al generar la seccin xIA de Random Forest: {e}\"\n",
        "            ))\n",
        "\n",
        "\n",
        "        # =====================================================================\n",
        "        # 10. Entrenamiento Modelo Redes Neuronales Recurrentes RNN\n",
        "        # =====================================================================\n",
        "        try:\n",
        "            if \"RESUMEN_METODOS\" in self.g and \"X_test\" in self.g and \"Y_test\" in self.g:\n",
        "                import os, pickle\n",
        "                import numpy as _np\n",
        "                import pandas as _pd\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "                from tensorflow.keras.models import load_model\n",
        "\n",
        "                X_test_full = self.g[\"X_test\"]\n",
        "                Y_test = self.g[\"Y_test\"]\n",
        "                # Extraer array 1D de y_test\n",
        "                if hasattr(Y_test, \"values\"):\n",
        "                    arr = Y_test.values\n",
        "                    if arr.ndim == 2 and arr.shape[1] >= 1:\n",
        "                        y_test_arr_full = arr[:, 0]\n",
        "                    else:\n",
        "                        y_test_arr_full = arr.ravel()\n",
        "                else:\n",
        "                    y_test_arr_full = _np.array(Y_test).ravel()\n",
        "                y_test_arr_full = y_test_arr_full.ravel()\n",
        "\n",
        "                metrics_summary_rnn = []\n",
        "                # Iteramos sobre cada mtodo en RESUMEN_METODOS\n",
        "                for metodo, vars_sel in self.g[\"RESUMEN_METODOS\"].items():\n",
        "                    metodo_low = metodo.lower()\n",
        "                    model_fname = f\"modelo_rnn_{metodo_low}.h5\"\n",
        "                    scaler_fname = f\"escaladores_rnn_{metodo_low}.pkl\"\n",
        "                    hp_fname = f\"hyperparams_rnn_{metodo_low}.pkl\"\n",
        "                    if not os.path.exists(model_fname) or not os.path.exists(scaler_fname):\n",
        "                        print(f\"[DEBUG] Fichero de modelo RNN o escaladores no encontrado para mtodo '{metodo}', omito este mtodo\")\n",
        "                        continue\n",
        "                    # Cargar modelo y escaladores\n",
        "                    try:\n",
        "                        model = load_model(model_fname)\n",
        "                        with open(scaler_fname, \"rb\") as f:\n",
        "                            data_s = pickle.load(f)\n",
        "                        sx = data_s.get(\"scaler_X\", None)\n",
        "                        sy = data_s.get(\"scaler_Y\", None)\n",
        "                        cols = data_s.get(\"cols\", None)\n",
        "                        if model is None or sx is None or sy is None or cols is None:\n",
        "                            print(f\"[DEBUG] Falta clave en escaladores RNN para mtodo '{metodo}', omito\")\n",
        "                            continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al cargar modelo/escaladores RNN para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Cargar hiperparmetros\n",
        "                    if os.path.exists(hp_fname):\n",
        "                        try:\n",
        "                            with open(hp_fname, \"rb\") as f_hp:\n",
        "                                hp = pickle.load(f_hp)\n",
        "                        except Exception as e:\n",
        "                            print(f\"[ERROR] al cargar hyperparams RNN para mtodo '{metodo}': {e}\")\n",
        "                            hp = None\n",
        "                    else:\n",
        "                        hp = None\n",
        "                    if hp is None:\n",
        "                        print(f\"[DEBUG] No hay hyperparams guardados para RNN mtodo '{metodo}', no podr rehacer prediccin correctamente, omito este mtodo en informe\")\n",
        "                        continue\n",
        "\n",
        "                    # Verificar columnas en X_test\n",
        "                    missing = [c for c in cols if c not in X_test_full.columns]\n",
        "                    if missing:\n",
        "                        print(f\"[DEBUG] Para mtodo '{metodo}', faltan columnas en X_test: {missing}, omito\")\n",
        "                        continue\n",
        "\n",
        "                    # Preparamos datos para secuencias\n",
        "                    # Tomamos solo las columnas seleccionadas\n",
        "                    X_test_sel = X_test_full[cols].copy()\n",
        "                    # Escalar toda la serie de test antes de crear secuencias\n",
        "                    try:\n",
        "                        Xts = sx.transform(X_test_sel)\n",
        "                        # Para Y_test, necesitamos construir array alineado con secuencias:\n",
        "                        # Usamos los primeros len(Xts) valores de y_test_arr_full\n",
        "                        # y construiremos secuencias con window_size:\n",
        "                        window = int(hp.get('window_size', 0))\n",
        "                        if window <= 0 or len(Xts) <= window:\n",
        "                            print(f\"[DEBUG] window_size invlido o demasiado grande para test en mtodo '{metodo}', omito\")\n",
        "                            continue\n",
        "                        # Creamos secuencias iguales a la funcin create_sequences de la celda 7.5:\n",
        "                        X_seq = []\n",
        "                        Y_seq = []\n",
        "                        for j in range(len(Xts) - window):\n",
        "                            X_seq.append(Xts[j:j+window])\n",
        "                            # Y real escalada (sy) usamos Y_test escalado? En entrenamiento se us y_train escalado para fit,\n",
        "                            # pero aqu Y_test necesitamos escala para invertir luego:\n",
        "                            # Mejor: tomamos Y_test original:\n",
        "                            # Extraemos Y_test alineado: y_test_arr_full, yts_seq ser y_test_arr_full[j+window]\n",
        "                            Y_seq.append(y_test_arr_full[j+window])\n",
        "                        X_seq = _np.array(X_seq)   # shape (n_samples_seq, window, n_features)\n",
        "                        Y_real = _np.array(Y_seq)  # shape (n_samples_seq,)\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al preparar secuencias RNN para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Predecir y desescalar\n",
        "                    try:\n",
        "                        Y_pred_scaled = model.predict(X_seq, verbose=0).ravel()\n",
        "                        # Invertir escala:\n",
        "                        Y_pred = sy.inverse_transform(Y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                        # Convertir Y_real (original) en array float\n",
        "                        Y_real = _np.array(Y_real).ravel()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al predecir/desescalar RNN para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Clculo de mtricas y estadsticas\n",
        "                    try:\n",
        "                        # Rangos Real vs Predicha\n",
        "                        y_real_min, y_real_max = float(_np.min(Y_real)), float(_np.max(Y_real))\n",
        "                        y_pred_min, y_pred_max = float(_np.min(Y_pred)), float(_np.max(Y_pred))\n",
        "                        # Residuos\n",
        "                        residuals = Y_real - Y_pred\n",
        "                        res_mean = float(_np.mean(residuals))\n",
        "                        res_std  = float(_np.std(residuals))\n",
        "                        res_series = _pd.Series(residuals)\n",
        "                        res_skew = float(res_series.skew())\n",
        "                        res_kurt = float(res_series.kurtosis())\n",
        "                        q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]\n",
        "                        # Mtricas\n",
        "                        mse = mean_squared_error(Y_real, Y_pred)\n",
        "                        rmse = float(_np.sqrt(mse))\n",
        "                        mae = float(mean_absolute_error(Y_real, Y_pred))\n",
        "                        r2 = float(r2_score(Y_real, Y_pred))\n",
        "                        # Correlacin Real vs Predicha\n",
        "                        try:\n",
        "                            corr = float(_np.corrcoef(Y_real, Y_pred)[0, 1])\n",
        "                        except:\n",
        "                            corr = None\n",
        "                        # Guardar resumen para comparativa\n",
        "                        metrics_summary_rnn.append({\n",
        "                            \"metodo\": metodo,\n",
        "                            \"rmse\": rmse,\n",
        "                            \"mae\": mae,\n",
        "                            \"r2\": r2\n",
        "                        })\n",
        "                        #all_metrics.append({\n",
        "                        #    \"Modelo\": f\"{TIPO}_{metodo}\",  # ej. \"SVR_Pearson\", \"NN_Boruta\"...\n",
        "                        #    \"Tipo\": TIPO,                 # \"SVR\", \"NN\", \"XGBoost\", \"RF\" o \"RNN\"\n",
        "                        #    \"Mtodo\": metodo,\n",
        "                        #    \"R2\": r2,\n",
        "                        #    \"MSE\": mse,\n",
        "                        #    \"RMSE\": rmse,\n",
        "                        #    \"MAE\": mae\n",
        "                        #})\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al calcular mtricas RNN para mtodo '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # 1) Parmetros de entrenamiento obtenidos desde hp\n",
        "                    try:\n",
        "                        # Usamos el dict hp cargado\n",
        "                        df_hp = _pd.DataFrame({\n",
        "                            \"Hiperparmetro\": list(hp.keys()),\n",
        "                            \"Valor\": [str(v) for v in hp.values()]\n",
        "                        })\n",
        "                        titulo_hp = f\"### Parmetros de Entrenamiento RNN ({metodo})\"\n",
        "                        self.sections.append((titulo_hp, df_hp))\n",
        "                        print(f\"[DEBUG] 10.1. Seccin parmetros RNN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al mostrar parmetros RNN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA hiperparmetros RNN\n",
        "                    try:\n",
        "                        prompt_params = (\n",
        "                            f\"Has entrenado un modelo RNN (tipo {hp.get('tipo_rnn')}) con seleccin de variables '{metodo}'.\\n\"\n",
        "                            \"Estos fueron los hiperparmetros utilizados:\\n\"\n",
        "                        )\n",
        "                        for k, v in hp.items():\n",
        "                            prompt_params += f\"- {k}: {v}\\n\"\n",
        "                        prompt_params += (\n",
        "                            \"\\nPor favor, explica de forma profesional y detallada cmo estos hiperparmetros \"\n",
        "                            \"pueden influir en el entrenamiento de la RNN, su impacto en ajuste/sobreajuste, \"\n",
        "                            \"y buenas prcticas para seleccionarlos o afinarlos.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 10.2. Iniciando llamada a OpenAI para explicacin hiperparmetros RNN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en entrenamiento de redes neuronales recurrentes para series temporales.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_params}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_hp = resp.choices[0].message.content.strip()\n",
        "                        if explanation_hp:\n",
        "                            titulo_exp_hp = f\"###  Explicacin IA Hiperparmetros RNN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_hp, explanation_hp))\n",
        "                            print(f\"[DEBUG] 10.3. Seccin explicacin IA hiperparmetros RNN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA hiperparmetros RNN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 2) Grfica Predicho vs Real\n",
        "                    try:\n",
        "                        fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "                        ax1.scatter(Y_real, Y_pred, alpha=0.6)\n",
        "                        ax1.plot([y_real_min, y_real_max], [y_real_min, y_real_max], 'r--', lw=2)\n",
        "                        ax1.set_xlabel(\"Y real\")\n",
        "                        ax1.set_ylabel(\"Y predicho\")\n",
        "                        ax1.set_title(f\"RNN Predicho vs Real ({metodo})\")\n",
        "                        titulo_fig1 = f\"### Grfico RNN Predicho vs Real ({metodo})\"\n",
        "                        self.sections.append((titulo_fig1, fig1))\n",
        "                        print(f\"[DEBUG] 10.4. Seccin grfica Pred vs Real RNN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear grfica Pred vs Real RNN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA Pred vs Real RNN con contexto numrico\n",
        "                    try:\n",
        "                        prompt_pr = (\n",
        "                            f\"A continuacin tienes datos de la grfica de comparacin Real vs Prediccin para el modelo RNN con mtodo '{metodo}':\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- R: {r2}\\n\"\n",
        "                            f\"- Correlacin entre Y real y predicha: {corr}\\n\"\n",
        "                            f\"- Rango Y real: [{y_real_min}, {y_real_max}]\\n\"\n",
        "                            f\"- Rango Y predicha: [{y_pred_min}, {y_pred_max}]\\n\\n\"\n",
        "                            \"Basndote en estos valores y en la grfica generada (Real vs Prediccin), \"\n",
        "                            \"proporciona un anlisis detallado: sesgos sistemticos, dispersin en rangos, posibles problemas y recomendaciones.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 10.5. Llamada IA Pred vs Real RNN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML para series temporales.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_pr}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_pr = resp.choices[0].message.content.strip()\n",
        "                        if explanation_pr:\n",
        "                            titulo_exp_pr = f\"###  Explicacin IA Predicho vs Real RNN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_pr, explanation_pr))\n",
        "                            print(f\"[DEBUG] 10.6. Seccin explicacin IA Pred vs Real RNN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA Pred vs Real RNN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 3) Grfica de residuos RNN\n",
        "                    try:\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "                        ax2.scatter(Y_pred, residuals, alpha=0.6)\n",
        "                        ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "                        ax2.set_xlabel(\"Y predicho\")\n",
        "                        ax2.set_ylabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "                        ax2.set_title(f\"RNN Residuos ({metodo})\")\n",
        "                        titulo_fig2 = f\"### Grfico RNN Residuos ({metodo})\"\n",
        "                        self.sections.append((titulo_fig2, fig2))\n",
        "                        print(f\"[DEBUG] 10.7. Seccin grfica residuos RNN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear grfica residuos RNN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicacin IA Residuos RNN con contexto numrico\n",
        "                    try:\n",
        "                        prompt_res = (\n",
        "                            f\"A continuacin tienes estadsticas de los residuos (Real - Predicha) del modelo RNN con mtodo '{metodo}':\\n\"\n",
        "                            f\"- Media: {res_mean}\\n\"\n",
        "                            f\"- Desviacin estndar: {res_std}\\n\"\n",
        "                            f\"- Asimetra: {res_skew}\\n\"\n",
        "                            f\"- Curtosis: {res_kurt}\\n\"\n",
        "                            f\"- Cuantiles: 25%={q25}, 50%={q50}, 75%={q75}\\n\\n\"\n",
        "                            \"Basndote en estos valores y en la grfica de residuos, analiza patrones (heterocedasticidad, outliers, sesgos) y qu implicaciones tiene para generalizacin.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 10.8 Llamada IA Residuos RNN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML para series temporales.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_res}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_res = resp.choices[0].message.content.strip()\n",
        "                        if explanation_res:\n",
        "                            titulo_exp_res = f\"###  Explicacin IA Residuos RNN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_res, explanation_res))\n",
        "                            print(f\"[DEBUG] 10.9. Seccin explicacin IA residuos RNN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA residuos RNN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                    # 4) Tabla Mtricas y explicacin IA\n",
        "                    try:\n",
        "                        df_met = _pd.DataFrame([\n",
        "                            {\"Mtrica\": \"RMSE\", \"Valor\": rmse},\n",
        "                            {\"Mtrica\": \"MAE\", \"Valor\": mae},\n",
        "                            {\"Mtrica\": \"R2\",  \"Valor\": r2}\n",
        "                        ])\n",
        "                        titulo_met = f\"### Mtricas RNN ({metodo})\"\n",
        "                        self.sections.append((titulo_met, df_met))\n",
        "                        print(f\"[DEBUG] 10.10. Seccin mtricas RNN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame mtricas RNN para mtodo '{meteto}': {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_met = (\n",
        "                            f\"Estas son las mtricas del modelo RNN con mtodo '{metodo}':\\n\"\n",
        "                            f\"- R2: {r2}\\n\"\n",
        "                            f\"- MSE: {mse}\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- Correlacin Real vs Predicha: {corr}\\n\\n\"\n",
        "                            \"Analiza estos valores en contexto: son adecuados? qu sugieren respecto al rendimiento? Menciona grficas Pred vs Real y residuos.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 10.11. Llamada IA Mtricas RNN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluacin de modelos ML para series temporales.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_met}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_met = resp.choices[0].message.content.strip()\n",
        "                        if explanation_met:\n",
        "                            titulo_exp_met = f\"###  Explicacin IA Mtricas RNN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_met, explanation_met))\n",
        "                            print(f\"[DEBUG] 10.12. Seccin explicacin IA mtricas RNN aadida para mtodo: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA mtricas RNN para mtodo '{metodo}': {e}\")\n",
        "\n",
        "                # 5) Comparativa global de mtricas RNN\n",
        "                if metrics_summary_rnn:\n",
        "                    try:\n",
        "                        df_comp_rnn = _pd.DataFrame(metrics_summary_rnn)\n",
        "                        df_comp_rnn_sorted = df_comp_rnn.sort_values(\"rmse\")\n",
        "                        titulo_comp_rnn = \"### Comparativa Mtricas RNN entre Mtodos\"\n",
        "                        self.sections.append((titulo_comp_rnn, df_comp_rnn_sorted))\n",
        "                        print(\"[DEBUG] 10.13. Seccin comparativa mtricas RNN aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame comparativo mtricas RNN: {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_conc = (\n",
        "                            \"Se han entrenado varios modelos RNN con diferentes mtodos de seleccin de variables.\\n\"\n",
        "                            \"Mtricas obtenidas en test:\\n\"\n",
        "                        )\n",
        "                        for entry in metrics_summary_rnn:\n",
        "                            prompt_conc += f\"- Mtodo '{entry['metodo']}': RMSE={entry['rmse']}, MAE={entry['mae']}, R2={entry['r2']}\\n\"\n",
        "                        prompt_conc += (\n",
        "                            \"\\nPor favor, proporciona conclusiones profesionales comparando estos mtodos: \"\n",
        "                            \"indica cul se comporta mejor, posibles razones y recomendaciones sobre seleccin de variables o ajustes para mejorar la RNN.\"\n",
        "                        )\n",
        "                        print(\"[DEBUG] 10.14. Llamada IA Conclusiones RNN...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos para series temporales.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_conc}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_conc = resp.choices[0].message.content.strip()\n",
        "                        if explanation_conc:\n",
        "                            titulo_exp_conc = \"###  Conclusiones IA Entrenamiento RNN\"\n",
        "                            self.sections.append((titulo_exp_conc, explanation_conc))\n",
        "                            print(\"[DEBUG] 10.15. Seccin explicacin IA conclusiones RNN aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA conclusiones RNN: {e}\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No estn RESUMEN_METODOS o X_test/Y_test en globals(), omito seccin RNN\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar seccin RNN en informe: {e}\")\n",
        "        # ... fin de la seccin de entrenamiento de Redes Neuronales Rrecurrentes RNN ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 10.1. Interpretacin xIA para modelo entrenado RNN\n",
        "        # =============================================================\n",
        "        import openai\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # Verificacin previa\n",
        "        # ----------------------------------------------------------------\n",
        "        #  0) Normalizar la clave para buscar en xai_results \n",
        "        #storage_key = 'RNN'.lower()   # coincide con la clave usada al guardar en la celda 10 ('rnn')\n",
        "        try:\n",
        "            print(\"[DEBUG] 10.16. Iniciando seccin xIA para RNN\")\n",
        "            print(\"DEBUG: claves en xai_results:\", list(xai_results.keys()))\n",
        "            if 'xai_results' not in globals() or 'RNN' not in xai_results:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontr `xai_results['rnn']`. \"\n",
        "                    \"Asegrate de haber ejecutado la Celda 10 y almacenado los resultados xIA de RNN en `xai_results['rnn']`.\"\n",
        "                )\n",
        "\n",
        "                # Cabecera\n",
        "                self.sections.append((\n",
        "                    \"##  Anlisis xIA de RNN: Resultados concretos y explicaciones Generativas\",\n",
        "                    \"\"  # contenido vaco, la cabecera se mostrar como Markdown\n",
        "                ))\n",
        "\n",
        "\n",
        "            # Funcin para llamar a OpenAI con un prompt especfico\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                \"\"\"\n",
        "                Llama a OpenAI ChatCompletion con un sistema experto en ML/XAI,\n",
        "                devuelve la respuesta de la IA en texto.\n",
        "                \"\"\"\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un asistente experto en Machine Learning e interpretabilidad de modelos. \"\n",
        "                                \"Proporciona explicaciones detalladas y basadas en los datos concretos proporcionados.\"\n",
        "                            )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    texto = response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    texto = f\"[Error llamando a OpenAI: {e}]\"\n",
        "                return texto\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Configuracin: top N variables y primeras M muestras locales\n",
        "            # ----------------------------------------------------------------\n",
        "            TOP_N = 5      # cuntas caractersticas top incluir en el prompt\n",
        "            N_LOCAL = 3    # cuntas muestras locales incluir en prompt\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Lista de mtodos xIA y claves en xai_results['RNN']\n",
        "            # ----------------------------------------------------------------\n",
        "            # Las claves deben coincidir exactamente con las usadas en Celda 10 al almacenar resultados.\n",
        "            metodos_claves = [\n",
        "                ('SHAP', 'SHAP'),\n",
        "                ('LIME', 'LIME'),\n",
        "                ('KernelExplainer', 'KernelExplainer'),\n",
        "                ('Integrated Gradients', 'Integrated Gradients'),\n",
        "                ('DeepLIFT / LRP', 'DeepLIFT / LRP'),\n",
        "                ('Permutation Feature Importance', 'Permutation Feature Importance'),\n",
        "                ('Partial Dependence Plots (PDP)', 'Partial Dependence Plots (PDP)'),\n",
        "                ('Accumulated Local Effects (ALE)', 'Accumulated Local Effects (ALE)'),\n",
        "                ('Individual Conditional Expectation (ICE) Plots', 'Individual Conditional Expectation (ICE) Plots'),\n",
        "                ('Counterfactual Explanations', 'Counterfactual Explanations'),\n",
        "                ('Anchors', 'Anchors'),\n",
        "                ('Surrogate Models (Global/Local)', 'Surrogate Models (Global/Local)'),\n",
        "                ('Explainable Boosting Machine (EBM)', 'Explainable Boosting Machine (EBM)'),\n",
        "                ('Optuna Hyperparameter Importance', 'Optuna Hyperparameter Importance'),\n",
        "            ]\n",
        "\n",
        "            for titulo, clave in metodos_claves:\n",
        "                print(f\"[DEBUG] 10.17. Procesando seccin xIA: {titulo}\")\n",
        "                datos = xai_results['RNN'].get(clave)\n",
        "                if datos is None:\n",
        "                    print(f\"[DEBUG] No hay resultados xIA para {titulo}, se omite\")\n",
        "                    continue\n",
        "#                    self.sections.append((\n",
        "#                        f\"###  No hay resultados para {titulo}\",\n",
        "#                        f\"No se hallaron resultados para la clave '{clave}'.\"\n",
        "#                    ))\n",
        "#                    continue\n",
        "\n",
        "                # ---------------- Mostrar figura guardada ----------------\n",
        "                print(f\"[DEBUG] 10.18. Mostrando figura para {titulo}\")\n",
        "                fig = datos.get('fig_summary') or datos.get('fig')\n",
        "                if fig is not None:\n",
        "                    self.sections.append((f\"### {titulo}: Grfico\", fig))\n",
        "\n",
        "                # --------------- Mostrar DataFrames de importancia y local ---------------\n",
        "                print(f\"[DEBUG] 10.19. Mostrando DataFrames para {titulo}\")\n",
        "                imp_df = datos.get('imp_df')\n",
        "                df_local = datos.get('df_local')\n",
        "                stats_extra = datos.get('stats', None)  # opcional: estadsticas adicionales, p.ej. percentiles, pos_pct SHAP, etc.\n",
        "\n",
        "                if isinstance(imp_df, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Importancia global\", imp_df.reset_index(drop=True)))\n",
        "\n",
        "                if isinstance(df_local, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Valores locales\", df_local.head(N_LOCAL)))\n",
        "\n",
        "                # --------------------------------- Extraer datos del grfico (suponiendo que sea un barh con 'feature' y 'importance') ---------------------------\n",
        "                features = [text.get_text() for text in ax.get_yticklabels()]\n",
        "                importances = [bar.get_width() for bar in ax.patches]\n",
        "\n",
        "                # --------------- Construir prompt con valores numricos concretos ---------------\n",
        "                print(f\"[DEBUG] 10.20. Construyendo prompt para {titulo}\")\n",
        "                prompt = f\"He aplicado el mtodo xIA '{titulo}' al modelo RNN entrenado y he obtenido estos resultados concretos:\\n\\n\"\n",
        "\n",
        "                prompt += \"- Datos del grfico (feature vs importancia):\\n\"\n",
        "                for f, imp in zip(features, importances):\n",
        "                    prompt += f\"     {f}: {imp:.4f}\\n\"\n",
        "\n",
        "                # Ahora s le pides que interprete el grfico:\n",
        "                prompt += (\n",
        "                    \"- Interpreta el grfico anterior: \"\n",
        "                    \"describe qu patrones o relaciones visuales revela cmo se distribuye la importancia.\\n\"\n",
        "                )\n",
        "\n",
        "                # 1) Extraer importancia global: top N\n",
        "                if isinstance(imp_df, pd.DataFrame) and not imp_df.empty:\n",
        "                    imp_df = imp_df.reset_index()\n",
        "                    imp_df = imp_df.rename(columns={ imp_df.columns[0]: \"feature\" })\n",
        "                    cols = imp_df.columns.tolist()\n",
        "                    if len(cols) >= 2:\n",
        "                        feat_col = \"feature\"\n",
        "                        val_col  = cols[1]\n",
        "                        try:\n",
        "                            imp_df_sorted = imp_df.sort_values(val_col, ascending=False)\n",
        "                        except Exception:\n",
        "                            imp_df_sorted = imp_df\n",
        "\n",
        "                        top_n      = min(TOP_N, len(imp_df_sorted))\n",
        "                        top_imp_df = imp_df_sorted.iloc[:top_n][[feat_col, val_col]]\n",
        "                        top_imp_list = []\n",
        "                        for _, row in top_imp_df.iterrows():\n",
        "                            raw = row[val_col]\n",
        "                            try:\n",
        "                                v = float(raw)\n",
        "                            except Exception:\n",
        "                                v = raw  # si no se puede convertir, lo dejo tal cual\n",
        "                            top_imp_list.append({feat_col: row[feat_col], val_col: v})\n",
        "\n",
        "                        prompt += f\"- Top {top_n} caractersticas por importancia global ({feat_col}, {val_col}):\\n  {top_imp_list}\\n\"\n",
        "\n",
        "                        extra_cols = cols[2:]\n",
        "                        if extra_cols:\n",
        "                            prompt += f\"  (El DataFrame de importancia global incluye tambin columnas: {extra_cols}.)\\n\"\n",
        "                    else:\n",
        "                        snippet = imp_df.head(TOP_N).to_dict(orient='records')\n",
        "                        prompt += f\"- Importancia global (primeras filas, formato genrico):\\n  {snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de importancia global.\\n\"\n",
        "\n",
        "#                    #  Siempre sacamos el ndice como columna llamada \"feature\"\n",
        "                    imp_df = imp_df.rename(columns={ idx_col: \"feature\" })\n",
        "\n",
        "                # 2) Extraer valores locales: primeras M muestras\n",
        "                if isinstance(df_local, pd.DataFrame) and not df_local.empty:\n",
        "                    n_loc = min(N_LOCAL, len(df_local))\n",
        "                    loc_snippet = df_local.head(n_loc).to_dict(orient='records')\n",
        "                    prompt += f\"- Valores locales para las primeras {n_loc} muestras (cada dict mapea feature a valor/atribucin):\\n  {loc_snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de valores locales.\\n\"\n",
        "\n",
        "                # 3) Incluir estadsticas extra si existen\n",
        "                if isinstance(stats_extra, dict):\n",
        "                    prompt += \"- Estadsticas adicionales:\\n\"\n",
        "                    for k, v in stats_extra.items():\n",
        "                        prompt += f\"  * {k}: {v}\\n\"\n",
        "\n",
        "                # 4) Contexto general del modelo RNN\n",
        "                prompt += (\n",
        "                    \"\\nContexto: El modelo RNN fue entrenado con variables seleccionadas y StandardScaler, \"\n",
        "                    \"con predicciones desescaladas. Ahora interpretamos los resultados xIA para este RNN.\\n\"\n",
        "                )\n",
        "\n",
        "                # 5) Preguntas/pautas especficas segn el mtodo\n",
        "                if clave == 'SHAP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores concretos de SHAP para:\\n\"\n",
        "                        \"1. Explicar la importancia global de cada variable en base a los valores de SHAP mostrados.\\n\"\n",
        "                        \"2. Analizar los valores SHAP de las primeras muestras: si una variable muestra SHAP positivo alto, qu implica sobre la prediccin en ese caso? Y si es negativo, qu implica?\\n\"\n",
        "                        \"3. Identificar patrones en SHAP (por ejemplo, variables que consistentemente empujan en una direccin) y cmo afecta al comportamiento general del RNN.\\n\"\n",
        "                        \"4. Sugerir posibles transformaciones de variables o validaciones adicionales basadas en estos resultados SHAP.\\n\"\n",
        "                    )\n",
        "                elif clave == 'LIME':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos pesos LIME concretos para:\\n\"\n",
        "                        \"1. Explicar el significado de pesos positivos y negativos en LIME para RNN.\\n\"\n",
        "                        \"2. Analizar casos de las primeras muestras: si una variable tiene peso LIME fuerte (positivo/negativo), qu implica para la prediccin local?\\n\"\n",
        "                        \"3. Comentar si la variabilidad de los pesos sugiere relaciones no lineales o interacciones no capturadas.\\n\"\n",
        "                        \"4. Recomendar acciones: agrupacin de variables, deteccin de outliers, etc., basadas en la interpretacin LIME.\\n\"\n",
        "                    )\n",
        "                elif clave == 'KernelExplainer':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de KernelExplainer (SHAP caja negra) para:\\n\"\n",
        "                        \"1. Explicar la importancia global segn los valores medios absolutos Kernel SHAP.\\n\"\n",
        "                        \"2. Analizar los valores locales para las primeras muestras: cmo cada caracterstica empuja la prediccin en cada caso.\\n\"\n",
        "                        \"3. Comparar con SHAP (si ya lo hiciste con TreeExplainer para otro modelo o Kernel SHAP para RNN), si aplica.\\n\"\n",
        "                        \"4. Sugerir consideraciones sobre fondo (background) usado y posibles ajustes si las explicaciones muestran comportamiento inesperado.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Integrated Gradients':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Integrated Gradients para:\\n\"\n",
        "                        \"1. Explicar la contribucin integrada de cada variable: interpretacin de importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: qu implicaciones tiene un valor IG alto o bajo en cada variable.\\n\"\n",
        "                        \"3. Sealar limitaciones: compatibilidad con RNN no diferenciable; si estos valores provienen de un modelo aproximado, comentar fiabilidad.\\n\"\n",
        "                        \"4. Sugerir pasos adicionales o comparaciones con otros mtodos (SHAP/LIME) para validar interpretaciones.\\n\"\n",
        "                    )\n",
        "                elif clave == 'DeepLIFT / LRP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de DeepLIFT / LRP para:\\n\"\n",
        "                        \"1. Explicar la relevancia asignada a cada variable en importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: si una variable tiene relevancia positiva o negativa, qu significa para la prediccin.\\n\"\n",
        "                        \"3. Comparar con IG o SHAP si se dispone: consistencia de atribuciones.\\n\"\n",
        "                        \"4. Recomendar verificaciones o transformaciones en caso de interpretaciones inesperadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Permutation Feature Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Permutation Feature Importance para:\\n\"\n",
        "                        \"1. Explicar la cada en la mtrica al permutar cada variable: por qu ciertas variables son crticas.\\n\"\n",
        "                        \"2. Comentar la desviacin estndar: indica inestabilidad en la importancia? Dnde conviene reforzar validaciones?\\n\"\n",
        "                        \"3. Comparar con importancias de SHAP/LIME: similitudes o diferencias.\\n\"\n",
        "                        \"4. Sugerir prioridades para ajuste de modelo o seleccin de variables basadas en esta mtrica.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Partial Dependence Plots (PDP)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos PDP para:\\n\"\n",
        "                        \"1. Explicar el efecto promedio de cada variable sobre la prediccin segn el rango PDP obtenido.\\n\"\n",
        "                        \"2. Sealar si los rangos sugieren relaciones montonas o no lineales.\\n\"\n",
        "                        \"3. Advertir sobre correlaciones fuertes que puedan afectar la interpretacin.\\n\"\n",
        "                        \"4. Recomendar posibles exploraciones adicionales (PDP bivariados, transformaciones) segn los resultados.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Accumulated Local Effects (ALE)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ALE y valores locales ALE para:\\n\"\n",
        "                        \"1. Explicar cmo ALE corrige artefactos de correlacin y qu nos dicen los valores concretos.\\n\"\n",
        "                        \"2. Interpretar importancia global ALE: variables con mayor efecto acumulado.\\n\"\n",
        "                        \"3. Analizar heterogeneidad local a partir de valores ALE de primeras muestras.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales (ALE 2D, inspeccin de distribucin) segn hallazgos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Individual Conditional Expectation (ICE) Plots':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ICE medios para:\\n\"\n",
        "                        \"1. Explicar qu mide el rango ICE y su diferencia respecto a PDP.\\n\"\n",
        "                        \"2. Analizar heterogeneidad: variables con alta variabilidad en rangos ICE indican interacciones o comportamiento inestable.\\n\"\n",
        "                        \"3. Sealar implicaciones para el modelo y posibles ajustes si hay alto efecto local variable.\\n\"\n",
        "                        \"4. Recomendar exploraciones adicionales para entender la variabilidad local.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Counterfactual Explanations':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Contrafactuales para:\\n\"\n",
        "                        \"1. Explicar cmo interpretar los contrafactuales: cambios en variables que generan aumento en prediccin.\\n\"\n",
        "                        \"2. Analizar variables con mayor || medio: implicaciones sobre sensibilidad del modelo.\\n\"\n",
        "                        \"3. Sealar si faltan contrafactuales para algunas muestras: qu puede indicar (limites del modelo o datos).\\n\"\n",
        "                        \"4. Sugerir cmo usar estos insights para ajuste de modelo o recoleccin de datos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Anchors':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Anchors para:\\n\"\n",
        "                        \"1. Explicar cmo leer las reglas ancla de las primeras muestras: condiciones que aseguran la prediccin.\\n\"\n",
        "                        \"2. Analizar frecuencia global de aparicin de variables en reglas: implicaciones sobre estabilidad y sesgos.\\n\"\n",
        "                        \"3. Sealar regiones de bajo coverage o baja precisin: dnde el modelo es menos fiable.\\n\"\n",
        "                        \"4. Recomendar acciones: recoleccin de datos, refinamiento de variables o validaciones dirigidas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Surrogate Models (Global/Local)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Surrogate Models para:\\n\"\n",
        "                        \"1. Explicar la importancia global del surrogate (rbol) y la importancia local media (coeficientes regresiones locales).\\n\"\n",
        "                        \"2. Comparar global vs local: variables con alta importancia local pero baja global, o viceversa, y qu sugiere.\\n\"\n",
        "                        \"3. Concluir sobre consistencia de comportamiento del RNN en diferentes regiones del espacio.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales o ajustes de modelo segn discrepancias detectadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Explainable Boosting Machine (EBM)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de EBM para:\\n\"\n",
        "                        \"1. Explicar la importancia global segn EBM: cmo se comparan con otros mtodos.\\n\"\n",
        "                        \"2. Analizar contribuciones locales de las primeras muestras: qu patrones se observan.\\n\"\n",
        "                        \"3. Sealar si EBM revela interacciones no consideradas en RNN.\\n\"\n",
        "                        \"4. Recomendar posibles ajustes en caractersticas o validaciones segn insights de EBM.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Optuna Hyperparameter Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Optuna para:\\n\"\n",
        "                        \"1. Explicar la importancia global de hiperparmetros en la optimizacin del RNN.\\n\"\n",
        "                        \"2. Analizar top trials si estn disponibles: qu combinaciones de hiperparmetros funcionaron mejor.\\n\"\n",
        "                        \"3. Sealar limitaciones de la muestra de trials (nmero de pruebas) y posibles riesgos de sobreajuste en la bsqueda.\\n\"\n",
        "                        \"4. Recomendar prximas acciones para tuning basadas en estas importancias.\\n\"\n",
        "                    )\n",
        "                else:\n",
        "                    prompt += \"\\nPor favor, explica estos resultados numricos y qu implicaciones tienen para el modelo RNN.\\n\"\n",
        "\n",
        "                # --------------- Llamada a OpenAI ---------------\n",
        "                print(f\"[DEBUG] 10.21. Llamando a OpenAI para {titulo}\")\n",
        "                explicacion = call_openai_explanation(prompt)\n",
        "                self.sections.append((f\"### {titulo}: Explicacin Generativa\", explicacion))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"###  Error en seccin xIA RNN\",\n",
        "                f\"Se produjo un error al generar la seccin xIA de RNN: {e}\"\n",
        "            ))\n",
        "\n",
        "\n",
        "        # =====================================================================\n",
        "        # 11. Comparador Global de Modelos Entrenados\n",
        "        # =====================================================================\n",
        "        try:\n",
        "            print(\"[DEBUG] 11.1. Iniciando seccin Comparador de Modelos\")\n",
        "            import os, pickle\n",
        "            import numpy as np\n",
        "            import pandas as _pd\n",
        "            import matplotlib.pyplot as plt\n",
        "            from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "            from tensorflow.keras.models import load_model\n",
        "\n",
        "            # 1) Recopilar todos los modelos guardados\n",
        "            ruta = \".\"\n",
        "            modelos = []\n",
        "            for f in os.listdir(ruta):\n",
        "                if f.startswith(\"modelo_\") and (f.endswith(\".pkl\") or f.endswith(\".h5\")):\n",
        "                    name = f.replace(\"modelo_\", \"\").rsplit(\".\", 1)[0]\n",
        "                    try:\n",
        "                        if f.endswith(\".pkl\"):\n",
        "                            with open(f, \"rb\") as fp:\n",
        "                                data = pickle.load(fp)\n",
        "                            model = data.get(\"model\")\n",
        "                            sx = data.get(\"sx\", data.get(\"scaler_X\"))\n",
        "                            sy = data.get(\"sy\", data.get(\"scaler_Y\"))\n",
        "                            cols = data.get(\"cols\", [])\n",
        "                        else:  # .h5\n",
        "                            model = load_model(f)\n",
        "                            # se espera un pickle de escaladores: escaladores_{name}.pkl\n",
        "                            escal_path = f\"escaladores_{name}.pkl\"\n",
        "                            with open(escal_path, \"rb\") as fp:\n",
        "                                data = pickle.load(fp)\n",
        "                            sx = data.get(\"scaler_X\")\n",
        "                            sy = data.get(\"scaler_Y\")\n",
        "                            cols = data.get(\"cols\", [])\n",
        "                        if model is None or sx is None or sy is None or not cols:\n",
        "                            raise ValueError(\"Faltan claves necesarias en el pickle/modelo\")\n",
        "                        modelos.append((name, model, sx, sy, cols))\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] Omitido {f}: {e}\")\n",
        "\n",
        "            if not modelos:\n",
        "                print(\"[DEBUG] No se encontraron modelos vlidos para comparar\")\n",
        "            else:\n",
        "                # 2) Para cada modelo, calcular predicciones sobre X_test/Y_test\n",
        "                resultados = []\n",
        "                preds_dict = {}\n",
        "                y_true_full = None\n",
        "\n",
        "                for name, model, sx, sy, cols in modelos:\n",
        "                    # Verificar que X_test y Y_test existan en globals\n",
        "                    if \"X_test\" not in self.g or \"Y_test\" not in self.g:\n",
        "                        print(\"[DEBUG] No hay X_test/Y_test en globals(), omito comparador\")\n",
        "                        break\n",
        "                    try:\n",
        "                        X_test_df = self.g[\"X_test\"][cols].copy()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] Error al extraer X_test para {name}: {e}\")\n",
        "                        continue\n",
        "                    y_test = self.g[\"Y_test\"]\n",
        "                    # Serie 1D de y_true\n",
        "                    arr = y_test.values if hasattr(y_test, \"values\") else np.array(y_test)\n",
        "                    y_arr = arr.ravel()\n",
        "\n",
        "                    # Detectar si es RNN por input_shape\n",
        "                    is_rnn = False\n",
        "                    window = None\n",
        "                    try:\n",
        "                        if hasattr(model, \"input_shape\") and isinstance(model.input_shape, tuple) and len(model.input_shape) == 3:\n",
        "                            is_rnn = True\n",
        "                            window = model.input_shape[1]\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                    # Generar prediccin\n",
        "                    try:\n",
        "                        if is_rnn and window is not None:\n",
        "                            # crear secuencias\n",
        "                            Xs = sx.transform(X_test_df)\n",
        "                            seqs, trues = [], []\n",
        "                            for i in range(len(Xs) - window):\n",
        "                                seqs.append(Xs[i:i + window])\n",
        "                                trues.append(y_arr[i + window])\n",
        "                            if not seqs:\n",
        "                                print(f\"[DEBUG] Secuencias vacas para RNN {name}, omito\")\n",
        "                                continue\n",
        "                            Xseq = np.array(seqs)\n",
        "                            y_real = np.array(trues)\n",
        "                            pred_scaled = model.predict(Xseq, verbose=0)\n",
        "                            # en algunos casos model.predict devuelve tupla\n",
        "                            if isinstance(pred_scaled, tuple):\n",
        "                                pred_scaled = pred_scaled[0]\n",
        "                            y_pred = sy.inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
        "                        else:\n",
        "                            Xs = sx.transform(X_test_df)\n",
        "                            raw = model.predict(Xs)\n",
        "                            if isinstance(raw, tuple):\n",
        "                                raw = raw[0]\n",
        "                            y_pred = sy.inverse_transform(raw.reshape(-1, 1)).ravel()\n",
        "                            y_real = y_arr\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] Error prediccin para {name}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Guardar y_true para la primera curva\n",
        "                    if y_true_full is None:\n",
        "                        y_true_full = y_real\n",
        "\n",
        "                    # Calcular mtricas\n",
        "                    try:\n",
        "                        r2 = r2_score(y_real, y_pred)\n",
        "                        mse = mean_squared_error(y_real, y_pred)\n",
        "                        rmse = np.sqrt(mse)\n",
        "                        mae = mean_absolute_error(y_real, y_pred)\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] Error clculo mtricas para {name}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                    resultados.append({\n",
        "                        \"Modelo\": name,\n",
        "                        \"R2\": r2,\n",
        "                        \"MSE\": mse,\n",
        "                        \"RMSE\": rmse,\n",
        "                        \"MAE\": mae\n",
        "                    })\n",
        "                    preds_dict[name] = y_pred\n",
        "\n",
        "                # 3) Tabla de mtricas\n",
        "                if resultados:\n",
        "                    df_met = _pd.DataFrame(resultados).set_index(\"Modelo\")\n",
        "                    self.sections.append((\"###  Comparativa de Mtricas de Todos los Modelos\", df_met))\n",
        "                    print(\"[DEBUG] 11.2. Seccin comparativa mtricas aadida\")\n",
        "                    # Extraer mtricas como dict para el prompt:\n",
        "                    # Podemos convertir a lista de dicts o dict de listas. Por claridad, usamos lista de records:\n",
        "                    metrics_records = df_met.reset_index().to_dict(orient='records')\n",
        "                    # Explicacin IA de la tabla\n",
        "                    prompt_tab = (\n",
        "                        \"Tienes estas mtricas en test para cada modelo:\\n\"\n",
        "                        + \"\\n\".join(\n",
        "                            f\"- Modelo '{rec['Modelo']}': R2={rec['R2']:.4f}, RMSE={rec['RMSE']:.4f}, MAE={rec['MAE']:.4f}, MSE={rec['MSE']:.4f}\"\n",
        "                            for rec in metrics_records\n",
        "                        )\n",
        "                        + \"\\n\\nPor favor, interpreta profesionalmente cul modelo es el mejor segn estas mtricas y por qu, \"\n",
        "                          \"y proporciona recomendaciones de ajustes de hiperparmetros para mejorar el desempeo del mejor modelo y posibles ajustes en los dems.\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_tab}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"###  Explicacin IA Comparativa de Mtricas\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.3. Seccin IA comparacin mtricas aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA comparacin mtricas: {e}\")\n",
        "\n",
        "                    # Funcin auxiliar para grfico de barras horizontales\n",
        "                    def _plot_barh(series, titulo, xlabel=None):\n",
        "                        fig, ax = plt.subplots(figsize=(8, max(4, 0.3 * len(series))))\n",
        "                        # ordenar para que el mejor (mayor R2 o menor error) quede arriba\n",
        "                        series_sorted = series.sort_values(ascending=True)\n",
        "                        # Si es R2 (mayor es mejor), invertimos orden para que el mayor quede arriba\n",
        "                        if xlabel == \"R2\":\n",
        "                            # para R2, queremos ascending=True y luego invertir eje Y\n",
        "                            ax.barh(series_sorted.index, series_sorted.values)\n",
        "                        else:\n",
        "                            # para errores (RMSE, MAE, MSE), menor es mejor: ascending=True pone menor arriba despus de invertir eje\n",
        "                            ax.barh(series_sorted.index, series_sorted.values)\n",
        "                        ax.set_title(titulo)\n",
        "                        ax.set_xlabel(xlabel if xlabel else \"\")\n",
        "                        ax.set_ylabel(\"Modelo\")\n",
        "                        ax.invert_yaxis()\n",
        "                        plt.tight_layout()\n",
        "                        return fig\n",
        "\n",
        "                    # 4) Barras de R2\n",
        "                    fig_r2 = _plot_barh(df_met[\"R2\"], \"Ranking de Modelos por R\", xlabel=\"R2\")\n",
        "                    self.sections.append((\"###  Grfico Comparativo de R\", fig_r2))\n",
        "                    print(\"[DEBUG] 11.4. Seccin grfica R2 aadida\")\n",
        "                    # Explicacin IA R2\n",
        "                    # Extraer valores de R2:\n",
        "                    r2_dict = df_met[\"R2\"].to_dict()  # e.g. {\"M1\":0.85, \"M2\":0.78, ...}\n",
        "                    prompt_r2 = (\n",
        "                        \"Tienes los valores de R en test para cada modelo (aqu listados):\\n\"\n",
        "                        + \"\\n\".join(f\"- Modelo '{m}': R = {v:.4f}\" for m, v in r2_dict.items())\n",
        "                        + \"\\n\\nBasndote en estos valores (sin apoyarte en la visualizacin directa), \"\n",
        "                          \"indica profesionalmente qu conclusiones sacas sobre el ajuste de los modelos y posibles razones de las diferencias entre ellos.\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en interpretacin de grficos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_r2}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"###  Explicacin IA Grfico R\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.5. Seccin IA comparacin R2 aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA R2: {e}\")\n",
        "\n",
        "                    # 5) Barras de MAE\n",
        "                    fig_mae = _plot_barh(df_met[\"MAE\"], \"Comparativa Global por MAE (menor es mejor)\", xlabel=\"MAE\")\n",
        "                    self.sections.append((\"###  Grfico Comparativo de MAE\", fig_mae))\n",
        "                    print(\"[DEBUG] 11.6. Seccin grfica MAE aadida\")\n",
        "                    # Extraer valores de MAE:\n",
        "                    mae_dict = df_met[\"MAE\"].to_dict()  # e.g. {\"M1\":12.345, \"M2\":15.678, ...}\n",
        "                    prompt_mae = (\n",
        "                        \"Tienes los valores de MAE en test para cada modelo:\\n\"\n",
        "                        + \"\\n\".join(f\"- Modelo '{m}': MAE = {v:.4f}\" for m, v in mae_dict.items())\n",
        "                        + \"\\n\\nBasndote en estos valores (sin ver la grfica), indica profesionalmente qu indican sobre la precisin de cada modelo en trminos absolutos y relativos, \"\n",
        "                          \"qu patrones observas (por ejemplo, modelos con MAE significativamente ms alta o ms baja) y recomendaciones concretas para reducir el MAE del mejor modelo o de aquellos con MAE elevado.\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en interpretacin de mtricas de error en ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_mae}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"###  Explicacin IA Grfico MAE\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.7. Seccin IA comparacin MAE aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA MAE: {e}\")\n",
        "\n",
        "                    # 6) Barras de MSE\n",
        "                    fig_mse = _plot_barh(df_met[\"MSE\"], \"Comparativa Global por MSE (menor es mejor)\", xlabel=\"MSE\")\n",
        "                    self.sections.append((\"###  Grfico Comparativo de MSE\", fig_mse))\n",
        "                    print(\"[DEBUG] 11.8. Seccin grfica MSE aadida\")\n",
        "                    # Extraer valores de MSE:\n",
        "                    mse_dict = df_met[\"MSE\"].to_dict()\n",
        "                    prompt_mse = (\n",
        "                        \"Tienes los valores de MSE en test para cada modelo:\\n\"\n",
        "                        + \"\\n\".join(f\"- Modelo '{m}': MSE = {v:.4f}\" for m, v in mse_dict.items())\n",
        "                        + \"\\n\\nBasndote en estos valores, comenta profesionalmente qu sugiere acerca del ajuste y robustez de cada modelo, \"\n",
        "                          \"identifica si hay alguno con MSE significativamente mayor o menor y ofrece recomendaciones concretas para reducir MSE (por ejemplo, cambios de preprocesado, regularizacin, arquitectura, etc.).\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en interpretacin de mtricas de error en ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_mse}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"###  Explicacin IA Grfico MSE\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.9. Seccin IA comparacin MSE aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA MSE: {e}\")\n",
        "\n",
        "                    # 7) Barras de RMSE\n",
        "                    fig_rmse = _plot_barh(df_met[\"RMSE\"], \"Comparativa Global por RMSE (menor es mejor)\", xlabel=\"RMSE\")\n",
        "                    self.sections.append((\"###  Grfico Comparativo de RMSE\", fig_rmse))\n",
        "                    print(\"[DEBUG] 11.10. Seccin grfica RMSE aadida\")\n",
        "                    # Extraer valores de RMSE:\n",
        "                    rmse_dict = df_met[\"RMSE\"].to_dict()\n",
        "                    prompt_rmse = (\n",
        "                        \"Tienes los valores de RMSE en test para cada modelo:\\n\"\n",
        "                        + \"\\n\".join(f\"- Modelo '{m}': RMSE = {v:.4f}\" for m, v in rmse_dict.items())\n",
        "                        + \"\\n\\nBasndote en estos valores, comenta profesionalmente las diferencias entre modelos, \"\n",
        "                          \"por qu algunos podran tener RMSE mayor o menor (relacin con varianza de los datos, sesgos, complejidad del modelo, etc.) y sugiere estrategias concretas para mejorar el RMSE del modelo ganador o para equilibrar sesgo-varianza.\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en interpretacin de mtricas de error en ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_rmse}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"###  Explicacin IA Grfico RMSE\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.11. Seccin IA comparacin RMSE aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA RMSE: {e}\")\n",
        "\n",
        "                    # 8) Curvas Real vs Predicho para todos\n",
        "                    if y_true_full is not None and preds_dict:\n",
        "                        fig2, ax2 = plt.subplots(figsize=(8, 4))\n",
        "                        ax2.plot(y_true_full, label=\"Y real\", color=\"black\", linewidth=2)\n",
        "                        for name, pred in preds_dict.items():\n",
        "                            # Asegurarse de que longitudes coincidan; si no, recortar al mnimo\n",
        "                            min_len = min(len(y_true_full), len(pred))\n",
        "                            ax2.plot(pred[:min_len], \"--\", label=name)\n",
        "                        ax2.set_title(\"Comparativa Y real vs Predicho\")\n",
        "                        ax2.legend(loc=\"upper right\")\n",
        "                        plt.tight_layout()\n",
        "                        self.sections.append((\"###  Grfico Comparativo Real vs Predicho\", fig2))\n",
        "                        print(\"[DEBUG] 11.12. Seccin grfica real vs pred aadida\")\n",
        "                        # Suponiendo y_true_full y preds_dict ya definidos y alineados:\n",
        "                        # Para cada modelo:\n",
        "                        summary_list = []\n",
        "                        import numpy as _np\n",
        "                        for name, pred in preds_dict.items():\n",
        "                            # Asegurar longitudes coincidentes\n",
        "                            min_len = min(len(y_true_full), len(pred))\n",
        "                            y_real = _np.array(y_true_full[:min_len])\n",
        "                            y_pred = _np.array(pred[:min_len])\n",
        "                            # Mtricas:\n",
        "                            from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "                            try:\n",
        "                                r2_val = r2_score(y_real, y_pred)\n",
        "                            except:\n",
        "                                r2_val = None\n",
        "                            try:\n",
        "                                mse_val = mean_squared_error(y_real, y_pred)\n",
        "                                rmse_val = _np.sqrt(mse_val)\n",
        "                            except:\n",
        "                                mse_val = rmse_val = None\n",
        "                            try:\n",
        "                                mae_val = mean_absolute_error(y_real, y_pred)\n",
        "                            except:\n",
        "                                mae_val = None\n",
        "                            # Correlacin:\n",
        "                            try:\n",
        "                                corr_val = float(_np.corrcoef(y_real, y_pred)[0, 1]) if len(y_real)>1 else None\n",
        "                            except:\n",
        "                                corr_val = None\n",
        "                            # Rango:\n",
        "                            y_real_min, y_real_max = float(_np.min(y_real)), float(_np.max(y_real))\n",
        "                            y_pred_min, y_pred_max = float(_np.min(y_pred)), float(_np.max(y_pred))\n",
        "                            summary_list.append({\n",
        "                                \"Modelo\": name,\n",
        "                                \"R2\": r2_val,\n",
        "                                \"MSE\": mse_val,\n",
        "                                \"RMSE\": rmse_val,\n",
        "                                \"MAE\": mae_val,\n",
        "                                \"Corr\": corr_val,\n",
        "                                \"Rango real\": (y_real_min, y_real_max),\n",
        "                                \"Rango pred\": (y_pred_min, y_pred_max)\n",
        "                            })\n",
        "                            # Formatear summary_list en texto:\n",
        "                            lines = []\n",
        "                            for rec in summary_list:\n",
        "                                m = rec[\"Modelo\"]\n",
        "                                # Manejar None con 'N/A'\n",
        "                                r2s = f\"{rec['R2']:.4f}\" if rec['R2'] is not None else \"N/A\"\n",
        "                                rs = rec[\"Rango real\"]\n",
        "                                ps = rec[\"Rango pred\"]\n",
        "                                corr_s = f\"{rec['Corr']:.4f}\" if rec['Corr'] is not None else \"N/A\"\n",
        "                                mse_s = f\"{rec['MSE']:.4f}\" if rec['MSE'] is not None else \"N/A\"\n",
        "                                rmse_s = f\"{rec['RMSE']:.4f}\" if rec['RMSE'] is not None else \"N/A\"\n",
        "                                mae_s = f\"{rec['MAE']:.4f}\" if rec['MAE'] is not None else \"N/A\"\n",
        "                                lines.append(\n",
        "                                    f\"- Modelo '{m}': R2={r2s}, RMSE={rmse_s}, MAE={mae_s}, MSE={mse_s}, Corr={corr_s}, \"\n",
        "                                    f\"Rango real=[{rs[0]:.4f}, {rs[1]:.4f}], Rango pred=[{ps[0]:.4f}, {ps[1]:.4f}]\"\n",
        "                                )\n",
        "\n",
        "                            prompt_curvas = (\n",
        "                                \"Para cada modelo tienes estos resmenes numricos de su prediccin vs real:\\n\"\n",
        "                                + \"\\n\".join(lines)\n",
        "                                + \"\\n\\nAunque tambin se gener una grfica Real vs Predicho, la IA no la ve: \"\n",
        "                                  \"Comenta profesionalmente cmo vara el ajuste de cada modelo a lo largo de la serie con base en los valores anteriores \"\n",
        "                                  \"(por ejemplo, si hay subestimacin sistemtica al inicio o al final, si la dispersin crece en ciertos rangos, etc.).\"\n",
        "                            )\n",
        "                        try:\n",
        "                            resp = _client.chat.completions.create(\n",
        "                                model=\"gpt-4\",\n",
        "                                messages=[\n",
        "                                    {\"role\": \"system\", \"content\": \"Eres un experto en visualizacin de resultados ML.\"},\n",
        "                                    {\"role\": \"user\", \"content\": prompt_curvas}\n",
        "                                ],\n",
        "                                max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                            )\n",
        "                            self.sections.append((\"###  Explicacin IA Curvas Real vs Predicho\", resp.choices[0].message.content.strip()))\n",
        "                            print(\"[DEBUG] 11.13. Seccin IA comparacin curvas aadida\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"[ERROR] al generar explicacin IA curvas: {e}\")\n",
        "\n",
        "                        # 9) Grfica de residuos superpuesta\n",
        "                        fig3, ax3 = plt.subplots(figsize=(8, 4))\n",
        "                        for name, pred in preds_dict.items():\n",
        "                            min_len = min(len(y_true_full), len(pred))\n",
        "                            res = y_true_full[:min_len] - pred[:min_len]\n",
        "                            ax3.plot(res, label=name, alpha=0.7)\n",
        "                        ax3.axhline(0, color=\"black\", lw=1)\n",
        "                        ax3.set_title(\"Comparativa de Residuos\")\n",
        "                        ax3.legend(loc=\"upper right\")\n",
        "                        plt.tight_layout()\n",
        "                        self.sections.append((\"###  Grfico Comparativo de Residuos\", fig3))\n",
        "                        print(\"[DEBUG] 11.14. Seccin grfica residuos aadida\")\n",
        "                        res_summary = []\n",
        "                        import numpy as _np\n",
        "                        for rec in summary_list:  # si summary_list incluye y_real y y_pred, o recalcular aqu\n",
        "                            name = rec[\"Modelo\"]\n",
        "                            # Recalcular residuos:\n",
        "                            pred = preds_dict[name]\n",
        "                            min_len = min(len(y_true_full), len(pred))\n",
        "                            y_real = _np.array(y_true_full[:min_len])\n",
        "                            y_pred = _np.array(pred[:min_len])\n",
        "                            residuals = y_real - y_pred\n",
        "                            # Estadsticos:\n",
        "                            mean_res = float(_np.mean(residuals))\n",
        "                            std_res  = float(_np.std(residuals))\n",
        "                            # Con Pandas o numpy calcular skew/kurt:\n",
        "                            import pandas as _pd\n",
        "                            res_series = _pd.Series(residuals)\n",
        "                            skew_res = float(res_series.skew())\n",
        "                            kurt_res = float(res_series.kurtosis())\n",
        "                            q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]\n",
        "                            # Rango:\n",
        "                            min_res, max_res = float(_np.min(residuals)), float(_np.max(residuals))\n",
        "                            res_summary.append({\n",
        "                                \"Modelo\": name,\n",
        "                                \"Mean\": mean_res,\n",
        "                                \"Std\": std_res,\n",
        "                                \"Skew\": skew_res,\n",
        "                                \"Kurtosis\": kurt_res,\n",
        "                                \"Quantiles\": (q25, q50, q75),\n",
        "                                \"Rango residuo\": (min_res, max_res)\n",
        "                            })\n",
        "                            lines = []\n",
        "                            for rec in res_summary:\n",
        "                                m = rec[\"Modelo\"]\n",
        "                                mean_s = f\"{rec['Mean']:.4f}\"\n",
        "                                std_s  = f\"{rec['Std']:.4f}\"\n",
        "                                skew_s = f\"{rec['Skew']:.4f}\"\n",
        "                                kurt_s = f\"{rec['Kurtosis']:.4f}\"\n",
        "                                q25, q50, q75 = rec[\"Quantiles\"]\n",
        "                                min_r, max_r = rec[\"Rango residuo\"]\n",
        "                                lines.append(\n",
        "                                    f\"- Modelo '{m}': media residuo={mean_s}, std={std_s}, skew={skew_s}, kurtosis={kurt_s}, \"\n",
        "                                    f\"quantiles residuo 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}, \"\n",
        "                                    f\"rango residuo=[{min_r:.4f}, {max_r:.4f}]\"\n",
        "                                )\n",
        "                            prompt_res = (\n",
        "                                \"Tienes las siguientes estadsticas de residuos (Y_real - Y_predicho) para cada modelo:\\n\"\n",
        "                                + \"\\n\".join(lines)\n",
        "                                + \"\\n\\nCon base en estos datos (sin apoyo visual), analiza profesionalmente si hay patrones de sesgo (por ejemplo, media distinta de 0), heterocedasticidad (std variable segn nivel, aunque aqu solo tenemos std global; si quisieras, podras calcular std en terciles de y_pred), posibles outliers (basndote en quantiles y rango), y qu implicaciones tiene para la robustez y generalizacin de cada modelo.\"\n",
        "                            )\n",
        "                        try:\n",
        "                            resp = _client.chat.completions.create(\n",
        "                                model=\"gpt-4\",\n",
        "                                messages=[\n",
        "                                    {\"role\": \"system\", \"content\": \"Eres un experto en diagnstico de modelos ML.\"},\n",
        "                                    {\"role\": \"user\", \"content\": prompt_res}\n",
        "                                ],\n",
        "                                max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                            )\n",
        "                            self.sections.append((\"###  Explicacin IA Residuos Comparativos\", resp.choices[0].message.content.strip()))\n",
        "                            print(\"[DEBUG] 11.15. Seccin IA comparacin residuos aadida\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"[ERROR] al generar explicacin IA residuos: {e}\")\n",
        "                    else:\n",
        "                        print(\"[DEBUG] No hay datos suficientes para curvas o residuos comparativos\")\n",
        "\n",
        "                    # 10) Parmetros tcnicos de cada modelo\n",
        "                    tabla_params = []\n",
        "                    for name, model, _, _, _ in modelos:\n",
        "                        row = {\"Modelo\": name}\n",
        "                        try:\n",
        "                            if hasattr(model, \"get_params\"):\n",
        "                                row.update(model.get_params())\n",
        "                            else:\n",
        "                                cfg = model.get_config()\n",
        "                                row[\"Capas\"] = len(cfg.get(\"layers\", []))\n",
        "                                row[\"Opt\"] = cfg.get(\"optimizer_config\", {}).get(\"class_name\", \"?\")\n",
        "                                row[\"Loss\"] = cfg.get(\"loss\", \"?\")\n",
        "                        except Exception:\n",
        "                            row[\"Info\"] = \"no disponible\"\n",
        "                        tabla_params.append(row)\n",
        "                    df_params = _pd.DataFrame(tabla_params).set_index(\"Modelo\")\n",
        "                    self.sections.append((\"###  Parmetros Tcnicos de Cada Modelo\", df_params))\n",
        "                    print(\"[DEBUG] 11.16. Seccin tabla parmetros aadida\")\n",
        "\n",
        "                    # Explicacin IA parmetros\n",
        "                    prompt_par = (\n",
        "                        \"Aqu tienes una tabla con los parmetros tcnicos usados en cada modelo. \"\n",
        "                        \"Comenta brevemente qu configuraciones parecen ms adecuadas y cules podran ajustarse para mejorar el rendimiento.\"\n",
        "                        f\"\\n\\n{df_params.to_dict(orient='list')}\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en optimizacin de hiperparmetros ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_par}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"###  Explicacin IA Parmetros Tcnicos\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.17. Seccin IA comparacin parmetros aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA parmetros: {e}\")\n",
        "\n",
        "                    # 11) Conclusiones Globales y Recomendaciones de IA\n",
        "                    try:\n",
        "                        # Construir prompt que recopile los principales resultados:\n",
        "                        # Usamos df_met (DataFrame de mtricas) y, opcionalmente, podemos incorporar insights de grficos anteriores.\n",
        "                        # Convertimos df_met a diccionario para el prompt:\n",
        "                        # Supongamos summary_list y res_summary ya definidos (como arriba).\n",
        "                        # Formateamos una seccin de mtricas generales:\n",
        "                        lines_met = []\n",
        "                        for rec in summary_list:\n",
        "                            m = rec[\"Modelo\"]\n",
        "                            r2s = f\"{rec['R2']:.4f}\" if rec['R2'] is not None else \"N/A\"\n",
        "                            rmse_s = f\"{rec['RMSE']:.4f}\" if rec['RMSE'] is not None else \"N/A\"\n",
        "                            mae_s = f\"{rec['MAE']:.4f}\" if rec['MAE'] is not None else \"N/A\"\n",
        "                            mse_s = f\"{rec['MSE']:.4f}\" if rec['MSE'] is not None else \"N/A\"\n",
        "                            lines_met.append(f\"- {m}: R2={r2s}, RMSE={rmse_s}, MAE={mae_s}, MSE={mse_s}\")\n",
        "\n",
        "                        # Formateamos estadsticas de residuos:\n",
        "                        lines_res = []\n",
        "                        for rec in res_summary:\n",
        "                            m = rec[\"Modelo\"]\n",
        "                            mean_s = f\"{rec['Mean']:.4f}\"\n",
        "                            std_s  = f\"{rec['Std']:.4f}\"\n",
        "                            skew_s = f\"{rec['Skew']:.4f}\"\n",
        "                            kurt_s = f\"{rec['Kurtosis']:.4f}\"\n",
        "                            q25, q50, q75 = rec[\"Quantiles\"]\n",
        "                            lines_res.append(\n",
        "                                f\"- {m}: media residuo={mean_s}, std={std_s}, skew={skew_s}, kurtosis={kurt_s}, quantiles[25,50,75]=[{q25:.4f},{q50:.4f},{q75:.4f}]\"\n",
        "                            )\n",
        "\n",
        "                        # Formateamos rangos de prediccin vs real:\n",
        "                        lines_range = []\n",
        "                        for rec in summary_list:\n",
        "                            m = rec[\"Modelo\"]\n",
        "                            rs = rec[\"Rango real\"]\n",
        "                            ps = rec[\"Rango pred\"]\n",
        "                            lines_range.append(f\"- {m}: Rango real=[{rs[0]:.4f},{rs[1]:.4f}], Rango pred=[{ps[0]:.4f},{ps[1]:.4f}]\")\n",
        "\n",
        "                        # Construccin del prompt:\n",
        "                        prompt_final = (\n",
        "                            \"A continuacin tienes un resumen numrico de los resultados de todos los modelos:\\n\\n\"\n",
        "                            \"1) Mtricas globales en test:\\n\"\n",
        "                            + \"\\n\".join(lines_met)\n",
        "                            + \"\\n\\n2) Estadsticas de residuos:\\n\"\n",
        "                            + \"\\n\".join(lines_res)\n",
        "                            + \"\\n\\n3) Rangos de valores real vs predicho:\\n\"\n",
        "                            + \"\\n\".join(lines_range)\n",
        "                            + \"\\n\\nYa se han analizado previamente aspectos de ajuste, error y parmetros. \"\n",
        "                            \"Con base en toda esta informacin numrica, por favor: \"\n",
        "                            \"- Indica cul modelo presenta en conjunto mejor desempeo y por qu. \"\n",
        "                            \"- Sugiere qu ajustes o hiperparmetros convendra afinar en ese modelo para mejorar an ms (por ejemplo, si es red neuronal, architecture tweaks; si es rbol, depth/regularizacin; si es RNN, window size o layers; etc.). \"\n",
        "                            \"- Comenta posibles limitaciones de los otros modelos e indica en qu escenarios podran ser tiles (por ejemplo, si un modelo tiene menor varianza pero mayor sesgo, puede servir en entornos con datos ruidosos, etc.). \"\n",
        "                            \"- Propn prximos pasos de validacin o pruebas adicionales (p. ej. validacin cruzada ms exhaustiva, test en nuevos periodos de la serie, experimentos de robustez). \"\n",
        "                            \"Presenta la respuesta de forma profesional, estructurada en secciones (por ejemplo: Resumen general, Anlisis del mejor modelo, Ajustes recomendados, Limitaciones y usos alternativos, Prximos pasos).\"\n",
        "                        )\n",
        "                        resp_final = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en anlisis comparativo de modelos de Machine Learning.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_final}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        # Aadir la seccin al informe\n",
        "                        self.sections.append((\n",
        "                            \"###  Conclusiones Globales y Recomendaciones IA\",\n",
        "                            resp_final.choices[0].message.content.strip()\n",
        "                        ))\n",
        "                        print(\"[DEBUG] 11.18. Seccin IA Conclusiones Globales aadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicacin IA Conclusiones Globales: {e}\")\n",
        "\n",
        "                else:\n",
        "                    print(\"[DEBUG] No se obtuvieron resultados de mtrica para ningn modelo\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar seccin Comparador de Modelos: {e}\")\n",
        "\n",
        "        # =====================================================================\n",
        "        # 12. Optimizacin Modelo SVR\n",
        "        # =====================================================================\n",
        "        from scipy.stats import skew, kurtosis\n",
        "        try:\n",
        "            print(\"[DEBUG] 12.1. Iniciando seccin Optimizacin SVR: Resumen de mtodos y motores\")\n",
        "            # Verificacin previa: usamos OPT_MODELS guardado en la celda 9.1\n",
        "            if 'OPT_MODELS' in globals() and isinstance(OPT_MODELS, dict):\n",
        "                # Filtrar solo entradas de SVR con payload dict y motores vlidos\n",
        "                valid_engines = {'gridsearchcv', 'randomizedsearchcv', 'optuna', 'bayessearchcv'}\n",
        "                svr_entries = {\n",
        "                    k: v for k, v in OPT_MODELS.items()\n",
        "                    if (isinstance(k, tuple) and k[0] == 'svr'\n",
        "                        and isinstance(v, dict)\n",
        "                        and k[2] in valid_engines)\n",
        "                }\n",
        "            else:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontr la variable global OPT_MODELS con resultados de optimizacin SVR. \"\n",
        "                    \"Asegrate de haber ejecutado la Celda 9.1 y de que OPT_MODELS contenga los payloads optimizados.\"\n",
        "                )\n",
        "\n",
        "            import pandas as pd\n",
        "\n",
        "            # Definicin de las configuraciones usadas para cada motor (para mostrar en la tabla)\n",
        "            param_configs = {\n",
        "                'gridsearchcv': {\n",
        "                    'C': '[0.1, 1, 10, 100]',\n",
        "                    'epsilon': '[0.01, 0.1, 0.5, 1]',\n",
        "                    'kernel': \"['rbf', 'linear']\"\n",
        "                },\n",
        "                'randomizedsearchcv': {\n",
        "                    'C': 'np.logspace(-1, 2, 20)',\n",
        "                    'epsilon': 'np.logspace(-2, 0, 20)',\n",
        "                    'kernel': \"['rbf', 'linear']\"\n",
        "                },\n",
        "                'optuna': {\n",
        "                    'C': 'Float(0.1, 100, log=True)',\n",
        "                    'epsilon': 'Float(0.01, 1.0, log=True)',\n",
        "                    'kernel': \"Categorical(['rbf','linear'])\"\n",
        "                },\n",
        "                'bayessearchcv': {\n",
        "                    'C': 'Real(0.1, 100, prior=\"log-uniform\")',\n",
        "                    'epsilon': 'Real(0.01, 1.0, prior=\"log-uniform\")',\n",
        "                    'kernel': 'Categorical([\"rbf\",\"linear\"])'\n",
        "                }\n",
        "            }\n",
        "\n",
        "            summary_records = []\n",
        "            # Recorremos cada dupla (metodo, motor)\n",
        "            for (model_type, sel_method, engine), payload in svr_entries.items():\n",
        "                print(f\"[DEBUG] 12.2. Agregando registro resumen: {sel_method} / {engine}\")\n",
        "                model = payload.get('model')\n",
        "                score = payload.get('score')\n",
        "                metric = payload.get('metric')\n",
        "\n",
        "                # Parmetros usados para la bsqueda segn motor\n",
        "                config_used = param_configs.get(engine, {})\n",
        "\n",
        "                # Hiperparmetros ptimos extrados del modelo\n",
        "                best_params = {param: getattr(model, param, None) for param in ['C', 'epsilon', 'kernel']}\n",
        "\n",
        "                # Construir fila\n",
        "                row = {\n",
        "                    'Seleccin X': sel_method,\n",
        "                    'Motor': engine,\n",
        "                    'Mtrica': metric,\n",
        "                    'Score': score,\n",
        "                }\n",
        "\n",
        "                # Aadir configuracin de bsqueda\n",
        "                for k, v in config_used.items():\n",
        "                    row[f'Grid_{k}'] = v\n",
        "                # Aadir mejores hiperparmetros\n",
        "                for k, v in best_params.items():\n",
        "                    row[f'Best_{k}'] = v\n",
        "\n",
        "                summary_records.append(row)\n",
        "\n",
        "            # Crear DataFrame\n",
        "            df_summary = pd.DataFrame(summary_records)\n",
        "            print(\"[DEBUG] 12.3. df_summary.columns:\", df_summary.columns.tolist())\n",
        "            # Reordenar columnas para claridad\n",
        "            desired_cols = [\n",
        "                'Seleccin X', 'Motor', 'Mtrica', 'Score',\n",
        "                'Grid_C', 'Grid_epsilon', 'Grid_kernel',\n",
        "                'Best_C', 'Best_epsilon', 'Best_kernel'\n",
        "            ]\n",
        "            available_cols = [c for c in desired_cols if c in df_summary.columns]\n",
        "            if not available_cols:\n",
        "                print(\"[WARNING] Ninguna de las columnas deseadas est presente en df_summary; mostrando todo el DataFrame.\")\n",
        "                df_to_show = df_summary\n",
        "            else:\n",
        "                df_to_show = df_summary[available_cols]\n",
        "\n",
        "            # Aadir seccin de tabla al informe\n",
        "            self.sections.append((\n",
        "                \"### SVR Optimizacin: Resumen de Mtodos y Motores\",\n",
        "                df_to_show.reset_index(drop=True)\n",
        "            ))\n",
        "            # ----------------------- 0. Anlisis Generativo con OpenAI -------------------------------------------------\n",
        "            # Preparar funcin de llamada a OpenAI\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un experto en optimizacin de hiperparmetros de modelos de ML. \"\n",
        "                                \"Proporciona anlisis profundo, interpretaciones y recomendaciones.\" )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS\n",
        "                    )\n",
        "                    return response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    return f\"[Error llamando a OpenAI: {e}]\"\n",
        "\n",
        "            # Construir prompt con contexto y todos los registros\n",
        "            records_text = []\n",
        "            for rec in summary_records:\n",
        "                part = (f\"Mtodo X: {rec['Seleccin X']}, Motor: {rec['Motor']}, Mtrica: {rec['Mtrica']}, Score: {rec['Score']:.4f}, \"\n",
        "                        f\"Configuracin de bsqueda: C={rec.get('Grid_C')}, epsilon={rec.get('Grid_epsilon')}, kernel={rec.get('Grid_kernel')}. \"\n",
        "                        f\"Hiperparmetros ptimos: C={rec.get('Best_C')}, epsilon={rec.get('Best_epsilon')}, kernel={rec.get('Best_kernel')}.\")\n",
        "                records_text.append(part)\n",
        "            prompt = (\n",
        "                \"He obtenido los siguientes resultados de optimizacin para SVR:\\n\" +\n",
        "                \"\\n\".join(records_text) +\n",
        "                \"\\n\\n\" +\n",
        "                \"1. Explica el funcionamiento y la estrategia de bsqueda de cada motor (GridSearchCV, RandomizedSearchCV, Optuna, BayesSearchCV).\\n\" +\n",
        "                \"2. Analiza comparativamente los scores obtenidos por cada optimizacin y justifica cul es la mejor configuracin.\\n\" +\n",
        "                \"3. Ofrece recomendaciones de posibles mejoras cambiando parmetros de ajuste u otros factores para incrementar el desempeo.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 12.4. Llamando a OpenAI para anlisis generativo SVR optimizacin\")\n",
        "            generative_analysis = call_openai_explanation(prompt)\n",
        "            self.sections.append((\n",
        "                \"### SVR Optimizacin: Anlisis Generativo\",\n",
        "                generative_analysis\n",
        "            ))\n",
        "            # ---- Fin bloque 0 ----\n",
        "\n",
        "            # --- 1. Curvas de Ajuste Real vs. Predicho y de Residuos ---\n",
        "            print(\"[DEBUG] 12.5. Iniciando seccin Calidad Ajuste SVR optimizado\")\n",
        "            # Seleccionar mejor registro por Score\n",
        "            best_rec = max(summary_records, key=lambda x: x['Score'])\n",
        "            sel_method, engine = best_rec['Seleccin X'], best_rec['Motor']\n",
        "\n",
        "            #  1.1 Normalizar payload \n",
        "            payload_raw = OPT_MODELS[('svr', sel_method, engine)]\n",
        "            p           = _normalize_payload(payload_raw)\n",
        "\n",
        "            model, sx, sy, raw_cols = p['model'], p['sx'], p['sy'], p['cols']\n",
        "            score, metric           = p['score'], p['metric']\n",
        "            best_params             = p['best_params']\n",
        "            #  Fin normalizacin \n",
        "\n",
        "            #  1.1.1 Sanitizacin y filtro unificado de columnas \n",
        "            sanitized_cols = [sanitize_name(c) for c in raw_cols]\n",
        "            missing = set(sanitized_cols) - set(X_test.columns)\n",
        "            if missing:\n",
        "                print(f\"[WARNING] SVR omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "            cols_valid = [c for c in sanitized_cols if c in X_test.columns]\n",
        "\n",
        "            # Reemplaza cualquier X_test_sel previo por esta versin saneada\n",
        "            X_test_sel = X_test[cols_valid].copy()\n",
        "            #  Fin de sanitizacin \n",
        "\n",
        "            #  1.1.1 Saneamiento y filtro de columnas \n",
        "            #import re\n",
        "            #def clean_name(s):\n",
        "            #    # idntico a tu celda 9.x\n",
        "            #    t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))\n",
        "            #    t = re.sub(r'_+', '_', t)\n",
        "            #    return t.strip('_')\n",
        "\n",
        "            # Limpio cada nombre y luego lo filtro\n",
        "            #cols = [ clean_name(c) for c in raw_cols ]\n",
        "            #cols_valid = [c for c in cols if c in X_test.columns]\n",
        "            #missing = set(cols) - set(cols_valid)\n",
        "            #if missing:\n",
        "            #    print(f\"[WARNING] SVR omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "\n",
        "            # Ahora ya puedo indexar con total seguridad\n",
        "            #X_test_sel = X_test[cols_valid].copy()\n",
        "            y_true     = Y_test.values.ravel()\n",
        "            X_test_scaled = sx.transform(X_test_sel)\n",
        "            y_pred     = sy.inverse_transform(\n",
        "                            model.predict(X_test_scaled).reshape(-1,1)\n",
        "                        ).ravel()\n",
        "            #  Fin saneamiento SVR \n",
        "\n",
        "            #  1.1 Normalizar payload\n",
        "            #payload_raw        = OPT_MODELS[('svr', sel_method, engine)]\n",
        "            #p                  = _normalize_payload(payload_raw)\n",
        "\n",
        "            #model, sx, sy, cols = p['model'], p['sx'], p['sy'], p['cols']\n",
        "            #score, metric      = p['score'], p['metric']\n",
        "            #best_params        = p['best_params']\n",
        "            #  Fin normalizacin\n",
        "\n",
        "            #  Unificar saneamiento de columnas segn entrenamiento \n",
        "            #import re\n",
        "            #def clean_name(s):\n",
        "            #    t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))\n",
        "            #    t = re.sub(r'_+', '_', t)\n",
        "            #    return t.strip('_')\n",
        "\n",
        "            #cols = [ clean_name(c) for c in cols ]\n",
        "            #  Fin unificacin \n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            #X_test_sel = X_test[cols].copy()\n",
        "            #y_true = Y_test.values.ravel()\n",
        "            #X_test_scaled = sx.transform(X_test_sel)\n",
        "            #y_pred = sy.inverse_transform(model.predict(X_test_scaled).reshape(-1,1)).ravel()\n",
        "\n",
        "            # Clculo de residuos y estadsticas\n",
        "            residuals = y_true - y_pred\n",
        "            mean_res = float(np.mean(residuals))\n",
        "            std_res = float(np.std(residuals))\n",
        "            skew_res = float(skew(residuals))\n",
        "            kurt_res = float(kurtosis(residuals))\n",
        "            q25, q50, q75 = [float(x) for x in np.quantile(residuals, [0.25, 0.5, 0.75])]\n",
        "\n",
        "            # Grfica Predicho vs Real\n",
        "            fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "            ax1.scatter(y_true, y_pred, alpha=0.6)\n",
        "            ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "            ax1.set_xlabel(\"Y real\")\n",
        "            ax1.set_ylabel(\"Y predicho\")\n",
        "            ax1.set_title(f\"SVR Optimizado (Mejor: {sel_method}-{engine}) Predicho vs Real\")\n",
        "            self.sections.append((\n",
        "                f\"### SVR Optimizado: Predicho vs Real ({sel_method}-{engine})\", fig1\n",
        "            ))\n",
        "\n",
        "            # Grfica Residuos\n",
        "            fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "            ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "            ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "            ax2.set_xlabel(\"Y predicho\")\n",
        "            ax2.set_ylabel(\"Residuo\")\n",
        "            ax2.set_title(f\"SVR Optimizado (Mejor) Residuos ({sel_method}-{engine})\")\n",
        "            self.sections.append((\n",
        "                f\"### SVR Optimizado: Residuos ({sel_method}-{engine})\", fig2\n",
        "            ))\n",
        "\n",
        "            # Tabla de estadsticas de residuos\n",
        "            df_stats = pd.DataFrame({\n",
        "                'Mtrica': ['Media', 'Desviacin', 'Skew', 'Kurtosis', '25%', '50%', '75%'],\n",
        "                'Valor': [mean_res, std_res, skew_res, kurt_res, q25, q50, q75]\n",
        "            })\n",
        "            self.sections.append((\n",
        "                f\"### SVR Optimizado: Estadsticas de Residuos ({sel_method}-{engine})\", df_stats\n",
        "            ))\n",
        "\n",
        "            # Anlisis generativo de calidad de ajuste\n",
        "            prompt2 = (\n",
        "                f\"Para el mejor modelo SVR optimizado con seleccin {sel_method} y motor {engine}, \"\n",
        "                f\"tienes los siguientes datos:\\n\"\n",
        "                f\"- Rango Y real: [{float(y_true.min()):.4f}, {float(y_true.max()):.4f}]\\n\"\n",
        "                f\"- Rango Y pred: [{float(y_pred.min()):.4f}, {float(y_pred.max()):.4f}]\\n\"\n",
        "                f\"- Estadsticas residuos: media={mean_res:.4f}, std={std_res:.4f}, skew={skew_res:.4f}, kurtosis={kurt_res:.4f}, \"\n",
        "                f\"quantiles 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}.\\n\\n\"\n",
        "                \"1. Analiza la calidad del ajuste basndote en la grfica Predicho vs Real y la de residuos.\\n\"\n",
        "                \"2. Comenta sobre sesgos sistemticos, heterocedasticidad y normalidad de errores.\\n\"\n",
        "                \"3. Ofrece recomendaciones para mejorar el fit si hay problemas detectados.\"\n",
        "            )\n",
        "            analysis2 = call_openai_explanation(prompt2)\n",
        "            self.sections.append((\n",
        "                \"### SVR Optimizado: Anlisis Calidad Ajuste\", analysis2\n",
        "            ))\n",
        "            # ---- Fin bloque 1 ----\n",
        "\n",
        "            # --- 2. Importancia Relativa de Hiperparmetros ---\n",
        "            import seaborn as sns\n",
        "            # 2.1 Heatmap Score vs Best_C y Best_epsilon\n",
        "            heat = df_summary.pivot(index='Best_C', columns='Best_epsilon', values='Score')\n",
        "            fig_heat, ax_heat = plt.subplots(figsize=(6,5))\n",
        "            sns.heatmap(heat, annot=True, fmt='.4f', ax=ax_heat)\n",
        "            ax_heat.set_title('SVR Optimizado: Heatmap Score vs C y ')\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Heatmap Score vs C y ', fig_heat\n",
        "            ))\n",
        "\n",
        "            # 2.2 Sensibilidad 10%\n",
        "            sens = []\n",
        "            for rec in summary_records:\n",
        "                for param in ['C','epsilon']:\n",
        "                    base = rec[f'Best_{param}']\n",
        "                    if base is None: continue\n",
        "                    for factor,label in [(1.1,'+10%'),(0.9,'-10%')]:\n",
        "                        sens.append({\n",
        "                            'Parmetro':param,\n",
        "                            'Cambio':label,\n",
        "                            '% Score':rec['Score']*(factor-1)*100,\n",
        "                            'Seleccin X':rec['Seleccin X'],\n",
        "                            'Motor':rec['Motor']\n",
        "                        })\n",
        "            df_sens = pd.DataFrame(sens)\n",
        "            fig_sens, ax_sens = plt.subplots(figsize=(6,4))\n",
        "            sns.barplot(data=df_sens, x='% Score', y='Parmetro', hue='Cambio', ax=ax_sens)\n",
        "            ax_sens.set_title('SVR Opt: Sensibilidad Score 10%')\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Sensibilidad del Score', fig_sens\n",
        "            ))\n",
        "\n",
        "            # 2.3 Anlisis IA de Importancia\n",
        "            def call_openai_explanation(prompt, model='gpt-4'):\n",
        "                resp = _client.chat.completions.create(\n",
        "                    model=model,\n",
        "                    messages=[\n",
        "                        {'role':'system','content':'Eres un experto en anlisis de hiperparmetros de ML.'},\n",
        "                        {'role':'user','content':prompt}\n",
        "                    ],\n",
        "                    temperature=TEMPERATURE_VAL,\n",
        "                    max_tokens=MAX_EXPLANATION_TOKENS\n",
        "                )\n",
        "                return resp.choices[0].message.content.strip()\n",
        "\n",
        "            lines = [f\"{r['Parmetro']} {r['Cambio']} => {r['% Score']:.2f}%\" for r in sens]\n",
        "            prompt_param = (\n",
        "                \"Sensibilidad de Score al 10% en C y :\\n\" + \"\\n\".join(lines) +\n",
        "                \"\\n\\nExplica qu hiperparmetro impulsa ms mejora y por qu, y sugiere focos de tuning.\"    )\n",
        "            print(\"[DEBUG] 12.6. Llamando IA importancia hiperparmetros\")\n",
        "            analysis_hp = call_openai_explanation(prompt_param)\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: IA Importancia Hiperparmetros', analysis_hp\n",
        "            ))\n",
        "            # ---- Fin bloque 2 ----\n",
        "\n",
        "            # --- 3. Distribucin de Mtricas en Validacin Cruzada ---\n",
        "            from sklearn.model_selection import cross_validate\n",
        "            import seaborn as sns\n",
        "            print(\"[DEBUG] 12.7. Calculando distribucin de mtricas CV para el mejor modelo optimizado\")\n",
        "\n",
        "            # 3. Identificar mejor modelo optimizado\n",
        "            best_rec       = max(summary_records, key=lambda x: x['Score'])\n",
        "            sel_method_cv, engine_cv = best_rec['Seleccin X'], best_rec['Motor']\n",
        "\n",
        "            # --- Normalizar extraccin de payload ---\n",
        "            payload_raw_cv = OPT_MODELS[('svr', sel_method_cv, engine_cv)]\n",
        "            p_cv           = _normalize_payload(payload_raw_cv)\n",
        "            model_cv, sx_cv, sy_cv, cols_cv = (\n",
        "                p_cv['model'], p_cv['sx'], p_cv['sy'], p_cv['cols']\n",
        "            )\n",
        "            # ----------------------------------------\n",
        "\n",
        "            # Preparar datos de entrenamiento escalados\n",
        "            X_cv = X_train[cols_cv].copy()\n",
        "            y_cv = Y_train.values.ravel()\n",
        "            X_cv_scaled = sx_cv.transform(X_cv)\n",
        "\n",
        "            # Cross-validate con mltiples mtricas\n",
        "            cv_results = cross_validate(\n",
        "                model_cv, X_cv_scaled, y_cv,\n",
        "                cv=5,\n",
        "                scoring={\n",
        "                    'r2':'r2',\n",
        "                    'neg_mse':'neg_mean_squared_error',\n",
        "                    'neg_mae':'neg_mean_absolute_error'\n",
        "                },\n",
        "                return_train_score=False\n",
        "            )\n",
        "\n",
        "            # Procesar resultados\n",
        "            r2_scores   = cv_results['test_r2']\n",
        "            mse_scores  = [-v for v in cv_results['test_neg_mse']]\n",
        "            mae_scores  = [-v for v in cv_results['test_neg_mae']]\n",
        "            rmse_scores = np.sqrt(mse_scores)\n",
        "\n",
        "            df_cv = pd.DataFrame({\n",
        "                'R2': r2_scores,\n",
        "                'MSE': mse_scores,\n",
        "                'MAE': mae_scores,\n",
        "                'RMSE': rmse_scores\n",
        "            })\n",
        "\n",
        "            # 3.1 Boxplot de mtricas por fold\n",
        "            fig_cv, ax_cv = plt.subplots(figsize=(6, 4))\n",
        "            sns.boxplot(data=df_cv, ax=ax_cv)\n",
        "            ax_cv.set_title('SVR Optimizado: Distribucin de Mtricas CV')\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Distribucin de Mtricas CV', fig_cv\n",
        "            ))\n",
        "\n",
        "            # 3.2 Tabla con media  desviacin\n",
        "            stats_cv = df_cv.agg(['mean', 'std']).T.reset_index().rename(columns={\n",
        "                'index':'Mtrica', 'mean':'Media', 'std':'Desviacin'\n",
        "            })\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Estadsticas CV por Fold', stats_cv\n",
        "            ))\n",
        "\n",
        "            # 3.3 Anlisis Generativo IA de estabilidad\n",
        "            prompt_cv = (\n",
        "                f\"Valores CV por fold (R2: {r2_scores.tolist()}, MAE: {mae_scores}, RMSE: {rmse_scores}).\\n\"\n",
        "                \"Qu nos dice la dispersin sobre la estabilidad del modelo?\"\n",
        "            )\n",
        "            print(\"[DEBUG] 12.8. Llamando IA para estabilidad en CV\")\n",
        "            analysis_cv = call_openai_explanation(prompt_cv)\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Anlisis Estabilidad CV', analysis_cv\n",
        "            ))\n",
        "            # ---- Fin bloque 3 ----\n",
        "\n",
        "            # --- 4. Seccin Calidad Ajuste Mejor Modelo Optimizado SVR ---\n",
        "            # --- 4.1. Curvas de Aprendizaje y Validacin para SVR Optimizado ---\n",
        "            from sklearn.model_selection import learning_curve, validation_curve\n",
        "            print(\"[DEBUG] 12.9. Calculando curvas de aprendizaje y validacin para SVR optimizado\")\n",
        "\n",
        "            # 4.1 Seleccionar el mejor payload para curvas de aprendizaje\n",
        "            best_rec       = max(summary_records, key=lambda x: x['Score'])\n",
        "            sel_method, engine = best_rec['Seleccin X'], best_rec['Motor']\n",
        "\n",
        "            # --- Normalizamos la extraccin del payload ---\n",
        "            payload_raw    = OPT_MODELS[('svr', sel_method, engine)]\n",
        "            p              = _normalize_payload(payload_raw)\n",
        "            model, sx, sy, cols = p['model'], p['sx'], p['sy'], p['cols']\n",
        "            # -----------------------------------------------\n",
        "\n",
        "            # Preparar datos de entrenamiento\n",
        "            X_train_sel = X_train[cols].copy()\n",
        "            y_train = Y_train.values.ravel()\n",
        "            X_train_scaled = sx.transform(X_train_sel)\n",
        "\n",
        "            # Curva de aprendizaje (R)\n",
        "            train_sizes, train_scores, val_scores = learning_curve(\n",
        "                model, X_train_scaled, y_train,\n",
        "                cv=3, scoring='r2', train_sizes=np.linspace(0.1, 1.0, 5), n_jobs=-1\n",
        "            )\n",
        "            train_mean = np.mean(train_scores, axis=1)\n",
        "            val_mean = np.mean(val_scores, axis=1)\n",
        "            fig_lc, ax_lc = plt.subplots(figsize=(6, 4))\n",
        "            ax_lc.plot(train_sizes, train_mean, 'o-', label='Train R')\n",
        "            ax_lc.plot(train_sizes, val_mean, 'o-', label='CV R')\n",
        "            ax_lc.set_title('Curva de Aprendizaje SVR Optimizado')\n",
        "            ax_lc.set_xlabel('Tamao del set de entrenamiento')\n",
        "            ax_lc.set_ylabel('R')\n",
        "            ax_lc.legend()\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Curva de Aprendizaje', fig_lc\n",
        "            ))\n",
        "\n",
        "            # --- 4.2. Curva de validacin para hiperparmetro C\n",
        "            param_range_C = np.logspace(-1, 2, 5)\n",
        "            train_scores_C, val_scores_C = validation_curve(\n",
        "                model, X_train_scaled, y_train,\n",
        "                param_name='C', param_range=param_range_C,\n",
        "                cv=3, scoring='r2', n_jobs=-1\n",
        "            )\n",
        "            fig_vc_C, ax_vc_C = plt.subplots(figsize=(6, 4))\n",
        "            ax_vc_C.plot(param_range_C, np.mean(train_scores_C, axis=1), 'o-', label='Train R')\n",
        "            ax_vc_C.plot(param_range_C, np.mean(val_scores_C, axis=1), 'o-', label='CV R')\n",
        "            ax_vc_C.set_xscale('log')\n",
        "            ax_vc_C.set_title('Curva de Validacin: C')\n",
        "            ax_vc_C.set_xlabel('C')\n",
        "            ax_vc_C.set_ylabel('R')\n",
        "            ax_vc_C.legend()\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Curva de Validacin C', fig_vc_C\n",
        "            ))\n",
        "\n",
        "            # --- 4.3. Curva de validacin para hiperparmetro epsilon\n",
        "            param_range_e = np.logspace(-2, 0, 5)\n",
        "            train_scores_e, val_scores_e = validation_curve(\n",
        "                model, X_train_scaled, y_train,\n",
        "                param_name='epsilon', param_range=param_range_e,\n",
        "                cv=3, scoring='r2', n_jobs=-1\n",
        "            )\n",
        "            fig_vc_e, ax_vc_e = plt.subplots(figsize=(6, 4))\n",
        "            ax_vc_e.plot(param_range_e, np.mean(train_scores_e, axis=1), 'o-', label='Train R')\n",
        "            ax_vc_e.plot(param_range_e, np.mean(val_scores_e, axis=1), 'o-', label='CV R')\n",
        "            ax_vc_e.set_xscale('log')\n",
        "            ax_vc_e.set_title('Curva de Validacin: Epsilon')\n",
        "            ax_vc_e.set_xlabel('Epsilon')\n",
        "            ax_vc_e.set_ylabel('R')\n",
        "            ax_vc_e.legend()\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Curva de Validacin Epsilon', fig_vc_e\n",
        "            ))\n",
        "\n",
        "            # --- 4.4. Interpretacin IA de las Curvas de Aprendizaje y Validacin ---\n",
        "            # Extraemos estadsticas para el prompt\n",
        "            lc_train = train_mean.tolist()\n",
        "            lc_val   = val_mean.tolist()\n",
        "            vc_C_train = np.mean(train_scores_C, axis=1).tolist()\n",
        "            vc_C_val   = np.mean(val_scores_C, axis=1).tolist()\n",
        "            vc_e_train = np.mean(train_scores_e, axis=1).tolist()\n",
        "            vc_e_val   = np.mean(val_scores_e, axis=1).tolist()\n",
        "            sizes = train_sizes.tolist()\n",
        "            C_vals = param_range_C.tolist()\n",
        "            e_vals = param_range_e.tolist()\n",
        "\n",
        "            prompt_curvas = (\n",
        "                \"He generado para el SVR optimizado las siguientes curvas:\\n\"\n",
        "                f\"- Curva de Aprendizaje (R): tamaos={sizes}, train={lc_train}, CV={lc_val}\\n\"\n",
        "                f\"- Curva de Validacin para C (R): C={C_vals}, train={vc_C_train}, CV={vc_C_val}\\n\"\n",
        "                f\"- Curva de Validacin para  (R): ={e_vals}, train={vc_e_train}, CV={vc_e_val}\\n\\n\"\n",
        "                \"1. Interpreta cada curva: qu nos dice sobre sesgo (under-/overfitting) y varianza?\\n\"\n",
        "                \"2. Seala si hay seales de under-/overfitting o alta varianza segn cada grfica.\\n\"\n",
        "                \"3. Concluye recomendaciones generales para mejorar el aprendizaje (por ejemplo, ajustar C, , ms datos, regularizacin, etc.).\"\n",
        "            )\n",
        "\n",
        "            print(\"[DEBUG] 12.10. Llamando a OpenAI para interpretacin IA de curvas\")\n",
        "            analisis_curvas = call_openai_explanation(prompt_curvas)\n",
        "            self.sections.append((\n",
        "                \"### SVR Optimizado: Interpretacin IA de Curvas\",\n",
        "                analisis_curvas\n",
        "            ))\n",
        "            # ---- Fin bloque 4 ----\n",
        "\n",
        "            # --- 5. Curvas de Calibracin y Prediccin de Intervalos ---\n",
        "            from sklearn.calibration import calibration_curve\n",
        "            print(\"[DEBUG] 12.11. Calculando curva de calibracin y prediccin de intervalos para el mejor modelo optimizado SVR\")\n",
        "\n",
        "            # Identificar mejor modelo optimizado\n",
        "            best_rec_ci       = max(summary_records, key=lambda x: x['Score'])\n",
        "            sel_method_ci, engine_ci = best_rec_ci['Seleccin X'], best_rec_ci['Motor']\n",
        "\n",
        "            # --- Normalizamos la extraccin del payload ---\n",
        "            payload_raw_ci   = OPT_MODELS[('svr', sel_method_ci, engine_ci)]\n",
        "            p_ci             = _normalize_payload(payload_raw_ci)\n",
        "            model_ci, sx_ci, sy_ci, cols_ci = p_ci['model'], p_ci['sx'], p_ci['sy'], p_ci['cols']\n",
        "            # -----------------------------------------------\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            X_test_ci = X_test[cols_ci].copy()\n",
        "            y_true_ci = Y_test.values.ravel()\n",
        "            X_test_scaled_ci = sx_ci.transform(X_test_ci)\n",
        "\n",
        "            #  Prediccin raw \n",
        "            y_pred_raw_ci = model_ci.predict(X_test_scaled_ci)\n",
        "\n",
        "            #  Desescalado (si tienes sy_ci) \n",
        "            if sy_ci is not None:\n",
        "                # sy_ci es tu StandardScaler para y\n",
        "                y_pred_ci = sy_ci.inverse_transform(\n",
        "                    y_pred_raw_ci.reshape(-1,1)\n",
        "                ).ravel()\n",
        "            else:\n",
        "                y_pred_ci = y_pred_raw_ci.ravel()\n",
        "\n",
        "            # 5.1 Curva de calibracin (binned reliability plot para regresin)\n",
        "            import pandas as _pd\n",
        "            bins = 10\n",
        "            print(\"[DEBUG] 12.12. Calculando curva de calibracin manual para regresin SVR optimizado\")\n",
        "            df_cal = _pd.DataFrame({'y_pred': y_pred_ci, 'y_true': y_true_ci})\n",
        "            # Crear bins segn cuantiles en prediccin\n",
        "            try:\n",
        "                df_cal['bin'] = _pd.qcut(df_cal['y_pred'], q=bins, duplicates='drop')\n",
        "            except Exception:\n",
        "                df_cal['bin'] = _pd.cut(df_cal['y_pred'], bins=bins)\n",
        "            # Agrupar para obtener promedio predicho vs observado\n",
        "            #grp = df_cal.groupby('bin').agg({'y_pred': 'mean', 'y_true': 'mean'})\n",
        "            grp = df_cal.groupby('bin', observed=True).agg({'y_pred': 'mean', 'y_true': 'mean'})\n",
        "            prob_pred = grp['y_pred'].values\n",
        "            prob_true = grp['y_true'].values\n",
        "            fig_cal, ax_cal = plt.subplots(figsize=(6, 4))\n",
        "            ax_cal.plot(prob_pred, prob_true, marker='o', linewidth=2)\n",
        "            ax_cal.plot([prob_pred.min(), prob_pred.max()], [prob_pred.min(), prob_pred.max()], 'k--')\n",
        "            ax_cal.set_xlabel('Prediccin promedio en bin')\n",
        "            ax_cal.set_ylabel('Valor real promedio')\n",
        "            ax_cal.set_title('SVR Optimizado: Curva de Calibracin')\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Curva de Calibracin', fig_cal\n",
        "            ))\n",
        "\n",
        "            # 5.2 Intervalos de prediccin 1 STD de residuos\n",
        "            residuals_ci = y_true_ci - y_pred_ci\n",
        "            std_res_ci = np.std(residuals_ci)\n",
        "            upper = y_pred_ci + std_res_ci\n",
        "            lower = y_pred_ci - std_res_ci\n",
        "            fig_int, ax_int = plt.subplots(figsize=(6, 4))\n",
        "            ax_int.plot(y_true_ci, label='Y real')\n",
        "            ax_int.plot(y_pred_ci, label='Prediccin')\n",
        "            ax_int.fill_between(range(len(y_pred_ci)), lower, upper, alpha=0.3, label='1 STD residuo')\n",
        "            ax_int.set_xlabel('ndice de muestra')\n",
        "            ax_int.set_ylabel('Valor')\n",
        "            ax_int.set_title('SVR Optimizado: Intervalos de Prediccin')\n",
        "            ax_int.legend()\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Intervalos de Prediccin', fig_int\n",
        "            ))\n",
        "\n",
        "            # 5.3 Anlisis Generativo IA de incertidumbre y calibracin\n",
        "            prompt_ci = (\n",
        "                f\"Curva de calibracin (prob_pred: {prob_pred.tolist()}, prob_true: {prob_true.tolist()}) y \"\n",
        "                f\"intervalos de prediccin 1 STD (std_res={std_res_ci:.4f}).\\n\"\n",
        "                \"1. Son fiables los intervalos de incertidumbre?\"\n",
        "                \"2. Observas infravaloracin de errores altos?\"\n",
        "            )\n",
        "            print(\"[DEBUG] 12.13. Llamando IA para anlisis de incertidumbre y calibracin SVR\")\n",
        "            analysis_ci = call_openai_explanation(prompt_ci)\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Anlisis de Incertidumbre y Calibracin', analysis_ci\n",
        "            ))\n",
        "            # ---- Fin bloque 5 ----\n",
        "\n",
        "            # --- Seccin 6: Resumen Ejecutivo y Road-Map de Siguientes Pasos ---\n",
        "            # 6.1. Bloque Markdown con puntos clave para stakeholders\n",
        "            summary_md = (\n",
        "                \"**Puntos Clave:**\\n\"\n",
        "                f\"- **Mejor combinacin:** {sel_method_ci}-{engine_ci} con score={best_rec_ci['Score']:.4f}.\\n\"\n",
        "                f\"- **Grado de robustez:** Variabilidad CV={val_mean.std():.4f}, residuos (std={std_res_ci:.4f}).\\n\"\n",
        "                f\"- **Puntos dbiles:** identificar picos de error y alta varianza en ciertos rangos.\\n\"\n",
        "                f\"- **Recomendaciones inmediatas:** ampliar CV, explorar HalvingGridSearchCV, recolectar ms datos.\"\n",
        "            )\n",
        "            self.sections.append((\n",
        "                '### Resumen Ejecutivo y Road-Map', summary_md\n",
        "            ))\n",
        "\n",
        "            # 6.2. Anlisis Generativo IA para desarrollar cada punto clave y generar resumen ejecutivo\n",
        "            prompt_exec = (\n",
        "                \"Eres un investigador cientfico para el Instituto de Procesos Sostenibles de la Universidad de Valladolid. \"\n",
        "                \"A continuacin se presentan los puntos clave de la optimizacin SVR:\\n\"\n",
        "                f\"{summary_md}\\n\"\n",
        "                \"Desarrolla un anlisis detallado en prrafos separados para cada punto (Mejor combinacin, Grado de robustez, Puntos dbiles, Recomendaciones inmediatas), \"\n",
        "                \"y finaliza con un resumen ejecutivo de 3 prrafos resumiendo hallazgos y prximos pasos para los stakeholders.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 12.14. Llamando IA para Resumen Ejecutivo y Road-Map SVR optimizacin\")\n",
        "            analysis_exec = call_openai_explanation(prompt_exec)\n",
        "            self.sections.append((\n",
        "                '### Resumen Ejecutivo IA', analysis_exec\n",
        "            ))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"###  Error en seccin Optimizacin SVR\",\n",
        "                f\"Se produjo un error al generar el resumen de mtodos y motores: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =============================================================================\n",
        "        # 13. Optimizacin Modelo Optimizacin NN\n",
        "        # =============================================================================\n",
        "        try:\n",
        "            print(\"[DEBUG] 13.1. Iniciando seccin Optimizacin NN: Resumen de mtodos y motores\")\n",
        "            # Verificacin previa: usamos OPT_MODELS guardado en la celda 9.2\n",
        "            if 'OPT_MODELS' in globals() and isinstance(OPT_MODELS, dict):\n",
        "                valid_engines = {'randomsearch', 'bayesianoptimization', 'hyperband', 'optuna'}\n",
        "                nn_entries = {\n",
        "                    k: v for k, v in OPT_MODELS.items()\n",
        "                    if (isinstance(k, tuple) and k[0] == 'nn' and isinstance(v, dict) and k[2] in valid_engines)\n",
        "                }\n",
        "            else:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontr la variable global OPT_MODELS con resultados de optimizacin NN. \"\n",
        "                    \"Asegrate de haber ejecutado la Celda 9.2 y de que OPT_MODELS contenga los payloads optimizados.\"\n",
        "                )\n",
        "\n",
        "            import pickle\n",
        "            import pandas as pd\n",
        "            import numpy as np\n",
        "            import matplotlib.pyplot as plt\n",
        "            import seaborn as sns\n",
        "            from scipy.stats import skew, kurtosis\n",
        "            from tensorflow.keras.models import load_model\n",
        "            import tensorflow as tf\n",
        "            from tensorflow import keras\n",
        "            from tensorflow.keras import layers as keras_layers\n",
        "            from scikeras.wrappers import KerasRegressor\n",
        "            from sklearn.model_selection import learning_curve\n",
        "\n",
        "            #import re\n",
        "\n",
        "            #def _clean_col(name):\n",
        "            #    # sustituye [, ], <, >, % por _\n",
        "            #    return re.sub(r'[\\[\\]<>%]', '_', str(name))\n",
        "\n",
        "            #  1.1.1 Sanitizacin y filtro unificado de columnas \n",
        "            #sanitized_cols = [sanitize_name(c) for c in raw_cols]\n",
        "            #missing = set(sanitized_cols) - set(X_test.columns)\n",
        "            #if missing:\n",
        "            #    print(f\"[WARNING] SVR omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "            #cols_valid = [c for c in sanitized_cols if c in X_test.columns]\n",
        "\n",
        "            #X_test_sel = X_test[cols_valid].copy()\n",
        "\n",
        "            # Parmetros de bsqueda usados en cada motor (para documentar)\n",
        "            param_configs = {\n",
        "                'randomsearch': { 'layers': 'hp.Int(\"layers\", 1, max_layers)', 'units': 'hp.Int(\"units_i\", min_units, max_units)', 'dropout': 'hp.Float(\"dropout_i\", 0.0, max_dropout)' },\n",
        "                'bayesianoptimization': { 'layers': 'hp.Int(\"layers\", 1, max_layers)', 'units': 'hp.Int(\"units_i\", min_units, max_units)', 'dropout': 'hp.Float(\"dropout_i\", 0.0, max_dropout)' },\n",
        "                'hyperband': { 'layers': 'hp.Int(\"layers\", 1, max_layers)', 'units': 'hp.Int(\"units_i\", min_units, max_units)', 'dropout': 'hp.Float(\"dropout_i\", 0.0, max_dropout)' },\n",
        "                'optuna': { 'layers': 'suggest_int(\"n_layers\", 1, max_layers)', 'units': 'suggest_int(\"n_units_l{i}\", min_units, max_units)', 'dropout': 'suggest_float(\"dropout_l{i}\", 0.0, max_dropout)' }\n",
        "            }\n",
        "\n",
        "            summary_records = []\n",
        "            # Iterar cada combinacin mtodo/motor\n",
        "            for (_, sel_method, engine), payload in nn_entries.items():\n",
        "                print(f\"[DEBUG] 13.2. Agregando registro resumen NN: {sel_method} / {engine}\")\n",
        "\n",
        "                # 1) mtricas\n",
        "                score = payload.get('score')\n",
        "                metric = payload.get('metric')\n",
        "\n",
        "                # Parmetros de bsqueda documentados\n",
        "                config_used = param_configs.get(engine, {})\n",
        "\n",
        "                # 3) hiperparmetros ptimos\n",
        "                if engine in (\"randomsearch\",\"bayesianoptimization\",\"hyperband\"):\n",
        "                    best_params = {\n",
        "                        \"layers\":  payload.get(\"layers\"),\n",
        "                        \"neurons\": payload.get(\"neurons\"),\n",
        "                        \"dropout\": payload.get(\"dropout\"),\n",
        "                        \"epochs\":  payload.get(\"epochs\")\n",
        "                    }\n",
        "                else:  # optuna\n",
        "                    best_params = {\n",
        "                        \"layers\":  payload.get(\"n_layers\"),\n",
        "                        \"neurons\": payload.get(\"n_units_l0\"),\n",
        "                        \"dropout\": payload.get(\"dropout_l0\"),\n",
        "                        \"epochs\":  payload.get(\"epochs\")\n",
        "                    }\n",
        "\n",
        "                # Construir fila de resumen\n",
        "                row = {\n",
        "                    'Seleccin X': sel_method,\n",
        "                    'Motor': engine,\n",
        "                    'Mtrica': metric,\n",
        "                    'Score': score\n",
        "                }\n",
        "                # Aadir configuracin de bsqueda al row\n",
        "                for k, v in config_used.items():\n",
        "                    row[f'Config_{k}'] = v\n",
        "                # Aadir mejores hiperparmetros al row\n",
        "                for k, v in best_params.items():\n",
        "                    row[f'Best_{k}'] = v\n",
        "\n",
        "                summary_records.append(row)\n",
        "\n",
        "            # Crear DataFrame de resumen y reordenar columnas\n",
        "            df_summary_nn = pd.DataFrame(summary_records)\n",
        "            desired_cols = [\n",
        "                'Seleccin X', 'Motor', 'Mtrica', 'Score',\n",
        "                'Config_layers', 'Config_units', 'Config_dropout',\n",
        "                'Best_layers', 'Best_neurons', 'Best_dropout', 'Best_epochs'\n",
        "            ]\n",
        "            available_cols = [c for c in desired_cols if c in df_summary_nn.columns]\n",
        "            df_to_show_nn = df_summary_nn[available_cols] if available_cols else df_summary_nn\n",
        "\n",
        "            # Aadir seccin de tabla al informe\n",
        "            self.sections.append((\n",
        "                \"### NN Optimizacin: Resumen de Mtodos y Motores\",\n",
        "                df_to_show_nn.reset_index(drop=True)\n",
        "            ))\n",
        "\n",
        "            #  Preseleccin global del mejor modelo NN \n",
        "            if df_summary_nn['Mtrica'].iloc[0] == 'R2':\n",
        "                idx_best_nn = df_summary_nn['Score'].idxmax()\n",
        "            else:\n",
        "                idx_best_nn = df_summary_nn['Score'].idxmin()\n",
        "\n",
        "            best_entry_nn = df_summary_nn.loc[idx_best_nn]\n",
        "            sel_method_nn = best_entry_nn['Seleccin X']\n",
        "            engine_nn     = best_entry_nn['Motor']\n",
        "\n",
        "            payload_raw = OPT_MODELS[('nn', sel_method, engine)]\n",
        "            p = _normalize_payload(payload_raw)\n",
        "            model        = p['model']\n",
        "            sx, sy, cols = p['sx'], p['sy'], p['cols']\n",
        "            score, metric, best_params = p['score'], p['metric'], p['best_params']\n",
        "\n",
        "            # Precargo payload_best para usar en todos los bloques\n",
        "            payload_best = p    # ya tienes el payload normalizado en p, no necesitas nn_entries aqu\n",
        "\n",
        "            # --- 0. Anlisis Generativo IA de Tabla de Resumen ---\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un experto en optimizacin de redes neuronales. \"\n",
        "                                \"Analiza los resultados y ofrece conclusiones detalladas.\" )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS\n",
        "                    )\n",
        "                    return response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    print(f\"[DEBUG] Exception atrapada en Optimizacin NN!: {type(e).__name__}: {e}\")\n",
        "                    return f\"[Error llamando a OpenAI: {e}]\"\n",
        "\n",
        "            # Construir prompt a partir de summary_records\n",
        "            records_text = []\n",
        "            for rec in summary_records:\n",
        "                records_text.append(\n",
        "                    f\"Mtodo X: {rec['Seleccin X']}, Motor: {rec['Motor']}, \"\n",
        "                    f\"Mtrica: {rec['Mtrica']}, Score: {rec['Score']:.4f}, \"\n",
        "                    f\"Config: layers={rec.get('Config_layers')}, units={rec.get('Config_units')}, dropout={rec.get('Config_dropout')}, \"\n",
        "                    f\"Best: layers={rec.get('Best_layers')}, neurons={rec.get('Best_neurons')}, \"\n",
        "                    f\"dropout={rec.get('Best_dropout')}, epochs={rec.get('Best_epochs')}\"\n",
        "                )\n",
        "            prompt = (\n",
        "                \"He obtenido los siguientes resultados de optimizacin para la Red Neuronal:\\n\"\n",
        "                + \"\\n\".join(records_text)\n",
        "                + \"\\n\\n\"\n",
        "                + \"1. Describe e interpreta cada combinacin mtodo/motor.\\n\"\n",
        "                + \"2. Seala cul configuracin ofrece mejor desempeo y por qu.\\n\"\n",
        "                + \"3. Proporciona recomendaciones para futuros ajustes de HPO.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.3. Llamando IA para anlisis generativo NN optimizacin\")\n",
        "            generative_analysis_nn = call_openai_explanation(prompt)\n",
        "            self.sections.append((\n",
        "                \"### NN Optimizacin: Anlisis Generativo\",\n",
        "                generative_analysis_nn\n",
        "            ))\n",
        "            # --- Fin Bloque 0 ---\n",
        "\n",
        "            # --- 1. Curvas de Ajuste Real vs. Predicho y Residuos ---\n",
        "            print(\"[DEBUG] 13.4. Entrando en Bloque 1: Curvas Predicho vs Real\")\n",
        "            from scipy.stats import skew, kurtosis\n",
        "            # Seleccionar mejor configuracin segn Score\n",
        "            if df_summary_nn.empty:\n",
        "                raise RuntimeError(\"No hay registros de optimizacin NN para generar curvas.\")\n",
        "            # Definir criterio de ordenamiento: maximizar R2, minimizar errores\n",
        "            if df_summary_nn['Mtrica'].iloc[0] == 'R2':\n",
        "                idx_best = df_summary_nn['Score'].idxmax()\n",
        "            else:\n",
        "                idx_best = df_summary_nn['Score'].idxmin()\n",
        "            best_row = df_summary_nn.loc[idx_best]\n",
        "            sel_method = best_row['Seleccin X']\n",
        "            engine = best_row['Motor']\n",
        "\n",
        "            #  1.1 Normalizar payload\n",
        "            payload_raw   = OPT_MODELS[('nn', sel_method, engine)]\n",
        "            p             = _normalize_payload(payload_raw)\n",
        "\n",
        "            model         = p['model']\n",
        "            sx, sy, cols  = p['sx'], p['sy'], p['cols']\n",
        "            score, metric = p['score'], p['metric']\n",
        "            best_params   = p['best_params']\n",
        "\n",
        "            import tensorflow as tf  # necesario para custom_objects al cargar el modelo\n",
        "            from tensorflow.keras.models import load_model\n",
        "\n",
        "            #  Sanitizacin unificada de columnas para NN \n",
        "            # 1) Partimos de `cols` (payload['cols'])\n",
        "            raw_cols_nn = cols  # o p['cols'] si lo extraes ah mismo\n",
        "\n",
        "            # 2) Aplicamos `sanitize_name` a cada nombre\n",
        "            sanitized_cols = [sanitize_name(c) for c in raw_cols_nn]\n",
        "\n",
        "            # 3) Detectamos columnas faltantes en X_test\n",
        "            missing = set(sanitized_cols) - set(X_test.columns)\n",
        "            if missing:\n",
        "                print(f\"[WARNING] NN omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "\n",
        "            # 4) Nos quedamos solo con las columnas vlidas\n",
        "            cols_valid = [c for c in sanitized_cols if c in X_test.columns]\n",
        "\n",
        "            # 5) Seleccin segura de test y prediccin\n",
        "            X_test_sel    = X_test[cols_valid].copy()\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            #  SANITIZACIN DE cols antes de indexar X_test\n",
        "            #cols_clean = [_clean_col(c) for c in cols]\n",
        "            #X_test_sel = X_test[cols_clean].copy()\n",
        "            #X_test_sel = X_test[cols].copy()\n",
        "            y_true = Y_test.values.ravel()\n",
        "            if sx is not None:\n",
        "                X_test_scaled = sx.transform(X_test_sel)\n",
        "            else:\n",
        "                # No se encontr scaler, usar datos sin transformar\n",
        "                X_test_scaled = X_test_sel.values\n",
        "                raw_pred = model.predict(X_test_scaled).ravel()\n",
        "            if sy is not None:\n",
        "                # Aplicar inverse transform si existe scaler de salida\n",
        "                y_pred = sy.inverse_transform(raw_pred.reshape(-1,1)).ravel()\n",
        "            else:\n",
        "                y_pred = raw_pred\n",
        "\n",
        "            # Grfica Predicho vs Real\n",
        "            fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "            ax1.scatter(y_true, y_pred, alpha=0.6)\n",
        "            ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "            ax1.set_xlabel(\"Y real\"); ax1.set_ylabel(\"Y predicho\")\n",
        "            ax1.set_title(f\"NN Optimizado ({sel_method}-{engine}) Predicho vs Real\")\n",
        "            self.sections.append((\n",
        "                f\"### NN Optimizacin: Predicho vs Real ({sel_method}-{engine})\", fig1\n",
        "            ))\n",
        "\n",
        "            # Grfica Residuos\n",
        "            residuals = y_true - y_pred\n",
        "            fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "            ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "            ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "            ax2.set_xlabel(\"Y predicho\"); ax2.set_ylabel(\"Residuo\")\n",
        "            ax2.set_title(f\"NN Optimizado ({sel_method}-{engine}) Residuos\")\n",
        "            self.sections.append((\n",
        "                f\"### NN Optimizacin: Residuos ({sel_method}-{engine})\", fig2\n",
        "            ))\n",
        "\n",
        "            # Tabla de estadsticas de residuos\n",
        "            stats_df = pd.DataFrame({\n",
        "                'Mtrica': ['Media', 'Desviacin', 'Skew', 'Kurtosis', '25%', '50%', '75%'],\n",
        "                'Valor': [residuals.mean(), residuals.std(), skew(residuals), kurtosis(residuals), *np.quantile(residuals, [0.25,0.5,0.75])]\n",
        "            })\n",
        "            self.sections.append((\n",
        "                f\"### NN Optimizacin: Estadsticas de Residuos ({sel_method}-{engine})\", stats_df\n",
        "            ))\n",
        "\n",
        "            # --- Anlisis Generativo IA de Curvas de Ajuste (NN) ---\n",
        "            # Construimos un nico f-string multilnea para asegurar que todo el texto llegue al modelo\n",
        "            prompt_curvas = f\"\"\"\n",
        "            Para el mejor modelo NN optimizado con seleccin {sel_method} y motor {engine} tenemos los siguientes datos:\n",
        "            - Rango Y real: [{float(y_true.min()):.4f}, {float(y_true.max()):.4f}]\n",
        "            - Rango Y predicho: [{float(y_pred.min()):.4f}, {float(y_pred.max()):.4f}]\n",
        "            - Estadsticas de residuos:\n",
        "               media = {residuals.mean():.4f}\n",
        "               std   = {residuals.std():.4f}\n",
        "               skew  = {skew(residuals):.4f}\n",
        "               kurtosis = {kurtosis(residuals):.4f}\n",
        "               quantiles: 25%={float(np.quantile(residuals, 0.25)):.4f},\n",
        "                          50%={float(np.quantile(residuals, 0.50)):.4f},\n",
        "                          75%={float(np.quantile(residuals, 0.75)):.4f}\n",
        "\n",
        "            1. Analiza detalladamente la grfica Predicho vs Real: di si hay desviaciones sistemticas, cun cerca estn los puntos de la diagonal, y si observas heterocedasticidad o patrones claros.\n",
        "            2. Analiza la grfica de residuos: describe la dispersin, si hay colas pesadas o asimetras.\n",
        "            3. Comenta sobre la normalidad de los errores y posibles fuentes de sesgo.\n",
        "            4. Propn recomendaciones prcticas para mejorar el ajuste (p. ej. refinar hiperparmetros, transformar variables, etc.).\n",
        "            \"\"\"\n",
        "            analysis_curvas = call_openai_explanation(prompt_curvas)\n",
        "            self.sections.append((\n",
        "                \"### NN Optimizacin: Anlisis Curvas Ajuste\", analysis_curvas\n",
        "            ))\n",
        "            # --- Fin bloque 1 ---\n",
        "\n",
        "            # --- 2. Importancia Relativa de Hiperparmetros ---\n",
        "            # 2.1 Heatmap Score vs Best_layers y Best_neurons\n",
        "            heat_nn = df_summary_nn.pivot(index='Best_layers', columns='Best_neurons', values='Score')\n",
        "            fig_heat_nn, ax_heat_nn = plt.subplots(figsize=(6,5))\n",
        "            sns.heatmap(heat_nn, annot=True, fmt='.4f', ax=ax_heat_nn)\n",
        "            ax_heat_nn.set_title('NN Optimizado: Heatmap Score vs Capas y Neuronas')\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Heatmap Score vs Capas y Neuronas', fig_heat_nn\n",
        "            ))\n",
        "\n",
        "            # 2.2 Sensibilidad 10%\n",
        "            sens_nn = []\n",
        "            for rec in summary_records:\n",
        "                for param in ['layers','neurons']:\n",
        "                    base = rec.get(f'Best_{param}')\n",
        "                    if base is None: continue\n",
        "                    for factor,label in [(1.1,'+10%'),(0.9,'-10%')]:\n",
        "                        sens_nn.append({\n",
        "                            'Parmetro': param,\n",
        "                            'Cambio': label,\n",
        "                            '% Score': rec['Score'] * (factor - 1) * 100,\n",
        "                            'Seleccin X': rec['Seleccin X'],\n",
        "                            'Motor': rec['Motor']\n",
        "                        })\n",
        "            df_sens_nn = pd.DataFrame(sens_nn)\n",
        "            fig_sens_nn, ax_sens_nn = plt.subplots(figsize=(6,4))\n",
        "            sns.barplot(data=df_sens_nn, x='% Score', y='Parmetro', hue='Cambio', ax=ax_sens_nn)\n",
        "            ax_sens_nn.set_title('NN Opt: Sensibilidad Score 10%')\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Sensibilidad del Score', fig_sens_nn\n",
        "            ))\n",
        "\n",
        "            # 2.3 Anlisis IA de Importancia Relativa\n",
        "            lines_nn = [f\"{r['Parmetro']} {r['Cambio']} => {r['% Score']:.2f}%\" for r in sens_nn]\n",
        "            prompt_hp_nn = (\n",
        "                \"Sensibilidad de Score al 10% en Capas y Neuronas:\\n\" +\n",
        "                \"\\n\".join(lines_nn) +\n",
        "                \"\\n\\nExplica qu hiperparmetro impulsa ms mejora y por qu, y sugiere focos de tuning futuros para la red neuronal.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.5. Llamando IA importancia hiperparmetros NN\")\n",
        "            analysis_hp_nn = call_openai_explanation(prompt_hp_nn)\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: IA Importancia Hiperparmetros', analysis_hp_nn\n",
        "            ))\n",
        "            # --- Fin bloque 2 ---\n",
        "\n",
        "            # --- 3. Distribucin de Mtricas en Validacin Cruzada (manual para Keras NN) ---\n",
        "            print(\"[DEBUG] 13.6. Calculando distribucin de mtricas CV para el mejor modelo NN optimizado (manual)\")\n",
        "\n",
        "            from sklearn.model_selection import KFold\n",
        "\n",
        "            # Identificar mejor configuracin NN\n",
        "            if df_summary_nn['Mtrica'].iloc[0] == 'R2':\n",
        "                idx_best = df_summary_nn['Score'].idxmax()\n",
        "            else:\n",
        "                idx_best = df_summary_nn['Score'].idxmin()\n",
        "            row_best = df_summary_nn.loc[idx_best]\n",
        "            sel_method_cv, engine_cv = row_best['Seleccin X'], row_best['Motor']\n",
        "\n",
        "            #  1.1 Normalizar payload\n",
        "            payload_raw = OPT_MODELS[('nn', sel_method, engine)]\n",
        "            p           = _normalize_payload(payload_raw)\n",
        "\n",
        "            model        = p['model']\n",
        "            sx, sy, cols = p['sx'], p['sy'], p['cols']\n",
        "            score        = p['score']\n",
        "            metric       = p['metric']\n",
        "            best_params  = p['best_params']\n",
        "\n",
        "            # Cargar metadatos y mejor modelo\n",
        "            payload_cv  = nn_entries[('nn', sel_method_cv, engine_cv)]\n",
        "            best_params = {\n",
        "                'layers':  payload_cv.get('layers')   or payload_cv.get('n_layers'),\n",
        "                'neurons': payload_cv.get('neurons')  or payload_cv.get('n_units_l0'),\n",
        "                'dropout': payload_cv.get('dropout')  or payload_cv.get('dropout_l0'),\n",
        "                'epochs':  payload_cv.get('epochs')\n",
        "            }\n",
        "\n",
        "            # Preparar datos\n",
        "            sx_cv, sy_cv, cols_cv = (\n",
        "                payload_cv.get('sx'),\n",
        "                payload_cv.get('sy'),\n",
        "                payload_cv.get('cols'),\n",
        "            )\n",
        "\n",
        "            #  Sanitizacin y filtro unificado de columnas para CV en NN \n",
        "            # 1) Partimos de cols_cv (del payload normalizado)\n",
        "            raw_cols_cv = cols_cv\n",
        "\n",
        "            # 2) Aplicamos sanitize_name a cada nombre\n",
        "            sanitized_cols_cv = [sanitize_name(c) for c in raw_cols_cv]\n",
        "\n",
        "            # 3) Detectamos columnas faltantes en X_train\n",
        "            missing_cv = set(sanitized_cols_cv) - set(X_train.columns)\n",
        "            if missing_cv:\n",
        "                print(f\"[WARNING] NN omitido estas columnas en CV por no existir en X_train: {sorted(missing_cv)}\")\n",
        "\n",
        "            # 4) Nos quedamos solo con las que s existen\n",
        "            cols_cv_valid = [c for c in sanitized_cols_cv if c in X_train.columns]\n",
        "\n",
        "            # 5) Ahora indexamos sin NameError\n",
        "            X_cv = X_train[cols_cv_valid].copy()\n",
        "            y_cv = Y_train.values.ravel()\n",
        "            X_cv_scaled = sx_cv.transform(X_cv.values) if sx_cv is not None else X_cv.values\n",
        "\n",
        "\n",
        "            #  SANITIZACIN DE cols_cv antes de indexar X_train\n",
        "            #cols_cv_clean = [_clean_col(c) for c in cols_cv]\n",
        "            #X_cv = X_train[cols_cv_clean].copy()\n",
        "            #X_cv = X_train[cols_cv].copy()\n",
        "            #y_cv = Y_train.values.ravel()\n",
        "            #X_cv_scaled = sx_cv.transform(X_cv) if sx_cv is not None else X_cv.values\n",
        "\n",
        "            # Manual K-Fold CV\n",
        "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            r2_scores, mse_scores, mae_scores = [], [], []\n",
        "            for train_idx, val_idx in kf.split(X_cv_scaled):\n",
        "                X_tr, X_val = X_cv_scaled[train_idx], X_cv_scaled[val_idx]\n",
        "                y_tr, y_val = y_cv[train_idx], y_cv[val_idx]\n",
        "                # Reconstruir modelo con mejores hiperparmetros\n",
        "                model_cv_fold = keras.Sequential()\n",
        "                model_cv_fold.add(layers.Input(shape=(X_tr.shape[1],)))\n",
        "                for _ in range(int(best_params['layers'])):\n",
        "                    model_cv_fold.add(layers.Dense(int(best_params['neurons']), activation='relu'))\n",
        "                    model_cv_fold.add(layers.Dropout(float(best_params['dropout'])))\n",
        "                model_cv_fold.add(layers.Dense(1))\n",
        "                model_cv_fold.compile(optimizer='adam', loss='mse')\n",
        "                model_cv_fold.fit(X_tr, y_tr, epochs=int(best_params['epochs']), verbose=0)\n",
        "                preds = model_cv_fold.predict(X_val).ravel()\n",
        "                r2_scores.append(r2_score(y_val, preds))\n",
        "                mse_scores.append(mean_squared_error(y_val, preds))\n",
        "                mae_scores.append(mean_absolute_error(y_val, preds))\n",
        "            rmse_scores = [np.sqrt(m) for m in mse_scores]\n",
        "\n",
        "            # DataFrame de resultados CV\n",
        "            import pandas as pd\n",
        "            cv_df_nn = pd.DataFrame({'R2': r2_scores, 'MSE': mse_scores, 'MAE': mae_scores, 'RMSE': rmse_scores})\n",
        "\n",
        "            # 3.1 Boxplot mtricas por fold\n",
        "            fig_cv_nn, ax_cv_nn = plt.subplots(figsize=(6,4))\n",
        "            sns.boxplot(data=cv_df_nn, ax=ax_cv_nn)\n",
        "            ax_cv_nn.set_title('NN Optimizado: Distribucin de Mtricas CV')\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Distribucin de Mtricas CV', fig_cv_nn\n",
        "            ))\n",
        "            # 3.2 Tabla media  desviacin\n",
        "            stats_cv_nn = cv_df_nn.agg(['mean','std']).T.reset_index().rename(columns={'index':'Mtrica','mean':'Media','std':'Desviacin'})\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Estadsticas CV por Fold', stats_cv_nn\n",
        "            ))\n",
        "            # 3.3 Anlisis IA de estabilidad\n",
        "            import numpy as np\n",
        "            # Generar prompt con mximo contexto\n",
        "            hyperparams = best_params\n",
        "            # Estadsticas agregadas para IA\n",
        "            data_summary = (\n",
        "                f\"- R2: media={cv_df_nn['R2'].mean():.4f}, std={cv_df_nn['R2'].std():.4f}\\n\"\n",
        "                f\"- MSE: media={cv_df_nn['MSE'].mean():.4f}, std={cv_df_nn['MSE'].std():.4f}\\n\"\n",
        "                f\"- MAE: media={cv_df_nn['MAE'].mean():.4f}, std={cv_df_nn['MAE'].std():.4f}\\n\"\n",
        "                f\"- RMSE: media={cv_df_nn['RMSE'].mean():.4f}, std={cv_df_nn['RMSE'].std():.4f}\"\n",
        "            )\n",
        "            prompt_cv_nn = (\n",
        "                f\"Para la red neuronal optimizada con mtodo '{sel_method_cv}' y motor '{engine_cv}', \"\n",
        "                f\"se realiz una validacin cruzada de 5 folds obteniendo los siguientes scores por fold:\\n\"\n",
        "                f\"- R2: {r2_scores}\\n\"\n",
        "                f\"- MSE: {mse_scores}\\n\"\n",
        "                f\"- MAE: {mae_scores}\\n\"\n",
        "                f\"- RMSE: {rmse_scores}\\n\\n\"\n",
        "                f\"Resumen estadstico por mtrica:\\n{data_summary}\\n\\n\"\n",
        "                \"Los hiperparmetros ptimos usados fueron:\\n\"\n",
        "                f\"- Capas: {hyperparams['layers']}\\n\"\n",
        "                f\"- Neuronas: {hyperparams['neurons']}\\n\"\n",
        "                f\"- Dropout: {hyperparams['dropout']}\\n\"\n",
        "                f\"- pocas: {hyperparams['epochs']}\\n\\n\"\n",
        "                \"1. Analiza detalladamente la dispersin de cada mtrica por fold y comenta sobre la robustez del modelo.\\n\"\n",
        "                \"2. Identifica posibles fuentes de variabilidad y su impacto en la generalizacin.\\n\"\n",
        "                \"3. Sugiere acciones concretas para mejorar la estabilidad del modelo \"\n",
        "                \"(p.ej., regularizacin adicional, recopilacin de ms datos, ajustes de HPO, etc.).\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.7. Llamando IA para estabilidad en CV NN (manual)\")\n",
        "\n",
        "            analysis_cv_nn = call_openai_explanation(prompt_cv_nn)\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Anlisis Estabilidad CV', analysis_cv_nn\n",
        "            ))\n",
        "            # --- Fin bloque 3 ---\n",
        "\n",
        "            # --- 4. Curvas de Aprendizaje y Validacin para NN Optimizado ---\n",
        "            print(\"[DEBUG] 13.8. Bloque 4: Curvas de Aprendizaje para NN optimizado sin wrapper SKLearn\")\n",
        "\n",
        "            import pickle, numpy as np\n",
        "            from sklearn.model_selection import KFold\n",
        "\n",
        "            # --- normalizo payload para extraer modelo y scalers ---\n",
        "            payload_raw = OPT_MODELS[('nn', sel_method_nn, engine_nn)]\n",
        "            p           = _normalize_payload(payload_raw)\n",
        "\n",
        "            model_best  = p['model']\n",
        "            sx          = p['sx']\n",
        "            sy          = p['sy']\n",
        "            cols        = p['cols']\n",
        "\n",
        "            # 1) Limpia nombres de columnas\n",
        "            sanitized_cols = [sanitize_name(c) for c in cols]\n",
        "\n",
        "            # 2) Filtra solo las que existen en X_train\n",
        "            effective_cols = [c for c in sanitized_cols if c in X_train.columns]\n",
        "            missing = set(sanitized_cols) - set(effective_cols)\n",
        "            if missing:\n",
        "                print(f\"[WARNING] NN omitido estas columnas en entrenamiento: {sorted(missing)}\")\n",
        "\n",
        "            # 3) Construye la matriz de entrenamiento\n",
        "            X_tr = X_train[effective_cols].values\n",
        "\n",
        "            #cols_clean = [_clean_col(c) for c in cols]\n",
        "            #X_tr = X_train[cols_clean].values\n",
        "            #X_tr = X_train[cols].values\n",
        "            y_tr = Y_train.values.ravel()\n",
        "            X_tr_scaled = sx.transform(X_tr) if sx is not None else X_tr\n",
        "            #  EXTRAER HIPERPARMETROS NORMALIZADOS \n",
        "            # payload_raw ya lo habrs definido as:\n",
        "            # payload_raw = OPT_MODELS[('nn', sel_method_nn, engine_nn)]\n",
        "            p = _normalize_payload(payload_raw)\n",
        "            best = p['best_params']\n",
        "            layers_opt  = int(best.get('layers',         1))   # por defecto 1 capa si no est\n",
        "            neurons_opt = int(best.get('units',         32))   # por defecto 32 neuronas\n",
        "            dropout_opt = float(best.get('dropout_rate', 0.0)) # por defecto 0.0 de dropout\n",
        "            #epochs_opt  = int(best.get('epochs',        10))   # por defecto 10 pocas\n",
        "            epochs_opt = min(int(best.get('epochs', 10)), 5)  # mx 5 para acelerar  Justificacin: Al ser solo para validacin de curvas, no necesitamos convergencia perfecta.\n",
        "\n",
        "            # Determinar mejor ndice segn mtrica\n",
        "            # En lugar de mirar en df_nn, usas directamente:\n",
        "            if best_entry_nn['Mtrica'].upper() == 'R2':\n",
        "                best_idx = idx_best_nn  # ya lo tienes\n",
        "            else:\n",
        "                best_idx = idx_best_nn\n",
        "\n",
        "            # Y cuando necesites la ruta:\n",
        "            # Normaliza el payload para extraer todo en variables claras\n",
        "            payload_raw = OPT_MODELS[('nn', sel_method_nn, engine_nn)]\n",
        "            p           = _normalize_payload(payload_raw)\n",
        "\n",
        "            model_best  = p['model']\n",
        "            sx          = p['sx']\n",
        "            sy          = p['sy']\n",
        "            cols        = p['cols']\n",
        "\n",
        "            # 4.1 Curva de Aprendizaje manual\n",
        "            #train_fracs = np.linspace(0.1,1.0,5)\n",
        "            train_fracs = np.linspace(0.1, 1.0, 3)  # [0.1, 0.55, 1.0]   Justificacin: Permite evaluar comportamiento inicial, medio y completo con solo 3 puntos.\n",
        "            train_scores, cv_scores = [], []\n",
        "            #kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "            kf = KFold(n_splits=2, shuffle=True, random_state=42)     #  Justificacin: 2-fold ya permite evaluar generalizacin y reduce el nmero de ciclos casi a la mitad.\n",
        "            for frac in train_fracs:\n",
        "                n = int(len(X_tr_scaled)*frac)\n",
        "                X_sub, y_sub = X_tr_scaled[:n], y_tr[:n]\n",
        "                s_tr, s_val = [], []\n",
        "                for tr_idx, val_idx in kf.split(X_sub):\n",
        "                    Xt, Xv = X_sub[tr_idx], X_sub[val_idx]\n",
        "                    yt, yv = y_sub[tr_idx], y_sub[val_idx]\n",
        "                    # Reentrenar modelo con los mismos HPO optimizados\n",
        "                    m = keras.Sequential()\n",
        "                    m.add(keras.Input(shape=(X_tr_scaled.shape[1],)))\n",
        "                    for _ in range(layers_opt):\n",
        "                        m.add(keras_layers.Dense(neurons_opt, activation='relu'))\n",
        "                        m.add(keras_layers.Dropout(dropout_opt))\n",
        "                    m.add(keras_layers.Dense(1))\n",
        "                    m.compile(optimizer='adam', loss='mse')\n",
        "                    m.fit(Xt, yt, epochs=epochs_opt, batch_size=32, verbose=0)\n",
        "                    s_tr.append(r2_score(yt, m.predict(Xt).ravel()))\n",
        "                    s_val.append(r2_score(yv, m.predict(Xv).ravel()))\n",
        "                train_scores.append(np.mean(s_tr))\n",
        "                cv_scores.append(np.mean(s_val))\n",
        "            fig_lc, ax_lc = plt.subplots(figsize=(6,4))\n",
        "            ax_lc.plot(train_fracs*len(X_tr_scaled), train_scores, 'o-', label='Train R')\n",
        "            ax_lc.plot(train_fracs*len(X_tr_scaled), cv_scores,    'o-', label='CV R')\n",
        "            ax_lc.set_title('NN Optimizado: Curva de Aprendizaje')\n",
        "            ax_lc.set_xlabel('Nmero de muestras de entrenamiento')\n",
        "            ax_lc.set_ylabel('R')\n",
        "            ax_lc.legend()\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Curva de Aprendizaje', fig_lc\n",
        "            ))\n",
        "\n",
        "            # 4.2 Curva de validacin para Layers\n",
        "            #param_L = list(range(1, min(6, X_tr_scaled.shape[1]+1)))\n",
        "            param_L = list(range(1, 4))  # solo 1 a 3 capas  Justificacin: Reduce significativamente las combinaciones sin comprometer la visualizacin de tendencias.\n",
        "            scores_tr_L, scores_cv_L = [], []\n",
        "            for L in param_L:\n",
        "                st, sv = [], []\n",
        "                for ti, vi in kf.split(X_tr_scaled):\n",
        "                    Xt, Xv = X_tr_scaled[ti], X_tr_scaled[vi]\n",
        "                    yt, yv = y_tr[ti], y_tr[vi]\n",
        "                    m = keras.Sequential()\n",
        "                    m.add(keras.Input(shape=(X_tr_scaled.shape[1],)))\n",
        "                    for _ in range(L):\n",
        "                        m.add(keras_layers.Dense(neurons_opt, activation='relu'))\n",
        "                        m.add(keras_layers.Dropout(dropout_opt))\n",
        "                    m.add(keras_layers.Dense(1))\n",
        "                    m.compile(optimizer='adam', loss='mse')\n",
        "                    m.fit(Xt, yt, epochs=epochs_opt, batch_size=32, verbose=0)\n",
        "                    st.append(r2_score(yt, m.predict(Xt).ravel()))\n",
        "                    sv.append(r2_score(yv, m.predict(Xv).ravel()))\n",
        "                scores_tr_L.append(np.mean(st))\n",
        "                scores_cv_L.append(np.mean(sv))\n",
        "            fig_vL, ax_vL = plt.subplots(figsize=(6,4))\n",
        "            ax_vL.plot(param_L, scores_tr_L, 'o-', label='Train R')\n",
        "            ax_vL.plot(param_L, scores_cv_L,'o-', label='CV R')\n",
        "            ax_vL.set_title('NN Opt: Curva Validacin Layers')\n",
        "            ax_vL.set_xlabel('Layers')\n",
        "            ax_vL.set_ylabel('R')\n",
        "            ax_vL.legend()\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Curva Validacin Layers', fig_vL\n",
        "            ))\n",
        "\n",
        "            # 4.3 Curva de validacin para Neurons\n",
        "            #param_N = list(np.linspace(10, X_tr_scaled.shape[1]*50, 5, dtype=int))\n",
        "            param_N = list(np.linspace(10, X_tr_scaled.shape[1]*20, 3, dtype=int))  # solo 3 valores de neuronas   Justificacin: Reduce significativamente las combinaciones sin comprometer la visualizacin de tendencias.\n",
        "            scores_tr_N, scores_cv_N = [], []\n",
        "            for N in param_N:\n",
        "                st, sv = [], []\n",
        "                for ti, vi in kf.split(X_tr_scaled):\n",
        "                    Xt, Xv = X_tr_scaled[ti], X_tr_scaled[vi]\n",
        "                    yt, yv = y_tr[ti], y_tr[vi]\n",
        "                    m = keras.Sequential()\n",
        "                    m.add(keras.Input(shape=(X_tr_scaled.shape[1],)))\n",
        "                    for _ in range(layers_opt):\n",
        "                        m.add(keras_layers.Dense(N, activation='relu'))\n",
        "                        m.add(keras_layers.Dropout(dropout_opt))\n",
        "                    m.add(keras_layers.Dense(1))\n",
        "                    m.compile(optimizer='adam', loss='mse')\n",
        "                    m.fit(Xt, yt, epochs=epochs_opt, batch_size=32, verbose=0)\n",
        "                    st.append(r2_score(yt, m.predict(Xt).ravel()))\n",
        "                    sv.append(r2_score(yv, m.predict(Xv).ravel()))\n",
        "                scores_tr_N.append(np.mean(st))\n",
        "                scores_cv_N.append(np.mean(sv))\n",
        "            fig_vN, ax_vN = plt.subplots(figsize=(6,4))\n",
        "            ax_vN.plot(param_N, scores_tr_N, 'o-', label='Train R')\n",
        "            ax_vN.plot(param_N, scores_cv_N,'o-', label='CV R')\n",
        "            ax_vN.set_title('NN Opt: Curva Validacin Neurons')\n",
        "            ax_vN.set_xlabel('Neurons')\n",
        "            ax_vN.set_ylabel('R')\n",
        "            ax_vN.legend()\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Curva Validacin Neurons', fig_vN\n",
        "            ))\n",
        "\n",
        "            # 4.4 Interpretacin IA de las curvas\n",
        "            prompt_curves = (\n",
        "                f\"NN optimizado (mtodo: {best_entry_nn['Seleccin X']}, motor: {best_entry_nn['Motor']}) gener estas curvas de R:\\n\"\n",
        "                f\"- Aprendizaje: tamaos={ (train_fracs*len(X_tr_scaled)).tolist() }, train={train_scores}, CV={cv_scores}\\n\"\n",
        "                f\"- Validacin Layers: valores={param_L}, train={scores_tr_L}, CV={scores_cv_L}\\n\"\n",
        "                f\"- Validacin Neurons: valores={param_N}, train={scores_tr_N}, CV={scores_cv_N}\\n\"\n",
        "                \"1. Interpreta cada curva, sealando indicios de underfitting/overfitting.\"\n",
        "                \"2. Describe patrones (brechas, picos) y posibles causas.\"\n",
        "                \"3. Recomienda ajustes precisos de HPO basados en estos hallazgos.\"\n",
        "                \"4. Seala limitaciones de datos o modelo evidentes en las curvas.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.9. Llamando a IA para interpretacin IA de curvas rediseado\")\n",
        "            analysis_curves = call_openai_explanation(prompt_curves)\n",
        "            self.sections.append((\n",
        "                \"### NN Optimizado: Interpretacin IA de Curvas\", analysis_curves\n",
        "            ))\n",
        "            # --- Fin bloque 4 ---\n",
        "\n",
        "            # ---- 5. Curvas de Calibracin y Prediccin de Intervalos para NN Optimizado ----\n",
        "            print(\"[DEBUG] 13.10. Calculando curva de calibracin y prediccin de intervalos para el mejor modelo optimizado NN\")\n",
        "            # Usamos el modelo y payload_best ya cargados\n",
        "\n",
        "            #  Sanitizacin unificada de columnas para la curva de calibracin \n",
        "            # 1) Partimos de las columnas del payload\n",
        "            raw_cols_ci = payload_best['cols']\n",
        "\n",
        "            # 2) Limpiamos cada nombre\n",
        "            sanitized_cols_ci = [sanitize_name(c) for c in raw_cols_ci]\n",
        "\n",
        "            # 3) Detectamos columnas faltantes\n",
        "            missing_ci = set(sanitized_cols_ci) - set(X_test.columns)\n",
        "            if missing_ci:\n",
        "                print(f\"[WARNING] NN omitido estas columnas en Calibracin por no existir en X_test: {sorted(missing_ci)}\")\n",
        "\n",
        "            # 4) Filtramos slo las que existen\n",
        "            effective_cols_ci = [c for c in sanitized_cols_ci if c in X_test.columns]\n",
        "\n",
        "            # 5) Seleccin segura de test\n",
        "            X_test_ci = X_test[effective_cols_ci].copy()\n",
        "            y_true_ci = Y_test.values.ravel()\n",
        "\n",
        "            # 6) Transformacin con scaler de entrada\n",
        "            if sx is not None:\n",
        "                X_scaled_ci = sx.transform(X_test_ci.values)\n",
        "            else:\n",
        "                X_scaled_ci = X_test_ci.values\n",
        "\n",
        "            # 7) Prediccin y desescalado\n",
        "            y_pred_raw_ci = model_best.predict(X_scaled_ci).ravel()\n",
        "            if sy is not None:\n",
        "                y_pred_ci = sy.inverse_transform(y_pred_raw_ci.reshape(-1,1)).ravel()\n",
        "            else:\n",
        "                y_pred_ci = y_pred_raw_ci\n",
        "\n",
        "            # Ahora y_true_ci y y_pred_ci estn listas para la curva de calibracin y los intervalos\n",
        "\n",
        "            #  SANITIZACIN de las cols del payload antes de usar en informe\n",
        "            #cols_ci_clean = [_clean_col(c) for c in payload_best['cols']]\n",
        "            #X_test_ci = X_test[cols_ci_clean].copy()\n",
        "            #X_test_ci = X_test[payload_best['cols']].copy()\n",
        "\n",
        "            #y_true_ci = Y_test.values.ravel()\n",
        "            #X_test_scaled_ci = sx.transform(X_test_ci) if sx else X_test_ci.values\n",
        "            #y_pred_ci = model_best.predict(X_test_scaled_ci).ravel()\n",
        "\n",
        "            # 5.1 Curva de calibracin manual para regresin\n",
        "            import pandas as _pd\n",
        "            bins = 10\n",
        "            _df_cal = _pd.DataFrame({'y_pred': y_pred_ci, 'y_true': y_true_ci})\n",
        "            try:\n",
        "                _df_cal['bin'] = _pd.qcut(_df_cal['y_pred'], q=bins, duplicates='drop')\n",
        "            except Exception:\n",
        "                _df_cal['bin'] = _pd.cut(_df_cal['y_pred'], bins=bins)\n",
        "            gr = _df_cal.groupby('bin', observed=True).agg({'y_pred':'mean','y_true':'mean'})\n",
        "            prob_pred = gr['y_pred'].values\n",
        "            prob_true = gr['y_true'].values\n",
        "            fig_cal, ax_cal = plt.subplots(figsize=(6,4))\n",
        "            ax_cal.plot(prob_pred, prob_true, marker='o', linewidth=2)\n",
        "            ax_cal.plot([prob_pred.min(), prob_pred.max()], [prob_pred.min(), prob_pred.max()], 'k--')\n",
        "            ax_cal.set_xlabel('Prediccin promedio por bin')\n",
        "            ax_cal.set_ylabel('Valor real promedio')\n",
        "            ax_cal.set_title('NN Optimizado: Curva de Calibracin')\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Curva de Calibracin', fig_cal\n",
        "            ))\n",
        "\n",
        "            # 5.2 Intervalos de prediccin 1 STD de residuos\n",
        "            residuals_ci = y_true_ci - y_pred_ci\n",
        "            std_res_ci = np.std(residuals_ci)\n",
        "            upper = y_pred_ci + std_res_ci\n",
        "            lower = y_pred_ci - std_res_ci\n",
        "            fig_int, ax_int = plt.subplots(figsize=(6,4))\n",
        "            ax_int.plot(y_true_ci, label='Y real')\n",
        "            ax_int.plot(y_pred_ci, label='Prediccin')\n",
        "            ax_int.fill_between(range(len(y_pred_ci)), lower, upper, alpha=0.3, label='1 STD residuo')\n",
        "            ax_int.set_xlabel('ndice de muestra')\n",
        "            ax_int.set_ylabel('Valor')\n",
        "            ax_int.set_title('NN Optimizado: Intervalos de Prediccin')\n",
        "            ax_int.legend()\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Intervalos de Prediccin', fig_int\n",
        "            ))\n",
        "\n",
        "            # 5.3 Anlisis Generativo IA de incertidumbre y calibracin\n",
        "            prompt_ci_nn = (\n",
        "                f\"Curva de calibracin (pred vs real): pred={prob_pred.tolist()}, real={prob_true.tolist()}\\n\"\n",
        "                f\"Intervalos 1 STD de residuos (std_res={std_res_ci:.4f}).\\n\"\n",
        "                \"1. Son fiables estos intervalos de incertidumbre?\\n\"\n",
        "                \"2. Se infravaloran errores altos?\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.11. Llamando IA para anlisis de incertidumbre y calibracin NN\")\n",
        "            analysis_ci_nn = call_openai_explanation(prompt_ci_nn)\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Anlisis de Incertidumbre y Calibracin', analysis_ci_nn\n",
        "            ))\n",
        "            # --- Fin bloque 5 ---\n",
        "\n",
        "            # --- Seccin 6: Resumen Ejecutivo y Road-Map de Siguientes Pasos ---\n",
        "            # Definimos mejor combinacin basada en el mejor registro\n",
        "            sel_method_nn = best_entry_nn['Seleccin X']\n",
        "            engine_nn    = best_entry_nn['Motor']\n",
        "            best_score_nn = best_entry_nn['Score']\n",
        "\n",
        "            # 6.1. Bloque Markdown con puntos clave para stakeholders\n",
        "            summary_md = (\n",
        "                \"**Puntos Clave:**\\n\"\n",
        "                f\"- **Mejor combinacin:** {sel_method_nn}-{engine_nn} con score={best_score_nn:.4f}.\\n\"\n",
        "                f\"- **Grado de robustez:** Variabilidad CV={np.std(cv_scores):.4f}, residuos (std={std_res_ci:.4f}).\\n\"\n",
        "                f\"- **Puntos dbiles:** picos de error en ciertos rangos y posible sobreajuste en tamaos de muestra altos.\\n\"\n",
        "                f\"- **Recomendaciones inmediatas:** ampliar validacin cruzada, explorar motores jerrquicos como HalvingGridSearchCV y recolectar ms datos.\"\n",
        "            )\n",
        "            self.sections.append((\n",
        "                '### Resumen Ejecutivo y Road-Map', summary_md\n",
        "            ))\n",
        "\n",
        "            # 6.2. Anlisis Generativo IA para desarrollar cada punto clave y generar resumen ejecutivo\n",
        "            prompt_exec = (\n",
        "                \"Eres un investigador cientfico del Instituto de Procesos Sostenibles de la Universidad de Valladolid. \"\n",
        "                \"A continuacin se presentan los puntos clave de la optimizacin de la red neuronal:\\n\"\n",
        "                f\"{summary_md}\\n\"\n",
        "                \"Desarrolla un anlisis detallado en prrafos separados para cada punto \"\n",
        "                \"(Mejor combinacin, Grado de robustez, Puntos dbiles, Recomendaciones inmediatas), \"\n",
        "                \"y finaliza con un resumen ejecutivo de tres prrafos que resalte los hallazgos y los prximos pasos para los stakeholders.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.12. Llamando IA para Resumen Ejecutivo y Road-Map NN optimizacin\")\n",
        "            analysis_exec = call_openai_explanation(prompt_exec)\n",
        "            self.sections.append((\n",
        "                '### Resumen Ejecutivo IA', analysis_exec\n",
        "            ))\n",
        "        # --- Fin Bloque 6 ---\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[DEBUG] Excepcin atrapada en Optimizacin NN: {type(e).__name__}: {e}\")\n",
        "            raise   # relanza la excepcin para que no se oculte\n",
        "#            self.sections.append((\n",
        "#                \"###  Error en seccin Optimizacin NN\",\n",
        "#                f\"Se produjo un error al generar el resumen de mtodos y motores NN: {e}\"\n",
        "#            ))\n",
        "\n",
        "        # =============================================================================\n",
        "        # 14. Optimizacin Modelo XGBoost\n",
        "        # =============================================================================\n",
        "        print(\"[DEBUG] 14.1. Iniciando seccin Optimizacin XGBoost: Resumen de mtodos y motores\")\n",
        "        try:\n",
        "            # Extraer resultados de OPT_MODELS para XGBoost\n",
        "            valid_engines_xgb = {'randomsearch', 'bayesian', 'hyperband', 'optuna'}\n",
        "            xgb_entries = {\n",
        "                k: v for k, v in OPT_MODELS.items()\n",
        "                if isinstance(k, tuple) and k[0] == 'xgb' and k[2] in valid_engines_xgb\n",
        "            }\n",
        "            if not xgb_entries:\n",
        "                raise RuntimeError(\"No se encontr optimizaciones XGBoost en OPT_MODELS\")\n",
        "\n",
        "            import pandas as pd\n",
        "            # Construir registros de resumen para XGBoost\n",
        "            summary_xgb = []\n",
        "            for (model_type, sel_method, engine), payload in xgb_entries.items():\n",
        "                model = payload.get('model')\n",
        "                score = payload.get('score')\n",
        "                metric = payload.get('metric')\n",
        "                # Determinar parmetros de bsqueda del payload unificando todas las claves posibles\n",
        "                params_cfg = (\n",
        "                    payload.get('param_dist')\n",
        "                    or payload.get('param_spaces')\n",
        "                    or payload.get('search_spaces')\n",
        "                    or payload.get('hpo_params')\n",
        "                    or {}\n",
        "                )\n",
        "                best_params = getattr(model, 'get_params', lambda: {})()\n",
        "                summary_xgb.append({\n",
        "                    'Seleccin X': sel_method,\n",
        "                    'Motor': engine,\n",
        "                    'Mtrica': metric,\n",
        "                    'Score': score,\n",
        "                    'Params_Bsqueda': params_cfg,\n",
        "                    'Best_n_estimators': best_params.get('n_estimators'),\n",
        "                    'Best_max_depth': best_params.get('max_depth'),\n",
        "                    'Best_learning_rate': best_params.get('learning_rate')\n",
        "                })\n",
        "\n",
        "            df_xgb = pd.DataFrame(summary_xgb)\n",
        "            # Aadir al informe\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizacin: Resumen de Mtodos y Motores',\n",
        "                df_xgb.reset_index(drop=True)\n",
        "            ))\n",
        "\n",
        "            # --- 0. Anlisis Generativo IA de Tabla de Resumen ---\n",
        "            # Llamada a OpenAI para anlisis de los resultados\n",
        "            lines = [\n",
        "                f\"Mtodo X: {r['Seleccin X']}, Motor: {r['Motor']}, Score: {r['Score']:.4f}, \" +\n",
        "                f\"Params Busqueda: {r.get('Params_Bsqueda', {})}, Best_Params: {r.get('Best_Params', {})}\"\n",
        "                for r in summary_xgb\n",
        "            ]\n",
        "\n",
        "            prompt_xgb = (\n",
        "                \"He obtenido los siguientes resultados de optimizacin para XGBoost:\" + \"\".join(lines) + \"\"\n",
        "                \"1. Explica la estrategia de bsqueda y ventajas de cada motor (Hyperband, RandomizedSearchCV, Optuna, BayesSearchCV).\\n\"\n",
        "                \"2. Compara los scores obtenidos y argumenta cul es la mejor configuracin.\\n\"\n",
        "                \"3. Sugiere mejoras especficas de HPO para XGBoost basadas en estos resultados.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.2. Llamando a OpenAI para anlisis generativo XGBoost optimizacin\")\n",
        "            analysis_xgb = call_openai_explanation(prompt_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizacin: Anlisis Generativo', analysis_xgb\n",
        "            ))\n",
        "            # --- Fin Bloque 0 ---\n",
        "\n",
        "            # --- 1. Curvas de Ajuste Real vs. Predicho y Residuos para XGBoost ---\n",
        "            from sklearn.metrics import r2_score\n",
        "            print(\"[DEBUG] 14.3. Iniciando bloque de Curvas Ajuste Real vs Predicho y Residuos para XGBoost optimizado\")\n",
        "            from scipy.stats import skew, kurtosis\n",
        "            import numpy as np\n",
        "\n",
        "            # 1. Seleccionar mejor configuracin y normalizar payload\n",
        "            best_idx    = df_xgb['Score'].idxmax() if df_xgb['Mtrica'].str.upper().iloc[0]=='R2' else df_xgb['Score'].idxmin()\n",
        "            best_row    = summary_xgb[best_idx]\n",
        "\n",
        "            payload_raw = OPT_MODELS[('xgb', best_row['Seleccin X'], best_row['Motor'])]\n",
        "            p           = _normalize_payload(payload_raw)\n",
        "\n",
        "            #  Sanitizacin de columnas para XGBoost \n",
        "            # 1) obtenemos la lista original de columnas entrenadas\n",
        "            raw_cols_xgb = p['cols'] if p['cols'] is not None else X_train.columns.tolist()\n",
        "            # 2) aplicamos sanitize_name a cada nombre\n",
        "            sanitized_cols = [sanitize_name(c) for c in raw_cols_xgb]\n",
        "            # 3) avisamos si faltan columnas en X_test\n",
        "            missing = set(sanitized_cols) - set(X_test.columns)\n",
        "            if missing:\n",
        "                print(f\"[WARNING] XGBoost omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "            # 4) nos quedamos solo con las columnas vlidas\n",
        "            cols_xgb = [c for c in sanitized_cols if c in X_test.columns]\n",
        "            #  Fin sanitizacin XGBoost \n",
        "\n",
        "            #  1.1.1 Sanitizacin y filtro unificado de columnas \n",
        "            #sanitized_cols = [sanitize_name(c) for c in raw_cols]\n",
        "            #missing = set(sanitized_cols) - set(X_test.columns)\n",
        "            #if missing:\n",
        "            #    print(f\"[WARNING] SVR omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "            #cols_xgb = [c for c in sanitized_cols if c in X_test.columns]\n",
        "\n",
        "            #X_test_sel = X_test[cols_valid].copy()\n",
        "\n",
        "            # AADIDO: Unificar saneamiento de nombres de columnas segn entrenamiento\n",
        "            #import re\n",
        "            #def clean_name(s):\n",
        "            #    t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))\n",
        "            #    t = re.sub(r'_+', '_', t)\n",
        "            #    return t.strip('_')\n",
        "\n",
        "            # 1) Modelo  si no est, error controlado\n",
        "            model_xgb = p['model']\n",
        "            if model_xgb is None:\n",
        "                raise RuntimeError(f\"No pude cargar el modelo para {best_row['Seleccin X']}-{best_row['Motor']}\")\n",
        "\n",
        "            # 2) Columnas  si no vienen, uso **todas** las de entrenamiento\n",
        "            #cols_xgb = p['cols'] if p['cols'] is not None else X_train.columns.tolist()\n",
        "\n",
        "            # REEMPLAZAR esta lnea original:\n",
        "            # cols_xgb = p['cols'] if p['cols'] is not None else X_train.columns.tolist()\n",
        "            # POR:\n",
        "            #raw_cols_xgb = p['cols'] if p['cols'] is not None else X_train.columns.tolist()\n",
        "            #cols_xgb     = [ clean_name(c) for c in raw_cols_xgb ]\n",
        "            # FIN AADIDO / REEMPLAZO\n",
        "\n",
        "            # 3) Escalador de salida  a la hora de inverse_transform\n",
        "            sy_xgb = p['sy']  # puede ser None\n",
        "\n",
        "            # 4) Score y best_params  nunca deberan ser None, pero por si acaso:\n",
        "            score_xgb       = p['score'] or 0.0\n",
        "            best_params_xgb = p['best_params'] or {}\n",
        "            metric_xgb      = p['metric']\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            #X_test_xgb = X_test[cols_xgb]\n",
        "            #y_true_xgb = Y_test.values.ravel()\n",
        "            #y_pred_xgb = model_xgb.predict(X_test_xgb)\n",
        "            #if p['sy'] is not None:\n",
        "            #    y_pred_xgb = p['sy'].inverse_transform(y_pred_xgb.reshape(-1,1)).ravel()\n",
        "            # si tu modelo no escala internamente:\n",
        "            #X_test_scaled_xgb = sx_xgb.transform(X_test_xgb)\n",
        "            #y_pred_xgb        = sy_xgb.inverse_transform(model_xgb.predict(X_test_scaled_xgb).reshape(-1,1)).ravel()\n",
        "\n",
        "            # 1) Filtramos y saneamos las columnas igual que en entrenamiento\n",
        "            X_test_xgb = X_test[cols_xgb]\n",
        "            # 2) Llevamos a numpy array para evitar la comprobacin de nombres\n",
        "            X_test_vals = X_test_xgb.values\n",
        "            y_pred_raw  = model_xgb.predict(X_test_vals)\n",
        "            # 3) Desescalamos si corresponde\n",
        "            if p['sy'] is not None:\n",
        "                y_pred_xgb = p['sy'].inverse_transform(y_pred_raw.reshape(-1,1)).ravel()\n",
        "            else:\n",
        "                y_pred_xgb = y_pred_raw\n",
        "            # 4) Tus valores reales siguen as:\n",
        "            y_true_xgb = Y_test.values.ravel()\n",
        "\n",
        "            # Grfica Predicho vs Real\n",
        "            fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "            ax1.scatter(y_true_xgb, y_pred_xgb, alpha=0.6)\n",
        "            ax1.plot([y_true_xgb.min(), y_true_xgb.max()], [y_true_xgb.min(), y_true_xgb.max()], 'r--', lw=2)\n",
        "            ax1.set_xlabel('Y real')\n",
        "            ax1.set_ylabel('Y predicho')\n",
        "            ax1.set_title(f\"XGBoost Optimizado: Predicho vs Real ({best_row['Seleccin X']}-{best_row['Motor']})\")\n",
        "            self.sections.append((\n",
        "                f\"### XGBoost Optimizado: Predicho vs Real ({best_row['Seleccin X']}-{best_row['Motor']})\", fig1\n",
        "            ))\n",
        "\n",
        "            # Grfica Residuos\n",
        "            residuals_xgb = y_true_xgb - y_pred_xgb\n",
        "            fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "            ax2.scatter(y_pred_xgb, residuals_xgb, alpha=0.6)\n",
        "            ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "            ax2.set_xlabel('Y predicho')\n",
        "            ax2.set_ylabel('Residuo')\n",
        "            ax2.set_title(f\"XGBoost Optimizado: Residuos ({best_row['Seleccin X']}-{best_row['Motor']})\")\n",
        "            self.sections.append((\n",
        "                f\"### XGBoost Optimizado: Residuos ({best_row['Seleccin X']}-{best_row['Motor']})\", fig2\n",
        "            ))\n",
        "\n",
        "            # Tabla estadsticas de residuos\n",
        "            df_res_stats = pd.DataFrame({\n",
        "                'Mtrica': ['Media','Desviacin','Skew','Kurtosis','25%','50%','75%'],\n",
        "                'Valor': [residuals_xgb.mean(), residuals_xgb.std(), skew(residuals_xgb), kurtosis(residuals_xgb), *np.quantile(residuals_xgb,[0.25,0.5,0.75])]\n",
        "            })\n",
        "            self.sections.append((\n",
        "                f\"### XGBoost Optimizado: Estadsticas de Residuos ({best_row['Seleccin X']}-{best_row['Motor']})\", df_res_stats\n",
        "            ))\n",
        "\n",
        "            # Anlisis generativo IA\n",
        "            prompt_xgb_curves = (\n",
        "                f\"Para el mejor XGBoost optimizado (seleccin {best_row['Seleccin X']}, motor {best_row['Motor']}), \"\n",
        "                f\"tienes los siguientes datos:\"\n",
        "                f\"- Rango Y real: [{y_true_xgb.min():.4f}, {y_true_xgb.max():.4f}]\\n\"\n",
        "                f\"- Rango Y pred: [{y_pred_xgb.min():.4f}, {y_pred_xgb.max():.4f}]\\n\"\n",
        "                f\"- Estadsticas residuos: media={residuals_xgb.mean():.4f}, std={residuals_xgb.std():.4f}, skew={skew(residuals_xgb):.4f}, kurtosis={kurtosis(residuals_xgb):.4f}\\.\\n\"\n",
        "                \"1. Analiza la calidad del ajuste basndote en Predicho vs Real y Residuos.\\n\"\n",
        "                \"2. Comenta sesgos sistemticos o heterocedasticidad.\\n\"\n",
        "                \"3. Recomienda acciones para mejorar el fit si hay problemas.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.4. Llamando IA para anlisis de curvas XGBoost\")\n",
        "            try:\n",
        "                analysis_xgb_curves = call_openai_explanation(prompt_xgb_curves)\n",
        "                self.sections.append((\n",
        "                    '### XGBoost Optimizado: Anlisis Calidad Ajuste',\n",
        "                    analysis_xgb_curves\n",
        "                ))\n",
        "            except Exception as e:\n",
        "                # aqu s podemos usar `e`\n",
        "                print(f\"[ERROR XGB  Curvas] {type(e).__name__}: {e}\")\n",
        "                self.sections.append((\n",
        "                    '###  Error XGBoost: Anlisis Calidad Ajuste',\n",
        "                    f\"Se produjo un error al generar las curvas de XGBoost: {type(e).__name__}: {e}\"\n",
        "                ))\n",
        "            # --- Fin bloque 1 ---\n",
        "\n",
        "            # --- 2. Importancia Relativa de Hiperparmetros para XGBoost Optimizado ---\n",
        "            print(\"[DEBUG] 14.5. Calculando importancia real de hiperparmetros para XGBoost optimizado\")\n",
        "\n",
        "            import pandas as pd\n",
        "            import numpy as np\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.metrics import r2_score\n",
        "            from xgboost import XGBRegressor\n",
        "\n",
        "            # Determinar la mejor configuracin desde df_xgb\n",
        "            if df_xgb['Mtrica'].str.upper().iloc[0] == 'R2':\n",
        "                idx_best = df_xgb['Score'].idxmax()\n",
        "            else:\n",
        "                idx_best = df_xgb['Score'].idxmin()\n",
        "\n",
        "            # Datos completos\n",
        "            X_full = X_train[cols_xgb]\n",
        "            y_full = Y_train.values.ravel()\n",
        "            base_params = best_params_xgb\n",
        "            base_score  = score_xgb\n",
        "\n",
        "            # Funcin para medir impacto de variar un hiperparmetro 10%\n",
        "            sens_list = []\n",
        "            for param in ['n_estimators', 'max_depth', 'learning_rate']:\n",
        "                base_val = base_params.get(param)\n",
        "                if base_val is None:\n",
        "                    continue\n",
        "                for factor, label in [(1.1, '+10%'), (0.9, '-10%')]:\n",
        "                    # Nuevo valor\n",
        "                    new_val = int(base_val * factor) if param in ['n_estimators', 'max_depth'] else base_val * factor\n",
        "                    # Divisin train/val\n",
        "                    X_tr, X_val, y_tr, y_val = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
        "                    # Reentrenar con el parmetro modificado\n",
        "                    m = XGBRegressor(**{**base_params, param: new_val}, random_state=42, verbosity=0)\n",
        "                    m.fit(X_tr, y_tr)\n",
        "                    y_pred = m.predict(X_val)\n",
        "                    score_new = r2_score(y_val, y_pred)\n",
        "                    sens_list.append({\n",
        "                        'Parmetro': param,\n",
        "                        'Cambio': label,\n",
        "                        'Score': score_new,\n",
        "                        'Delta': score_new - base_score\n",
        "                    })\n",
        "\n",
        "            # Crear DataFrame de sensibilidad\n",
        "            df_sens_xgb = pd.DataFrame(sens_list)\n",
        "            df_sens_xgb['% Delta'] = df_sens_xgb['Delta'] * 100\n",
        "\n",
        "            # 2.1 Tabla de sensibilidad real\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Sensibilidad Real del Score 10%',\n",
        "                df_sens_xgb[['Parmetro', 'Cambio', 'Score', '% Delta']]\n",
        "            ))\n",
        "\n",
        "            # 2.2 Heatmap de sensibilidad\n",
        "            pivot_xgb = df_sens_xgb.pivot(index='Parmetro', columns='Cambio', values='% Delta')\n",
        "            fig_sens_xgb, ax_sens_xgb = plt.subplots(figsize=(6, 4))\n",
        "            sns.heatmap(pivot_xgb, annot=True, fmt='.2f', ax=ax_sens_xgb)\n",
        "            ax_sens_xgb.set_title('XGBoost Optimizado: Heatmap Sensibilidad Real 10%')\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Heatmap Sensibilidad',\n",
        "                fig_sens_xgb\n",
        "            ))\n",
        "\n",
        "            # 2.3 Anlisis Generativo IA de importancia real\n",
        "            lines_xgb = [\n",
        "            f\"{row['Parmetro']} {row['Cambio']}  % = {row['% Delta']:.2f}%\"\n",
        "            for _, row in df_sens_xgb.iterrows()\n",
        "            ]\n",
        "            prompt_hp_xgb = (\n",
        "                \"Sensibilidad del Score tras reentrenar XGBoost variando 10% cada hiperparmetro:\\n\" +\n",
        "                \"\\n\".join(lines_xgb) +\n",
        "                \"\\n\\n1. Explica cul hiperparmetro impacta ms y por qu.\\n\"\n",
        "                \"2. Sugiere en qu enfocarte en prximos HPO basndote en estos resultados.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.6. Llamando IA para importancia hiperparmetros XGBoost\")\n",
        "            analysis_hp_xgb = call_openai_explanation(prompt_hp_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: IA Importancia Real Hiperparmetros',\n",
        "                analysis_hp_xgb\n",
        "            ))\n",
        "            # ---- Fin bloque 2 ----\n",
        "\n",
        "            # --- 3. Distribucin de Mtricas en Validacin Cruzada para XGBoost Optimizado ---\n",
        "            from sklearn.model_selection import cross_validate\n",
        "            import numpy as np\n",
        "\n",
        "            print(\"[DEBUG] 14.7. Calculando distribucin de mtricas CV para XGBoost optimizado\")\n",
        "\n",
        "            # 3.1 Identificar el mejor modelo segn Score (R2 se maximiza, errores se minimizan)\n",
        "            if df_xgb['Mtrica'].str.upper().iloc[0] == 'R2':\n",
        "                best_idx = df_xgb['Score'].idxmax()\n",
        "            else:\n",
        "                best_idx = df_xgb['Score'].idxmin()\n",
        "            best_row = df_xgb.loc[best_idx]\n",
        "\n",
        "            payload_raw_cv  = OPT_MODELS[('xgb', best_row['Seleccin X'], best_row['Motor'])]\n",
        "            p_cv            = _normalize_payload(payload_raw_cv)\n",
        "            model_cv, cols_cv = p_cv['model'], p_cv['cols']\n",
        "\n",
        "            # Preparamos datos de entrenamiento escalados\n",
        "            X_cv = X_train[cols_cv]\n",
        "            y_cv = Y_train.values.ravel()\n",
        "\n",
        "            # Ejecutamos crossvalidate con 5 folds\n",
        "            cv_results = cross_validate(\n",
        "                model_cv, X_cv, y_cv,\n",
        "                cv=5,\n",
        "                scoring={\n",
        "                    'R2': 'r2',\n",
        "                    'neg_MSE': 'neg_mean_squared_error',\n",
        "                    'neg_MAE': 'neg_mean_absolute_error'\n",
        "                },\n",
        "                return_train_score=False\n",
        "            )\n",
        "\n",
        "            # Convertimos a mtricas positivas\n",
        "            r2_scores   = cv_results['test_R2']\n",
        "            mse_scores  = -cv_results['test_neg_MSE']\n",
        "            mae_scores  = -cv_results['test_neg_MAE']\n",
        "            rmse_scores = np.sqrt(mse_scores)\n",
        "\n",
        "            import pandas as _pd\n",
        "\n",
        "            # 3.2 Boxplot de mtricas por fold\n",
        "            df_cv = _pd.DataFrame({\n",
        "                'R2':   r2_scores,\n",
        "                'MSE':  mse_scores,\n",
        "                'MAE':  mae_scores,\n",
        "                'RMSE': rmse_scores\n",
        "            })\n",
        "            fig_cv, ax_cv = plt.subplots(figsize=(6,4))\n",
        "            sns.boxplot(data=df_cv, ax=ax_cv)\n",
        "            ax_cv.set_title('XGBoost Optimizado: Distribucin de Mtricas CV')\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Distribucin de Mtricas CV',\n",
        "                fig_cv\n",
        "            ))\n",
        "\n",
        "            # 3.3 Tabla con media  desviacin en folds\n",
        "            stats_cv = df_cv.agg(['mean','std']).T.reset_index().rename(columns={\n",
        "                'index':'Mtrica','mean':'Media','std':'Desviacin'\n",
        "            })\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Estadsticas CV por Fold',\n",
        "                stats_cv\n",
        "            ))\n",
        "\n",
        "            # 3.4 Anlisis Generativo IA de estabilidad\n",
        "            prompt_cv_xgb = (\n",
        "                f\"Validacin cruzada 5-folds para XGBoost optimizado \"\n",
        "                f\"(seleccin={best_row['Seleccin X']}, motor={best_row['Motor']}):\\n\"\n",
        "                f\"- R2 scores: {r2_scores.tolist()}\\n\"\n",
        "                f\"- MSE scores: {mse_scores.tolist()}\\n\"\n",
        "                f\"- MAE scores: {mae_scores.tolist()}\\n\"\n",
        "                f\"- RMSE scores: {rmse_scores.tolist()}\\n\\n\"\n",
        "                \"1. Qu nos dice la dispersin de cada mtrica sobre la estabilidad del modelo?\\n\"\n",
        "                \"2. Identifica fuentes de variabilidad que puedan afectar la generalizacin.\\n\"\n",
        "                \"3. Sugiere acciones concretas (p.ej., ms regularizacin, ms datos, ajuste de HPO) para mejorar la robustez.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.8. Llamando IA para estabilidad en CV XGBoost\")\n",
        "            analysis_cv_xgb = call_openai_explanation(prompt_cv_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Anlisis Estabilidad CV',\n",
        "                analysis_cv_xgb\n",
        "            ))\n",
        "            # --- Fin Bloque 3 ---\n",
        "\n",
        "            # --- 4. Curvas de Aprendizaje y Validacin para XGBoost Optimizado ---\n",
        "            # --- 4. Curvas de Aprendizaje y Validacin para XGBoost Optimizado ---\n",
        "            from sklearn.model_selection import learning_curve, validation_curve\n",
        "            import numpy as np\n",
        "\n",
        "            print(\"[DEBUG] 14.9. Calculando curvas de aprendizaje y validacin para XGBoost optimizado\")\n",
        "\n",
        "            # Seleccionar mejor configuracin segn mtrica\n",
        "            if df_xgb['Mtrica'].str.upper().iloc[0] == 'R2':\n",
        "                best_idx = df_xgb['Score'].idxmax()\n",
        "            else:\n",
        "                best_idx = df_xgb['Score'].idxmin()\n",
        "            best_row = summary_xgb[best_idx]\n",
        "            payload_raw_lc = OPT_MODELS[('xgb', best_row['Seleccin X'], best_row['Motor'])]\n",
        "            p_lc           = _normalize_payload(payload_raw_lc)\n",
        "            model_best, cols_xgb = p_lc['model'], p_lc['cols']\n",
        "\n",
        "            # Preparar datos de entrenamiento\n",
        "            X_tr = X_train[cols_xgb]\n",
        "            y_tr = Y_train.values.ravel()\n",
        "\n",
        "            # 4.1 Curva de Aprendizaje (R)\n",
        "            train_sizes, train_scores, cv_scores = learning_curve(\n",
        "                model_best, X_tr, y_tr,\n",
        "                cv=3,\n",
        "                train_sizes=np.linspace(0.1, 1.0, 5),\n",
        "                scoring='r2',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            train_mean = np.mean(train_scores, axis=1)\n",
        "            cv_mean    = np.mean(cv_scores,   axis=1)\n",
        "\n",
        "            fig_lc_xgb, ax_lc_xgb = plt.subplots(figsize=(6,4))\n",
        "            ax_lc_xgb.plot(train_sizes, train_mean, 'o-', label='Train R')\n",
        "            ax_lc_xgb.plot(train_sizes, cv_mean,    'o-', label='CV R')\n",
        "            ax_lc_xgb.set_title('XGBoost Optimizado: Curva de Aprendizaje')\n",
        "            ax_lc_xgb.set_xlabel('Tamao del set de entrenamiento')\n",
        "            ax_lc_xgb.set_ylabel('R')\n",
        "            ax_lc_xgb.legend()\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Curva de Aprendizaje',\n",
        "                fig_lc_xgb\n",
        "            ))\n",
        "\n",
        "            # 4.2 Curva de Validacin para max_depth\n",
        "            param_range_depth = np.arange(3, 16, 2)\n",
        "            depth_tr, depth_cv = validation_curve(\n",
        "                model_best, X_tr, y_tr,\n",
        "                param_name='max_depth',\n",
        "                param_range=param_range_depth,\n",
        "                cv=3,\n",
        "                scoring='r2',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            fig_vc_d, ax_vc_d = plt.subplots(figsize=(6,4))\n",
        "            ax_vc_d.plot(param_range_depth, np.mean(depth_tr, axis=1), 'o-', label='Train R')\n",
        "            ax_vc_d.plot(param_range_depth, np.mean(depth_cv, axis=1), 'o-', label='CV R')\n",
        "            ax_vc_d.set_title('XGBoost Opt.: Curva Validacin max_depth')\n",
        "            ax_vc_d.set_xlabel('max_depth')\n",
        "            ax_vc_d.set_ylabel('R')\n",
        "            ax_vc_d.legend()\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Curva de Validacin max_depth',\n",
        "                fig_vc_d\n",
        "            ))\n",
        "\n",
        "            # 4.3 Curva de Validacin para learning_rate\n",
        "            param_range_lr = np.linspace(0.01, 0.3, 5)\n",
        "            lr_tr, lr_cv = validation_curve(\n",
        "                model_best, X_tr, y_tr,\n",
        "                param_name='learning_rate',\n",
        "                param_range=param_range_lr,\n",
        "                cv=3,\n",
        "                scoring='r2',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            fig_vc_lr, ax_vc_lr = plt.subplots(figsize=(6,4))\n",
        "            ax_vc_lr.plot(param_range_lr, np.mean(lr_tr, axis=1), 'o-', label='Train R')\n",
        "            ax_vc_lr.plot(param_range_lr, np.mean(lr_cv, axis=1), 'o-', label='CV R')\n",
        "            ax_vc_lr.set_xscale('log')\n",
        "            ax_vc_lr.set_title('XGBoost Opt.: Curva Validacin learning_rate')\n",
        "            ax_vc_lr.set_xlabel('learning_rate')\n",
        "            ax_vc_lr.set_ylabel('R')\n",
        "            ax_vc_lr.legend()\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Curva de Validacin learning_rate',\n",
        "                fig_vc_lr\n",
        "            ))\n",
        "\n",
        "            # 4.4 Interpretacin IA de las curvas\n",
        "            prompt_curvas_xgb = (\n",
        "                f\"Para el modelo XGBoost optimizado (seleccin={best_row['Seleccin X']}, motor={best_row['Motor']}), se generaron estas curvas de R:\\n\"\n",
        "                f\"- Aprendizaje: tamaos={train_sizes.tolist()}, train={train_mean.tolist()}, cv={cv_mean.tolist()}\\n\"\n",
        "                f\"- Validacin max_depth: depths={param_range_depth.tolist()}, train={np.mean(depth_tr,axis=1).tolist()}, cv={np.mean(depth_cv,axis=1).tolist()}\\n\"\n",
        "                f\"- Validacin learning_rate: rates={param_range_lr.tolist()}, train={np.mean(lr_tr,axis=1).tolist()}, cv={np.mean(lr_cv,axis=1).tolist()}\\n\\n\"\n",
        "                \"1. Interpreta cada curva sealando indicios de underfitting o overfitting.\\n\"\n",
        "                \"2. Describe patrones (brechas entre train y cv, picos, cadas).\\n\"\n",
        "                \"3. Recomienda ajustes precisos de HPO (max_depth, learning_rate, regularizacin).\\n\"\n",
        "                \"4. Indica posibles limitaciones de datos o modelo evidentes.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.10. Llamando a OpenAI para interpretacin IA de curvas XGBoost\")\n",
        "            analysis_curvas_xgb = call_openai_explanation(prompt_curvas_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Interpretacin IA de Curvas',\n",
        "                analysis_curvas_xgb\n",
        "            ))\n",
        "            # --- Fin Bloque 4 ---\n",
        "\n",
        "            # ---- 5. Curvas de Calibracin y Prediccin de Intervalos para XGBoost Optimizado ----\n",
        "            from sklearn.calibration import calibration_curve\n",
        "            import pandas as _pd\n",
        "            import numpy as np\n",
        "\n",
        "            print(\"[DEBUG] 14.11. Calculando curva de calibracin y prediccin de intervalos para el mejor modelo optimizado XGBoost\")\n",
        "\n",
        "            # 5.1 Identificar mejor modelo\n",
        "            if df_xgb['Mtrica'].str.upper().iloc[0] == 'R2':\n",
        "                best_idx = df_xgb['Score'].idxmax()\n",
        "            else:\n",
        "                best_idx = df_xgb['Score'].idxmin()\n",
        "            best = summary_xgb[best_idx]\n",
        "            payload_raw_ci = OPT_MODELS[('xgb', best['Seleccin X'], best['Motor'])]\n",
        "            p_ci           = _normalize_payload(payload_raw_ci)\n",
        "            model_ci, cols_ci = p_ci['model'], p_ci['cols']\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            X_test_ci   = X_test[cols_ci]\n",
        "            y_true_ci   = Y_test.values.ravel()\n",
        "            y_pred_ci   = model_ci.predict(X_test_ci)\n",
        "\n",
        "            # 5.2 Curva de calibracin (binned reliability plot)\n",
        "            bins = 10\n",
        "            df_cal = _pd.DataFrame({'y_pred': y_pred_ci, 'y_true': y_true_ci})\n",
        "            try:\n",
        "                df_cal['bin'] = _pd.qcut(df_cal['y_pred'], q=bins, duplicates='drop')\n",
        "            except ValueError:\n",
        "                df_cal['bin'] = _pd.cut(df_cal['y_pred'], bins=bins)\n",
        "            grp = df_cal.groupby('bin', observed=True).agg({'y_pred':'mean','y_true':'mean'})\n",
        "            prob_pred = grp['y_pred'].values\n",
        "            prob_true = grp['y_true'].values\n",
        "\n",
        "            fig_cal, ax_cal = plt.subplots(figsize=(6,4))\n",
        "            ax_cal.plot(prob_pred, prob_true, marker='o', linewidth=2)\n",
        "            ax_cal.plot([prob_pred.min(), prob_pred.max()],\n",
        "                        [prob_pred.min(), prob_pred.max()],\n",
        "                        'k--')\n",
        "            ax_cal.set_xlabel('Prediccin promedio por bin')\n",
        "            ax_cal.set_ylabel('Valor real promedio')\n",
        "            ax_cal.set_title('XGBoost Opt.: Curva de Calibracin')\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Curva de Calibracin',\n",
        "                fig_cal\n",
        "            ))\n",
        "\n",
        "            # 5.3 Intervalos de prediccin 1 STD de residuos\n",
        "            residuals_ci = y_true_ci - y_pred_ci\n",
        "            std_res_ci   = np.std(residuals_ci)\n",
        "            upper = y_pred_ci + std_res_ci\n",
        "            lower = y_pred_ci - std_res_ci\n",
        "\n",
        "            fig_int, ax_int = plt.subplots(figsize=(6,4))\n",
        "            ax_int.plot(y_true_ci, label='Y real')\n",
        "            ax_int.plot(y_pred_ci, label='Prediccin')\n",
        "            ax_int.fill_between(range(len(y_pred_ci)), lower, upper,\n",
        "                                alpha=0.3, label='1 STD residuo')\n",
        "            ax_int.set_xlabel('ndice de muestra')\n",
        "            ax_int.set_ylabel('Valor')\n",
        "            ax_int.set_title('XGBoost Opt.: Intervalos de Prediccin')\n",
        "            ax_int.legend()\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Intervalos de Prediccin',\n",
        "                fig_int\n",
        "            ))\n",
        "\n",
        "            # 5.4 Anlisis Generativo IA de incertidumbre y calibracin\n",
        "            prompt_ci_xgb = (\n",
        "                f\"Para el XGBoost optimizado (mtodo={best['Seleccin X']}, motor={best['Motor']}):\\n\"\n",
        "                f\"- Curva de calibracin: pred={prob_pred.tolist()}, real={prob_true.tolist()}\\n\"\n",
        "                f\"- Intervalos 1 STD de residuos (std={std_res_ci:.4f})\\n\\n\"\n",
        "                \"1. Son fiables estos intervalos de incertidumbre?\\n\"\n",
        "                \"2. Observas infravaloracin de errores altos o patrones heterocedsticos?\\n\"\n",
        "                \"3. Sugiere mejoras en HPO o en la modelizacin para fortalecer la confianza de las predicciones.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.12. Llamando IA para anlisis de incertidumbre y calibracin XGBoost\")\n",
        "            analysis_ci_xgb = call_openai_explanation(prompt_ci_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Anlisis de Incertidumbre y Calibracin',\n",
        "                analysis_ci_xgb\n",
        "            ))\n",
        "            # --- Fin Bloque 5 ---\n",
        "\n",
        "            # --- 6. Resumen Ejecutivo y Road-Map de Siguientes Pasos para XGBoost Optimizado ---\n",
        "            # 6.1. Identificamos la mejor configuracin\n",
        "            if df_xgb['Mtrica'].str.upper().iloc[0] == 'R2':\n",
        "                best_idx_xgb = df_xgb['Score'].idxmax()\n",
        "            else:\n",
        "                best_idx_xgb = df_xgb['Score'].idxmin()\n",
        "            best_xgb = summary_xgb[best_idx_xgb]\n",
        "            sel_xgb, eng_xgb, best_score_xgb = best_xgb['Seleccin X'], best_xgb['Motor'], best_xgb['Score']\n",
        "\n",
        "            # 6.2. Creamos el bloque Markdown con los puntos clave\n",
        "            summary_md_xgb = (\n",
        "                \"**Puntos Clave Optimizacin XGBoost:**\\n\\n\"\n",
        "                f\"- **Mejor combinacin:** Mtodo de seleccin `{sel_xgb}` + motor `{eng_xgb}`  **Score** = {best_score_xgb:.4f}\\n\"\n",
        "                f\"- **Robustez del modelo:** CV y residuos muestran desviacin estndar de aproximadamente _X_ (reemplazar con valor real).\\n\"\n",
        "                \"- **Puntos dbiles detectados:** posibles indicios de sobreajuste en rangos altos de `max_depth` o `learning_rate`, y variabilidad en ciertos folds.\\n\"\n",
        "                \"- **Recomendaciones inmediatas:**\\n\"\n",
        "                \"  1. Ampliar validacin cruzada (p.ej., HalvingGridSearchCV o K=510 folds).\\n\"\n",
        "                \"  2. Explorar rangos ms finos de `learning_rate` en [0.01, 0.1] y `max_depth` en [3, 10].\\n\"\n",
        "                \"  3. Evaluar regularizacin L1/L2 (`reg_alpha`, `reg_lambda`) para atenuar overfitting.\\n\"\n",
        "                \"  4. Considerar ensamblados ligeros (p.ej., LightGBM, CatBoost) y comparacin de rendimiento.\\n\"\n",
        "            )\n",
        "\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Resumen Ejecutivo y Road-Map',\n",
        "                summary_md_xgb\n",
        "            ))\n",
        "\n",
        "            # 6.3. Anlisis Generativo con OpenAI\n",
        "            prompt_exec_xgb = (\n",
        "                \"Eres un investigador del Instituto de Procesos Sostenibles de la Universidad de Valladolid. \"\n",
        "                \"A continuacin tienes los puntos clave de la optimizacin XGBoost:\\n\\n\"\n",
        "                f\"{summary_md_xgb}\\n\\n\"\n",
        "                \"Por favor:\\n\"\n",
        "                \"1. Desarrolla en un prrafo cada uno de los puntos clave (Mejor combinacin, Robustez, Puntos dbiles, Recomendaciones).\\n\"\n",
        "                \"2. Finaliza con un **resumen ejecutivo** de tres prrafos dirigido a stakeholders acadmicos, resaltando hallazgos y pasos siguientes.\\n\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.13. Llamando IA para Resumen Ejecutivo XGBoost\")\n",
        "            analysis_exec_xgb = call_openai_explanation(prompt_exec_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Anlisis Ejecutivo IA',\n",
        "                analysis_exec_xgb\n",
        "            ))\n",
        "            # --- Fin Bloque 6 ---\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR XGBoost] {type(e).__name__}: {e}\")\n",
        "            self.sections.append((\n",
        "                '###  Error en seccin Optimizacin XGBoost',\n",
        "                f\"Se produjo un error al generar XGBoost: {type(e).__name__}: {e}\"\n",
        "            ))\n",
        "\n",
        "        #print(\"[DEBUG] ReportBuilder.build_sections end\")\n",
        "\n",
        "        # =============================================================================\n",
        "        # 15. Optimizacin Modelo Random Forest\n",
        "        # =============================================================================\n",
        "        try:\n",
        "            print(\"[DEBUG] 15.1. Iniciando seccin Optimizacin Random Forest: Resumen de mtodos y motores\")\n",
        "            # Verificar que OPT_MODELS existe y es dict\n",
        "            if 'OPT_MODELS' not in globals() or not isinstance(OPT_MODELS, dict):\n",
        "                raise RuntimeError(\"No se encontr OPT_MODELS con resultados de optimizacin RF\")\n",
        "\n",
        "            import pandas as pd\n",
        "            # Filtrar entradas de Random Forest\n",
        "            valid_engines_rf = {'randomsearch', 'bayesianoptimization', 'hyperband', 'optuna'}\n",
        "            rf_entries = {\n",
        "                k: v for k, v in OPT_MODELS.items()\n",
        "                if isinstance(k, tuple) and k[0] == 'rf' and k[2] in valid_engines_rf\n",
        "            }\n",
        "            if not rf_entries:\n",
        "                raise RuntimeError(\"No se encontraron optimizaciones Random Forest en OPT_MODELS\")\n",
        "\n",
        "            # Construir lista de registros resumen\n",
        "            summary_rf = []\n",
        "            for (_, sel_method, engine), payload in rf_entries.items():\n",
        "                score = payload.get('score')\n",
        "                metric = payload.get('metric')\n",
        "                params_search = payload.get('param_dist', {}) or payload.get('search_spaces', {})\n",
        "                best_params = payload.get('best_params', {})\n",
        "                summary_rf.append({\n",
        "                    'Seleccin X': sel_method,\n",
        "                    'Motor':      engine,\n",
        "                    'Mtrica':    metric,\n",
        "                    'Score':      score,\n",
        "                    'Params_Bsqueda': params_search,\n",
        "                    'Best_Params':     best_params\n",
        "                })\n",
        "\n",
        "            # DataFrame resumen\n",
        "            df_rf = pd.DataFrame(summary_rf)\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimizacin: Resumen de Mtodos y Motores',\n",
        "                df_rf.reset_index(drop=True)\n",
        "            ))\n",
        "\n",
        "            # --- 0. Anlisis Generativo IA de la Tabla Resumen ---\n",
        "            def call_openai_explanation(prompt: str, model: str = \"gpt-4\", temperature: float = TEMPERATURE_VAL, max_tokens: int = MAX_EXPLANATION_TOKENS) -> str:\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un experto en optimizacin de hiperparmetros de modelos de Machine Learning. \"\n",
        "                                \"Proporciona anlisis profundo, interpretaciones y recomendaciones basadas en los datos proporcionados.\")},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS\n",
        "                    )\n",
        "                    return response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    return f\"[Error llamando a OpenAI: {e}]\"\n",
        "            lines = [\n",
        "                f\"Mtodo X: {r['Seleccin X']}, Motor: {r['Motor']}, Score: {r['Score']:.4f}, \" +\n",
        "                f\"Params: {r['Params_Bsqueda']}, Best: {r['Best_Params']}\"\n",
        "                for r in summary_rf\n",
        "            ]\n",
        "            prompt_rf = (\n",
        "                \"He obtenido los siguientes resultados de optimizacin para Random Forest:\\n\" +\n",
        "                \"\\n\".join(lines) + \"\\n\\n\"\n",
        "                \"1. Explica la estrategia de cada motor (RandomSearch, BayesianOptimization, Hyperband, Optuna).\\n\"\n",
        "                \"2. Compara los scores y justifica la mejor configuracin.\\n\"\n",
        "                \"3. Sugiere mejoras especficas de HPO para Random Forest.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 15.2. Llamando a OpenAI para anlisis generativo RF optimizacin\")\n",
        "            analysis_rf = call_openai_explanation(prompt_rf)\n",
        "\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimizacin: Anlisis Generativo',\n",
        "                analysis_rf\n",
        "            ))\n",
        "            # --- Fin Bloque 0 ---\n",
        "\n",
        "            # --- 1. Curvas de Ajuste Real vs. Predicho y Residuos para Random Forest Optimizado ---\n",
        "            from sklearn.metrics import r2_score\n",
        "            from scipy.stats import skew, kurtosis\n",
        "            print(\"[DEBUG] 15.3. Iniciando bloque de Curvas Ajuste Real vs Predicho y Residuos para Random Forest optimizado\")\n",
        "\n",
        "            # 1. Seleccionar mejor configuracin y normalizar payload\n",
        "            best_idx  = df_rf['Score'].idxmax()\n",
        "            best_row  = summary_rf[best_idx]\n",
        "\n",
        "            payload_raw = OPT_MODELS[('rf', best_row['Seleccin X'], best_row['Motor'])]\n",
        "            p = _normalize_payload(payload_raw)\n",
        "\n",
        "            model_rf     = p['model']\n",
        "            sx_rf, sy_rf = p['sx'], p['sy']\n",
        "            cols_rf      = p['cols']\n",
        "            score_rf     = p['score']\n",
        "            metric_rf    = p['metric']\n",
        "            best_params_rf = p['best_params']\n",
        "\n",
        "            #  15.1 Sanitizacin unificada de columnas para RF \n",
        "            sanitized_cols_rf = [sanitize_name(c) for c in cols_rf]\n",
        "            missing_rf = set(sanitized_cols_rf) - set(X_test.columns)\n",
        "            if missing_rf:\n",
        "                print(f\"[WARNING] RF omitido estas columnas por no existir en X_test: {sorted(missing_rf)}\")\n",
        "            cols_valid_rf = [c for c in sanitized_cols_rf if c in X_test.columns]\n",
        "\n",
        "            # Seleccin segura de test\n",
        "            X_test_sel = X_test[cols_valid_rf].copy()\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            #X_test_sel = X_test[cols_rf]\n",
        "            y_true = Y_test.values.ravel()\n",
        "            if sx_rf and sy_rf:\n",
        "                X_scaled = sx_rf.transform(X_test_sel)\n",
        "                y_pred = sy_rf.inverse_transform(model_rf.predict(X_scaled).reshape(-1,1)).ravel()\n",
        "            elif sx_rf:\n",
        "                X_scaled = sx_rf.transform(X_test_sel)\n",
        "                y_pred = model_rf.predict(X_scaled)\n",
        "            else:\n",
        "                y_pred = model_rf.predict(X_test_sel)\n",
        "\n",
        "            # Grfica Predicho vs Real\n",
        "            fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "            ax1.scatter(y_true, y_pred, alpha=0.6)\n",
        "            ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "            ax1.set_xlabel(\"Y real\")\n",
        "            ax1.set_ylabel(\"Y predicho\")\n",
        "            ax1.set_title(f\"RF Optimizado: Predicho vs Real ({best_row['Seleccin X']}-{best_row['Motor']})\")\n",
        "            self.sections.append((\n",
        "                f\"### Random Forest Optimizado: Predicho vs Real ({best_row['Seleccin X']}-{best_row['Motor']})\", fig1\n",
        "            ))\n",
        "\n",
        "            # Grfica Residuos\n",
        "            residuals = y_true - y_pred\n",
        "            fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "            ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "            ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "            ax2.set_xlabel(\"Y predicho\")\n",
        "            ax2.set_ylabel(\"Residuo\")\n",
        "            ax2.set_title(f\"RF Optimizado: Residuos ({best_row['Seleccin X']}-{best_row['Motor']})\")\n",
        "            self.sections.append((\n",
        "                f\"### Random Forest Optimizado: Residuos ({best_row['Seleccin X']}-{best_row['Motor']})\", fig2\n",
        "            ))\n",
        "\n",
        "            # Estadsticas de residuos\n",
        "            mean_res = float(residuals.mean())\n",
        "            std_res = float(residuals.std())\n",
        "            skew_res = float(skew(residuals))\n",
        "            kurt_res = float(kurtosis(residuals))\n",
        "            q25, q50, q75 = [float(x) for x in np.quantile(residuals, [0.25, 0.5, 0.75])]\n",
        "            df_stats = pd.DataFrame({\n",
        "                'Mtrica':['Media','Desviacin','Skew','Kurtosis','25%','50%','75%'],\n",
        "                'Valor':[mean_res,std_res,skew_res,kurt_res,q25,q50,q75]\n",
        "            })\n",
        "            self.sections.append((\n",
        "                f\"### Random Forest Optimizado: Estadsticas de Residuos ({best_row['Seleccin X']}-{best_row['Motor']})\", df_stats\n",
        "            ))\n",
        "\n",
        "            # Anlisis generativo IA de calidad de ajuste\n",
        "            print(\"[DEBUG] 15.4. Llamando IA para anlisis de curvas Random Forest\")\n",
        "            prompt_curves_rf = (\n",
        "                f\"Para el Random Forest optimizado (seleccin {best_row['Seleccin X']}, motor {best_row['Motor']}), tiene:\\n\"\n",
        "                f\"- Rango Y real: [{y_true.min():.4f}, {y_true.max():.4f}]\\n\"\n",
        "                f\"- Rango Y pred: [{y_pred.min():.4f}, {y_pred.max():.4f}]\\n\"\n",
        "                f\"- Residuales: media={mean_res:.4f}, std={std_res:.4f}, skew={skew_res:.4f}, kurtosis={kurt_res:.4f}, quantiles 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}.\\n\"\n",
        "                \"1. Analiza la grfica Predicho vs Real y comenta sobre sesgo o varianza.\\n\"\n",
        "                \"2. Interpreta la distribucin de residuos: sesgos sistemticos, heterocedasticidad y normalidad.\\n\"\n",
        "                \"3. Sugiere acciones para mejorar el ajuste si detectas problemas (p.ej. ms rboles, regularizacin, ms datos...).\"\n",
        "            )\n",
        "            analysis_curves_rf = call_openai_explanation(prompt_curves_rf)\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimizado: Anlisis Calidad Ajuste', analysis_curves_rf\n",
        "            ))\n",
        "            # --- Fin bloque 1 ---\n",
        "\n",
        "            # --- 2. Importancia Relativa de Hiperparmetros para Random Forest Optimizado ---\n",
        "            from sklearn import utils\n",
        "            import seaborn as sns\n",
        "\n",
        "            print(\"[DEBUG] 15.5. Iniciando bloque de Importancia Relativa de Hiperparmetros para RF optimizado\")\n",
        "\n",
        "            # Construir matriz Score vs n_estimators y max_depth para heatmap\n",
        "            # (si existen ambos hiperparmetros en Best_Params)\n",
        "            params_for_heat = ['n_estimators', 'max_depth']\n",
        "            heat_records = []\n",
        "            for r in summary_rf:\n",
        "                bp = r['Best_Params']\n",
        "                if all(p in bp for p in params_for_heat):\n",
        "                    heat_records.append({\n",
        "                        'n_estimators': bp['n_estimators'],\n",
        "                        'max_depth':    bp['max_depth'],\n",
        "                        'Score':        r['Score']\n",
        "                    })\n",
        "            if heat_records:\n",
        "                df_heat = pd.DataFrame(heat_records)\n",
        "                heat = df_heat.pivot(index='n_estimators', columns='max_depth', values='Score')\n",
        "                fig_heat, ax_heat = plt.subplots(figsize=(6,5))\n",
        "                sns.heatmap(heat, annot=True, fmt='.4f', ax=ax_heat)\n",
        "                ax_heat.set_title('RF Optimizado: Heatmap Score vs n_estimators y max_depth')\n",
        "                self.sections.append((\n",
        "                    '### RF Optimizado: Heatmap Score vs n_estimators & max_depth', fig_heat\n",
        "                ))\n",
        "\n",
        "            # Sensibilidad 10% sobre cada hiperparmetro de cada entrada\n",
        "            sens = []\n",
        "            for r in summary_rf:\n",
        "                base_score = r['Score']\n",
        "                bp = r['Best_Params']\n",
        "                for param, base_val in bp.items():\n",
        "                    if isinstance(base_val, (int, float)):\n",
        "                        for factor, label in [(1.1, '+10%'), (0.9, '-10%')]:\n",
        "                            sens.append({\n",
        "                                'Parmetro': param,\n",
        "                                'Cambio':    label,\n",
        "                                '%  Score': (base_score * factor - base_score) / abs(base_score) * 100,\n",
        "                                'Seleccin X': r['Seleccin X'],\n",
        "                                'Motor':      r['Motor']\n",
        "                            })\n",
        "            if sens:\n",
        "                df_sens = pd.DataFrame(sens)\n",
        "                fig_sens, ax_sens = plt.subplots(figsize=(6,4))\n",
        "                sns.barplot(data=df_sens, x='%  Score', y='Parmetro', hue='Cambio', ax=ax_sens)\n",
        "                ax_sens.set_title('RF Optimizado: Sensibilidad del Score 10%')\n",
        "                self.sections.append((\n",
        "                    '### RF Optimizado: Sensibilidad del Score', fig_sens\n",
        "                ))\n",
        "\n",
        "            # IA: explicar qu parmetro impulsa ms mejora y por qu\n",
        "            print(\"[DEBUG] 15.6. Llamando IA para importancia de hiperparmetros RF\")\n",
        "            lines = [f\"{row['Parmetro']} {row['Cambio']}  {row['%  Score']:.2f}%\"\n",
        "                    for row in sens if row['Seleccin X']==best_row['Seleccin X'] and row['Motor']==best_row['Motor']]\n",
        "            prompt_hp_rf = (\n",
        "                \"Sensibilidad del Score al 10% para el mejor RF optimizado \"\n",
        "                f\"(seleccin {best_row['Seleccin X']}, motor {best_row['Motor']}):\\n\" +\n",
        "                \"\\n\".join(lines) +\n",
        "                \"\\n\\n1. Qu hiperparmetro impulsa ms la mejora y por qu?\\n\"\n",
        "                \"2. Sugiere focos de ajuste prioritarios basados en esta sensibilidad.\"\n",
        "            )\n",
        "            analysis_hp_rf = call_openai_explanation(prompt_hp_rf)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: IA Importancia Hiperparmetros', analysis_hp_rf\n",
        "            ))\n",
        "            # ---- Fin bloque 2 ----\n",
        "\n",
        "            # --- 3. Distribucin de Mtricas en Validacin Cruzada para Random Forest Optimizado ---\n",
        "            from sklearn.model_selection import cross_validate\n",
        "            print(\"[DEBUG] 15.7. Calculando distribucin de mtricas CV para Random Forest optimizado\")\n",
        "\n",
        "            # Identificar mejor configuracin\n",
        "            best_idx_rf = df_rf['Score'].idxmax()\n",
        "            best_row_rf = summary_rf[best_idx_rf]\n",
        "\n",
        "            payload_raw = OPT_MODELS[('rf', best_row_rf['Seleccin X'], best_row_rf['Motor'])]\n",
        "            p = _normalize_payload(payload_raw)\n",
        "\n",
        "            model_cv = p['model']\n",
        "            sx_cv    = p['sx']\n",
        "            cols_cv  = p['cols']\n",
        "\n",
        "            # Preparar datos de entrenamiento escalados\n",
        "            X_cv = X_train[cols_cv].copy()\n",
        "            y_cv = Y_train.values.ravel()\n",
        "            if sx_cv:\n",
        "                X_cv_scaled = sx_cv.transform(X_cv)\n",
        "            else:\n",
        "                X_cv_scaled = X_cv\n",
        "\n",
        "            # Cross-validate con mtricas mltiples\n",
        "            cv_results_rf = cross_validate(\n",
        "                model_cv, X_cv_scaled, y_cv,\n",
        "                cv=5,\n",
        "                scoring={\n",
        "                    'r2':'r2',\n",
        "                    'neg_mse':'neg_mean_squared_error',\n",
        "                    'neg_mae':'neg_mean_absolute_error'\n",
        "                },\n",
        "                return_train_score=False\n",
        "            )\n",
        "\n",
        "            # Procesar resultados\n",
        "            r2_scores_rf  = cv_results_rf['test_r2']\n",
        "            mse_scores_rf = [-v for v in cv_results_rf['test_neg_mse']]\n",
        "            mae_scores_rf = [-v for v in cv_results_rf['test_neg_mae']]\n",
        "            rmse_scores_rf = np.sqrt(mse_scores_rf)\n",
        "\n",
        "            df_cv_rf = pd.DataFrame({\n",
        "                'R2': r2_scores_rf,\n",
        "                'MSE': mse_scores_rf,\n",
        "                'MAE': mae_scores_rf,\n",
        "                'RMSE': rmse_scores_rf\n",
        "            })\n",
        "\n",
        "            # 3.1 Boxplot de mtricas por fold\n",
        "            fig_cv_rf, ax_cv_rf = plt.subplots(figsize=(6,4))\n",
        "            sns.boxplot(data=df_cv_rf, ax=ax_cv_rf)\n",
        "            ax_cv_rf.set_title('RF Optimizado: Distribucin de Mtricas CV')\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimizado: Distribucin de Mtricas CV', fig_cv_rf\n",
        "            ))\n",
        "\n",
        "            # 3.2 Tabla con media  desviacin\n",
        "            stats_cv_rf = df_cv_rf.agg(['mean','std']).T.reset_index().rename(columns={\n",
        "                'index':'Mtrica','mean':'Media','std':'Desviacin'\n",
        "            })\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimizado: Estadsticas CV por Fold', stats_cv_rf\n",
        "            ))\n",
        "\n",
        "            # 3.3 Anlisis Generativo IA de Estabilidad CV\n",
        "            print(\"[DEBUG] 15.8. Llamando IA para estabilidad CV Random Forest\")\n",
        "            prompt_cv_rf = (\n",
        "                f\"Validacin cruzada 5 folds RF optimizado (Seleccin {best_row_rf['Seleccin X']}, Motor {best_row_rf['Motor']}):\\n\"\n",
        "                f\"- R2 por fold: {r2_scores_rf.tolist()} \\n\"\n",
        "                f\"- MAE por fold: {mae_scores_rf}\\n\"\n",
        "                f\"- RMSE por fold: {rmse_scores_rf}\\n\"\n",
        "                \"Analiza la dispersin de cada mtrica y comenta sobre la estabilidad y generalizacin del modelo.\"\n",
        "            )\n",
        "            analysis_cv_rf = call_openai_explanation(prompt_cv_rf)\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimizado: Anlisis Estabilidad CV', analysis_cv_rf\n",
        "            ))\n",
        "            # --- Fin Bloque 3 ---\n",
        "\n",
        "            # --- 4. Curvas de Aprendizaje y Validacin para Random Forest Optimizado ---\n",
        "            from sklearn.model_selection import learning_curve, validation_curve\n",
        "            import numpy as np\n",
        "\n",
        "            print(\"[DEBUG] 15.9. Iniciando bloque de Curvas de Aprendizaje y Validacin para Random Forest optimizado\")\n",
        "\n",
        "            # Seleccionar mejor configuracin segn Score\n",
        "            best_idx = df_rf['Score'].idxmax()\n",
        "            best_row = summary_rf[best_idx]\n",
        "\n",
        "            payload_raw = OPT_MODELS[('rf', best_row['Seleccin X'], best_row['Motor'])]\n",
        "            p = _normalize_payload(payload_raw)\n",
        "\n",
        "            model_rf = p['model']\n",
        "            sx_rf    = p['sx']\n",
        "            cols_rf  = p['cols']\n",
        "\n",
        "            # Preparar datos de entrenamiento escalados\n",
        "            X_train_sel = X_train[cols_rf]\n",
        "            y_train = Y_train.values.ravel()\n",
        "            X_train_scaled = sx_rf.transform(X_train_sel) if sx_rf else X_train_sel\n",
        "\n",
        "            # 4.1 Curva de Aprendizaje (R)\n",
        "            train_sizes, train_scores, val_scores = learning_curve(\n",
        "                model_rf, X_train_scaled, y_train,\n",
        "                cv=5, scoring='r2', train_sizes=np.linspace(0.1,1.0,5), n_jobs=-1\n",
        "            )\n",
        "            train_mean = np.mean(train_scores, axis=1)\n",
        "            val_mean   = np.mean(val_scores,   axis=1)\n",
        "            fig_lc, ax_lc = plt.subplots(figsize=(6,4))\n",
        "            ax_lc.plot(train_sizes, train_mean, 'o-', label='Train R')\n",
        "            ax_lc.plot(train_sizes, val_mean,   'o-', label='CV R')\n",
        "            ax_lc.set_title('RF Optimizado: Curva de Aprendizaje')\n",
        "            ax_lc.set_xlabel('Tamao del set de entrenamiento')\n",
        "            ax_lc.set_ylabel('R')\n",
        "            ax_lc.legend()\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Curva de Aprendizaje', fig_lc\n",
        "            ))\n",
        "\n",
        "            # Funcin auxiliar para Curvas de Validacin\n",
        "            def plot_vc(param_name, param_range):\n",
        "                  # Ajuste: validation_curve devuelve solo train_scores y test_scores\n",
        "                train_scores_vc, val_scores_vc = validation_curve(\n",
        "                    model_rf, X_train_scaled, y_train,\n",
        "                    param_name=param_name, param_range=param_range,\n",
        "                    cv=5, scoring='r2', n_jobs=-1\n",
        "                )\n",
        "                fig, ax = plt.subplots(figsize=(6,4))\n",
        "                ax.plot(param_range, np.mean(train_scores_vc, axis=1), 'o-', label='Train R')\n",
        "                ax.plot(param_range, np.mean(val_scores_vc, axis=1), 'o-', label='CV R')\n",
        "                ax.set_title(f\"RF Optimizado: Curva de Validacin {param_name}\")\n",
        "                ax.set_xlabel(param_name)\n",
        "                ax.set_ylabel('R')\n",
        "                if param_name == 'n_estimators':\n",
        "                    ax.set_xscale('log')\n",
        "                ax.legend()\n",
        "                return fig, train_scores_vc, val_scores_vc\n",
        "\n",
        "            # 4.2 Curva de Validacin n_estimators\n",
        "            param_range_n = np.arange(50, 501, 50)\n",
        "            fig_vc_n, tsn, vsn = plot_vc('n_estimators', param_range_n)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Curva de Validacin n_estimators', fig_vc_n\n",
        "            ))\n",
        "\n",
        "            # 4.3 Curva de Validacin max_depth\n",
        "            param_range_md = np.arange(3, 16, 2)\n",
        "            fig_vc_md, tsm, vsm = plot_vc('max_depth', param_range_md)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Curva de Validacin max_depth', fig_vc_md\n",
        "            ))\n",
        "\n",
        "            # 4.4 Interpretacin IA de Curvas\n",
        "            print(\"[DEBUG] 15.10. Llamando IA para anlisis de curvas Random Forest\")\n",
        "            prompt_curvas_rf = (\n",
        "                f\"Random Forest optimizado (Seleccin {best_row['Seleccin X']}, Motor {best_row['Motor']}):\\n\"\n",
        "                f\"- Curva de Aprendizaje: tamaos={train_sizes.tolist()}, train={train_mean.tolist()}, cv={val_mean.tolist()}\\n\"\n",
        "                f\"- Curva de Validacin n_estimators: {param_range_n.tolist()}, train={np.mean(tsn,axis=1).tolist()}, cv={np.mean(vsn,axis=1).tolist()}\\n\"\n",
        "                f\"- Curva de Validacin max_depth: {param_range_md.tolist()}, train={np.mean(tsm,axis=1).tolist()}, cv={np.mean(vsm,axis=1).tolist()}\\n\\n\"\n",
        "                \"1. Analiza cada curva e identifica underfitting o overfitting.\\n\"\n",
        "                \"2. Seala brechas clave entre entrenamiento y validacin y su impacto en la generalizacin.\\n\"\n",
        "                \"3. Recomienda ajustes concretos de n_estimators y max_depth para mejorar el modelo.\"\n",
        "            )\n",
        "            analysis_vc_rf = call_openai_explanation(prompt_curvas_rf)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Interpretacin IA de Curvas de Aprendizaje y Validacin', analysis_vc_rf\n",
        "            ))\n",
        "            # --- Fin Bloque 4 ---\n",
        "\n",
        "            # --- 5. Curvas de Calibracin y Prediccin de Intervalos para Random Forest Optimizado ---\n",
        "            from sklearn.calibration import calibration_curve\n",
        "            import pandas as _pd\n",
        "\n",
        "            print(\"[DEBUG] 15.11. Iniciando bloque de Curvas de Calibracin y Prediccin de Intervalos para Random Forest optimizado\")\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            y_true_rf = Y_test.values.ravel()\n",
        "            y_pred_scaled = model_rf.predict(X_test[cols_rf])\n",
        "            y_pred_rf = sy_rf.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel() if sy_rf else y_pred_scaled\n",
        "\n",
        "            # 5.1 Curva de calibracin (binned reliability plot para regresin)\n",
        "            bins = 10\n",
        "            df_cal = _pd.DataFrame({'y_pred': y_pred_rf, 'y_true': y_true_rf})\n",
        "            try:\n",
        "                df_cal['bin'] = _pd.qcut(df_cal['y_pred'], q=bins, duplicates='drop')\n",
        "            except:\n",
        "                df_cal['bin'] = _pd.cut(df_cal['y_pred'], bins=bins)\n",
        "            grp = df_cal.groupby('bin', observed=True).agg({'y_pred': 'mean', 'y_true': 'mean'})\n",
        "            prob_pred, prob_true = grp['y_pred'].values, grp['y_true'].values\n",
        "            fig_cal, ax_cal = plt.subplots(figsize=(6, 4))\n",
        "            ax_cal.plot(prob_pred, prob_true, marker='o', linewidth=2)\n",
        "            ax_cal.plot([prob_pred.min(), prob_pred.max()], [prob_pred.min(), prob_pred.max()], 'k--')\n",
        "            ax_cal.set_xlabel('Prediccin promedio en bin')\n",
        "            ax_cal.set_ylabel('Valor real promedio')\n",
        "            ax_cal.set_title('RF Optimizado: Curva de Calibracin')\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Curva de Calibracin', fig_cal\n",
        "            ))\n",
        "\n",
        "            # 5.2 Intervalos de prediccin 1 STD de residuos\n",
        "            residuals_rf = y_true_rf - y_pred_rf\n",
        "            std_res_rf = np.std(residuals_rf)\n",
        "            upper_rf = y_pred_rf + std_res_rf\n",
        "            lower_rf = y_pred_rf - std_res_rf\n",
        "            fig_int, ax_int = plt.subplots(figsize=(6, 4))\n",
        "            ax_int.plot(y_true_rf, label='Y real')\n",
        "            ax_int.plot(y_pred_rf, label='Prediccin')\n",
        "            ax_int.fill_between(range(len(y_pred_rf)), lower_rf, upper_rf, alpha=0.3, label='1 STD residuo')\n",
        "            ax_int.set_xlabel('ndice de muestra')\n",
        "            ax_int.set_ylabel('Valor')\n",
        "            ax_int.set_title('RF Optimizado: Intervalos de Prediccin')\n",
        "            ax_int.legend()\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Intervalos de Prediccin', fig_int\n",
        "            ))\n",
        "\n",
        "            # 5.3 Anlisis Generativo IA de Incertidumbre y Calibracin\n",
        "            print(\"[DEBUG] 15.12. Llamando IA para anlisis de incertidumbre y calibracin RF optimizado\")\n",
        "            prompt_ci_rf = (\n",
        "                f\"Curva de calibracin (pred:{prob_pred.tolist()}, real:{prob_true.tolist()}) y \"\n",
        "                f\"intervalos 1 STD (std_res={std_res_rf:.4f}).\\n\"\n",
        "                \"1. Evala la fiabilidad de los intervalos de incertidumbre.\\n\"\n",
        "                \"2. Identifica infravaloracin de errores altos o patrones de heterocedasticidad.\\n\"\n",
        "                \"3. Sugiere mejoras para la calibracin y estimacin de incertidumbre.\"\n",
        "            )\n",
        "            analysis_ci_rf = call_openai_explanation(prompt_ci_rf)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Anlisis de Incertidumbre y Calibracin', analysis_ci_rf\n",
        "            ))\n",
        "            # --- Fin Bloque 5 ---\n",
        "\n",
        "            # --- 6. Resumen Ejecutivo y Road-Map de Siguientes Pasos para Random Forest Optimizado ---\n",
        "            print(\"[DEBUG] 15.13. Iniciando bloque de Resumen Ejecutivo y Road-Map para Random Forest optimizado\")\n",
        "            # Variables clave\n",
        "            sel_rf    = best_row['Seleccin X']\n",
        "            eng_rf    = best_row['Motor']\n",
        "            best_score_rf = best_row['Score']\n",
        "            # Estadsticas de residuos y validacin\n",
        "            cv_std    = float(np.std(val_mean))\n",
        "            res_std   = std_res_rf\n",
        "            res_kurt  = kurtosis(residuals_rf)\n",
        "            q25_rf, q50_rf, q75_rf = [float(x) for x in np.quantile(residuals_rf, [0.25,0.5,0.75])]\n",
        "\n",
        "            summary_md_rf = (\n",
        "                \"**Puntos Clave Optimizacin Random Forest:**\\n \"\n",
        "                f\"- **Mejor combinacin:** Seleccin `{sel_rf}` + motor `{eng_rf}`  **Score** = {best_score_rf:.4f}\\n\"\n",
        "                f\"- **Robustez del modelo:** desviacin estndar CV = {cv_std:.4f}, std residuos = {res_std:.4f}\\n\"\n",
        "                f\"- **Curtosis de residuos:** {res_kurt:.4f}, quantiles (25%,50%,75%) = ({q25_rf:.4f},{q50_rf:.4f},{q75_rf:.4f})\\n\"\n",
        "                \"- **Recomendaciones inmediatas:**\\n\"\n",
        "                \"  1. Ajustar `n_estimators` y `max_depth` segn el balance sesgo-varianza observado.\\n\"\n",
        "                \"  2. Explorar regularizacin adicional (`min_samples_split`, `min_samples_leaf`) para reducir varianza.\\n\"\n",
        "                \"  3. Ampliar validacin cruzada a 710 folds e incluir `oob_score` para medir robustez.\\n\"\n",
        "                \"  4. Considerar ensambles adicionales (e.g., GradientBoosting, LightGBM) para comparar rendimiento\"\n",
        "            )\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Resumen Ejecutivo y Road-Map', summary_md_rf\n",
        "            ))\n",
        "\n",
        "            print(\"[DEBUG] 15.14. Llamando IA para Resumen Ejecutivo y Road-Map RF optimizado\")\n",
        "            prompt_exec_rf = (\n",
        "                \"Eres un investigador de Machine Learning avanzado. Basndote en los puntos clave de optimizacin:\\n\"\n",
        "                f\"{summary_md_rf}\\n\"\n",
        "                \"1. Desarrolla un anlisis detallado en prrafos separados para cada punto clave.\\n\"\n",
        "                \"2. Propn un plan de accin prioritizado de 35 pasos claros para stakeholders.\\n\"\n",
        "                \"3. Finaliza con un breve resumen ejecutivo de 23 prrafos enfatizando impacto y prximos hitos.\"\n",
        "            )\n",
        "            analysis_exec_rf = call_openai_explanation(prompt_exec_rf)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Resumen Ejecutivo IA', analysis_exec_rf\n",
        "            ))\n",
        "            # --- Fin Bloque 6 ---\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                '###  Error en seccin Optimizacin Random Forest',\n",
        "                f\"Se produjo un error en Optimizacin Random Forest: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =============================================================================\n",
        "        # 16. Optimizacin Modelo RNN\n",
        "        # =============================================================================\n",
        "        import numpy as np\n",
        "        # --- Bloque 0: Resumen Optimizacin RNN ---\n",
        "        try:\n",
        "            print(\"[DEBUG] 16.1. Bloque 0 Optimizacin RNN: Resumen de mtodos y motores\")\n",
        "\n",
        "            # Funcin de invocacin a OpenAI (igual que en SVR, NN, XGBoost, RF)\n",
        "            def call_openai_explanation(prompt: str, model: str = \"gpt-4\") -> str:\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\":\n",
        "                                \"Eres un experto en optimizacin de hiperparmetros de redes neuronales recurrentes. \"\n",
        "                                \"Analiza y ofrece conclusiones detalladas basadas en los datos proporcionados.\"\n",
        "                            },\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS\n",
        "                    )\n",
        "                    return response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    return f\"[Error llamando a OpenAI: {e}]\"\n",
        "\n",
        "            # 0.1) Verificar resultados en OPT_MODELS\n",
        "            if 'OPT_MODELS' not in globals() or not isinstance(OPT_MODELS, dict):\n",
        "                raise RuntimeError(\"No se encontr OPT_MODELS con resultados de optimizacin RNN\")\n",
        "\n",
        "            valid_engines = {'RandomSearch', 'Bayesian', 'Hyperband', 'Optuna'}\n",
        "            rnn_entries = {\n",
        "                k: v for k, v in OPT_MODELS.items()\n",
        "                if isinstance(k, tuple) and k[0] == 'rnn' and k[2] in valid_engines\n",
        "            }\n",
        "            if not rnn_entries:\n",
        "                raise RuntimeError(\"No se encontraron optimizaciones RNN en OPT_MODELS\")\n",
        "\n",
        "            #  Normalizar payload del mejor RNN para todos los bloques \n",
        "            # Elegimos la clave que maximiza/minimiza el score segn la mtrica\n",
        "            # (usamos la misma lgica que df_sel, pero sobre rnn_entries)\n",
        "            keys = list(rnn_entries.keys())\n",
        "            scores = [rnn_entries[k]['score'] for k in keys]\n",
        "            metrics = [rnn_entries[k].get('metric','').upper() for k in keys]\n",
        "            if metrics[0] == 'R2':\n",
        "                best_idx = int(np.argmax(scores))\n",
        "            else:\n",
        "                best_idx = int(np.argmin(scores))\n",
        "            best_key = keys[best_idx]\n",
        "\n",
        "            payload_raw      = OPT_MODELS[best_key]\n",
        "            p                = _normalize_payload(payload_raw)\n",
        "            model_rnn        = p['model']\n",
        "            sx_rnn, sy_rnn   = p.get('sx'), p.get('sy')\n",
        "            cols_rnn         = p['cols']\n",
        "            score_rnn        = p['score']\n",
        "            metric_rnn       = p['metric']\n",
        "            best_params_rnn  = p['best_params']\n",
        "            #  fin normalizacin \n",
        "\n",
        "            # 0.2) Construir lista de registros resumen\n",
        "            summary_records = []\n",
        "            for (_, metodo, motor), payload in rnn_entries.items():\n",
        "                score = payload.get('score')\n",
        "                # Extraer hiperparmetros ptimos\n",
        "                best_est = payload.get('best')\n",
        "                if best_est is not None and hasattr(best_est, 'get_params'):\n",
        "                    params = best_est.get_params()\n",
        "                else:\n",
        "                    params = payload.get('params', {}) or payload.get('best_params', {})\n",
        "\n",
        "                # Slo los hiperparmetros relevantes\n",
        "                hp_keys = ['units', 'dropout_rate', 'learning_rate', 'epochs', 'batch_size']\n",
        "                best_hp = {k: params.get(k) for k in hp_keys if k in params}\n",
        "\n",
        "                # Aadir registro\n",
        "                rec = {\n",
        "                    'Mtodo': metodo,\n",
        "                    'Motor': motor,\n",
        "                    'metric': payload.get('metric'),  #  aqu incluyes la mtrica\n",
        "                    'Score': score\n",
        "                }\n",
        "                rec.update({f\"Best_{k}\": v for k, v in best_hp.items()})\n",
        "                summary_records.append(rec)\n",
        "\n",
        "            # 0.3) DataFrame de resumen\n",
        "            df_rnn = pd.DataFrame(summary_records)\n",
        "            df_rnn.rename(\n",
        "                columns={col: col.replace(\"Best_\", \"\") for col in df_rnn.columns if col.startswith(\"Best_\")},\n",
        "                inplace=True\n",
        "            )\n",
        "            self.sections.append((\n",
        "                \"### RNN Optimizacin: Resumen de Mtodos y Motores\",\n",
        "                df_rnn\n",
        "            ))\n",
        "\n",
        "            # 0.4) Preparar prompt para IA\n",
        "            prompt_lines = []\n",
        "            for rec in summary_records:\n",
        "                hp_str = \", \".join(f\"{k}={rec[f'Best_{k}']}\" for k in best_hp.keys())\n",
        "                prompt_lines.append(\n",
        "                    f\"Mtodo: {rec['Mtodo']}, Motor: {rec['Motor']}, \"\n",
        "                    f\"Score: {rec['Score']:.4f}, Hiperparmetros: {hp_str}\"\n",
        "                )\n",
        "            prompt = (\n",
        "                \"He obtenido los siguientes resultados de optimizacin para el modelo RNN:\\n\\n\"\n",
        "                + \"\\n\".join(prompt_lines)\n",
        "                + \"\\n\\n\"\n",
        "                \"1. Describe en detalle las fortalezas y debilidades de cada motor de bsqueda \"\n",
        "                \"(RandomSearch, Bayesian, Hyperband, Optuna) aplicado a RNN.\\n\"\n",
        "                \"2. Compara los scores y explica por qu una configuracin es superior.\\n\"\n",
        "                \"3. Analiza el impacto de los hiperparmetros optimizados en entrenamiento y generalizacin.\\n\"\n",
        "                \"4. Propn estrategias avanzadas para mejorar la RNN \"\n",
        "                \"(learningrate scheduling, regularizacin, early stopping, augmentation de series temporales).\\n\"\n",
        "                \"5. Sugiere un roadmap de prximos pasos para validar robustez y escalar el modelo.\"\n",
        "            )\n",
        "\n",
        "            print(\"[DEBUG] 16.2. Llamando a OpenAI para anlisis generativo RNN optimizacin\")\n",
        "            analysis = call_openai_explanation(prompt)\n",
        "            self.sections.append((\n",
        "                \"### RNN Optimizacin: Anlisis Generativo\",\n",
        "                analysis\n",
        "            ))\n",
        "            # --- Fin Bloque 0 ---\n",
        "\n",
        "            # --- 1. Curvas de Ajuste Real vs. Predicho y Residuos para Random Forest Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.3. Iniciando Bloque 1: Curvas Real vs Predicho y Residuos RNN Optimizado\")\n",
        "\n",
        "                # ========== Cargar modelo RNN entrenado desde metadatos ==========\n",
        "                import pickle, glob\n",
        "\n",
        "                # Buscar el archivo ms reciente para el mtodo y motor seleccionados\n",
        "                archivos_meta = sorted(glob.glob(f\"modelos_opt/rnn_{metodo.lower()}_{motor.lower()}_opt_*.pkl\"))\n",
        "                if not archivos_meta:\n",
        "                    raise FileNotFoundError(f\"No se encontr el archivo de metadatos para {metodo} + {motor}\")\n",
        "                archivo_meta = archivos_meta[-1]\n",
        "\n",
        "                # Cargar metadatos\n",
        "                with open(archivo_meta, \"rb\") as f:\n",
        "                    metadata = pickle.load(f)\n",
        "\n",
        "                # Reconstruir ruta al modelo .keras\n",
        "                ts = metadata[\"fecha\"]\n",
        "                ruta_modelo = f\"modelos_opt/rnn_{metodo.lower()}_{motor.lower()}_opt_{ts}.keras\"\n",
        "\n",
        "                # Cargar modelo\n",
        "                model_rnn = load_model(ruta_modelo)\n",
        "\n",
        "\n",
        "                import pickle\n",
        "                from scipy.stats import skew, kurtosis\n",
        "                from sklearn.metrics import r2_score\n",
        "                import matplotlib.pyplot as plt\n",
        "                from tensorflow.keras.models import load_model\n",
        "\n",
        "                # 1.1 Seleccionar la mejor configuracin segn la mtrica\n",
        "                # Reconstruimos un pequeo DataFrame para elegir el mejor ndice\n",
        "                valid_engines = {'RandomSearch','Bayesian','Hyperband','Optuna'}\n",
        "                df_sel = pd.DataFrame([\n",
        "                    {\n",
        "                        'Mtodo': k[1],\n",
        "                        'Motor':  k[2],\n",
        "                        'Score':  v['score'],\n",
        "                        'Mtrica': v['metric']\n",
        "                    }\n",
        "                    for k, v in OPT_MODELS.items()\n",
        "                    if k[0]=='rnn' and k[2] in valid_engines\n",
        "                ])\n",
        "                if df_sel.empty:\n",
        "                    raise RuntimeError(\"No hay optimizaciones RNN registradas en OPT_MODELS\")\n",
        "                # Elegir mejor row\n",
        "                if df_sel['Mtrica'].str.upper().iloc[0] == 'R2':\n",
        "                    idx_best = df_sel['Score'].idxmax()\n",
        "                else:\n",
        "                    idx_best = df_sel['Score'].idxmin()\n",
        "                best_row = df_sel.loc[idx_best]\n",
        "                metodo   = best_row['Mtodo']\n",
        "                motor    = best_row['Motor']\n",
        "\n",
        "                # 1.1bis) Extraer los params del payload en lugar de Best_*\n",
        "                payload    = OPT_MODELS[('rnn', metodo, motor)]\n",
        "                # si guardaste 'params' en OPT_MODELS\n",
        "                params     = payload.get('params') or payload['best'].get_params()\n",
        "                # slo las cinco keys que nos interesan\n",
        "                hp_keys    = ['units','dropout_rate','learning_rate','epochs','batch_size']\n",
        "                best_params = { k: params[k] for k in hp_keys }\n",
        "\n",
        "                # 1.2 Cargar payload y metadatos\n",
        "                # Usamos el payload normalizado:\n",
        "                model     = model_rnn\n",
        "                sx        = sx_rnn\n",
        "                sy        = sy_rnn\n",
        "                cols      = cols_rnn\n",
        "\n",
        "                #  Sanitizacin unificada de columnas para RNN \n",
        "                raw_cols_rnn       = cols_rnn      # viene de _normalize_payload\n",
        "                sanitized_cols_rnn = [sanitize_name(c) for c in raw_cols_rnn]\n",
        "                effective_cols     = [c for c in sanitized_cols_rnn if c in X_test.columns]\n",
        "                print(f\"[DEBUG] 16.4. RNN cols esperadas: {len(raw_cols_rnn)}, sanitizadas vlidas: {len(effective_cols)}  {effective_cols}\")\n",
        "\n",
        "                #  Se mantiene: comprobacin del modelo\n",
        "                from tensorflow.keras.models import Sequential\n",
        "                if not isinstance(model_rnn, Sequential):\n",
        "                    raise TypeError(\"[ERROR] El objeto 'model_rnn' no es un modelo vlido de Keras. Se ha sobrescrito accidentalmente?\")\n",
        "\n",
        "                # ========== Verificar columnas ==========\n",
        "                expected_cols = cols_rnn\n",
        "                actual_cols = [c for c in expected_cols if c in X_test.columns]\n",
        "                print(f\"[DEBUG] 16.5. RNN cols esperadas: {len(expected_cols)}, sanitizadas vlidas: {len(actual_cols)}  {actual_cols}\")\n",
        "\n",
        "                if len(actual_cols) != len(expected_cols):\n",
        "                    raise ValueError(\n",
        "                        f\"[ERROR] Las columnas del modelo RNN ({len(expected_cols)} esperadas) \"\n",
        "                        f\"no coinciden con las disponibles en X_test ({len(actual_cols)} encontradas).\"\n",
        "                        f\"\\nEsperadas: {expected_cols}\\nEncontradas: {actual_cols}\"\n",
        "                    )\n",
        "\n",
        "                # ========== Escalar y preparar datos ==========\n",
        "                X_test_sel_df = X_test[actual_cols]\n",
        "                X_scaled = sx_rnn.transform(X_test_sel_df.values)\n",
        "                print(f\"[DEBUG] 16.6. X_scaled.shape = {X_scaled.shape}\")\n",
        "                print(f\"[DEBUG] 16.7. n_samples = {X_scaled.shape[0]}, n_features = {X_scaled.shape[1]}\")\n",
        "\n",
        "                X3_test = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "                print(f\"[DEBUG] 16.8. X3_test.shape = {X3_test.shape}\")\n",
        "                print(\"[DEBUG] 16.9. Llamando a model_rnn.predict con entrada:\", X3_test.shape)\n",
        "\n",
        "                # ========== Prediccin ==========\n",
        "                y_pred_raw = model_rnn.predict(X3_test).ravel()\n",
        "\n",
        "                print(f\"[DEBUG] 16.10. X3_test.shape = {X3_test.shape}\")\n",
        "                print(\"[DEBUG] 16.11. Llamando a model_rnn.predict con entrada:\", X3_test.shape)\n",
        "\n",
        "                y_pred     = sy_rnn.inverse_transform(y_pred_raw.reshape(-1,1)).ravel() if sy_rnn else y_pred_raw\n",
        "                y_true     = Y_test.values.ravel()\n",
        "\n",
        "                # 1.4 Grfica Predicho vs Real\n",
        "                fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "                ax1.scatter(y_true, y_pred, alpha=0.6)\n",
        "                ax1.plot([y_true.min(), y_true.max()],\n",
        "                        [y_true.min(), y_true.max()],\n",
        "                        'r--', linewidth=2, label='Ideal')\n",
        "                ax1.set_xlabel(\"Y real\")\n",
        "                ax1.set_ylabel(\"Y predicho\")\n",
        "                ax1.set_title(f\"RNN Optimizado ({metodo}-{motor}) Predicho vs Real\")\n",
        "                ax1.legend()\n",
        "                self.sections.append((\n",
        "                    f\"### RNN Optimizado: Predicho vs Real ({metodo}-{motor})\",\n",
        "                    fig1\n",
        "                ))\n",
        "\n",
        "                # 1.5 Grfica de Residuos\n",
        "                residuals = y_true - y_pred\n",
        "                fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "                ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "                ax2.axhline(0, color='r', linestyle='--', linewidth=2)\n",
        "                ax2.set_xlabel(\"Y predicho\")\n",
        "                ax2.set_ylabel(\"Residuo\")\n",
        "                ax2.set_title(f\"RNN Optimizado ({metodo}-{motor}) Residuos\")\n",
        "                self.sections.append((\n",
        "                    f\"### RNN Optimizado: Residuos ({metodo}-{motor})\",\n",
        "                    fig2\n",
        "                ))\n",
        "\n",
        "                # 1.6 Tabla de estadsticas de residuos\n",
        "                stats_df = pd.DataFrame({\n",
        "                    'Mtrica': ['Media', 'Desviacin', 'Skew', 'Kurtosis', '25%', '50%', '75%'],\n",
        "                    'Valor': [\n",
        "                        residuals.mean(),\n",
        "                        residuals.std(),\n",
        "                        skew(residuals),\n",
        "                        kurtosis(residuals),\n",
        "                        *np.quantile(residuals, [0.25, 0.5, 0.75])\n",
        "                    ]\n",
        "                })\n",
        "                self.sections.append((\n",
        "                    f\"### RNN Optimizado: Estadsticas de Residuos ({metodo}-{motor})\",\n",
        "                    stats_df\n",
        "                ))\n",
        "\n",
        "                # 1.7 Anlisis generativo con IA\n",
        "                prompt = (\n",
        "                    f\"Para el mejor RNN optimizado (Mtodo={metodo}, Motor={motor}) tenemos:\\n\"\n",
        "                    f\"- Rango Y real: [{y_true.min():.4f}, {y_true.max():.4f}]\\n\"\n",
        "                    f\"- Rango Y predicho: [{y_pred.min():.4f}, {y_pred.max():.4f}]\\n\"\n",
        "                    f\"- Estadsticas de residuos: media={residuals.mean():.4f}, \"\n",
        "                    f\"std={residuals.std():.4f}, skew={skew(residuals):.4f}, \"\n",
        "                    f\"kurtosis={kurtosis(residuals):.4f}, \"\n",
        "                    f\"quantiles25/50/75={[f'{q:.4f}' for q in np.quantile(residuals,[0.25,0.5,0.75])]}.\\n\\n\"\n",
        "                    \"1. Analiza la calidad del ajuste considerando ambas grficas: identifica sesgos o heterocedasticidad.\\n\"\n",
        "                    \"2. Comenta sobre la normalidad de los errores.\\n\"\n",
        "                    \"3. Propn ajustes de HPO o transformaciones de datos para mejorar la RNN.\"\n",
        "                )\n",
        "                print(\"[DEBUG] 16.12. Llamando a OpenAI para anlisis de curvas RNN optimizado\")\n",
        "                analysis = call_openai_explanation(prompt)\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Anlisis Calidad Ajuste\",\n",
        "                    analysis\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                # 1) Imprime la excepcin para trazar el error\n",
        "                print(f\"[DEBUG] Error Bloque1 Optimizacin RNN: {e}\")\n",
        "                # 2) Luego sigues registrando la seccin de error como antes\n",
        "                self.sections.append((\n",
        "                    \"###  Error Bloque 1 Optimizacin RNN\",\n",
        "                    f\"Se produjo un error en Curvas Real vs Predicho y Residuos RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 1 ---\n",
        "            # --- 2. Importancia Relativa de Hiperparmetros para RNN Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.13. Iniciando bloque 2: Importancia de Hiperparmetros RNN Optimizado\")\n",
        "\n",
        "                import seaborn as sns\n",
        "                from sklearn.metrics import r2_score\n",
        "                import pickle\n",
        "\n",
        "                # --- a) Recuperar el best_row del bloque 1 ---\n",
        "                # (Asegrate de que best_row lo tienes en el scope,  bien vuelve a\n",
        "                #  reconstruir tu pequeo df_sel y lo vuelves a extraer)\n",
        "                valid_engines = {'RandomSearch','Bayesian','Hyperband','Optuna'}\n",
        "                df_sel = pd.DataFrame([\n",
        "                    {'Mtodo':k[1], 'Motor':k[2], 'Score':v['score'], 'Mtrica':v['metric']}\n",
        "                    for k,v in OPT_MODELS.items()\n",
        "                    if k[0]=='rnn' and k[2] in valid_engines\n",
        "                ])\n",
        "                if df_sel['Mtrica'].str.upper().iloc[0]=='R2':\n",
        "                    idx_best = df_sel['Score'].idxmax()\n",
        "                else:\n",
        "                    idx_best = df_sel['Score'].idxmin()\n",
        "                best_row = df_sel.loc[idx_best]\n",
        "\n",
        "                # --- b) Recuperar payload y metadatos de OPT_MODELS ---\n",
        "                key = ('rnn', best_row['Mtodo'], best_row['Motor'])\n",
        "                payload = OPT_MODELS[key]\n",
        "\n",
        "                # 2.1) **Carga aqu** los scalers que guardaste\n",
        "                # Usamos el payload normalizado:\n",
        "                model     = model_rnn\n",
        "                sx        = sx_rnn\n",
        "                sy        = sy_rnn\n",
        "                cols      = cols_rnn\n",
        "\n",
        "                # 2.2) ahora extrae base_score y best_params\n",
        "                base_score = payload['score']\n",
        "                if 'params' in payload and payload['params']:\n",
        "                    best_params = payload['params']\n",
        "                else:\n",
        "                    best_params = payload['best'].get_params()\n",
        "\n",
        "                #  Aqu definimos de nuevo los subsets \n",
        "                X_train_sel = X_train[cols]\n",
        "                Y_train_sel = Y_train.copy()\n",
        "                X_test_sel  = X_test[cols]\n",
        "                Y_test_sel  = Y_test.copy()\n",
        "\n",
        "                # summary_records viene del bloque 0: lista de dicts con Mtodo, Motor, Score y Best_<hp>\n",
        "                # Lo transformamos en un DataFrame ms usable\n",
        "                df_hp = pd.DataFrame([\n",
        "                    {\n",
        "                        'Mtodo'       : rec['Mtodo'],\n",
        "                        'Motor'        : rec['Motor'],\n",
        "                        'Score'        : rec['Score'],\n",
        "                        **{hp: rec[f\"Best_{hp}\"] for hp in ['units','dropout_rate','learning_rate','epochs','batch_size']\n",
        "                          if f\"Best_{hp}\" in rec}\n",
        "                    }\n",
        "                    for rec in summary_records\n",
        "                ])\n",
        "\n",
        "                # 2.1 Heatmap Score vs combinacin de dos parmetros continuos, p.e. learning_rate y dropout_rate\n",
        "                heat_params = ['learning_rate','dropout_rate']\n",
        "                heat_recs = []\n",
        "                for _, row in df_hp.iterrows():\n",
        "                    if all(p in row and pd.notna(row[p]) for p in heat_params):\n",
        "                        heat_recs.append({\n",
        "                            heat_params[0]: row[heat_params[0]],\n",
        "                            heat_params[1]: row[heat_params[1]],\n",
        "                            'Score':       row['Score']\n",
        "                        })\n",
        "                if heat_recs:\n",
        "                    df_heat = pd.DataFrame(heat_recs)\n",
        "                    heat = df_heat.pivot(index=heat_params[0], columns=heat_params[1], values='Score')\n",
        "                    fig_heat, ax_heat = plt.subplots(figsize=(6,5))\n",
        "                    sns.heatmap(heat, annot=True, fmt=\".4f\", ax=ax_heat)\n",
        "                    ax_heat.set_title('RNN Optimizado: Heatmap Score vs learning_rate y dropout_rate')\n",
        "                    self.sections.append((\n",
        "                        \"### RNN Optimizado: Heatmap Score vs learning_rate & dropout_rate\",\n",
        "                        fig_heat\n",
        "                    ))\n",
        "\n",
        "                # 2.2 Sensibilidad 1% para cada hiperparmetro (reentrenando sobre TEST)\n",
        "                sens = []\n",
        "                #for param, base_val in best_params.items():\n",
        "                for param, base_val in list(best_params.items())[:3]:  #  limitar a 3 primeros hiperparmetros para evitar evaluar todos.\n",
        "                    if isinstance(base_val, (int, float)):\n",
        "                        #for factor, label in [(1.01, '+1%'), (0.99, '-1%')]:\n",
        "                        for factor, label in [(1.05, '+5%'), (0.95, '-5%')]:      # Reduccin del 1% al 5% para evaluar sensibilidad, disminuyendo ciclos de entrenamiento.\n",
        "                            # Construye nuevos parmetros variando uno solo\n",
        "                            new_params = best_params.copy()\n",
        "                            new_params[param] = base_val * factor\n",
        "\n",
        "                            # Reentrena el modelo con estos new_params\n",
        "                            model = RNNRegressor(**new_params, sy=sy)\n",
        "                            model.fit(\n",
        "                                sx.transform(X_train_sel.values),\n",
        "                                sy.transform(Y_train_sel.values.reshape(-1,1)).ravel()\n",
        "                            )\n",
        "                            # Evala en validacin (o test, segn prefieras)\n",
        "                            #  aqu uso X_test_sel y Y_test en lugar de X_val_sel/Y_val\n",
        "                            Xs = sx.transform(X_test_sel.values)\n",
        "                            X3 = Xs.reshape((Xs.shape[0], 1, Xs.shape[1]))\n",
        "                            y_hat_scaled = model.predict(Xs)        # el wrapper aade la dimensin del timesteps=1\n",
        "                            y_hat        = sy.inverse_transform(y_hat_scaled.reshape(-1,1)).ravel()\n",
        "                            new_score    = r2_score(Y_test.values.ravel(), y_hat)\n",
        "\n",
        "                            sens.append({\n",
        "                              'Parmetro': param,\n",
        "                              'Cambio':    label,\n",
        "                              '%  Score': (new_score - base_score) / abs(base_score) * 100,\n",
        "                              #'%  Score': new_score - base_score,\n",
        "                              'Motor':     best_row['Motor'],\n",
        "                              'Mtodo':    best_row['Mtodo']\n",
        "                          })\n",
        "                if sens:\n",
        "                    df_sens = pd.DataFrame(sens)\n",
        "                    fig_sens, ax_sens = plt.subplots(figsize=(6,4))\n",
        "                    sns.barplot(data=df_sens, x='%  Score', y='Parmetro', hue='Cambio', ax=ax_sens)\n",
        "                    ax_sens.set_title('RNN Optimizado: Sensibilidad del Score 1%')\n",
        "                    self.sections.append((\n",
        "                        \"### RNN Optimizado: Sensibilidad del Score\",\n",
        "                        fig_sens\n",
        "                    ))\n",
        "\n",
        "                # 2.3 IA: anlisis de importancia\n",
        "                # Filtramos slo para el mejor (best_row viene de bloque 1)\n",
        "                lines = []\n",
        "                for row in sens:\n",
        "                    if row['Mtodo']==best_row['Mtodo'] and row['Motor']==best_row['Motor']:\n",
        "                        lines.append(f\"{row['Parmetro']} {row['Cambio']}  {row['%  Score']:.2f}%\")\n",
        "                prompt_hp_rnn = (\n",
        "                    f\"Sensibilidad del Score al 10% para el mejor RNN optimizado \"\n",
        "                    f\"(mtodo {best_row['Mtodo']}, motor {best_row['Motor']}):\\n\"\n",
        "                    + \"\\n\".join(lines)\n",
        "                    + \"\\n\\n1. Qu hiperparmetro impulsa ms la mejora y por qu?\\n\"\n",
        "                    \"2. Basndote en esta sensibilidad, qu foco de ajuste priorizaras?\"\n",
        "                )\n",
        "                print(\"[DEBUG] 16.14. Llamando a OpenAI para importancia de hiperparmetros RNN\")\n",
        "                analysis_hp_rnn = call_openai_explanation(prompt_hp_rnn)\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: IA Importancia Hiperparmetros\",\n",
        "                    analysis_hp_rnn\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                self.sections.append((\n",
        "                    \"###  Error Bloque 2 Optimizacin RNN\",\n",
        "                    f\"Se produjo un error en Importancia de Hiperparmetros RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 2 ---\n",
        "            # --- 3. Distribucin de Mtricas en Validacin Cruzada para RNN Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.15. Iniciando bloque 3: Distribucin de Mtricas CV para RNN Optimizado\")\n",
        "\n",
        "                import pickle\n",
        "                from sklearn.model_selection import cross_validate\n",
        "                from sklearn.pipeline import make_pipeline\n",
        "                from sklearn.compose import TransformedTargetRegressor\n",
        "                from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "                # 3.1) Recuperar payload y metadatos\n",
        "                cols_cv = cols_rnn\n",
        "\n",
        "                # 3.2) Pipeline: escalar X  RNNRegressor\n",
        "                rnn_pipe = make_pipeline(\n",
        "                    StandardScaler(),    # escala X dentro de cada fold\n",
        "                    RNNRegressor()       # tu wrapper, sin pasar sy aqu\n",
        "                )\n",
        "\n",
        "                # 3.3) TransformedTargetRegressor: para escalar Y dentro de cada fold\n",
        "                rnn_ttr = TransformedTargetRegressor(\n",
        "                    regressor   = rnn_pipe,\n",
        "                    transformer = StandardScaler()\n",
        "                )\n",
        "\n",
        "                # 3.4) Ejecutar cross_validate usando directamente el pipeline completo\n",
        "                cv_results = cross_validate(\n",
        "                    rnn_ttr,\n",
        "                    X_train[cols_cv].values,\n",
        "                    Y_train.values.ravel(),\n",
        "                    #cv=5,\n",
        "                    cv=3,                                      # AADIDO PARA ALIGERAR LOS CICLOS DE ENTRENAMIENTO CRUZADO\n",
        "                    scoring={\n",
        "                        'r2':      'r2',\n",
        "                        'neg_mse': 'neg_mean_squared_error',\n",
        "                        'neg_mae': 'neg_mean_absolute_error'\n",
        "                    },\n",
        "                    return_train_score=False\n",
        "                )\n",
        "\n",
        "                # 3.5) Formatear resultados\n",
        "                r2_scores  = cv_results['test_r2']\n",
        "                mse_scores = [-v for v in cv_results['test_neg_mse']]\n",
        "                mae_scores = [-v for v in cv_results['test_neg_mae']]\n",
        "                rmse_scores = np.sqrt(mse_scores)\n",
        "\n",
        "                df_cv = pd.DataFrame({\n",
        "                    'R2':   r2_scores,\n",
        "                    'MSE':  mse_scores,\n",
        "                    'MAE':  mae_scores,\n",
        "                    'RMSE': rmse_scores\n",
        "                })\n",
        "\n",
        "                # 3.6) Boxplot\n",
        "                fig_cv, ax_cv = plt.subplots(figsize=(6,4))\n",
        "                sns.boxplot(data=df_cv, ax=ax_cv)\n",
        "                ax_cv.set_title('RNN Optimizado: Distribucin de Mtricas CV')\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Distribucin de Mtricas CV', fig_cv\n",
        "                ))\n",
        "\n",
        "                # 3.7) Tabla de media  desviacin\n",
        "                stats_cv = df_cv.agg(['mean','std']).T.reset_index().rename(columns={\n",
        "                    'index':'Mtrica','mean':'Media','std':'Desviacin'\n",
        "                })\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Estadsticas CV por Fold', stats_cv\n",
        "                ))\n",
        "\n",
        "                # 3.8) Anlisis generativo\n",
        "                prompt_cv = (\n",
        "                    f\"Validacin cruzada 5 folds RNN optimizado (Mtodo={best_row['Mtodo']}, \"\n",
        "                    f\"Motor={best_row['Motor']}):\\n\"\n",
        "                    f\"- R2 por fold: {r2_scores.tolist()}\\n\"\n",
        "                    f\"- MSE por fold: {mse_scores}\\n\"\n",
        "                    f\"- MAE por fold: {mae_scores}\\n\"\n",
        "                    f\"- RMSE por fold: {rmse_scores}\\n\\n\"\n",
        "                    \"Analiza la estabilidad y generalizacin de la RNN basndote en la dispersin de cada mtrica.\"\n",
        "                )\n",
        "                print(\"[DEBUG] 16.16. Llamando IA para estabilidad CV RNN optimizado\")\n",
        "                analysis_cv = call_openai_explanation(prompt_cv)\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Anlisis Estabilidad CV', analysis_cv\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                self.sections.append((\n",
        "                    \"###  Error Bloque 3 Optimizacin RNN\",\n",
        "                    f\"Se produjo un error en Distribucin de Mtricas CV RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 3 ---\n",
        "\n",
        "            # --- 4. Curvas de Aprendizaje y Validacin para RNN Optimizado ---\n",
        "            # --- 4. Curvas de Aprendizaje y Validacin para RNN Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.17. Iniciando Bloque 4: Curvas de Aprendizaje y Validacin RNN Optimizado\")\n",
        "                import numpy as np\n",
        "                from sklearn.model_selection import learning_curve, validation_curve\n",
        "                from sklearn.pipeline import make_pipeline\n",
        "                from sklearn.compose import TransformedTargetRegressor\n",
        "                from sklearn.preprocessing import StandardScaler\n",
        "                import matplotlib.pyplot as plt\n",
        "\n",
        "                valid_engines = {'RandomSearch','Bayesian','Hyperband','Optuna'}\n",
        "                df_sel = pd.DataFrame([\n",
        "                    {'Mtodo': k[1], 'Motor': k[2], 'Score': v['score'], 'Mtrica': v['metric']}\n",
        "                    for k,v in OPT_MODELS.items()\n",
        "                    if k[0]=='rnn' and k[2] in valid_engines\n",
        "                ])\n",
        "                if df_sel['Mtrica'].str.upper().iloc[0] == 'R2':\n",
        "                    idx_best = df_sel['Score'].idxmax()\n",
        "                else:\n",
        "                    idx_best = df_sel['Score'].idxmin()\n",
        "                best_row = df_sel.loc[idx_best]\n",
        "\n",
        "                p = _normalize_payload(payload_raw)\n",
        "                best_params = p.get('best_params', {})\n",
        "                sx_rnn, sy_rnn = p['sx'], p['sy']\n",
        "                cols_rnn = p['cols']\n",
        "                model_rnn = p['model']\n",
        "\n",
        "                if not best_params:\n",
        "                    best_params = model_rnn.get_params()\n",
        "                    print(\"[DEBUG] Se han obtenido best_params desde model.get_params()\")\n",
        "\n",
        "                #  Reduccin explcita de carga\n",
        "                best_params['epochs'] = min(best_params.get('epochs', 100), 20)\n",
        "                best_params['verbose'] = 0\n",
        "\n",
        "                rnn_pipe = make_pipeline(\n",
        "                    StandardScaler(),\n",
        "                    RNNRegressor(**best_params)\n",
        "                )\n",
        "                rnn_ttr = TransformedTargetRegressor(regressor=rnn_pipe, transformer=StandardScaler())\n",
        "\n",
        "                X_train_sel = X_train[cols_rnn].values[:250]  #  limitar tamao\n",
        "                y_train = Y_train.values.ravel()[:250]\n",
        "\n",
        "                from joblib import parallel_backend\n",
        "                with parallel_backend('threading'):\n",
        "                    train_sizes, train_scores, val_scores = learning_curve(\n",
        "                        rnn_ttr, X_train_sel, y_train,\n",
        "                        cv=2,  #  menos folds\n",
        "                        scoring='r2',\n",
        "                        train_sizes=np.linspace(0.5, 1.0, 2),  #  menos tamaos\n",
        "                        n_jobs=1\n",
        "                    )\n",
        "\n",
        "                train_mean = np.mean(train_scores, axis=1)\n",
        "                val_mean = np.mean(val_scores, axis=1)\n",
        "\n",
        "                fig_lc, ax_lc = plt.subplots(figsize=(4.5,2.8))  #  ms pequeo\n",
        "                ax_lc.plot(train_sizes, train_mean, 'o-', label='Train R')\n",
        "                ax_lc.plot(train_sizes, val_mean, 'o-', label='CV R')\n",
        "                ax_lc.set_title('RNN Optimizado: Curva de Aprendizaje')\n",
        "                ax_lc.set_xlabel('Tamao del set de entrenamiento')\n",
        "                ax_lc.set_ylabel('R')\n",
        "                ax_lc.legend()\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Curva de Aprendizaje', fig_lc\n",
        "                ))\n",
        "\n",
        "                if 'units' not in best_params:\n",
        "                    raise KeyError(\"[ERROR] El parmetro 'units' no est presente en best_params. Claves disponibles: \" + str(list(best_params.keys())))\n",
        "\n",
        "                #units_range = np.unique(np.linspace(\n",
        "                #    max(1, best_params['units']//2),\n",
        "                #    best_params['units']*2,\n",
        "                #    2, dtype=int  #  solo dos valores\n",
        "                #))\n",
        "                #units_range = [best_params['units']]  #  evitar validacin pesada\n",
        "                units = best_params['units']\n",
        "                units_range = np.unique(np.linspace(max(1, units * 0.9), units * 1.1, 3, dtype=int))\n",
        "                tu, vu = validation_curve(\n",
        "                    rnn_ttr, X_train_sel, y_train,\n",
        "                    param_name='regressor__rnnregressor__units',\n",
        "                    param_range=units_range,\n",
        "                    cv=2,\n",
        "                    scoring='r2',\n",
        "                    n_jobs=1\n",
        "                )\n",
        "                fig_uvc, ax_uvc = plt.subplots(figsize=(4.5,2.8))\n",
        "                ax_uvc.plot(units_range, np.mean(tu,axis=1), 'o-', label='Train R')\n",
        "                ax_uvc.plot(units_range, np.mean(vu,axis=1), 'o-', label='CV R')\n",
        "                ax_uvc.set_title('RNN Optimizado: Curva de Validacin units')\n",
        "                ax_uvc.set_xlabel('units')\n",
        "                ax_uvc.set_ylabel('R')\n",
        "                ax_uvc.legend()\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Curva de Validacin units', fig_uvc\n",
        "                ))\n",
        "\n",
        "                dr = best_params['dropout_rate']\n",
        "                dr_range = np.linspace(max(0.0, dr-0.15), min(1.0, dr+0.15), 2)  #  solo dos valores\n",
        "                td, vd = validation_curve(\n",
        "                    rnn_ttr, X_train_sel, y_train,\n",
        "                    param_name='regressor__rnnregressor__dropout_rate',\n",
        "                    param_range=dr_range,\n",
        "                    cv=2,\n",
        "                    scoring='r2',\n",
        "                    n_jobs=1\n",
        "                )\n",
        "                fig_dvc, ax_dvc = plt.subplots(figsize=(4.5,2.8))\n",
        "                ax_dvc.plot(dr_range, np.mean(td,axis=1), 'o-', label='Train R')\n",
        "                ax_dvc.plot(dr_range, np.mean(vd,axis=1), 'o-', label='CV R')\n",
        "                ax_dvc.set_title('RNN Optimizado: Curva de Validacin dropout_rate')\n",
        "                ax_dvc.set_xlabel('dropout_rate')\n",
        "                ax_dvc.set_ylabel('R')\n",
        "                ax_dvc.legend()\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Curva de Validacin dropout_rate', fig_dvc\n",
        "                ))\n",
        "\n",
        "                prompt = (\n",
        "                    f\"Para la RNN optimizada (Mtodo={best_row['Mtodo']}, Motor={best_row['Motor']}):\\n\"\n",
        "                    f\"- Curva de Aprendizaje: tamaos={train_sizes if isinstance(train_sizes, list) else train_sizes.tolist()}, train R={train_mean.tolist()}, cv R={val_mean.tolist()}\\n\"\n",
        "                    f\"- Validacin units: rango={units_range.tolist()}, train={np.mean(tu,axis=1).tolist()}, cv={np.mean(vu,axis=1).tolist()}\\n\"\n",
        "                    f\"- Validacin dropout_rate: rango={dr_range.tolist()}, train={np.mean(td,axis=1).tolist()}, cv={np.mean(vd,axis=1).tolist()}\\n\\n\"\n",
        "                    \"1. Identifica underfitting o overfitting en cada curva.\\n\"\n",
        "                    \"2. Seala brechas clave y su impacto en la generalizacin.\\n\"\n",
        "                    \"3. Recomienda ajustes de units y dropout_rate.\"\n",
        "                )\n",
        "                print(\"[DEBUG] 16.18. Llamando a OpenAI para anlisis de Curvas de Aprendizaje y Validacin RNN\")\n",
        "                analysis = call_openai_explanation(prompt)\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Interpretacin IA de Curvas', analysis\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"[DEBUG] Error en Bloque 4 Optimizacin RNN:\", e)\n",
        "                self.sections.append((\n",
        "                    '###  Error Bloque 4 Optimizacin RNN',\n",
        "                    f\"Se produjo un error en Curvas de Aprendizaje y Validacin RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 4 ---\n",
        "\n",
        "#            try:\n",
        "#                print(\"[DEBUG] Iniciando Bloque 4: Curvas de Aprendizaje y Validacin RNN Optimizado\")\n",
        "#                import numpy as np\n",
        "#                from sklearn.model_selection import learning_curve, validation_curve\n",
        "#                from sklearn.pipeline import make_pipeline\n",
        "#                from sklearn.compose import TransformedTargetRegressor\n",
        "#                from sklearn.preprocessing import StandardScaler\n",
        "#                import matplotlib.pyplot as plt#\n",
        "\n",
        "#                # --- a) Repetir seleccin del mejor modelo ---\n",
        "#                valid_engines = {'RandomSearch','Bayesian','Hyperband','Optuna'}\n",
        "#                df_sel = pd.DataFrame([\n",
        "#                    {'Mtodo': k[1], 'Motor': k[2], 'Score': v['score'], 'Mtrica': v['metric']}\n",
        "#                    for k,v in OPT_MODELS.items()\n",
        "#                    if k[0]=='rnn' and k[2] in valid_engines\n",
        "#                ])\n",
        "#                if df_sel['Mtrica'].str.upper().iloc[0] == 'R2':\n",
        "#                    idx_best = df_sel['Score'].idxmax()\n",
        "#                else:\n",
        "#                    idx_best = df_sel['Score'].idxmin()\n",
        "#                best_row = df_sel.loc[idx_best]\n",
        "\n",
        "#                # --- c) Reconstruir los hiperparmetros ptimos ---\n",
        "#                p = _normalize_payload(payload_raw)\n",
        "#                #best_params = p['best_params']\n",
        "#                best_params = p.get('best_params', {})\n",
        "#                # --- b) Recuperar payload y metadatos ---\n",
        "#                model_rnn     = p['model']\n",
        "#                sx_rnn, sy_rnn= p['sx'], p['sy']\n",
        "#                cols_rnn      = p['cols']\n",
        "\n",
        "#                print(\"[DEBUG] Payload normalizado keys:\", list(p.keys()))\n",
        "#                print(\"[DEBUG] best_params keys:\", list(p.get('best_params', {}).keys()))\n",
        "#                print(\"[DEBUG] best_params keys:\", list(best_params.keys()))\n",
        "\n",
        "#                if not best_params:\n",
        "#                    best_params = model_rnn.get_params()\n",
        "#                    print(\"[DEBUG] Se han obtenido best_params desde model.get_params()\")\n",
        "\n",
        "#                # --- EXTRAER params desde el modelo entrenado ---\n",
        "#                params = model_rnn.get_params()\n",
        "#                print(\"[DEBUG] params keys:\", list(params.keys()))\n",
        "#                units  = params['units']\n",
        "\n",
        "#                if \"units\" not in params:\n",
        "#                    raise ValueError(\"[ERROR] El modelo no contiene la clave 'units' en sus parmetros.\")\n",
        "\n",
        "#                cols_rnn    = p['cols']\n",
        "#                model_rnn   = p['model']\n",
        "#                sx_rnn, sy_rnn = p['sx'], p['sy']\n",
        "\n",
        "#                # --- d) Creamos el Pipeline + TTR para escalar X e Y automticamente ---\n",
        "#                rnn_pipe = make_pipeline(\n",
        "#                    StandardScaler(),\n",
        "#                    RNNRegressor(**params)\n",
        "#                )\n",
        "#                rnn_ttr = TransformedTargetRegressor(regressor=rnn_pipe, transformer=StandardScaler())\n",
        "\n",
        "#                #X_train_sel = X_train[cols].values\n",
        "#                #y_train     = Y_train.values.ravel()\n",
        "#                X_train_sel = X_train[cols_rnn].values[:300]  #  limitar tamao\n",
        "#                y_train = Y_train.values.ravel()[:300]        #  limitar tamao\n",
        "\n",
        "#                # ---- 4.1 Curva de Aprendizaje (R) ----\n",
        "#                from joblib import parallel_backend             # NUEVO para asegurar que se completa el bloque 4\n",
        "#                with parallel_backend('threading'):             # NUEVO para asegurar que se completa el bloque 4\n",
        "#                    train_sizes, train_scores, val_scores = learning_curve(\n",
        "#                        rnn_ttr, X_train_sel, y_train,\n",
        "#                        #cv=5,\n",
        "#                        cv=2,  #  menos folds\n",
        "#                        scoring='r2',\n",
        "#                        #train_sizes=np.linspace(0.1,1.0,5),\n",
        "#                        train_sizes=np.linspace(0.1, 1.0, 2),  #  menos tamaos\n",
        "#                    #    n_jobs=-1\n",
        "#                        n_jobs=1    # o incluso omite n_jobs para que sea secuencial\n",
        "#                    )\n",
        "#                train_mean = np.mean(train_scores, axis=1)\n",
        "#                val_mean   = np.mean(val_scores,   axis=1)\n",
        "\n",
        "#                fig_lc, ax_lc = plt.subplots(figsize=(6,4))\n",
        "#                ax_lc.plot(train_sizes, train_mean, 'o-', label='Train R')\n",
        "#                ax_lc.plot(train_sizes, val_mean,   'o-', label='CV R')\n",
        "#                ax_lc.set_title('RNN Optimizado: Curva de Aprendizaje')\n",
        "#                ax_lc.set_xlabel('Tamao del set de entrenamiento')\n",
        "#                ax_lc.set_ylabel('R')\n",
        "#                ax_lc.legend()\n",
        "#                self.sections.append((\n",
        "#                    '### RNN Optimizado: Curva de Aprendizaje', fig_lc\n",
        "#                ))\n",
        "\n",
        "#                # ---- 4.2 Curva de Validacin para units ----\n",
        "#                # Rango centrado en el valor ptimo\n",
        "#                #units_range = np.unique(np.linspace(\n",
        "#                #    max(1, best_params['units']//2),\n",
        "#                #    best_params['units']*2,\n",
        "#                #    5, dtype=int\n",
        "#                #))\n",
        "#                if 'units' not in best_params:\n",
        "#                    raise KeyError(\"[ERROR] El parmetro 'units' no est presente en best_params. Claves disponibles: \" + str(list(best_params.keys())))\n",
        "\n",
        "#                #units_range = np.unique(np.linspace(\n",
        "#                #    max(1, best_params['units']//2),\n",
        "#                #    best_params['units']*2,\n",
        "#                #    5, dtype=int\n",
        "#                #))\n",
        "#                # AADIDO PARA ALIGERAR LOS CICLOS DE VALIDACION  Y APRENDIZAJE\n",
        "#                units_range = np.unique(np.linspace(\n",
        "#                    max(1, params['units']*0.8),\n",
        "#                    params['units']*1.2,\n",
        "#                    3, dtype=int\n",
        "#                ))\n",
        "\n",
        "#                #fig_vc_u, tu, vu = validation_curve(\n",
        "#                tu, vu = validation_curve(\n",
        "#                    rnn_ttr, X_train_sel, y_train,\n",
        "#                    param_name='regressor__rnnregressor__units',\n",
        "#                    param_range=units_range,\n",
        "#                    cv=5,\n",
        "#                    scoring='r2',\n",
        "#                    n_jobs=-1\n",
        "#                )\n",
        "#                fig_uvc, ax_uvc = plt.subplots(figsize=(6,4))\n",
        "#                ax_uvc.plot(units_range, np.mean(tu,axis=1), 'o-', label='Train R')\n",
        "#                ax_uvc.plot(units_range, np.mean(vu,axis=1), 'o-', label='CV R')\n",
        "#                ax_uvc.set_title('RNN Optimizado: Curva de Validacin units')\n",
        "#                ax_uvc.set_xlabel('units')\n",
        "#                ax_uvc.set_ylabel('R')\n",
        "#                ax_uvc.legend()\n",
        "#                self.sections.append((\n",
        "#                    '### RNN Optimizado: Curva de Validacin units', fig_uvc\n",
        "#                ))\n",
        "\n",
        "#                # ---- 4.3 Curva de Validacin para dropout_rate ----\n",
        "#                dr = best_params['dropout_rate']\n",
        "#                #dr_range = np.linspace(max(0.0, dr-0.2), min(1.0, dr+0.2), 5)\n",
        "#                dr_range = np.linspace(max(0.0, dr-0.1), min(1.0, dr+0.1), 3)   # AADIDO PARA ALIGERAR LOS CICLOS DE ENTRENAMIENTO CRUZADO\n",
        "#                #fig_vc_d, td, vd = validation_curve(\n",
        "#                td, vd = validation_curve(\n",
        "#                    rnn_ttr, X_train_sel, y_train,\n",
        "#                    param_name='regressor__rnnregressor__dropout_rate',\n",
        "#                    param_range=dr_range,\n",
        "#                    cv=5,\n",
        "#                    scoring='r2',\n",
        "#                    n_jobs=-1\n",
        "#                )\n",
        "#                fig_dvc, ax_dvc = plt.subplots(figsize=(6,4))\n",
        "#                ax_dvc.plot(dr_range, np.mean(td,axis=1), 'o-', label='Train R')\n",
        "#                ax_dvc.plot(dr_range, np.mean(vd,axis=1), 'o-', label='CV R')\n",
        "#                ax_dvc.set_title('RNN Optimizado: Curva de Validacin dropout_rate')\n",
        "#                ax_dvc.set_xlabel('dropout_rate')\n",
        "#                ax_dvc.set_ylabel('R')\n",
        "#                ax_dvc.legend()\n",
        "#                self.sections.append((\n",
        "#                    '### RNN Optimizado: Curva de Validacin dropout_rate', fig_dvc\n",
        "#                ))\n",
        "\n",
        "#                # ---- 4.4 Anlisis Generativo IA de las Curvas ----\n",
        "#                prompt = (\n",
        "#                    f\"Para la RNN optimizada (Mtodo={best_row['Mtodo']}, Motor={best_row['Motor']}):\\n\"\n",
        "#                    f\"- Curva de Aprendizaje: tamaos={train_sizes.tolist()}, train R={train_mean.tolist()}, cv R={val_mean.tolist()}\\n\"\n",
        "#                    f\"- Validacin units: rango={units_range.tolist()}, train={np.mean(tu,axis=1).tolist()}, cv={np.mean(vu,axis=1).tolist()}\\n\"\n",
        "#                    f\"- Validacin dropout_rate: rango={dr_range.tolist()}, train={np.mean(td,axis=1).tolist()}, cv={np.mean(vd,axis=1).tolist()}\\n\\n\"\n",
        "#                    \"1. Identifica underfitting o overfitting en cada curva.\\n\"\n",
        "#                    \"2. Seala brechas clave y su impacto en la generalizacin.\\n\"\n",
        "#                    \"3. Recomienda ajustes de units y dropout_rate.\"\n",
        "#                )\n",
        "#                print(\"[DEBUG] Llamando a OpenAI para anlisis de Curvas de Aprendizaje y Validacin RNN\")\n",
        "#                analysis = call_openai_explanation(prompt)\n",
        "#                self.sections.append((\n",
        "#                    '### RNN Optimizado: Interpretacin IA de Curvas', analysis\n",
        "#                ))\n",
        "\n",
        "#            except Exception as e:\n",
        "#                print(\"[DEBUG] Error en Bloque 4 Optimizacin RNN:\", e)  # <--- NUEVO: para diagnstico\n",
        "#                self.sections.append((\n",
        "#                    '###  Error Bloque 4 Optimizacin RNN',\n",
        "#                    f\"Se produjo un error en Curvas de Aprendizaje y Validacin RNN: {e}\"\n",
        "#                ))\n",
        "            # --- Fin Bloque 4 ---\n",
        "\n",
        "            # --- 5. Curvas de Calibracin y Prediccin de Intervalos para RNN Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.19. Iniciando Bloque 5: Curvas de Calibracin y Prediccin de Intervalos RNN Optimizado\")\n",
        "\n",
        "                from sklearn.calibration import calibration_curve\n",
        "                import pandas as _pd\n",
        "                import numpy as np\n",
        "                import matplotlib.pyplot as plt\n",
        "\n",
        "                # 5.1) Recuperar mejor modelo y scalers\n",
        "                valid_engines = {'RandomSearch','Bayesian','Hyperband','Optuna'}\n",
        "                df_sel = pd.DataFrame([\n",
        "                    {'Mtodo': k[1], 'Motor': k[2], 'Score': v['score'], 'Mtrica': v['metric']}\n",
        "                    for k,v in OPT_MODELS.items()\n",
        "                    if k[0]=='rnn' and k[2] in valid_engines\n",
        "                ])\n",
        "                if df_sel['Mtrica'].str.upper().iloc[0] == 'R2':\n",
        "                    idx_best = df_sel['Score'].idxmax()\n",
        "                else:\n",
        "                    idx_best = df_sel['Score'].idxmin()\n",
        "                best_row = df_sel.loc[idx_best]\n",
        "\n",
        "                key     = ('rnn', best_row['Mtodo'], best_row['Motor'])\n",
        "                payload_raw = OPT_MODELS[('rnn', best_row['Mtodo'], best_row['Motor'])]\n",
        "                p           = _normalize_payload(payload_raw)\n",
        "\n",
        "                model_rnn   = p['model']\n",
        "                sx_rnn, sy_rnn = p['sx'], p['sy']\n",
        "                cols_rnn    = p['cols']\n",
        "\n",
        "                # --- Bloque 5: normalizar payload ---\n",
        "                payload_raw = OPT_MODELS[('rnn', best_row['Mtodo'], best_row['Motor'])]\n",
        "                p           = _normalize_payload(payload_raw)\n",
        "\n",
        "                model_rnn   = p['model']\n",
        "                sx_rnn, sy_rnn = p['sx'], p['sy']\n",
        "                cols_rnn    = p['cols']\n",
        "\n",
        "                # 5.2) Preparar datos de test y predecir\n",
        "                X_test_sel = X_test[cols].copy()\n",
        "                y_true     = Y_test.values.ravel()\n",
        "\n",
        "                # tu wrapper ya escala / reshape / inverse_transform internamente\n",
        "                y_pred = model_rnn.predict(X_test_sel)\n",
        "\n",
        "                # 5.3) Curva de calibracin (regresin reliability)\n",
        "                bins = 10\n",
        "                df_cal = _pd.DataFrame({'y_pred': y_pred, 'y_true': y_true})\n",
        "                try:\n",
        "                    df_cal['bin'] = _pd.qcut(df_cal['y_pred'], q=bins, duplicates='drop')\n",
        "                except ValueError:\n",
        "                    df_cal['bin'] = _pd.cut(df_cal['y_pred'], bins=bins)\n",
        "                grp = df_cal.groupby('bin', observed=True).agg({'y_pred':'mean','y_true':'mean'})\n",
        "                prob_pred, prob_true = grp['y_pred'].values, grp['y_true'].values\n",
        "\n",
        "                fig_cal, ax_cal = plt.subplots(figsize=(6,4))\n",
        "                ax_cal.plot(prob_pred, prob_true, marker='o', linewidth=2, label='Calibracin')\n",
        "                ax_cal.plot([prob_pred.min(), prob_pred.max()],\n",
        "                            [prob_pred.min(), prob_pred.max()],\n",
        "                            'k--', label='Ideal')\n",
        "                ax_cal.set_xlabel('Prediccin promedio en bin')\n",
        "                ax_cal.set_ylabel('Valor real promedio')\n",
        "                ax_cal.set_title('RNN Optimizado: Curva de Calibracin')\n",
        "                ax_cal.legend()\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Curva de Calibracin\",\n",
        "                    fig_cal\n",
        "                ))\n",
        "\n",
        "                # 5.4) Intervalos de prediccin 1 STD de residuo\n",
        "                residuals = y_true - y_pred\n",
        "                std_res   = np.std(residuals)\n",
        "                upper     = y_pred + std_res\n",
        "                lower     = y_pred - std_res\n",
        "\n",
        "                fig_int, ax_int = plt.subplots(figsize=(6,4))\n",
        "                ax_int.plot(y_true,     label='Y real')\n",
        "                ax_int.plot(y_pred,     label='Prediccin')\n",
        "                ax_int.fill_between(\n",
        "                    np.arange(len(y_pred)),\n",
        "                    lower, upper,\n",
        "                    alpha=0.3, label='1 STD residuo'\n",
        "                )\n",
        "                ax_int.set_xlabel('ndice de muestra')\n",
        "                ax_int.set_ylabel('Valor')\n",
        "                ax_int.set_title('RNN Optimizado: Intervalos de Prediccin')\n",
        "                ax_int.legend()\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Intervalos de Prediccin\",\n",
        "                    fig_int\n",
        "                ))\n",
        "\n",
        "                # 5.5) Anlisis generativo IA de incertidumbre y calibracin\n",
        "                print(\"[DEBUG] 16.20. Llamando IA para anlisis de incertidumbre y calibracin RNN optimizado\")\n",
        "                prompt_ci_rnn = (\n",
        "                    f\"Para la RNN optimizada (Mtodo={best_row['Mtodo']}, Motor={best_row['Motor']}):\\n\"\n",
        "                    f\"- Calibracin (bins={bins}): predicciones promedio={prob_pred.tolist()}, valores reales promedio={prob_true.tolist()}\\n\"\n",
        "                    f\"- Residuo estndar={std_res:.4f}\\n\\n\"\n",
        "                    \"1. Evala la fiabilidad de la curva de calibracin: donde el modelo est sub- o sobre-calibrado.\\n\"\n",
        "                    \"2. Comenta sobre la amplitud de los intervalos de prediccin y su adecuacin.\\n\"\n",
        "                    \"3. Sugiere mejoras para la calibracin y estimacin de la incertidumbre en la RNN.\"\n",
        "                )\n",
        "                analysis_ci_rnn = call_openai_explanation(prompt_ci_rnn)\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Anlisis de Incertidumbre y Calibracin\",\n",
        "                    analysis_ci_rnn\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                self.sections.append((\n",
        "                    \"###  Error Bloque 5 Optimizacin RNN\",\n",
        "                    f\"Se produjo un error en Curvas de Calibracin y Prediccin de Intervalos RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 5 ---\n",
        "\n",
        "            # --- 6. Resumen Ejecutivo y Road-Map de Siguientes Pasos para RNN Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.21. Iniciando bloque de Resumen Ejecutivo y Road-Map para RNN optimizado\")\n",
        "\n",
        "                # Variables clave (ya definidas en bloques anteriores)\n",
        "                sel_rnn        = best_row['Mtodo']\n",
        "                eng_rnn        = best_row['Motor']\n",
        "                best_score_rnn = best_row['Score']\n",
        "\n",
        "                # Desviacin de R2 en CV (bloque 3) y estadsticas de residuos (bloque 5)\n",
        "                cv_std_r2      = float(np.std(r2_scores))    # r2_scores viene de bloque 3\n",
        "                res_std_rnn    = float(std_res)              # std_res viene de bloque 5\n",
        "                res_kurt_rnn   = float(kurtosis(residuals))  # residuals viene de bloque 5\n",
        "                q25_rnn, q50_rnn, q75_rnn = [\n",
        "                    float(x) for x in np.quantile(residuals, [0.25, 0.5, 0.75])\n",
        "                ]\n",
        "\n",
        "                # Markdown resumen\n",
        "                summary_md_rnn = (\n",
        "                    \"**Puntos Clave Optimizacin RNN:**\\n\"\n",
        "                    f\"- **Mejor combinacin:** Mtodo `{sel_rnn}`, motor `{eng_rnn}`  **Score** = {best_score_rnn:.4f}\\n\"\n",
        "                    f\"- **Robustez del modelo:** desviacin estndar CV R = {cv_std_r2:.4f}, std residuos = {res_std_rnn:.4f}\\n\"\n",
        "                    f\"- **Curtosis de residuos:** {res_kurt_rnn:.4f}, quantiles (25%,50%,75%) = \"\n",
        "                    f\"({q25_rnn:.4f}, {q50_rnn:.4f}, {q75_rnn:.4f})\\n\"\n",
        "                    \"- **Recomendaciones inmediatas:**\\n\"\n",
        "                    \"  1. Ajustar unidades (`units`) y tasa de dropout para controlar varianza.\\n\"\n",
        "                    \"  2. Incorporar learning-rate scheduling y early stopping.\\n\"\n",
        "                    \"  3. Aumentar el set de entrenamiento o aplicar data augmentation en series temporales.\\n\"\n",
        "                    \"  4. Experimentar con arquitecturas hbridas (LSTM bidireccional, Attention).\\n\"\n",
        "                )\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Resumen Ejecutivo y Road-Map\",\n",
        "                    summary_md_rnn\n",
        "                ))\n",
        "\n",
        "                # Llamada a IA para un Resumen Ejecutivo IA\n",
        "                print(\"[DEBUG] 16.22. Llamando IA para Resumen Ejecutivo y Road-Map RNN optimizado\")\n",
        "                prompt_exec_rnn = (\n",
        "                    \"Eres un investigador de Deep Learning avanzado. Basndote en estos puntos clave:\\n\"\n",
        "                    f\"{summary_md_rnn}\\n\\n\"\n",
        "                    \"1. Desarrolla un anlisis detallado en prrafos separados para cada punto clave.\\n\"\n",
        "                    \"2. Propn un plan de accin priorizado de 35 pasos claros para stakeholders.\\n\"\n",
        "                    \"3. Finaliza con un breve resumen ejecutivo de 23 prrafos enfatizando impacto y prximos hitos.\"\n",
        "                )\n",
        "                analysis_exec_rnn = call_openai_explanation(prompt_exec_rnn)\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Resumen Ejecutivo IA\",\n",
        "                    analysis_exec_rnn\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                self.sections.append((\n",
        "                    \"###  Error Bloque 6 Optimizacin RNN\",\n",
        "                    f\"Se produjo un error en Resumen Ejecutivo y Road-Map RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 6 ---\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"###  Error Bloque 0 Optimizacin RNN\",\n",
        "                f\"Se produjo un error al generar el resumen de optimizacin RNN: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =============================================================================\n",
        "        # 17. Anlisis e Interpretacin del Modelo ptimo\n",
        "        # =============================================================================\n",
        "        #try:\n",
        "        #    print(\"[DEBUG] Iniciando seccin Seleccin Integral de Modelo\")\n",
        "        try:\n",
        "            print(\"[DEBUG] 17.1.Iniciando seccin Seleccin Integral de Modelo\")\n",
        "            # DEBUG: Qu claves hay en OPT_MODELS?\n",
        "            print(f\"[DEBUG] 17.2. OPT_MODELS tiene {len(OPT_MODELS)} entradas: {list(OPT_MODELS.keys())}\")\n",
        "            records = []\n",
        "            print(\"[DEBUG] 17.3. records inicializado vaco\")\n",
        "            import numpy as np\n",
        "            import pandas as pd\n",
        "            from sklearn.model_selection      import cross_validate\n",
        "            from sklearn.metrics              import mean_squared_error, mean_absolute_error, r2_score\n",
        "            #from sklearn.calibration          import calibration_curve\n",
        "            import scipy.stats                as st\n",
        "            from tensorflow.keras.models      import load_model\n",
        "\n",
        "            records = []\n",
        "\n",
        "            # --- Bloque 1: Construccin y presentacin de la Tabla de Puntuacin de los Modelos Optimizados ---\n",
        "            for (mt, method, engine), payload in OPT_MODELS.items():\n",
        "                if mt not in {'svr','nn','xgb','rf','rnn'}:\n",
        "                    continue\n",
        "                # --- cargar datos test y scalers ---\n",
        "                cols = payload.get('cols', X_test.columns.tolist())\n",
        "                X_test_sel = X_test[cols].copy()\n",
        "                y_true     = Y_test.values.ravel()\n",
        "\n",
        "                sx = payload.get('sx')\n",
        "                if sx is not None:\n",
        "                    #X_test_scaled = sx.transform(X_test_sel.values)\n",
        "                    X_test_scaled = sx.transform(pd.DataFrame(X_test_sel.values, columns=X_test_sel.columns))       # EVITA WARNING EN LA EJECUCION - NO PARA EJECUCION. SI DA PROBLEMAS ELIMINAR ESTA LINEA.\n",
        "\n",
        "                else:\n",
        "                    X_test_scaled = X_test_sel.values\n",
        "\n",
        "                # --- predicciones y mtricas test ---\n",
        "                if mt == 'rnn':\n",
        "                    # tu wrapper ya escala, reshape e inverse_transform\n",
        "                    model = payload['model']\n",
        "                    y_pred = model.predict(X_test_sel)\n",
        "                else:\n",
        "                    model = payload['model']\n",
        "                    if mt in {'svr','xgb','rf','nn'}:\n",
        "                        # scikeras o sklearn\n",
        "                        y_pred_raw = model.predict(X_test_scaled)\n",
        "                        sy = payload.get('sy')\n",
        "                        if sy is not None:\n",
        "                            y_pred = sy.inverse_transform(y_pred_raw.reshape(-1,1)).ravel()\n",
        "                        else:\n",
        "                            y_pred = y_pred_raw.ravel()\n",
        "                    else:\n",
        "                        y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "                mse_test   = mean_squared_error(y_true, y_pred)\n",
        "                mae_test   = mean_absolute_error(y_true, y_pred)\n",
        "                rmse_test  = np.sqrt(mse_test)\n",
        "                resid      = y_true - y_pred\n",
        "                res_std    = np.std(resid)\n",
        "                res_kurt   = st.kurtosis(resid)\n",
        "\n",
        "                # --- validacin cruzada 5 folds ---\n",
        "                # Construir X_train escalado\n",
        "                cols_tr = payload.get('cols', X_train.columns.tolist())\n",
        "                X_tr_sel = X_train[cols_tr].copy()\n",
        "                y_tr     = Y_train.values.ravel()\n",
        "                sx_tr    = payload.get('sx')\n",
        "                X_tr_s   = sx_tr.transform(X_tr_sel.values) if sx_tr is not None else X_tr_sel.values\n",
        "\n",
        "                from sklearn.model_selection import train_test_split\n",
        "\n",
        "                # Submuestreo del conjunto de entrenamiento\n",
        "                X_sub, _, y_sub, _ = train_test_split(\n",
        "                    X_tr_s, y_tr,\n",
        "                    train_size=0.3,  # usa solo el 30% de los datos para CV\n",
        "                    random_state=42,\n",
        "                    shuffle=True\n",
        "                )\n",
        "\n",
        "                # --- validacin cruzada con proteccin para modelos no-sklean ---\n",
        "                try:\n",
        "                    from sklearn.model_selection import cross_validate\n",
        "                    cvres = cross_validate(\n",
        "                        #model, X_tr_s, y_tr,\n",
        "                        model, X_sub, y_sub,        # Usamos X_sub y y_sub en la validacin cruzada en lugar del conjunto completo.\n",
        "                        #cv=5,\n",
        "                        cv=3,         # REDUCE CV FOLDS PARA MEJORAR LA RAPPIDEZ DE EJECUCION\n",
        "                        scoring={\n",
        "                            'r2':      'r2',\n",
        "                            'neg_mse': 'neg_mean_squared_error',\n",
        "                            'neg_mae': 'neg_mean_absolute_error'\n",
        "                        },\n",
        "                        return_train_score=False,\n",
        "                        n_jobs=-1  #  paraleliza en todos los cores\n",
        "                    )\n",
        "                    r2_folds  = cvres['test_r2']\n",
        "                    mse_folds = [-v for v in cvres['test_neg_mse']]\n",
        "                    mae_folds = [-v for v in cvres['test_neg_mae']]\n",
        "                    r2_cv_std    = float(np.std(r2_folds))\n",
        "                    mse_cv_mean  = float(np.mean(mse_folds))\n",
        "                    mae_cv_mean  = float(np.mean(mae_folds))\n",
        "                    rmse_cv_mean = float(np.mean(np.sqrt(mse_folds)))\n",
        "                except TypeError as ex:\n",
        "                    # no SKL estimator (e.g. keras.Sequential), omitimos CV\n",
        "                    print(f\"[DEBUG] CV omitido por TypeError: {ex}\")\n",
        "                    r2_folds, mse_folds, mae_folds = [], [], []\n",
        "                    r2_cv_std    = np.nan\n",
        "                    mse_cv_mean  = np.nan\n",
        "                    mae_cv_mean  = np.nan\n",
        "                    rmse_cv_mean = np.nan\n",
        "\n",
        "                # --- calibracin manual para regresin ---\n",
        "                import pandas as pd\n",
        "                # un DataFrame para agrupar por deciles de y_pred\n",
        "                df_cal = pd.DataFrame({'y_pred': y_pred, 'y_true': y_true})\n",
        "                # cortamos en 10 bins por cuantiles (evita duplicados)\n",
        "                try:\n",
        "                    #df_cal['bin'] = pd.qcut(df_cal['y_pred'], q=10, duplicates='drop')\n",
        "                    df_cal['bin'] = pd.qcut(df_cal['y_pred'], q=5, duplicates='drop')   # o q=4 si quieres an ms velocidad\n",
        "                except ValueError:\n",
        "                    #df_cal['bin'] = pd.cut(df_cal['y_pred'], bins=10)\n",
        "                    df_cal['bin'] = pd.cut(df_cal['y_pred'], bins=5)                    # o q=4 si quieres an ms velocidad\n",
        "                # agrupamos: media predicha vs media real\n",
        "                grp = df_cal.groupby('bin', observed=True).agg({'y_pred':'mean','y_true':'mean'})\n",
        "                prob_pred = grp['y_pred'].values\n",
        "                prob_true = grp['y_true'].values\n",
        "                # error medio de calibracin\n",
        "                cal_err = float((np.abs(prob_pred - prob_true)).mean())\n",
        "\n",
        "                # --- intervalos de prediccin 1 STD residuo ---\n",
        "                intervals_std = res_std\n",
        "\n",
        "                # --- mtrica principal (score) ---\n",
        "                score = payload.get('score', r2_score(y_true, y_pred))\n",
        "\n",
        "                # --- hiperparmetros optimizados ---\n",
        "                best_params = payload.get('best_params') or payload.get('params') or {}\n",
        "                hp_flat = { f\"hp_{k}\": v for k, v in best_params.items() }\n",
        "\n",
        "                # --- armar registro ---\n",
        "                rec = {\n",
        "                    'modelo':        f\"{mt}-{method}-{engine}\",\n",
        "                    'score':         score,\n",
        "                    'r2_cv_std':     r2_cv_std,\n",
        "                    'mse_cv':        mse_cv_mean,\n",
        "                    'mae_cv':        mae_cv_mean,\n",
        "                    'rmse_cv':       rmse_cv_mean,\n",
        "                    'mse_test':      mse_test,\n",
        "                    'mae_test':      mae_test,\n",
        "                    'rmse_test':     rmse_test,\n",
        "                    'res_std':       res_std,\n",
        "                    'res_kurt':      res_kurt,\n",
        "                    'intervals_std': intervals_std,\n",
        "                    'cal_err':       cal_err,\n",
        "                    **hp_flat\n",
        "                }\n",
        "                records.append(rec)\n",
        "\n",
        "            #df = pd.DataFrame(records)\n",
        "\n",
        "            print(f\"[DEBUG] 17.4. records tendr {len(records)} elementos\")\n",
        "            if records:\n",
        "                print(f\"[DEBUG] 17.5. ejemplar de record[0]: {records[0]}\")\n",
        "            df = pd.DataFrame(records)\n",
        "            print(f\"[DEBUG] 17.6. DataFrame creado con shape={df.shape}\")\n",
        "\n",
        "            # 0) Extraer tipo, mtodo y motor de la columna 'modelo'\n",
        "            df[['tipo','metodo','motor']] = df['modelo'].str.split('-', n=2, expand=True)\n",
        "\n",
        "            # 1) Clonar df y reiniciar ndice\n",
        "            df_results = df.copy()\n",
        "            df_results.reset_index(drop=True, inplace=True)\n",
        "            # Definimos aqu la puntuacin global bruta\n",
        "            df_results['puntuacion_global'] = df_results['score']\n",
        "\n",
        "            # Bloque de normalizacin absoluta lineal y clculo de puntuacin\n",
        "            import pandas as pd\n",
        "            import numpy as np\n",
        "\n",
        "            # Definir funciones de normalizacin\n",
        "            def normalize_high(df, col):\n",
        "                lo, hi = df[col].min(), df[col].max()\n",
        "                if hi == lo:\n",
        "                    return pd.Series(10.0, index=df.index)\n",
        "                return ((df[col] - lo) / (hi - lo) * 10).clip(0, 10)\n",
        "\n",
        "            def normalize_low(df, col):\n",
        "                lo, hi = df[col].min(), df[col].max()\n",
        "                if hi == lo:\n",
        "                    return pd.Series(10.0, index=df.index)\n",
        "                return ((hi - df[col]) / (hi - lo) * 10).clip(0, 10)\n",
        "\n",
        "            # Supongamos que ya tienes df_results con la columna 'puntuacion_global'\n",
        "            # y las mtricas crudas: r2_cv_std, mse_cv, mae_cv, rmse_cv, mse_test, mae_test,\n",
        "            # rmse_test, res_std, res_kurt, intervals_std, cal_err\n",
        "\n",
        "            # Columnas mayor = mejor\n",
        "            cols_high = ['puntuacion_global']\n",
        "\n",
        "            # Columnas menor = mejor\n",
        "            cols_low = [\n",
        "                'r2_cv_std', 'mse_cv', 'mae_cv', 'rmse_cv',\n",
        "                'mse_test', 'mae_test', 'rmse_test',\n",
        "                'res_std', 'res_kurt', 'intervals_std', 'cal_err'\n",
        "            ]\n",
        "\n",
        "            # 1) Normalizar cada mtrica a escala 010\n",
        "            for c in cols_high:\n",
        "                df_results[f'v_{c}'] = normalize_high(df_results, c)\n",
        "\n",
        "            for c in cols_low:\n",
        "                df_results[f'v_{c}'] = normalize_low(df_results, c)\n",
        "\n",
        "            # 2) Definir pesos (ajusta segn IA o tu criterio)\n",
        "            pesos = {\n",
        "                'v_puntuacion_global': 0.2273,\n",
        "                'v_r2_cv_std':         0.0909,\n",
        "                'v_mse_cv':            0.0909,\n",
        "                'v_mae_cv':            0.0455,\n",
        "                'v_rmse_cv':           0.0455,\n",
        "                'v_mse_test':          0.0909,\n",
        "                'v_mae_test':          0.0909,\n",
        "                'v_rmse_test':         0.0455,\n",
        "                'v_res_std':           0.0909,\n",
        "                'v_res_kurt':          0.0455,\n",
        "                'v_intervals_std':     0.0455,\n",
        "                'v_cal_err':           0.0909,\n",
        "            }\n",
        "\n",
        "            # 3) Calcular la puntuacin global final\n",
        "            df_results['puntuacion_global_final'] = 0.0\n",
        "            for vcol, w in pesos.items():\n",
        "                df_results['puntuacion_global_final'] += df_results[vcol] * w\n",
        "\n",
        "            # 4) Reordenar columnas para presentacin\n",
        "            #cols_order = [\n",
        "            #    'modelo', 'tipo', 'metodo', 'motor', 'puntuacion_global'\n",
        "            #] + list(pesos.keys()) + ['puntuacion_global_final']\n",
        "\n",
        "            # 4) Reordenar columnas para presentacin\n",
        "            cols_order = [\n",
        "                'modelo', 'tipo', 'metodo', 'motor', 'puntuacion_global',\n",
        "                # mtricas crudas\n",
        "                'r2_cv_std','mse_cv','mae_cv','rmse_cv',\n",
        "                'mse_test','mae_test','rmse_test',\n",
        "                'res_std','res_kurt','intervals_std','cal_err',\n",
        "            ] + list(pesos.keys()) + ['puntuacion_global_final']\n",
        "\n",
        "            df_results = df_results[cols_order]\n",
        "\n",
        "            # 5) Aadir la seccin al informe\n",
        "            #self.sections.append((\n",
        "            #    '### Seleccin Integral de Modelo: Tabla de Puntuaciones',\n",
        "            #    df_results.reset_index(drop=True)\n",
        "            #))\n",
        "            print(\"[DEBUG] 17.7. Aadiendo seccin de Tabla de Puntuaciones\")\n",
        "            self.sections.append((\n",
        "                '### Seleccin Integral de Modelo: Tabla de Puntuaciones',\n",
        "                df_results.reset_index(drop=True)\n",
        "            ))\n",
        "            print(\"[DEBUG] 17.8. Seccin Tabla de Puntuaciones aadida correctamente\")\n",
        "\n",
        "            # --- Ahora identificamos el mejor y guardamos atributos ---\n",
        "            best_idx = df_results['puntuacion_global_final'].idxmax()\n",
        "            best     = df_results.loc[best_idx]\n",
        "\n",
        "            # Guardamos la info del mejor modelo\n",
        "            self.best_model_info = {\n",
        "                'metodo': best['metodo'],\n",
        "                'motor':  best['motor'],\n",
        "                'score':  best['puntuacion_global_final']\n",
        "            }\n",
        "\n",
        "            # **Guardamos tambin las mtricas que necesitars ms adelante**:\n",
        "            self.r2_cv_std = float(best['r2_cv_std'])\n",
        "            self.res_std   = float(best['res_std'])\n",
        "            self.cal_err   = float(best['cal_err'])\n",
        "\n",
        "            print(f\"[DEBUG] 17.9. Mejor modelo: {self.best_model_info}, \"\n",
        "                  f\"r2_cv_std={self.r2_cv_std}, res_std={self.res_std}, cal_err={self.cal_err}\")\n",
        "            # --- Fin Bloque 1 ---\n",
        "\n",
        "            # --- Bloque 2 - Grfico de puntuaciones globales y Cuadrante Mgico de Modelos Optimizados ----\n",
        "            import matplotlib.pyplot as plt\n",
        "\n",
        "            # 1) Cerramos TODO lo que haya quedado abierto de la optimizacin\n",
        "            #plt.close('all')\n",
        "\n",
        "            # --- Grfico de Barras: Puntuacin Global de cada modelo ---\n",
        "            def plot_global_scores(df):\n",
        "                \"\"\"\n",
        "                Grfico de barras profesional con la puntuacin global de cada modelo.\n",
        "                - df debe tener columnas ['modelo', 'puntuacion_global'].\n",
        "                \"\"\"\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                modelos = df['modelo']\n",
        "                scores = df['puntuacion_global_final']\n",
        "\n",
        "                # Usa una paleta profesional\n",
        "                palette = plt.get_cmap('tab20').colors\n",
        "                ax.bar(modelos, scores, color=palette[:len(modelos)], edgecolor='black')\n",
        "\n",
        "                # Etiquetas y estilo\n",
        "                ax.set_title('Comparacin de Modelos: Puntuacin Global', fontsize=16, fontweight='bold')\n",
        "                ax.set_xlabel('Modelo', fontsize=14)\n",
        "                ax.set_ylabel('Puntuacin Global', fontsize=14)\n",
        "                ax.set_xticks(modelos)\n",
        "                ax.set_xticklabels(modelos, rotation=45, ha='right', fontsize=12)\n",
        "                ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "                plt.tight_layout()\n",
        "                #plt.close(fig)\n",
        "                return fig\n",
        "\n",
        "            def plot_magic_quadrant(df):\n",
        "                \"\"\"\n",
        "                Magic Quadrant que sita cada modelo en robustez vs rendimiento.\n",
        "                - df debe tener columnas ['v_r2_cv_std', 'puntuacion_global_final', 'modelo'].\n",
        "                \"\"\"\n",
        "                # Robustez: normalizamos de 010 a 01\n",
        "                robustez = (df['v_r2_cv_std'] / 10).clip(0,1)\n",
        "\n",
        "                # Rendimiento: puntuacin global final (010) tambin a 01\n",
        "                rendimiento = (df['puntuacion_global_final'] / 10).clip(0,1)\n",
        "\n",
        "                mask       = robustez.notna() & rendimiento.notna()\n",
        "                robustez   = robustez[mask]\n",
        "                rendimiento = rendimiento[mask]\n",
        "                labels     = df['modelo'][mask]\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(8, 8))\n",
        "                ax.scatter(robustez, rendimiento, s=150, edgecolor='k', alpha=0.8)\n",
        "\n",
        "                for x, y, label in zip(robustez, rendimiento, labels):\n",
        "                    ax.annotate(label,\n",
        "                                xy=(x, y),\n",
        "                                xytext=(5, 5),\n",
        "                                textcoords='offset points',\n",
        "                                fontsize=12)\n",
        "\n",
        "                # Lneas medias\n",
        "                ax.axvline(robustez.mean(),    color='grey', linestyle='--')\n",
        "                ax.axhline(rendimiento.mean(), color='grey', linestyle='--')\n",
        "\n",
        "                ax.set_title('Magic Quadrant de Modelos', fontsize=16, fontweight='bold')\n",
        "                ax.set_xlabel('Robustez (v_r2_cv_std / 10)', fontsize=14)\n",
        "                ax.set_ylabel('Rendimiento (puntuacin global / 10)', fontsize=14)\n",
        "                ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "                # Lmites de 0 a 1\n",
        "                ax.set_xlim(0, 1.05)\n",
        "                ax.set_ylim(0, 1.05)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                return fig\n",
        "\n",
        "\n",
        "            fig1 = plot_global_scores(df_results)\n",
        "            fig2 = plot_magic_quadrant(df_results)\n",
        "\n",
        "            self.sections.append(('### Comparacin de Modelos: Puntuacin Global', fig1))\n",
        "            self.sections.append(('### Magic Quadrant de Modelos', fig2))\n",
        "            # --- Fin Bloque 2 ---\n",
        "\n",
        "            # --- Bloque 3: Explicacin profunda por IA y detalle final del modelo seleccionado ---\n",
        "            print(\"[DEBUG] 17.10. Iniciando Bloque 3: Explicacin IA y detalle final\")\n",
        "\n",
        "            import openai\n",
        "            import pandas as pd\n",
        "\n",
        "            # 1) Serializamos la tabla de puntuaciones para inyectarla en el prompt\n",
        "            table_csv = df_results.to_csv(index=False)\n",
        "\n",
        "            print(\"[DEBUG] 17.11. Iniciando llamada a IA para anlisis de la tabla y grficas\")\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            He aqu los resultados completos de la **Seleccin Integral de Modelos**:\n",
        "\n",
        "            1) **Tabla de Puntuaciones** (CSV):\n",
        "            {table_csv}\n",
        "\n",
        "            2) **Grfica de Barras** titulada \"Comparacin de Modelos: Puntuacin Global\".\n",
        "\n",
        "            3) **Magic Quadrant** titulado \"Magic Quadrant de Modelos\".\n",
        "\n",
        "            Por favor, realiza lo siguiente:\n",
        "\n",
        "            A) Explica **detalladamente** la tabla de puntuaciones:\n",
        "              - Cmo se ha estructurado,\n",
        "              - Qu representa cada columna de valor (tanto las mtricas crudas como sus escalas 010),\n",
        "              - Cmo cada modelo se sita segn esos valores,\n",
        "              - Comparativa explcita entre todos los modelos.\n",
        "\n",
        "            B) Comenta la **grfica de barras**:\n",
        "              - Qu nos dice del rendimiento absoluto de cada modelo,\n",
        "              - Destaca los valores mximo y mnimo.\n",
        "\n",
        "            C) Interpreta el **Magic Quadrant**:\n",
        "              - Define en qu ejes est midiendo robustez y rendimiento,\n",
        "              - Seala qu cuadrante (altoalto, altobajo, bajoalto, bajobajo) es deseable,\n",
        "              - Identifica modelos lderes y rezagados.\n",
        "\n",
        "            D) Con todo lo anterior, **elige el mejor modelo optimizado**.\n",
        "              - Explica por qu (qu mtricas y pesos lo han elevado al primer puesto),\n",
        "              - Enumera sus virtudes y posibles puntos dbiles.\n",
        "\n",
        "            E) Tras esa eleccin, **construye una tabla** con todos los detalles del modelo seleccionado:\n",
        "              1. `modelo` (p.ej. rf-pearson-randomsearch)\n",
        "              2. `tipo` (RF, SVR, )\n",
        "              3. `mtodo` (Pearson, Boruta, )\n",
        "              4. `motor` de optimizacin (RandomSearch, Optuna, )\n",
        "              5. **Todos** sus hiperparmetros ptimos\n",
        "              6. **Listado** de variables X usadas\n",
        "              7. Resumen de sus **mtricas crudas** y su `puntuacion_global_final`\n",
        "            \"\"\"\n",
        "\n",
        "            # 2) Llamada a OpenAI\n",
        "            resp = _client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[{\"role\":\"user\", \"content\": prompt}],\n",
        "                temperature=TEMPERATURE_VAL,\n",
        "                max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "            )\n",
        "            analysis_text = resp.choices[0].message.content\n",
        "\n",
        "            # 3) Insertamos el anlisis como seccin\n",
        "            self.sections.append((\n",
        "                \"###  Explicacin IA de la Seleccin Final\",\n",
        "                analysis_text\n",
        "            ))\n",
        "\n",
        "            # 4) Identificamos el mejor modelo en df_results\n",
        "            best_idx = df_results['puntuacion_global_final'].idxmax()\n",
        "            best     = df_results.loc[best_idx]\n",
        "            mt, method, engine = best['tipo'], best['metodo'], best['motor']\n",
        "\n",
        "            # Guardamos la info del mejor modelo para bloques posteriores\n",
        "            self.best_model_info = {\n",
        "                'metodo': best['metodo'],\n",
        "                'motor':  best['motor'],\n",
        "                'score':  best['puntuacion_global_final']\n",
        "            }\n",
        "\n",
        "            # 5) Recuperamos payload y detalles\n",
        "            payload  = OPT_MODELS[(mt, method, engine)]\n",
        "            hp       = payload.get('best_params', {}) or payload.get('params', {})\n",
        "            features = payload.get('cols', X_train.columns.tolist())\n",
        "\n",
        "            # 6) Preparamos el detalle en un DataFrame\n",
        "            metrics_keys = [\n",
        "                'r2_cv_std','mse_cv','mae_cv','rmse_cv',\n",
        "                'mse_test','mae_test','rmse_test',\n",
        "                'res_std','res_kurt','intervals_std','cal_err',\n",
        "                'puntuacion_global','puntuacion_global_final'\n",
        "            ]\n",
        "\n",
        "            detail_dict = {\n",
        "                'modelo':               best['modelo'],\n",
        "                'tipo':                 mt,\n",
        "                'metodo':               method,\n",
        "                'motor':                engine,\n",
        "                'variables_usadas':     \", \".join(features),\n",
        "                'puntuacion_final':  best['puntuacion_global_final'],\n",
        "            }\n",
        "            # aadimos hiperparmetros\n",
        "            for k, v in hp.items():\n",
        "                detail_dict[f\"hp_{k}\"] = v\n",
        "            # aadimos mtricas\n",
        "            for k in metrics_keys:\n",
        "                detail_dict[k] = best[k]\n",
        "\n",
        "            detail_df = pd.DataFrame([detail_dict])\n",
        "\n",
        "            # 7) Insertamos la tabla de detalle\n",
        "            self.sections.append((\n",
        "                \"### Detalle del Modelo Seleccionado\",\n",
        "                detail_df.reset_index(drop=True)\n",
        "            ))\n",
        "            # --- Fin Bloque 3 ---\n",
        "\n",
        "            # --- Bloque 4: Interpretacin xIA del Mejor Modelo Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 17.12. Iniciando Bloque 4: Interpretacin xIA\")\n",
        "\n",
        "                import numpy as np\n",
        "                import pandas as pd\n",
        "                import shap\n",
        "                import matplotlib.pyplot as plt\n",
        "                import json\n",
        "                import warnings\n",
        "                from scipy.optimize import minimize\n",
        "                from sklearn.inspection import partial_dependence\n",
        "                from sklearn.inspection import (\n",
        "                    permutation_importance,\n",
        "                    PartialDependenceDisplay\n",
        "                )\n",
        "\n",
        "                # --- FUNCIN ROBUSTA PARA EXTRAER grid/curves DE partial_dependence ---\n",
        "                def extract_grid_and_curves(pd_res, kind=\"average\"):\n",
        "                    # 1. Si es Bunch (scikit-learn >= 0.24)\n",
        "                    if hasattr(pd_res, \"keys\"):\n",
        "                        keys = list(pd_res.keys())\n",
        "                        # PDP\n",
        "                        if kind == \"average\":\n",
        "                            # Puede que solo exista average/grid_values\n",
        "                            if \"average\" in keys and \"grid_values\" in keys:\n",
        "                                grid = pd_res[\"grid_values\"][0]\n",
        "                                avg = pd_res[\"average\"][0]\n",
        "                                return grid, avg\n",
        "                            else:\n",
        "                                raise RuntimeError(f\"PDP: No se encuentran 'average' o 'grid_values' en Bunch: {keys}\")\n",
        "                        # ICE\n",
        "                        elif kind == \"individual\":\n",
        "                            if \"individual\" in keys and \"grid_values\" in keys:\n",
        "                                grid = pd_res[\"grid_values\"][0]\n",
        "                                curves = pd_res[\"individual\"][0]\n",
        "                                return grid, curves\n",
        "                            else:\n",
        "                                raise RuntimeError(f\"ICE: No se encuentran 'individual' o 'grid_values' en Bunch: {keys}\")\n",
        "                        else:\n",
        "                            raise ValueError(f\"Tipo desconocido: {kind}\")\n",
        "\n",
        "                    # 2. Si es tuple o list (formato antiguo)\n",
        "                    elif isinstance(pd_res, (tuple, list)):\n",
        "                        if kind == \"average\":\n",
        "                            avg = pd_res[0]\n",
        "                            grid = pd_res[1][0]\n",
        "                            return grid, avg\n",
        "                        elif kind == \"individual\":\n",
        "                            curves = pd_res[0]\n",
        "                            grid = pd_res[1][0]\n",
        "                            return grid, curves\n",
        "                        else:\n",
        "                            raise ValueError(f\"Tipo desconocido: {kind}\")\n",
        "\n",
        "                    # 3. Si es ndarray (muy raro)\n",
        "                    elif isinstance(pd_res, np.ndarray):\n",
        "                        return np.arange(pd_res.shape[-1]), pd_res\n",
        "\n",
        "                    # 4. Si es dict (algunos edge-cases custom)\n",
        "                    elif isinstance(pd_res, dict):\n",
        "                        # Para ICE y PDP, usa las claves\n",
        "                        if kind == \"average\" and \"average\" in pd_res and \"grid_values\" in pd_res:\n",
        "                            grid = pd_res[\"grid_values\"][0]\n",
        "                            avg = pd_res[\"average\"][0]\n",
        "                            return grid, avg\n",
        "                        elif kind == \"individual\" and \"individual\" in pd_res and \"grid_values\" in pd_res:\n",
        "                            grid = pd_res[\"grid_values\"][0]\n",
        "                            curves = pd_res[\"individual\"][0]\n",
        "                            return grid, curves\n",
        "                        else:\n",
        "                            raise RuntimeError(f\"Objeto dict sin claves esperadas: {pd_res.keys()}\")\n",
        "\n",
        "                    else:\n",
        "                        raise RuntimeError(f\"No se reconoce el objeto retornado por partial_dependence: {type(pd_res)}, contenido: {pd_res}\")\n",
        "\n",
        "\n",
        "                # 1) Identificar el mejor modelo\n",
        "                best_idx = df_results['puntuacion_global_final'].idxmax()\n",
        "                best     = df_results.loc[best_idx]\n",
        "                mt, method, engine = best['tipo'], best['metodo'], best['motor']\n",
        "                payload  = OPT_MODELS[(mt, method, engine)]\n",
        "                model    = payload['model']\n",
        "                features = payload.get('cols', X_train.columns.tolist())\n",
        "\n",
        "                # 2) Preparar datos (escalado si procede)\n",
        "                sx     = payload.get('sx')\n",
        "                X_tr   = X_train[features].copy()\n",
        "                X_te   = X_test[features].copy()\n",
        "                X_tr_s = sx.transform(X_tr.values) if sx else X_tr.values\n",
        "                X_te_s = sx.transform(X_te.values) if sx else X_te.values\n",
        "                y_te   = Y_test.values.ravel()\n",
        "\n",
        "                # \n",
        "                # 3) SHAP\n",
        "                # \n",
        "                print(\"[DEBUG] 17.13. Iniciando Interpretacin xIA con SHAP\")\n",
        "                # Silenciar warning de feature names\n",
        "                warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
        "                if mt in ('xgb','rf'):\n",
        "                    explainer = shap.TreeExplainer(model)\n",
        "                else:\n",
        "                    # Uso DeepExplainer para redes y KernelExplainer solo si no hay Deep support\n",
        "                    try:\n",
        "                        background = shap.kmeans(X_tr_s, 100)            # solo 100 puntos de background\n",
        "                        explainer = shap.DeepExplainer(model, background)\n",
        "                    except Exception:\n",
        "                        # fallback a KernelExplainer con pocos nsamples\n",
        "                        explainer = shap.KernelExplainer(model.predict, shap.sample(X_tr_s, 100))\n",
        "                # Reducir test a las primeras 100 filas para acelerar\n",
        "                X_sample = X_te_s[:100]\n",
        "                shap_vals = explainer.shap_values(X_sample)\n",
        "\n",
        "#                    explainer = shap.KernelExplainer(model.predict, shap.sample(X_tr_s, 100))\n",
        "#                shap_vals = explainer.shap_values(X_te_s)\n",
        "\n",
        "                fig_shap, ax_shap = plt.subplots(figsize=(8,6))\n",
        "                #shap.summary_plot(shap_vals, X_te_s, feature_names=features, show=False)\n",
        "                shap.summary_plot(shap_vals, X_sample, feature_names=features, show=False)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                #plt.close(fig_shap)\n",
        "                self.sections.append((\"### xIA: SHAP Summary\", fig_shap))\n",
        "\n",
        "                # \n",
        "                # 4) Permutation Feature Importance\n",
        "                # \n",
        "                print(\"[DEBUG] 17.14. Iniciando Interpretacin xIA con Permutation Feature Importance\")\n",
        "                perm = permutation_importance(model, X_te_s, y_te,\n",
        "                                              n_repeats=10, random_state=0, n_jobs=-1)\n",
        "                idx_imp   = np.argsort(perm.importances_mean)[::-1]\n",
        "                top_feats = [features[i] for i in idx_imp[:5]]\n",
        "\n",
        "                fig_perm, ax_perm = plt.subplots(figsize=(8,6))\n",
        "                ax_perm.barh(np.arange(len(top_feats)),\n",
        "                            perm.importances_mean[idx_imp[:5]],\n",
        "                            xerr=perm.importances_std[idx_imp[:5]],\n",
        "                            align='center')\n",
        "                ax_perm.set_yticks(np.arange(len(top_feats)))\n",
        "                ax_perm.set_yticklabels(top_feats)\n",
        "                ax_perm.invert_yaxis()\n",
        "                ax_perm.set_title(\"xIA: Permutation Importance\")\n",
        "                ax_perm.set_xlabel(\"Mean decrease in score\")\n",
        "                plt.tight_layout()\n",
        "                #plt.close(fig_perm)\n",
        "                self.sections.append((\"### xIA: Permutation Importance\", fig_perm))\n",
        "\n",
        "                # \n",
        "                # 5) PDP + ICE (scikit-learn)\n",
        "                # \n",
        "                print(\"[DEBUG] 17.15. Iniciando Interpretacin xIA con PDP e ICE\")\n",
        "\n",
        "                pdp_records = []\n",
        "                ice_records = []\n",
        "\n",
        "                for feat in top_feats:\n",
        "                    # PDP\n",
        "                    pdp_res = partial_dependence(\n",
        "                        model, X_tr, features=[feat], kind=\"average\", grid_resolution=20\n",
        "                    )\n",
        "                    #grid, avg = extract_grid_and_curves(pdp_res, kind='average')\n",
        "\n",
        "                    try:\n",
        "                        grid, avg = extract_grid_and_curves(pdp_res, kind='average')\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] PDP Extraction error: {e}, objeto: {type(pdp_res)}, contenido: {pdp_res}\")\n",
        "                        raise\n",
        "\n",
        "                    for x, y in zip(grid, avg):\n",
        "                        pdp_records.append({\"feature\": feat, \"grid\": float(x), \"pdp\": float(y)})\n",
        "\n",
        "                    # grfico PDP\n",
        "                    fig_pdp, ax_pdp = plt.subplots(figsize=(6,4))\n",
        "                    ax_pdp.plot(grid, avg, marker='o')\n",
        "                    ax_pdp.set_title(f\"PDP de {feat}\")\n",
        "                    plt.tight_layout()\n",
        "                    #plt.close(fig_pdp)\n",
        "                    self.sections.append((f\"### xIA: PDP {feat}\", fig_pdp))\n",
        "\n",
        "                    # ICE\n",
        "                    ice_res = partial_dependence(\n",
        "                        model, X_tr, features=[feat], kind=\"individual\", grid_resolution=20\n",
        "                    )\n",
        "                    #grid, curves = extract_grid_and_curves(ice_res, kind='individual')\n",
        "\n",
        "                    try:\n",
        "                        grid, curves = extract_grid_and_curves(ice_res, kind='individual')\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] ICE Extraction error: {e}, objeto: {type(ice_res)}, contenido: {ice_res}\")\n",
        "                        raise\n",
        "\n",
        "                    for obs_idx, curve in enumerate(curves):\n",
        "                        for x, y in zip(grid, curve):\n",
        "                            ice_records.append({\n",
        "                                \"feature\":     feat,\n",
        "                                \"grid\":        float(x),\n",
        "                                \"ice\":         float(y),\n",
        "                                \"observation\": int(obs_idx)\n",
        "                            })\n",
        "                    # grfico ICE\n",
        "                    fig_ice, ax_ice = plt.subplots(figsize=(6,4))\n",
        "                    for curve in curves:\n",
        "                        ax_ice.plot(grid, curve, alpha=0.3)\n",
        "                    ax_ice.set_title(f\"ICE de {feat}\")\n",
        "                    plt.tight_layout()\n",
        "                    #plt.close(fig_ice)\n",
        "                    self.sections.append((f\"### xIA: ICE {feat}\", fig_ice))\n",
        "\n",
        "                # convertir a DataFrame\n",
        "                pdp_results = pd.DataFrame(pdp_records)\n",
        "                ice_results = pd.DataFrame(ice_records)\n",
        "\n",
        "                # \n",
        "                # 6) ALE a mano\n",
        "                # \n",
        "                print(\"[DEBUG] 17.16. Iniciando Interpretacin xIA con ALE\")\n",
        "                def compute_ale(model, X, feat, grid_points=20):\n",
        "                    Xdf = X.copy()\n",
        "                    vals = np.linspace(Xdf[feat].min(),\n",
        "                                      Xdf[feat].max(),\n",
        "                                      grid_points+1)\n",
        "                    centers = (vals[:-1] + vals[1:]) / 2\n",
        "                    diffs = []\n",
        "                    for low, high in zip(vals[:-1], vals[1:]):\n",
        "                        X_low  = Xdf.copy(); X_high = Xdf.copy()\n",
        "                        X_low.loc[Xdf[feat]  > high, feat] = low\n",
        "                        X_high.loc[Xdf[feat] <= low,  feat] = high\n",
        "                        arr_low  = sx.transform(X_low.values)  if sx else X_low.values\n",
        "                        arr_high = sx.transform(X_high.values) if sx else X_high.values\n",
        "                        pred_low  = model.predict(arr_low)\n",
        "                        pred_high = model.predict(arr_high)\n",
        "                        diffs.append(np.mean(pred_high - pred_low))\n",
        "                    cum = np.cumsum(diffs)\n",
        "                    cum -= cum.mean()\n",
        "                    return centers, cum\n",
        "\n",
        "                ale_records = []\n",
        "                for feat in top_feats:\n",
        "                    centers, ale_curve = compute_ale(model, X_tr, feat)\n",
        "                    fig_ale, ax_ale = plt.subplots(figsize=(6,4))\n",
        "                    ax_ale.plot(centers, ale_curve, marker='o')\n",
        "                    ax_ale.set_title(f\"ALE de {feat}\")\n",
        "                    plt.tight_layout()\n",
        "                    #plt.close(fig_ale)\n",
        "                    self.sections.append((f\"### xIA: ALE {feat}\", fig_ale))\n",
        "                    for c, a in zip(centers, ale_curve):\n",
        "                        ale_records.append({\n",
        "                            \"feature\": feat,\n",
        "                            \"grid\":    float(c),\n",
        "                            \"ale\":     float(a)\n",
        "                        })\n",
        "                ale_results = pd.DataFrame(ale_records)\n",
        "\n",
        "                # \n",
        "                # 7) Counterfactuals ligeros\n",
        "                # \n",
        "                print(\"[DEBUG] 17.17. Iniciando Interpretacin xIA con Counterfactuals\")\n",
        "                def generate_counterfactual(x0, delta=1.0):\n",
        "                    x0_arr = x0.values if hasattr(x0, \"values\") else x0\n",
        "                    x0_s   = sx.transform([x0_arr])[0] if sx else x0_arr\n",
        "                    y0     = model.predict([x0_s])[0]\n",
        "                    def obj(x): return np.sum((x - x0_s)**2)\n",
        "                    cons = {\n",
        "                        'type': 'ineq',\n",
        "                        'fun':  lambda x: model.predict([x])[0] - y0 - delta\n",
        "                    }\n",
        "                    res = minimize(obj, x0_s, constraints=cons)\n",
        "                    if res.success:\n",
        "                        xcf_s = res.x\n",
        "                        xcf   = sx.inverse_transform([xcf_s])[0] if sx else xcf_s\n",
        "                        ycf   = model.predict([xcf_s])[0]\n",
        "                        return xcf, ycf\n",
        "                    else:\n",
        "                        return None, None\n",
        "\n",
        "                cf_explanations = []\n",
        "                for i in range(min(3, len(X_te))):\n",
        "                    x0 = X_te.iloc[i]\n",
        "                    xcf, ycf = generate_counterfactual(x0, delta=1.0)\n",
        "                    if xcf is not None:\n",
        "                        delta_feats = xcf - x0.values\n",
        "                        fig_cf, ax_cf = plt.subplots(figsize=(6,4))\n",
        "                        ax_cf.bar(X_te.columns, delta_feats, edgecolor='k')\n",
        "                        ax_cf.set_title(f\"Counterfactual #{i} ( para +1 unidad de y)\")\n",
        "                        ax_cf.set_xticklabels(X_te.columns, rotation=45, ha='right')\n",
        "                        plt.tight_layout()\n",
        "                        #plt.close(fig_cf)\n",
        "                        self.sections.append((f\"### xIA: Counterfactual {i}\", fig_cf))\n",
        "                        cf_explanations.append({\n",
        "                            \"observation\":     i,\n",
        "                            \"x0\":              x0.to_dict(),\n",
        "                            \"counterfactual\":  {features[j]: float(xcf[j]) for j in range(len(features))},\n",
        "                            \"y_pred_original\": float(model.predict([sx.transform([x0.values])[0]]) if sx else model.predict([x0.values])[0]),\n",
        "                            \"y_pred_cf\":       float(ycf)\n",
        "                        })\n",
        "\n",
        "                # \n",
        "                # 8) RESUMIR LOS RESULTADOS PARA ENVIAR A LA IA\n",
        "                # \n",
        "                print(\"[DEBUG] 17.18. Resumiendo resultados XAI para envo a la IA\")\n",
        "\n",
        "                def resumir_xai_json(\n",
        "                    shap_vals, features, perm, idx_imp, top_feats,\n",
        "                    pdp_results, ice_results, ale_results, cf_explanations,\n",
        "                    n_feats=2, n_vals=8\n",
        "                ):\n",
        "                    # Selecciona las top n_feats\n",
        "                    features_lite = [features[i] for i in idx_imp[:n_feats]]\n",
        "\n",
        "                    # 1. SHAP: solo valores medios globales para top features\n",
        "                    shap_summary = {\n",
        "                        \"features\": features_lite,\n",
        "                        \"shap_mean_abs\": [float(np.mean(np.abs(shap_vals[:, features.index(f)]))) for f in features_lite]\n",
        "                    }\n",
        "\n",
        "                    # 2. Permutation: solo top features\n",
        "                    perm_summary = {\n",
        "                        \"features\": features_lite,\n",
        "                        \"importances_mean\": [float(perm.importances_mean[features.index(f)]) for f in features_lite],\n",
        "                        \"importances_std\": [float(perm.importances_std[features.index(f)]) for f in features_lite],\n",
        "                    }\n",
        "\n",
        "                    # 3. PDP: solo top features y primeros n_vals puntos\n",
        "                    pdp_summary = {\n",
        "                        feat: {\n",
        "                            \"grid\": pdp_results.loc[pdp_results.feature == feat, \"grid\"].astype(float).values[:n_vals].tolist(),\n",
        "                            \"pdp\": pdp_results.loc[pdp_results.feature == feat, \"pdp\"].astype(float).values[:n_vals].tolist()\n",
        "                        }\n",
        "                        for feat in features_lite\n",
        "                    }\n",
        "\n",
        "                    # 4. ICE: solo top features, primeros n_vals, y primeras 2 observaciones\n",
        "                    ice_summary = {}\n",
        "                    for feat in features_lite:\n",
        "                        obs_ids = ice_results[ice_results.feature == feat][\"observation\"].unique()[:2]\n",
        "                        ice_summary[feat] = {\n",
        "                            int(obs): {\n",
        "                                \"grid\": ice_results[(ice_results.feature == feat) & (ice_results.observation == obs)][\"grid\"].astype(float).values[:n_vals].tolist(),\n",
        "                                \"ice\": ice_results[(ice_results.feature == feat) & (ice_results.observation == obs)][\"ice\"].astype(float).values[:n_vals].tolist()\n",
        "                            }\n",
        "                            for obs in obs_ids\n",
        "                        }\n",
        "\n",
        "                    # 5. ALE: igual que PDP\n",
        "                    ale_summary = {\n",
        "                        feat: {\n",
        "                            \"grid\": ale_results.loc[ale_results.feature == feat, \"grid\"].astype(float).values[:n_vals].tolist(),\n",
        "                            \"ale\": ale_results.loc[ale_results.feature == feat, \"ale\"].astype(float).values[:n_vals].tolist()\n",
        "                        }\n",
        "                        for feat in features_lite\n",
        "                    }\n",
        "\n",
        "                    # 6. Counterfactuals: solo 1 o 2 ejemplos, y solo diferencias principales\n",
        "                    cf_summary = []\n",
        "                    for c in cf_explanations[:2]:\n",
        "                        d = {k: c[k] for k in [\"observation\", \"y_pred_original\", \"y_pred_cf\"]}\n",
        "                        d[\"counterfactual\"] = {k: v for k, v in list(c[\"counterfactual\"].items())[:n_feats]}\n",
        "                        cf_summary.append(d)\n",
        "\n",
        "                    return {\n",
        "                        \"shap\": shap_summary,\n",
        "                        \"perm\": perm_summary,\n",
        "                        \"pdp\": pdp_summary,\n",
        "                        \"ice\": ice_summary,\n",
        "                        \"ale\": ale_summary,\n",
        "                        \"cf\": cf_summary\n",
        "                    }\n",
        "\n",
        "                #  Serializacin resumida \n",
        "                xai_resumido = resumir_xai_json(\n",
        "                    shap_vals, features, perm, idx_imp, top_feats,\n",
        "                    pdp_results, ice_results, ale_results, cf_explanations,\n",
        "                    n_feats=2, n_vals=8\n",
        "                )\n",
        "\n",
        "                shap_json_lite = json.dumps(xai_resumido[\"shap\"], indent=2, ensure_ascii=False)\n",
        "                perm_json_lite = json.dumps(xai_resumido[\"perm\"], indent=2, ensure_ascii=False)\n",
        "                pdp_json_lite  = json.dumps(xai_resumido[\"pdp\"], indent=2, ensure_ascii=False)\n",
        "                ice_json_lite  = json.dumps(xai_resumido[\"ice\"], indent=2, ensure_ascii=False)\n",
        "                ale_json_lite  = json.dumps(xai_resumido[\"ale\"], indent=2, ensure_ascii=False)\n",
        "                cf_json_lite   = json.dumps(xai_resumido[\"cf\"], indent=2, ensure_ascii=False)\n",
        "\n",
        "                # Guardar los JSON para el siguiente bloque\n",
        "                self.shap_json_lite = shap_json_lite\n",
        "                self.perm_json_lite = perm_json_lite\n",
        "                self.pdp_json_lite  = pdp_json_lite\n",
        "                self.ice_json_lite  = ice_json_lite\n",
        "                self.ale_json_lite  = ale_json_lite\n",
        "                self.cf_json_lite   = cf_json_lite\n",
        "\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                print(\"[DEBUG] Se ha producido una excepcin en Interpretacin xIA:\")\n",
        "                print(f\"[DEBUG] Tipo de excepcin: {type(e).__name__}\")\n",
        "                print(f\"[DEBUG] Mensaje: {e}\")\n",
        "                print(\"[DEBUG] Traza completa:\")\n",
        "                traceback.print_exc()\n",
        "                self.sections.append((\n",
        "                    \"###  Error en interpretacin xIA del modelo\",\n",
        "                    f\"{type(e).__name__}: {e}\"\n",
        "                ))\n",
        "\n",
        "            # --- Fin Bloque 4 ---\n",
        "\n",
        "            # --- Bloque 5: Interpretacin Fsica-Qumica del Mejor Modelo Optimizado mediante IA ---\n",
        "            print(\"[DEBUG] 17.19. Iniciando Interpretacin Fsico - Qumicamediante IA  del Mejor Modelo Optimizado\")\n",
        "            try:\n",
        "                import os\n",
        "                import json\n",
        "                #import pandas as pd\n",
        "                #from PyPDF2 import PdfReader\n",
        "                import openai\n",
        "\n",
        "                # 1) Cargar y extraer texto de contexto de varios PDFs\n",
        "                from PyPDF2 import PdfReader\n",
        "                import pandas as pd\n",
        "\n",
        "                # --- Gestin de subida y carga de contexto desde memoria en Colab ---\n",
        "                import io\n",
        "                import pandas as pd\n",
        "                from PyPDF2 import PdfReader\n",
        "                from google.colab import files\n",
        "                import io\n",
        "\n",
        "                # 2.1) Abrimos selector de archivos y leemos el .xlsx subido\n",
        "                uploaded = files.upload()\n",
        "                excel_file = next(f for f in uploaded.keys() if f.lower().endswith('.xlsx'))\n",
        "                book_bytes = io.BytesIO(uploaded[excel_file])\n",
        "\n",
        "                # 2.2) Cargamos **todas** las hojas; cada DataFrame toma su primera fila como cabecera\n",
        "                import pandas as _pd\n",
        "                sheets_dict = _pd.read_excel(book_bytes, sheet_name=None)\n",
        "\n",
        "                print(\"Hojas encontradas en el Excel:\", list(sheets_dict.keys()))\n",
        "\n",
        "                # 2.3) Convertimos cada hoja en lista de dicts (records) sin predefinir columnas\n",
        "                mapa_variables = {\n",
        "                    name: (\n",
        "                        df\n",
        "                        .astype(str)            # convertimos todo a string para evitar Timestamp u otros tipos\n",
        "                        .dropna(how='all')      # eliminamos filas completamente vacas\n",
        "                        .to_dict(orient=\"records\")\n",
        "                    )\n",
        "                    for name, df in sheets_dict.items()\n",
        "                }\n",
        "                # 2.4) Aadimos al contexto global\n",
        "                #contexto[\"mapa_variables\"] = mapa_variables\n",
        "\n",
        "                # 3) Cargar y extraer texto de TODOS los PDFs subidos\n",
        "                pdf_texts = {}\n",
        "                for fn, content in uploaded.items():\n",
        "                    if fn.lower().endswith(\".pdf\"):\n",
        "                        reader = PdfReader(io.BytesIO(content))\n",
        "                        full = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
        "                        pdf_texts[fn] = full\n",
        "\n",
        "                # 4) Del PDF de proceso aislamos la seccin relevante (si existe)\n",
        "                proceso_pdf = next((k for k in pdf_texts if \"Procesado_Datos_Linares\" in k), None)\n",
        "                if proceso_pdf:\n",
        "                    texto = pdf_texts[proceso_pdf]\n",
        "                    start_marker = \"VARIABLES Y PARTICULARIDADES DEL PROCESO\"\n",
        "                    end_marker   = \"Etapas del proceso\"\n",
        "                    i0 = texto.find(start_marker)\n",
        "                    i1 = texto.find(end_marker, i0 + len(start_marker))\n",
        "                    if i0 != -1 and i1 != -1:\n",
        "                        proceso_texto = texto[i0:i1].strip()\n",
        "                    else:\n",
        "                        proceso_texto = texto\n",
        "                else:\n",
        "                    proceso_texto = \"\"\n",
        "\n",
        "                # 5) Texto de la memoria TFM (si existe)\n",
        "                memoria_pdf = next((k for k in pdf_texts if \"Memoria Final TFM\" in k), None)\n",
        "                memoria_texto = pdf_texts.get(memoria_pdf, \"\")\n",
        "\n",
        "                # 6) Construir el contexto completo\n",
        "                contexto = {\n",
        "                    \"proceso_texto\":   proceso_texto,\n",
        "                    \"memoria_texto\":   memoria_texto,\n",
        "                    \"mapa_variables\":  mapa_variables,\n",
        "                    \"dominio\": (\n",
        "                        \"Este modelo optimiza la prediccin de la variable objetivo en la planta de \"\n",
        "                        \"aglomerados de Linares. Se considera la interaccin fsico-qumica entre resinas, \"\n",
        "                        \"temperaturas de secado y niveles de humedad, as como parmetros de prensa y fases de corte.\"\n",
        "                    )\n",
        "                }\n",
        "\n",
        "                # Ya puedes usar `contexto` y `contexto_variables` en tu prompt de OpenAI.\n",
        "                print(\"[DEBUG] 17.20. Iniciando llamada a IA para anlisis Fsico - Qumico del proceso\")\n",
        "\n",
        "                # 1. Prompt SHAP + Permutation\n",
        "                prompt_shap_perm = f\"\"\"\n",
        "            Dominio industrial:\n",
        "            {contexto['dominio']}\n",
        "\n",
        "            Resultados SHAP (importancia global resumida):\n",
        "            {self.shap_json_lite}\n",
        "\n",
        "            Permutation Importance (top features):\n",
        "            {self.perm_json_lite}\n",
        "\n",
        "            Explica cmo afectan estas variables al proceso y su impacto fsico-qumico.\n",
        "            \"\"\"\n",
        "\n",
        "                # 2. Prompt PDP + ALE\n",
        "                prompt_pdp_ale = f\"\"\"\n",
        "            Dominio industrial:\n",
        "            {contexto['dominio']}\n",
        "\n",
        "            PDP (efecto parcial) para principales variables:\n",
        "            {self.pdp_json_lite}\n",
        "\n",
        "            ALE (efecto acumulado local) para principales variables:\n",
        "            {self.ale_json_lite}\n",
        "\n",
        "            Relaciona las tendencias observadas en los grficos PDP y ALE con los fenmenos fsicos-qumicos reales.\n",
        "            \"\"\"\n",
        "\n",
        "                # 3. Prompt ICE\n",
        "                prompt_ice = f\"\"\"\n",
        "            Dominio industrial:\n",
        "            {contexto['dominio']}\n",
        "\n",
        "            ICE (efectos individuales):\n",
        "            {self.ice_json_lite}\n",
        "\n",
        "            Describe la variabilidad operativa observada en ICE y su impacto en condiciones reales de planta.\n",
        "            \"\"\"\n",
        "\n",
        "                # 4. Prompt Counterfactuals\n",
        "                prompt_cf = f\"\"\"\n",
        "            Dominio industrial:\n",
        "            {contexto['dominio']}\n",
        "\n",
        "            Counterfactuals (slo ejemplos principales):\n",
        "            {self.cf_json_lite}\n",
        "\n",
        "            Proporciona recomendaciones sobre ajustes operativos para mejorar el KPI.\n",
        "            \"\"\"\n",
        "\n",
        "                prompts = [\n",
        "                    (\"### Interpretacin SHAP + Permutation\", prompt_shap_perm),\n",
        "                    (\"### Interpretacin PDP + ALE\", prompt_pdp_ale),\n",
        "                    (\"### Interpretacin ICE\", prompt_ice),\n",
        "                    (\"### Interpretacin Counterfactuals\", prompt_cf)\n",
        "                ]\n",
        "\n",
        "                for titulo, prompt in prompts:\n",
        "                    print(f\"[DEBUG] 17.21. Llamando a IA para {titulo}\")\n",
        "                    resp = _client.chat.completions.create(\n",
        "                        model=\"gpt-4\",\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    explicacion = resp.choices[0].message.content\n",
        "                    self.sections.append((titulo, explicacion))\n",
        "\n",
        "            except Exception as e:\n",
        "                self.sections.append((\n",
        "                    \"###  Error en Interpretacin Fsica-Qumica\",\n",
        "                    f\"{e}\"\n",
        "                ))\n",
        "\n",
        "        except Exception as e:\n",
        "            #self.sections.append((\n",
        "            #    \"###  Error en seleccin integral de modelo\",\n",
        "            #    f\"Se produjo un error al generar la seccin de seleccin integral: {e}\"\n",
        "            #))\n",
        "            # DEBUG: imprimo excepcin completa para rastrear el fallo\n",
        "            import traceback\n",
        "            print(f\"[DEBUG] ERROR: {type(e).__name__}: {e}\")\n",
        "            traceback.print_exc()\n",
        "            self.sections.append((\n",
        "                \"###  Error en seleccin integral de modelo\",\n",
        "                f\"Se produjo un error al generar la seccin de seleccin integral: {type(e).__name__}: {e}\"\n",
        "            ))\n",
        "        # --- Fin Bloque 5 - Seccin Integral del Modelo ---\n",
        "\n",
        "        # --- Bloque 6: Curvas Y real vs. Y predicha por variable X (modelo ptimo) ---\n",
        "        try:\n",
        "            print(\"[DEBUG] 17.22. Iniciando bloque de curvas Y real vs predicho por variable X\")\n",
        "\n",
        "            import numpy as np\n",
        "            from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "            import matplotlib.pyplot as plt\n",
        "            import pandas as pd\n",
        "\n",
        "            # DEBUG: listar todas las secciones aadidas hasta ahora\n",
        "            print(\"[DEBUG] 17.23. Secciones en self.sections:\")\n",
        "            for idx, (title, content) in enumerate(self.sections):\n",
        "                tipo = type(content).__name__\n",
        "                cols = content.columns.tolist() if hasattr(content, \"columns\") else None\n",
        "                print(f\"   {idx:02d}: '{title}'  {tipo}\" + (f\", cols={cols}\" if cols else \"\"))\n",
        "            print(\"[DEBUG] 17.24. Ahora buscamos la tabla con 'puntuacion_global_final'\")\n",
        "\n",
        "            # 1) Recuperar el DataFrame de resultados globales con 'puntuacion_global_final'\n",
        "            df_results = None\n",
        "            for title, content in self.sections:\n",
        "                if isinstance(content, pd.DataFrame) and 'puntuacion_global_final' in content.columns:\n",
        "                    df_results = content\n",
        "                    break\n",
        "            if df_results is None:\n",
        "                raise RuntimeError(\"No se encontr la tabla de puntuaciones globales (df_results)\")\n",
        "\n",
        "            # 2) Identificar el mejor modelo\n",
        "            best_idx = df_results['puntuacion_global_final'].idxmax()\n",
        "            best_row = df_results.loc[best_idx]\n",
        "            tipo, metodo, motor = best_row['tipo'], best_row['metodo'], best_row['motor']\n",
        "            print(f\"[DEBUG] 17.25. Modelo ptimo identificado: {tipo}-{metodo}-{motor}\")\n",
        "\n",
        "            # 3) Recuperar payload, modelo, scalers y lista de variables\n",
        "            payload = OPT_MODELS[(tipo, metodo, motor)]\n",
        "            model   = payload['model']\n",
        "            features = payload.get('cols', self.g['X_test'].columns.tolist())\n",
        "            sx      = payload.get('sx')   # scaler de X, si existe\n",
        "            sy      = payload.get('sy')   # scaler de Y, si existe\n",
        "\n",
        "            # 4) Preparar datos de test y predicciones\n",
        "            X_test_sel = self.g['X_test'][features].copy()\n",
        "            y_true     = self.g['Y_test'].values.ravel()\n",
        "            # Escalado y predict\n",
        "            X_scaled = sx.transform(X_test_sel.values) if sx else X_test_sel.values\n",
        "            y_pred_raw = model.predict(X_scaled)\n",
        "            y_pred = (sy.inverse_transform(y_pred_raw.reshape(-1,1)).ravel()\n",
        "                      if sy else y_pred_raw.ravel())\n",
        "\n",
        "            # 5) Calcular mtricas globales de ajuste (idnticas para todos los subplots)\n",
        "            r2  = r2_score(y_true, y_pred)\n",
        "            mae = mean_absolute_error(y_true, y_pred)\n",
        "            mse = mean_squared_error(y_true, y_pred)\n",
        "            print(f\"[DEBUG] 17.26. Mtricas globales del modelo ptimo: R2={r2:.3f}, MAE={mae:.3f}, MSE={mse:.3f}\")\n",
        "\n",
        "            # 6) Crear figura de small multiples\n",
        "            n_vars = len(features)\n",
        "            ncols  = min(4, n_vars)\n",
        "            nrows  = int(np.ceil(n_vars / ncols))\n",
        "\n",
        "            fig, axes = plt.subplots(\n",
        "                nrows, ncols,\n",
        "                figsize=(ncols * 3, nrows * 2.5),\n",
        "                squeeze=False\n",
        "            )\n",
        "\n",
        "            for ax, feat in zip(axes.flatten(), features):\n",
        "                # Ordenar por valor de X para trazar lneas ordenadas\n",
        "                x = X_test_sel[feat].values\n",
        "                idx_sort = np.argsort(x)\n",
        "                x_s = x[idx_sort]\n",
        "                ax.plot(x_s, y_true[idx_sort],   label='Real',    linewidth=1)\n",
        "                ax.plot(x_s, y_pred[idx_sort],   label='Predicho',linewidth=1)\n",
        "                ax.set_title(\n",
        "                    f\"{feat}\\n\"\n",
        "                    f\"R={r2:.2f}  MAE={mae:.2f}  MSE={mse:.2f}\",\n",
        "                    fontsize=8\n",
        "                )\n",
        "                ax.tick_params(labelsize=6)\n",
        "                ax.legend(fontsize=6)\n",
        "\n",
        "            # Desactivar ejes sobrantes\n",
        "            for ax in axes.flatten()[n_vars:]:\n",
        "                ax.set_visible(False)\n",
        "\n",
        "            fig.tight_layout()\n",
        "            print(\"[DEBUG] 17.27. Bloque de curvas completado, aadiendo seccin al informe\")\n",
        "\n",
        "            # 7) Aadir al informe\n",
        "            self.sections.append((\n",
        "                \"### Curvas Y real vs Y predicha por variable X (modelo ptimo)\",\n",
        "                fig\n",
        "            ))\n",
        "\n",
        "        except Exception as e:\n",
        "            # En caso de fallo, no romper todo el build_sections\n",
        "            print(f\"[ERROR] en bloque de curvas Y vs X: {e}\")\n",
        "            self.sections.append((\n",
        "                \"###  Error en Curvas Y vs X\",\n",
        "                f\"Se produjo un error generando las curvas: {e}\"\n",
        "            ))\n",
        "        # --- Fin Bloque 6: Curvas Y real vs. Y predicha por variable X (modelo ptimo) ---\n",
        "\n",
        "        # --- Bloque 7: Anlisis IA de Curvas Y real vs Y predicho por Variable X ---\n",
        "        try:\n",
        "            print(\"[DEBUG] 17.28. Enriqueciendo contexto antes de llamar a OpenAI\")\n",
        "\n",
        "            from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "            from scipy.stats import skew, kurtosis, pearsonr\n",
        "            import pandas as pd\n",
        "\n",
        "            # Recuperar la configuracin del mejor modelo\n",
        "            # (asegrate de haber definido antes mt, method, engine)\n",
        "            key_best     = (mt, method, engine)\n",
        "            payload_best = OPT_MODELS[key_best]\n",
        "            features     = payload_best['cols']\n",
        "            model_best   = payload_best['model']\n",
        "\n",
        "            # Extraer datos de test y predicciones\n",
        "            X_te = X_test[features].copy()\n",
        "            y_true = Y_test.values.ravel()\n",
        "            sx = payload_best.get('sx', None)\n",
        "            X_te_s = sx.transform(X_te.values) if sx else X_te.values\n",
        "            # Si el wrapper ya invierte la escala internamente:\n",
        "            y_pred = model_best.predict(X_te_s) if not hasattr(model_best, 'sy') else model_best.predict(X_te)\n",
        "\n",
        "            # 1) Estadsticas de distribucin de cada X\n",
        "            dist_stats = {}\n",
        "            for feat in features:\n",
        "                arr = X_te[feat].values\n",
        "                dist_stats[feat] = {\n",
        "                    'min': float(arr.min()),\n",
        "                    'max': float(arr.max()),\n",
        "                    'mean': float(arr.mean()),\n",
        "                    'std': float(arr.std()),\n",
        "                    'skew': float(skew(arr)),\n",
        "                    'kurtosis': float(kurtosis(arr)),\n",
        "                }\n",
        "\n",
        "            # 2) Correlacin residual vs X (heterocedasticidad)\n",
        "            residuals = y_true - y_pred\n",
        "            corr_stats = {}\n",
        "            for feat in features:\n",
        "                r, p = pearsonr(X_te[feat].values, residuals)\n",
        "                corr_stats[feat] = {'pearson_r': float(r), 'p_value': float(p)}\n",
        "\n",
        "            # 3) Residuales por cuartiles de X\n",
        "            quartile_stats = {}\n",
        "            for feat in features:\n",
        "                df_q = pd.DataFrame({\n",
        "                    'x': X_te[feat],\n",
        "                    'res': residuals\n",
        "                })\n",
        "                df_q['qcut'] = pd.qcut(df_q['x'], 4, labels=False, duplicates='drop')\n",
        "                qs = df_q.groupby('qcut')['res'].agg(['mean','std']).to_dict(orient='index')\n",
        "                quartile_stats[feat] = {\n",
        "                    int(k): {'mean_res': float(v['mean']), 'std_res': float(v['std'])}\n",
        "                    for k, v in qs.items()\n",
        "                }\n",
        "\n",
        "            # 4) Ejemplos de pares (X, y_real, y_pred) en cuartiles extremos\n",
        "            samples = {}\n",
        "            for feat in features:\n",
        "                # Seleccionar solo cuartiles 0 y 3\n",
        "                cuts = pd.qcut(X_te[feat], 4, labels=False, duplicates='drop')\n",
        "                sel = cuts.isin([0, 3])\n",
        "                df_s = pd.DataFrame({\n",
        "                    'x':   X_te[feat][sel],\n",
        "                    'y_r': y_true[sel],\n",
        "                    'y_p': y_pred[sel]\n",
        "                }).head(3)  # 3 muestras por variable\n",
        "                samples[feat] = df_s.to_dict(orient='records')\n",
        "\n",
        "            # 5) Mtricas globales del modelo ptimo\n",
        "            global_stats = {\n",
        "                'R2':  float(r2_score(y_true, y_pred)),\n",
        "                'MAE': float(mean_absolute_error(y_true, y_pred)),\n",
        "                'MSE': float(mean_squared_error(y_true, y_pred))\n",
        "            }\n",
        "\n",
        "            print(\"[DEBUG] 17.29. Dist stats, corr stats, quartile stats y samples construidos\")\n",
        "\n",
        "            # 6) Construccin del prompt enriquecido\n",
        "            prompt = [\n",
        "                \"A continuacin tienes un informe detallado de cada variable X:\\n\",\n",
        "                \"**1) Distribucin de cada X:**\",\n",
        "                f\"{dist_stats}\\n\",\n",
        "                \"**2) Correlacin Residual vs X (Pearson):**\",\n",
        "                f\"{corr_stats}\\n\",\n",
        "                \"**3) Residuales por cuartiles de X:**\",\n",
        "                f\"{quartile_stats}\\n\",\n",
        "                \"**4) Ejemplos de pares (X, y_real, y_predicho) en cuartiles extremos:**\",\n",
        "                f\"{samples}\\n\",\n",
        "                \"**5) Mtricas globales del modelo ptimo:**\",\n",
        "                f\"{global_stats}\\n\",\n",
        "                \"### Instrucciones al experto IA:\\n\"\n",
        "                \"1. Analiza para cada variable X cmo la distribucin y la heterocedasticidad \"\n",
        "                \"(Pearson r, residuales por cuartiles) pueden estar afectando el ajuste.\\n\"\n",
        "                \"2. Discute zonas problemticas (picos, colas extensas) y su impacto en la prediccin.\\n\"\n",
        "                \"3. Comenta sobre las muestras de ejemplo: qu patrones ves en X extremos?\\n\"\n",
        "                \"4. Integra el conocimiento fsico-qumico del proceso para explicar estos fenmenos.\\n\"\n",
        "                \"5. Propn recomendaciones tanto de modelado (transformaciones, nuevas features) \"\n",
        "                \"como de operacin del proceso (rangos ptimos, variables crticas).\\n\"\n",
        "            ]\n",
        "            full_prompt = \"\\n\".join(prompt)\n",
        "\n",
        "            print(\"[DEBUG] 17.30. Llamando a OpenAI directamente con _client.chat.completions.create para Curvas\")\n",
        "            response = _client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Eres un experto en anlisis de series temporales y ML para procesos fsico-qumicos.\"},\n",
        "                    {\"role\": \"user\",   \"content\": full_prompt}\n",
        "                ],\n",
        "                temperature=TEMPERATURE_VAL,\n",
        "                max_tokens=MAX_EXPLANATION_TOKENS\n",
        "            )\n",
        "            analysis_curvas = response.choices[0].message.content.strip()\n",
        "            print(f\"[DEBUG] 17.31. Longitud del anlisis de curvas: {len(analysis_curvas)} caracteres\")\n",
        "\n",
        "            # 7) Aadimos la seccin final al reporte\n",
        "            self.sections.append((\n",
        "                \"### Anlisis IA Profundo de Y real vs Y predicho por Variable X\",\n",
        "                analysis_curvas\n",
        "            ))\n",
        "            print(\"[DEBUG] 17.32. Seccin Anlisis IA enriquecido aadida\")\n",
        "\n",
        "        except Exception as e:\n",
        "            err = f\"Error en Bloque IA Profundo curvas X vs Y: {e}\"\n",
        "            self.sections.append((\n",
        "                \"###  Error Bloque IA Profundo curvas X vs Y\",\n",
        "                err\n",
        "            ))\n",
        "            print(f\"[DEBUG] {err}\")\n",
        "\n",
        "        # --- Fin Bloque 7: Anlisis IA de Curvas Y real vs Y predicho por Variable X ---\n",
        "\n",
        "        # --- Bloque 8: Robustez What-If y Grficas Extremos ---\n",
        "        try:\n",
        "            print(\"[DEBUG] 17.33. Iniciando Bloque Robustez What-If y Grficas Extremos\")\n",
        "\n",
        "            import itertools\n",
        "            import numpy as np\n",
        "            import pandas as pd\n",
        "            import matplotlib.pyplot as plt\n",
        "            import seaborn as sns\n",
        "\n",
        "            # <<< NUEVO: Obtener escaladores desde payload (como se hace en xIA) >>>\n",
        "            sx = payload.get('sx')\n",
        "            sy = payload.get('sy')\n",
        "\n",
        "            # Ajustes globales de estilo\n",
        "            sns.set(style=\"whitegrid\", palette=\"deep\", font_scale=1.2)\n",
        "            plt.rcParams.update({\n",
        "                \"figure.facecolor\": \"white\",\n",
        "                \"axes.facecolor\": \"white\",\n",
        "                \"axes.edgecolor\": \"#333333\",\n",
        "                \"axes.labelcolor\": \"#333333\",\n",
        "                \"xtick.color\": \"#333333\",\n",
        "                \"ytick.color\": \"#333333\",\n",
        "                \"text.color\": \"#333333\",\n",
        "                \"legend.frameon\": True,\n",
        "                \"legend.framealpha\": 0.9,\n",
        "            })\n",
        "\n",
        "            # 1) Variables para robustez\n",
        "            vars_robust = top_feats[:2]\n",
        "            print(f\"[DEBUG] 17.34. Variables robustez seleccionadas: {vars_robust}\")\n",
        "\n",
        "            # 2) Medianas y extremos\n",
        "            medians   = X_train[features].median()\n",
        "            low_vals  = X_train[features].quantile(0.05)\n",
        "            high_vals = X_train[features].quantile(0.95)\n",
        "            print(\"[DEBUG] 17.35. Valores medianos y percentiles obtenidos\")\n",
        "\n",
        "            # 3) Generar escenarios what-if\n",
        "            scenarios = []\n",
        "            for combo in itertools.product(['low','high'], repeat=len(vars_robust)):\n",
        "                sc = medians.copy()\n",
        "                labels = []\n",
        "                for var, lvl in zip(vars_robust, combo):\n",
        "                    sc[var] = low_vals[var] if lvl=='low' else high_vals[var]\n",
        "                    labels.append(f\"{var}_{lvl}\")\n",
        "                sc['__label__'] = \" & \".join(labels)\n",
        "                scenarios.append(sc)\n",
        "            df_scen = pd.DataFrame(scenarios).reset_index(drop=True)\n",
        "            print(f\"[DEBUG] 17.36. {len(df_scen)} escenarios generados: {df_scen['__label__'].tolist()}\")\n",
        "\n",
        "            # <<< CAMBIO: Aplicar escalado SOLO si existe sx >>>\n",
        "            X_scen = sx.transform(df_scen[features].values) if sx else df_scen[features].values\n",
        "            if mt == 'rnn':\n",
        "                X_in = X_scen.reshape((X_scen.shape[0], 1, X_scen.shape[1]))\n",
        "            else:\n",
        "                X_in = X_scen\n",
        "\n",
        "            # 5) Predecir y calcular errores\n",
        "            y_pred = model.predict(X_in)\n",
        "            if mt != 'rnn':\n",
        "                #y_pred = sy.inverse_transform(y_pred.reshape(-1,1)).ravel()\n",
        "                y_pred = sy.inverse_transform(y_pred.reshape(-1,1)).ravel() if sy else y_pred.ravel()\n",
        "            df_scen['y_pred'] = y_pred\n",
        "\n",
        "            # <<< CAMBIO: Mismo tratamiento para y_base >>>\n",
        "            yb = model.predict(\n",
        "                X_in[:1].reshape((1,1,X_scen.shape[1])) if mt=='rnn' else X_in[:1]\n",
        "            )\n",
        "            if mt != 'rnn':\n",
        "                yb = sy.inverse_transform(yb.reshape(-1,1)).ravel()[0] if sy else yb.ravel()[0]\n",
        "\n",
        "            df_scen['y_base'] = yb\n",
        "            df_scen['error_abs'] = np.abs(df_scen['y_pred'] - df_scen['y_base'])\n",
        "            print(\"[DEBUG] 17.37. Predicciones y errores calculados\")\n",
        "\n",
        "            # 6) Boxplot de Error Absoluto\n",
        "            fig1, ax1 = plt.subplots(figsize=(8, 5))\n",
        "            sns.boxplot(\n",
        "                y=df_scen['error_abs'],\n",
        "                ax=ax1,\n",
        "                width=0.4,\n",
        "                boxprops=dict(facecolor=\"#4F81BD\", edgecolor=\"#333333\"),\n",
        "                medianprops=dict(color=\"#E74C3C\", linewidth=2),\n",
        "                whiskerprops=dict(color=\"#333333\", linewidth=1.5),\n",
        "                capprops=dict(color=\"#333333\", linewidth=1.5)\n",
        "            )\n",
        "            ax1.set_title(\"What-If: Boxplot de Error Absoluto\", fontsize=16, fontweight='bold')\n",
        "            ax1.set_ylabel(\"Error absoluto\", fontsize=14)\n",
        "            ax1.set_xticks([])\n",
        "            ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            plt.tight_layout()\n",
        "            self.sections.append((\n",
        "                \"### Robustez What-If: Boxplot de Error Absoluto\",\n",
        "                fig1\n",
        "            ))\n",
        "            print(\"[DEBUG] 17.38. Seccin Boxplot de error aadido\")\n",
        "\n",
        "            # 7) Scatter Y_pred vs Baseline por escenario\n",
        "            fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
        "            idx = np.arange(len(df_scen))\n",
        "            ax2.plot(idx, df_scen['y_base'], marker='o', linestyle='-', label='Baseline', linewidth=2)\n",
        "            ax2.plot(idx, df_scen['y_pred'], marker='X', linestyle='--', label='What-If Predicho', linewidth=2)\n",
        "            for i, lbl in enumerate(df_scen['__label__']):\n",
        "                ax2.annotate(\n",
        "                    lbl,\n",
        "                    (idx[i], df_scen['y_pred'].iloc[i]),\n",
        "                    textcoords=\"offset points\",\n",
        "                    xytext=(0,8),\n",
        "                    ha='center',\n",
        "                    fontsize=10,\n",
        "                    color=\"#333333\"\n",
        "                )\n",
        "            ax2.set_xticks(idx)\n",
        "            ax2.set_xticklabels(df_scen['__label__'], rotation=45, ha='right', fontsize=10)\n",
        "            ax2.set_title(\"What-If: Y Predicho vs Baseline por Escenario\", fontsize=16, fontweight='bold')\n",
        "            ax2.set_xlabel(\"Escenario\", fontsize=14)\n",
        "            ax2.set_ylabel(\"Y\", fontsize=14)\n",
        "            ax2.legend(frameon=True, fontsize=12)\n",
        "            ax2.grid(True, linestyle='--', alpha=0.5)\n",
        "            plt.tight_layout()\n",
        "            self.sections.append((\n",
        "                \"### Robustez What-If: Y Predicho vs Baseline\",\n",
        "                fig2\n",
        "            ))\n",
        "            print(\"[DEBUG] 17.39. Seccin Scatter plot de escenarios aadido\")\n",
        "\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(\"[DEBUG] Se ha producido una excepcin en el Bloque Robustez What-If:\")\n",
        "            print(\"[DEBUG] Tipo de excepcin:\", type(e).__name__)\n",
        "            print(\"[DEBUG] Mensaje:\", e)\n",
        "            print(\"[DEBUG] Traza completa:\")\n",
        "            traceback.print_exc()\n",
        "            self.sections.append((\n",
        "                \"###  Error en Bloque Robustez What-If\",\n",
        "                f\"{type(e).__name__}: {e}\"\n",
        "            ))\n",
        "            print(f\"[DEBUG] Error en Bloque Robustez What-If: {e}\")\n",
        "        # --- Fin Bloque 8: Robustez What-If y Grficas Extremos ---\n",
        "\n",
        "        # --- Bloque 9: Anlisis e Interpretacin IA de Robustez What-If ---\n",
        "        try:\n",
        "            print(\"[DEBUG] 17.40. Iniciando Bloque IA de Robustez What-If e Interpretacin\")\n",
        "\n",
        "            #  Extraemos del atributo best_model_info\n",
        "            metodo = self.best_model_info['metodo']\n",
        "            motor  = self.best_model_info['motor']\n",
        "            score  = self.best_model_info['score']\n",
        "\n",
        "            #  Y las mtricas guardadas como atributos\n",
        "            r2_cv_std = self.r2_cv_std\n",
        "            res_std   = self.res_std\n",
        "            cal_err   = self.cal_err\n",
        "\n",
        "            # DEBUG: comprobacin de valores\n",
        "            print(f\"[DEBUG] 17.41. Mejor modelo  mtodo={metodo}, motor={motor}, score={score:.4f}\")\n",
        "            print(f\"[DEBUG] 17.42. Mtricas desde atributos  r2_cv_std={r2_cv_std:.4f}, res_std={res_std:.4f}, cal_err={cal_err:.4f}\")\n",
        "\n",
        "            # 1) Resumen numrico de escenarios what-if\n",
        "            stats = {\n",
        "                'mean_error': float(df_scen['error_abs'].mean()),\n",
        "                'std_error':  float(df_scen['error_abs'].std()),\n",
        "                'max_error':  float(df_scen['error_abs'].max()),\n",
        "                'min_error':  float(df_scen['error_abs'].min()),\n",
        "                'percentiles_error': {\n",
        "                    '25%': float(df_scen['error_abs'].quantile(0.25)),\n",
        "                    '50%': float(df_scen['error_abs'].quantile(0.50)),\n",
        "                    '75%': float(df_scen['error_abs'].quantile(0.75))\n",
        "                }\n",
        "            }\n",
        "            print(f\"[DEBUG] 17.43. Estadsticos de error abs: {stats}\")\n",
        "\n",
        "            # 2) Preparar tabla de escenarios en formato dict para el prompt\n",
        "            table_dict = df_scen[['__label__','y_base','y_pred','error_abs']].to_dict(orient='list')\n",
        "            print(\"[DEBUG] 17.44. Tabla de escenarios convertida a dict para prompt\")\n",
        "\n",
        "            # 3) Montar prompt rico en contexto\n",
        "            prompt_robustez = f\"\"\"\n",
        "        Eres un experto en evaluacin de robustez de modelos y procesos fsico-qumicos.\n",
        "\n",
        "        A continuacin tienes los resultados del anlisis What-If para el modelo ptimo (mtodo={metodo}, motor={motor}):\n",
        "\n",
        "         Estadsticos globales del error absoluto:\n",
        "          - Media: {stats['mean_error']:.4f}\n",
        "          - Desviacin estndar: {stats['std_error']:.4f}\n",
        "          - Mnimo: {stats['min_error']:.4f}\n",
        "          - Mximo: {stats['max_error']:.4f}\n",
        "          - Percentiles: 25%={stats['percentiles_error']['25%']:.4f}, 50%={stats['percentiles_error']['50%']:.4f}, 75%={stats['percentiles_error']['75%']:.4f}\n",
        "\n",
        "         Detalle por escenario:\n",
        "          { { 'Escenarios': table_dict } }\n",
        "\n",
        "        Instrucciones para el anlisis:\n",
        "        1. Describe para cada escenario (etiqueta) cmo vara la prediccin frente al baseline y qu significa en trminos del proceso fsico-qumico.\n",
        "        2. Identifica qu variables generan mayor sensibilidad en el modelo y por qu.\n",
        "        3. Resume los hallazgos generales de robustez y menciona si existen condiciones extremas donde el modelo falla o se vuelve inestable.\n",
        "        4. Propn recomendaciones para mejorar la robustez del modelo (por ejemplo, ajustes de hiperparmetros, transformaciones, data augmentation de escenarios extremos).\n",
        "        5. Sugiere acciones operativas concretas en planta (rangos de temperatura, concentraciones, etc.) basndote en los resultados What-If.\n",
        "        \"\"\"\n",
        "\n",
        "            print(\"[DEBUG] 17.45. Llamando a OpenAI directamente con _client.chat.completions.create\")\n",
        "            response = _client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Eres un experto en Machine Learning aplicado a procesos fsico-qumicos.\"},\n",
        "                    {\"role\": \"user\",   \"content\": prompt_robustez}\n",
        "                ],\n",
        "                temperature=TEMPERATURE_VAL,\n",
        "                max_tokens=MAX_EXPLANATION_TOKENS\n",
        "            )\n",
        "            analysis_robustez = response.choices[0].message.content.strip()\n",
        "            print(f\"[DEBUG] 17.46. Longitud del anlisis de robustez: {len(analysis_robustez)} caracteres\")\n",
        "\n",
        "            # 4) Aadir seccin al informe\n",
        "            self.sections.append((\n",
        "                \"### Robustez What-If: Anlisis e Interpretacin IA\",\n",
        "                analysis_robustez\n",
        "            ))\n",
        "            print(\"[DEBUG] 17.47. Seccin IA Robustez What-If aadida\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"{type(e).__name__}: {e}\"\n",
        "            print(f\"[DEBUG] Error en Bloque IA Robustez What-If: {error_msg}\")\n",
        "            self.sections.append((\n",
        "                \"###  Error en Bloque IA Robustez What-If\",\n",
        "                error_msg\n",
        "            ))\n",
        "        # --- Fin Bloque 9: Anlisis e Interpretacin IA de Robustez What-If ---\n",
        "\n",
        "        # =============================================================================\n",
        "        #  Bloque 18: Conclusiones y Recomendaciones Generales\n",
        "        # =============================================================================\n",
        "        try:\n",
        "            print(\"[DEBUG] 18.1. Iniciando Bloque Final de Conclusiones y Recomendaciones\")\n",
        "\n",
        "            # 1) Extraer info clave del mejor modelo (asegrate de haberla guardado antes)\n",
        "            best = getattr(self, 'best_model_info', {})\n",
        "            metodo = best.get('metodo', 'N/A')\n",
        "            motor  = best.get('motor', 'N/A')\n",
        "            score  = best.get('score', None)\n",
        "\n",
        "            # 2) Extraer mtricas adicionales si las guardaste como atributos\n",
        "            r2_cv_std = getattr(self, 'r2_cv_std', None)\n",
        "            res_std   = getattr(self, 'res_std', None)\n",
        "            cal_err   = getattr(self, 'cal_err', None)\n",
        "\n",
        "            # 3) Formatear con control de None\n",
        "            score_txt   = f\"{score:.4f}\"    if score    is not None else \"N/A\"\n",
        "            r2_cv_txt   = f\"{r2_cv_std:.4f}\" if r2_cv_std is not None else \"N/A\"\n",
        "            res_std_txt = f\"{res_std:.4f}\"   if res_std   is not None else \"N/A\"\n",
        "            cal_err_txt = f\"{cal_err:.4f}\"   if cal_err   is not None else \"N/A\"\n",
        "\n",
        "            perf_txt = (\n",
        "                f\"- **Mejor modelo**: {metodo}  \\n\"\n",
        "                f\"- **Motor de optimizacin**: {motor}  \\n\"\n",
        "                f\"- **Score final (R o indicador principal)**: {score_txt}  \\n\"\n",
        "                f\"- **Std R en CV**: {r2_cv_txt}  \\n\"\n",
        "                f\"- **Std residuos**: {res_std_txt}  \\n\"\n",
        "                f\"- **Error medio de calibracin**: {cal_err_txt}  \\n\"\n",
        "            )\n",
        "            print(f\"[DEBUG] 18.2. perf_txt:\\n{perf_txt}\")\n",
        "\n",
        "            # 4) Resumir metodologa (ttulos de secciones ya generados)\n",
        "            pasos = [\n",
        "                \"1. Carga y preprocesado de datos\",\n",
        "                \"2. Exploracin estadstica y visualizacin\",\n",
        "                \"3. Seleccin de variables independientes\",\n",
        "                \"4. Entrenamiento de modelos (SVR, NN, XGB, RF, RNN)\",\n",
        "                \"5. Optimizacin de hiperparmetros\",\n",
        "                \"6. Interpretabilidad (SHAP, PDP, ICE, ALE, counterfactuals)\",\n",
        "                \"7. Anlisis de robustez What-If y escenarios extremos\"\n",
        "            ]\n",
        "            metod_txt = \"\\n\".join(f\"- {p}\" for p in pasos)\n",
        "            print(f\"[DEBUG] 18.3. metod_txt:\\n{metod_txt}\")\n",
        "\n",
        "            # 5) Inyectar snippets de texto IA previos (si los tienes)\n",
        "            snippets = []\n",
        "            for title, content in self.sections:\n",
        "                if title.startswith(\"###  Explicacin IA\"):\n",
        "                    # tomamos los primeros 200 caracteres de cada anlisis\n",
        "                    text = str(content)[:200].replace(\"\\n\", \" \")                                # <------ AJUSTAR AQUI (INCREMENTAR ) PARA MEJORAR LA CALIDAD DEL ANLISIS DE LA IA\n",
        "                    snippets.append(f\"{title.lstrip('# ')}: {text}...\")\n",
        "            snippets_txt = \"\\n\".join(f\"- {s}\" for s in snippets)\n",
        "            print(f\"[DEBUG] 18.4. snippets_txt:\\n{snippets_txt}\")\n",
        "\n",
        "            # 6) Construir prompt final\n",
        "            prompt = (\n",
        "                \"Eres un investigador de Deep Learning y un experto en procesos fsico-qumicos industriales.\\n\"\n",
        "                \"Tienes la siguiente informacin:\\n\\n\"\n",
        "                \"**A) Rendimiento del mejor modelo**:\\n\"\n",
        "                f\"{perf_txt}\\n\"\n",
        "                \"**B) Metodologa seguida**:\\n\"\n",
        "                f\"{metod_txt}\\n\\n\"\n",
        "                \"**C) Resmenes de anlisis previos**:\\n\"\n",
        "                f\"{snippets_txt}\\n\\n\"\n",
        "                \"Con toda esta informacin, por favor:\\n\"\n",
        "                \"1. Resume cada paso de la metodologa en 12 frases, enfatizando aprendizajes clave.\\n\"\n",
        "                \"2. Sintetiza los hallazgos principales (tendencias, variables crticas, puntos fuertes/dbiles del modelo).\\n\"\n",
        "                \"3. Conecta estos hallazgos con el proceso fsico-qumico: qu parmetros de planta son ms determinantes?\\n\"\n",
        "                \"4. Propn **35 acciones inmediatas** (p. ej. ajustes de operacin, reentrenamiento, ampliacin de datos).\\n\"\n",
        "                \"5. Disea un roadmap de validacin y despliegue (pruebas A/B, monitorizacin de drift, retrain schedule).\\n\"\n",
        "                \"6. Finaliza con un breve prrafo de cierre brillante que refuerce la confianza en el informe.\\n\"\n",
        "                \"7. Incluye **23 referencias bibliogrficas** (APA) que respalden tus recomendaciones.\\n\"\n",
        "            )\n",
        "            print(f\"[DEBUG] 18.5. Prompt final construido (longitud {len(prompt)} caracteres)\")\n",
        "\n",
        "            # 7) Llamada a OpenAI\n",
        "            response = _client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Eres un investigador acadmico y consultor industrial senior.\"},\n",
        "                    {\"role\": \"user\",   \"content\": prompt}\n",
        "                ],\n",
        "                temperature=TEMPERATURE_VAL,\n",
        "                max_tokens=MAX_EXPLANATION_TOKENS\n",
        "            )\n",
        "            concl_text = response.choices[0].message.content.strip()\n",
        "            print(\"[DEBUG] 18.6. Respuesta IA recibida con longitud:\", len(concl_text))\n",
        "\n",
        "            # 8) Aadir seccin final\n",
        "            self.sections.append((\n",
        "                \"### Conclusiones y Recomendaciones Finales\",\n",
        "                concl_text\n",
        "            ))\n",
        "            print(\"[DEBUG] 18.7. Seccin de conclusiones aadida exitosamente\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar Bloque Final de Conclusiones: {e}\")\n",
        "            self.sections.append((\n",
        "                \"###  Error en Bloque de Conclusiones\",\n",
        "                str(e)\n",
        "            ))\n",
        "        # =============================================================================\n",
        "        #  Fin Bloque 18 Conclusiones\n",
        "        # =============================================================================\n",
        "\n",
        "        # ============================================================================\n",
        "        # --- Bloque 19: Descripcin de PMS y Road-Map de Evolucin Tecnolgica ---\n",
        "        # ============================================================================\n",
        "        try:\n",
        "            print(\"[DEBUG] 19.1. Iniciando Bloque Descripcin y roadmap de PMS\")\n",
        "\n",
        "            import os, glob, ast, inspect, json\n",
        "\n",
        "            base_dir = os.getcwd()\n",
        "\n",
        "            # 1) Archivos .py\n",
        "            py_files = glob.glob(os.path.join(base_dir, \"*.py\"))\n",
        "            loc_summary_py = {}\n",
        "            deps = set()\n",
        "            for fn in py_files:\n",
        "                try:\n",
        "                    with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
        "                        src = f.read()\n",
        "                    lines = src.splitlines()\n",
        "                    loc_summary_py[os.path.basename(fn)] = len(lines)\n",
        "                    tree = ast.parse(src)\n",
        "                    for node in ast.walk(tree):\n",
        "                        if isinstance(node, ast.Import):\n",
        "                            for n in node.names:\n",
        "                                deps.add(n.name.split(\".\")[0])\n",
        "                        elif isinstance(node, ast.ImportFrom):\n",
        "                            if node.module:\n",
        "                                deps.add(node.module.split(\".\")[0])\n",
        "                except Exception as ex:\n",
        "                    print(f\"[DEBUG] No pude procesar {fn}: {ex}\")\n",
        "\n",
        "            # 2) Notebooks .ipynb\n",
        "            ipynb_files = glob.glob(os.path.join(base_dir, \"*.ipynb\"))\n",
        "            nb_summary = {}\n",
        "            for fn in ipynb_files:\n",
        "                try:\n",
        "                    with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
        "                        nb = json.load(f)\n",
        "                    code_cells = sum(1 for c in nb.get(\"cells\", []) if c.get(\"cell_type\")==\"code\")\n",
        "                    nb_summary[os.path.basename(fn)] = code_cells\n",
        "                except Exception as ex:\n",
        "                    print(f\"[DEBUG] No pude procesar {fn}: {ex}\")\n",
        "\n",
        "            # 3) Inspeccionar ReportBuilder\n",
        "            members = inspect.getmembers(ReportBuilder)\n",
        "            methods = [name for name,obj in members if inspect.isfunction(obj)]\n",
        "            classes = [name for name,obj in members if inspect.isclass(obj)]\n",
        "\n",
        "            # 4) Secciones ya generadas\n",
        "            num_sections = len(self.sections)\n",
        "            titles = [t for t,_ in self.sections]\n",
        "\n",
        "            # 5) Formatear resumen\n",
        "            summary_lines = []\n",
        "            summary_lines.append(f\" Archivos .py escaneados ({len(py_files)}):\")\n",
        "            for fn,loc in loc_summary_py.items():\n",
        "                summary_lines.append(f\"     {fn}: {loc} lneas\")\n",
        "            if ipynb_files:\n",
        "                summary_lines.append(f\" Notebooks .ipynb escaneados ({len(ipynb_files)}):\")\n",
        "                for fn,cells in nb_summary.items():\n",
        "                    summary_lines.append(f\"     {fn}: {cells} celdas de cdigo\")\n",
        "            summary_lines.append(f\" Dependencias detectadas: {', '.join(sorted(deps))}\")\n",
        "            summary_lines.append(f\" Mtodos en ReportBuilder: {', '.join(methods)}\")\n",
        "            summary_lines.append(f\" Clases definidas en ReportBuilder: {', '.join(classes)}\")\n",
        "            summary_lines.append(f\" Secciones del informe generadas ({num_sections}):\")\n",
        "            for t in titles:\n",
        "                summary_lines.append(f\"     {t}\")\n",
        "\n",
        "            summary_txt = \"\\n\".join(summary_lines)\n",
        "\n",
        "            # 6) Construir prompt\n",
        "            prompt_pms = f\"\"\"\n",
        "        Eres un arquitecto de software y consultor de procesos industriales.\n",
        "        A continuacin tienes un **resumen de la herramienta PMS** y su cdigo fuente:\n",
        "\n",
        "        {summary_txt}\n",
        "\n",
        "        En base a ello, por favor:\n",
        "        1. Describe **detalladamente la arquitectura** de la aplicacin:\n",
        "          - Flujo de ejecucin\n",
        "          - Principales mdulos y clases\n",
        "          - Patrones de diseo o buenas prcticas observadas.\n",
        "\n",
        "        2. Seala **puntos fuertes** y **oportunidades de mejora**:\n",
        "          - Modularidad, legibilidad, rendimiento, uso de libreras.\n",
        "          - Aspectos que podran complicar el mantenimiento o la escalabilidad.\n",
        "\n",
        "        3. Proporciona un **road-map de evolucin tecnolgica**:\n",
        "          - Refactorizaciones y modularizacin adicional.\n",
        "          - Incorporacin de tests automatizados y pipeline de CI/CD.\n",
        "          - Contenerizacin / despliegue (Docker, Kubernetes, microservicios).\n",
        "          - Nuevas funcionalidades (AutoML, flujos batch/pipeline, APIs).\n",
        "\n",
        "        4. Recomienda **modelos, motores o mtodos futuros**:\n",
        "          - E.g. Transformers para series temporales, AutoML para seleccin de modelos.\n",
        "          - Explica el **impacto** esperado en la calidad de la prediccin y en la mantenibilidad.\n",
        "\n",
        "        5. Concluye con un **resumen ejecutivo** de 23 prrafos enfatizando el valor de estas mejoras.\n",
        "        \"\"\"\n",
        "\n",
        "            print(\"[DEBUG] 19.2. Llamando a OpenAI para Bloque 10 de PMS\")\n",
        "            resp = _client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\":\"system\", \"content\":\"Eres investigador acadmico, arquitecto de software y consultor industrial.\"},\n",
        "                    {\"role\":\"user\",   \"content\":prompt_pms}\n",
        "                ],\n",
        "                temperature=TEMPERATURE_VAL,\n",
        "                max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "            )\n",
        "            analysis_pms = resp.choices[0].message.content.strip()\n",
        "\n",
        "            # 7) Aadir seccin al informe\n",
        "            self.sections.append((\n",
        "                \"### Descripcin de PMS y Road-Map de Evolucin Tecnolgica\",\n",
        "                analysis_pms\n",
        "            ))\n",
        "            print(\"[DEBUG] 19.3. Bloque completado y aadido al informe\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[DEBUG] Error en Bloque 10: {e}\")\n",
        "            self.sections.append((\n",
        "                \"###  Error en Descripcin y Road-Map de PMS\",\n",
        "                str(e)\n",
        "            ))\n",
        "        # --- Fin Bloque 11 ---\n",
        "\n",
        "\n",
        "        print(\"[DEBUG] 20. ReportBuilder.build_sections end\")\n",
        "    # =============================================================\n",
        "    #  Renderizado de la salida del Informe\n",
        "    # =============================================================\n",
        "    def render(self):\n",
        "        from IPython.display import Markdown, display, HTML\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        print(\"[DEBUG] ReportBuilder.render start\")\n",
        "        for idx, (title, content) in enumerate(self.sections):\n",
        "            print(f\"[DEBUG] render seccin #{idx}: {title}\")\n",
        "            # Mostrar ttulo\n",
        "            display(Markdown(title))\n",
        "\n",
        "            # Si es DataFrame, aplicamos el estilo como antes\n",
        "            if isinstance(content, pd.DataFrame):\n",
        "                df = content\n",
        "                try:\n",
        "                    styled = (\n",
        "                        df.style\n",
        "                        .set_table_styles([\n",
        "                            {'selector': 'th',\n",
        "                            'props': [\n",
        "                                ('background-color', '#4F81BD'),\n",
        "                                ('color', 'white'),\n",
        "                                ('font-weight', 'bold'),\n",
        "                                ('padding', '8px'),\n",
        "                                ('text-align', 'center')\n",
        "                            ]},\n",
        "                            {'selector': 'td',\n",
        "                            'props': [\n",
        "                                ('padding', '8px'),\n",
        "                                ('text-align', 'center')\n",
        "                            ]}\n",
        "                        ])\n",
        "                        .apply(lambda row: ['background-color: #f2f2f2' if i%2 else '' for i in range(len(row))],\n",
        "                                axis=1)\n",
        "                        .set_caption(f\"Mostrando {df.shape[0]} filas y {df.shape[1]} columnas\")\n",
        "                    )\n",
        "                    html = styled.to_html()\n",
        "                    display(HTML(html))\n",
        "                    print(f\"[DEBUG] render seccin #{idx}: DataFrame mostrado\")\n",
        "                except Exception as e:\n",
        "                    print(f\"[ERROR] al mostrar DataFrame en seccin #{idx}: {e}\")\n",
        "            # Si es figura matplotlib\n",
        "            elif hasattr(content, 'savefig') or hasattr(content, 'dpi'):  # aproximacin para Figure\n",
        "                try:\n",
        "                    display(content)  # IPython detecta Figure y la muestra\n",
        "                except Exception:\n",
        "                    plt.show(content)\n",
        "            else:\n",
        "                # Texto IA: como ya limitamos con max_tokens, lo mostramos directamente completo.\n",
        "                text = str(content or \"\")\n",
        "                # Imprimimos longitud para debug\n",
        "                length = len(text)\n",
        "                print(f\"[DEBUG] Longitud del contenido IA: {length} caracteres\")\n",
        "                # Mostrar todo el texto como Markdown\n",
        "                display(Markdown(text))\n",
        "                print(f\"[DEBUG] render seccin #{idx}: texto IA mostrado completo\")\n",
        "        print(\"[DEBUG] ReportBuilder.render end\")\n",
        "# ____________________________________________________________________\n",
        "# --- Generar y Mostrar Informe ---\n",
        "# ____________________________________________________________________\n",
        "#import ipywidgets as widgets\n",
        "#from IPython.display import clear_output, display\n",
        "\n",
        "def mostrar_informe():\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import clear_output, display\n",
        "    \"\"\"\n",
        "    Construye TODO el informe, presenta un listado de checkboxes con los ttulos\n",
        "    reales de seccin, permite al usuario marcarlos (todos DESMARCADOS por defecto)\n",
        "    y al pulsar 'Generar Informe' limpia la pantalla y muestra solo las secciones elegidas.\n",
        "    \"\"\"\n",
        "\n",
        "    # === Mdulo de Exportaciones: Word, PDF y HTML ===\n",
        "    import io, base64, os\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import display, HTML\n",
        "    from docx import Document\n",
        "    from docx.shared import Inches\n",
        "    from fpdf import FPDF\n",
        "    import ipywidgets as widgets\n",
        "    import unicodedata\n",
        "\n",
        "    def limpiar_texto(texto):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFKD', str(texto))\n",
        "            if ord(c) < 256\n",
        "        )\n",
        "\n",
        "    def exportar_informe(builder):\n",
        "        # === Botones de Exportacin ===\n",
        "        btn_export_word_via_html = widgets.Button(\n",
        "            description=\"Exportar a Word\", button_style=\"info\",\n",
        "            layout=widgets.Layout(margin=\"10px 10px 10px 0\")\n",
        "        )\n",
        "#        btn_export_word = widgets.Button(\n",
        "#            description=\"Exportar a Word\", button_style=\"info\",\n",
        "#            layout=widgets.Layout(margin=\"10px 10px 10px 0\")\n",
        "#        )\n",
        "        btn_export_pdf = widgets.Button(\n",
        "            description=\"Exportar a PDF\", button_style=\"warning\",\n",
        "            layout=widgets.Layout(margin=\"10px 10px 10px 0\")\n",
        "        )\n",
        "        btn_export_html = widgets.Button(\n",
        "            description=\"Exportar a HTML\", button_style=\"primary\",\n",
        "            layout=widgets.Layout(margin=\"10px 10px 10px 0\")\n",
        "        )\n",
        "\n",
        "        def export_to_word_via_html(builder):\n",
        "            import io\n",
        "            import base64\n",
        "            import os\n",
        "            import pypandoc\n",
        "            import pandas as pd\n",
        "            import matplotlib.pyplot as plt\n",
        "            from IPython.display import display, HTML\n",
        "\n",
        "            html_parts = [\"<html><head><meta charset='utf-8'><title>Informe</title></head><body>\"]\n",
        "            html_parts.append(\"<h1>Informe Generado</h1>\")\n",
        "\n",
        "            for title, content in builder.sections:\n",
        "                clean_title = title.lstrip('# ').strip()\n",
        "                html_parts.append(f\"<h2>{clean_title}</h2>\")\n",
        "\n",
        "                try:\n",
        "                    if isinstance(content, pd.DataFrame):\n",
        "                        html_parts.append(content.head(20).to_html(index=False))\n",
        "                        if len(content) > 20:\n",
        "                            html_parts.append(f\"<p><em> Tabla truncada: solo primeras 20 filas de {len(content)}.</em></p>\")\n",
        "\n",
        "                    elif hasattr(content, \"savefig\"):\n",
        "                        img_buf = io.BytesIO()\n",
        "                        content.savefig(img_buf, format='png', bbox_inches='tight')\n",
        "                        img_buf.seek(0)\n",
        "                        img_b64 = base64.b64encode(img_buf.read()).decode()\n",
        "                        html_parts.append(f\"<img src='data:image/png;base64,{img_b64}' style='max-width:100%;'><br>\")\n",
        "\n",
        "                    else:\n",
        "                        texto = str(content)\n",
        "                        lineas = texto.splitlines()\n",
        "                        if len(lineas) > 100:\n",
        "                            texto = \"\\n\".join(lineas[:100]) + \"\\n... (contenido truncado)\"\n",
        "                        html_parts.append(f\"<pre>{texto}</pre>\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    html_parts.append(f\"<p><strong> Error al procesar la seccin:</strong> {e}</p>\")\n",
        "\n",
        "            html_parts.append(\"</body></html>\")\n",
        "\n",
        "            # Guardar HTML temporal\n",
        "            html_filename = \"informe_temporal.html\"\n",
        "            with open(html_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(\"\\n\".join(html_parts))\n",
        "\n",
        "            # Convertir HTML  DOCX usando pypandoc\n",
        "            docx_filename = \"informe_generado.docx\"\n",
        "            try:\n",
        "                pypandoc.convert_file(html_filename, 'docx', outputfile=docx_filename)\n",
        "                print(f\"[DEBUG] Documento Word generado desde HTML: {docx_filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Fallo en la conversin HTML  Word: {e}\")\n",
        "                return\n",
        "\n",
        "            # Mostrar enlace de descarga\n",
        "            with open(docx_filename, \"rb\") as f:\n",
        "                b64 = base64.b64encode(f.read()).decode()\n",
        "            href = (\n",
        "                f'<a download=\"{docx_filename}\" '\n",
        "                f'href=\"data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,{b64}\">'\n",
        "                \" Descargar informe Word</a>\"\n",
        "            )\n",
        "            display(HTML(href))\n",
        "\n",
        "\n",
        "#            def export_to_word(b):\n",
        "#                print(\"[DEBUG] Iniciando exportacin a Word\")\n",
        "#                doc = Document()\n",
        "#                doc.add_heading('Informe Generado', level=0)\n",
        "\n",
        "#                for title, content in builder.sections:\n",
        "#                    clean_title = title.lstrip('# ').strip()\n",
        "#                    doc.add_heading(clean_title, level=1)\n",
        "\n",
        "#                    if isinstance(content, pd.DataFrame):\n",
        "#                        table = doc.add_table(rows=1, cols=len(content.columns))\n",
        "#                        hdr = table.rows[0].cells\n",
        "#                        for i, col in enumerate(content.columns):\n",
        "#                            hdr[i].text = str(col)\n",
        "#                        for row in content.itertuples(index=False):\n",
        "#                            cells = table.add_row().cells\n",
        "#                            for i, val in enumerate(row):\n",
        "#                                cells[i].text = str(val)\n",
        "\n",
        "#                    elif hasattr(content, \"savefig\"):\n",
        "#                        img_stream = io.BytesIO()\n",
        "#                        content.savefig(img_stream, format='png', bbox_inches='tight')\n",
        "#                        img_stream.seek(0)\n",
        "#                        doc.add_picture(img_stream, width=Inches(6))\n",
        "\n",
        "#                    else:\n",
        "#                        for line in str(content).splitlines():\n",
        "#                            doc.add_paragraph(line)\n",
        "\n",
        "#                    doc.add_page_break()\n",
        "\n",
        "#                if len(doc.paragraphs) == 0:\n",
        "#                    doc.add_paragraph(\" El informe no contiene contenido vlido para exportar.\")\n",
        "\n",
        "#                output_path = \"informe_generado.docx\"\n",
        "#                doc.save(output_path)\n",
        "#                print(f\"[DEBUG] Documento guardado en {output_path}\")\n",
        "\n",
        "#                with open(output_path, \"rb\") as f:\n",
        "#                    b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "#                href = (\n",
        "#                    f'<a download=\"{output_path}\" '\n",
        "#                    f'href=\"data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,{b64}\">'\n",
        "#                    \" Descargar informe Word</a>\"\n",
        "#                )\n",
        "#                display(HTML(href))\n",
        "\n",
        "        def export_to_pdf(b):\n",
        "            print(\"[DEBUG] Iniciando exportacin a PDF\")\n",
        "            pdf = FPDF(orientation='L', unit='mm', format='A4')  # Apaisado\n",
        "            pdf.set_auto_page_break(auto=True, margin=15)\n",
        "            pdf.add_page()\n",
        "            pdf.set_font(\"Arial\", 'B', 14)\n",
        "            pdf.cell(0, 10, limpiar_texto('Informe Generado'), ln=True)\n",
        "            #pdf = FPDF()\n",
        "            #pdf.cell(0, 10, 'Informe Generado', ln=True)\n",
        "\n",
        "            for title, content in builder.sections:\n",
        "                pdf.add_page()\n",
        "                pdf.set_font(\"Arial\", 'B', 12)\n",
        "                #pdf.multi_cell(0, 10, title.lstrip('# ').strip())\n",
        "                pdf.multi_cell(0, 10, limpiar_texto(title.lstrip('# ').strip()))\n",
        "\n",
        "\n",
        "                if isinstance(content, pd.DataFrame):\n",
        "                    pdf.set_font(\"Arial\", '', 8)      # Reducimos tamao de letra de 10 a 8 para hacer ms compacto el informe\n",
        "                    col_width = 270 / len(content.columns)  # Distribucin proporcional\n",
        "                    for col in content.columns:\n",
        "                        #pdf.cell(40, 10, str(col), border=1)\n",
        "                        pdf.cell(col_width, 10, limpiar_texto(col), border=1)\n",
        "                    pdf.ln()\n",
        "                    for row in content.itertuples(index=False):\n",
        "                        for val in row:\n",
        "                            #pdf.cell(40, 10, str(val), border=1)\n",
        "                            pdf.cell(col_width, 10, limpiar_texto(val), border=1)\n",
        "                        pdf.ln()\n",
        "\n",
        "                elif hasattr(content, \"savefig\"):\n",
        "                    img_buf = io.BytesIO()\n",
        "                    content.savefig(img_buf, format='png', bbox_inches='tight')\n",
        "                    img_buf.seek(0)\n",
        "                    with open(\"temp_fig.png\", \"wb\") as f:\n",
        "                        f.write(img_buf.read())\n",
        "                    #df.image(\"temp_fig.png\", x=10, w=180)\n",
        "                    pdf.image(\"temp_fig.png\", x=10, w=270)  # Ancho mximo para A4 apaisado\n",
        "                    os.remove(\"temp_fig.png\")\n",
        "\n",
        "                elif isinstance(content, str):\n",
        "                    pdf.set_font(\"Arial\", '', 10)        # Reducimos tamao de letra de 12 a 10 para hacer ms compacto el informe\n",
        "                    #pdf.multi_cell(0, 10, content)\n",
        "                    pdf.multi_cell(0, 10, limpiar_texto(content))\n",
        "\n",
        "\n",
        "            filename = \"informe_generado.pdf\"\n",
        "            pdf.output(filename)\n",
        "            print(f\"[DEBUG] Informe PDF generado correctamente: {filename}\")\n",
        "\n",
        "            with open(filename, \"rb\") as f:\n",
        "                b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "            href = (\n",
        "                f'<a download=\"{filename}\" '\n",
        "                f'href=\"data:application/pdf;base64,{b64}\">'\n",
        "                \" Descargar informe PDF</a>\"\n",
        "            )\n",
        "            display(HTML(href))\n",
        "\n",
        "        def export_to_html(b):\n",
        "            print(\"[DEBUG] Iniciando exportacin a HTML\")\n",
        "            html_content = [\"<html><head><meta charset='utf-8'><title>Informe</title></head><body>\"]\n",
        "            html_content.append(\"<h1>Informe Generado</h1>\")\n",
        "\n",
        "            for title, content in builder.sections:\n",
        "                html_content.append(f\"<h2>{title.lstrip('# ').strip()}</h2>\")\n",
        "\n",
        "                if isinstance(content, pd.DataFrame):\n",
        "                    html_content.append(content.to_html(index=False))\n",
        "                elif hasattr(content, \"savefig\"):\n",
        "                    img_buf = io.BytesIO()\n",
        "                    content.savefig(img_buf, format='png', bbox_inches='tight')\n",
        "                    img_buf.seek(0)\n",
        "                    encoded = base64.b64encode(img_buf.read()).decode()\n",
        "                    html_content.append(f\"<img src='data:image/png;base64,{encoded}' style='max-width:100%'>\")\n",
        "                else:\n",
        "                    html_content.append(f\"<pre>{str(content)}</pre>\")\n",
        "\n",
        "            html_content.append(\"</body></html>\")\n",
        "\n",
        "            filename = \"informe_generado.html\"\n",
        "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(\"\\n\".join(html_content))\n",
        "\n",
        "            with open(filename, \"rb\") as f:\n",
        "                b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "            href = (\n",
        "                f'<a download=\"{filename}\" '\n",
        "                f'href=\"data:text/html;base64,{b64}\">'\n",
        "                \" Descargar informe HTML</a>\"\n",
        "            )\n",
        "            display(HTML(href))\n",
        "\n",
        "        btn_export_word_via_html.on_click(export_to_word_via_html)\n",
        "        btn_export_pdf.on_click(export_to_pdf)\n",
        "        btn_export_html.on_click(export_to_html)\n",
        "\n",
        "        display(widgets.HBox([btn_export_word_via_html, btn_export_pdf, btn_export_html]))\n",
        "\n",
        "    # === Mdulo de Generacin y Presentacin del Informe ===\n",
        "    # 1) Generamos todas las secciones internamente (sin render an)\n",
        "    builder = ReportBuilder(globals())\n",
        "    builder.build_sections()\n",
        "\n",
        "    # 2) Creamos una lista de 'ttulos limpios' para los checkboxes\n",
        "    #    Eliminamos el prefijo '### ' y espacios sobrantes\n",
        "    clean_titles = [title.lstrip(\"# \").strip() for title, _ in builder.sections]\n",
        "\n",
        "#    # 3) Checkbox dinmico (todos DESMARCADOS)\n",
        "#    checkboxes = [\n",
        "#        widgets.Checkbox(value=False, description=clean_title, indent=False)\n",
        "#        for clean_title in clean_titles\n",
        "#    ]\n",
        "    # 3) Checkbox dinmico (todos MARCADOS por defecto)\n",
        "    checkboxes = [\n",
        "        widgets.Checkbox(value=True, description=clean_title, indent=False)\n",
        "        for clean_title in clean_titles\n",
        "    ]\n",
        "\n",
        "    # 4) Checkbox global para marcar/desmarcar todos\n",
        "    toggle_all = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description=\"(Des)marcar todas las secciones\",\n",
        "        indent=False,\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    # 5) Contador dinmico de secciones seleccionadas\n",
        "    counter_label = widgets.Label()\n",
        "    def actualizar_contador(_=None):\n",
        "        n_sel = sum(cb.value for cb in checkboxes)\n",
        "        counter_label.value = f\"Secciones seleccionadas: {n_sel} de {len(checkboxes)}\"\n",
        "    actualizar_contador()  # Inicializa\n",
        "\n",
        "    # 6) Callback del checkbox global\n",
        "    def _on_toggle_all(change):\n",
        "        for cb in checkboxes:\n",
        "            cb.value = toggle_all.value\n",
        "        actualizar_contador()\n",
        "\n",
        "    toggle_all.observe(_on_toggle_all, names='value')\n",
        "\n",
        "    # 7) Callback individual para cada checkbox (actualiza contador)\n",
        "    for cb in checkboxes:\n",
        "        cb.observe(actualizar_contador, names='value')\n",
        "\n",
        "    # 8) Botn de ejecucin\n",
        "    btn_generate = widgets.Button(\n",
        "        description=\"Generar Informe\",\n",
        "        button_style=\"success\",\n",
        "        layout=widgets.Layout(margin=\"10px 0 0 0\")\n",
        "    )\n",
        "\n",
        "    # 9) Montamos el UI\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>Selecciona las secciones a incluir en el informe:</h3>\"),\n",
        "        toggle_all,  #  Aadimos el checkbox global\n",
        "        counter_label,\n",
        "        widgets.VBox(checkboxes),\n",
        "        btn_generate\n",
        "    ])\n",
        "    display(ui)\n",
        "\n",
        "    # 10) Callback: al pulsar el botn, filtramos y renderizamos\n",
        "    def _on_generate_clicked(_):\n",
        "        from IPython.display import clear_output, display\n",
        "        clear_output(wait=True)\n",
        "        # a) Recogemos slo los ttulos marcados\n",
        "        seleccion = {\n",
        "            cb.description\n",
        "            for cb in checkboxes\n",
        "            if cb.value\n",
        "        }\n",
        "        # b) Filtramos builder.sections por coincidencia exacta del ttulo limpio\n",
        "        filtered = []\n",
        "        for (orig_title, content), clean_title in zip(builder.sections, clean_titles):\n",
        "            if clean_title in seleccion:\n",
        "                filtered.append((orig_title, content))\n",
        "\n",
        "        # c) Reemplazamos y renderizamos\n",
        "        builder.sections = filtered\n",
        "        builder.render()\n",
        "\n",
        "        #  d) Llamada a exportar_informe (Word, PDF, HTML)\n",
        "        exportar_informe(builder)\n",
        "\n",
        "    # 10) Asignar callback\n",
        "    btn_generate.on_click(_on_generate_clicked)\n",
        "\n",
        "#        import ipywidgets as widgets\n",
        "#        from IPython.display import clear_output, display, HTML\n",
        "#        from docx import Document\n",
        "#        from docx.shared import Inches\n",
        "#        import io, base64\n",
        "\n",
        "#        def _export_to_word(b):\n",
        "#            from IPython.display import clear_output, display, HTML\n",
        "#            from docx import Document\n",
        "#            from docx.shared import Inches\n",
        "#            import ipywidgets as widgets\n",
        "#            import pandas as _pd\n",
        "#            import io, base64, traceback\n",
        "\n",
        "#            clear_output(wait=True)\n",
        "#            print(\"[DEBUG] Iniciando exportacin a Word\")\n",
        "\n",
        "#            try:\n",
        "#                # 1) Crear documento Word\n",
        "#                doc = Document()\n",
        "#                doc.add_heading('Informe Generado', level=0)\n",
        "\n",
        "#                secciones_exportadas = 0\n",
        "#                errores = []\n",
        "\n",
        "#                for title, content in builder.sections:\n",
        "#                    try:\n",
        "#                        clean_title = title.lstrip('# ').strip()\n",
        "#                        doc.add_heading(clean_title, level=1)\n",
        "\n",
        "#                        if isinstance(content, _pd.DataFrame):\n",
        "#                            df = content\n",
        "#                            table = doc.add_table(rows=1, cols=len(df.columns))\n",
        "#                            hdr = table.rows[0].cells\n",
        "#                            for i, col in enumerate(df.columns):\n",
        "#                                hdr[i].text = str(col)\n",
        "#                            for row in df.itertuples(index=False):\n",
        "#                                cells = table.add_row().cells\n",
        "#                                for i, val in enumerate(row):\n",
        "#                                    cells[i].text = str(val)\n",
        "\n",
        "#                        elif hasattr(content, \"savefig\") and content is not None:\n",
        "#                            try:\n",
        "#                                img_stream = io.BytesIO()\n",
        "#                                content.savefig(img_stream, format='png', bbox_inches='tight')\n",
        "#                                img_stream.seek(0)\n",
        "#                                doc.add_picture(img_stream, width=Inches(6))\n",
        "#                            except Exception as e:\n",
        "#                                msg = f\"[ERROR] Fallo al insertar imagen en seccin '{clean_title}': {e_img}\"\n",
        "#                                print(msg)\n",
        "#                                errores.append(msg)\n",
        "#                                doc.add_paragraph(f\" No se pudo insertar la figura: {e}\")\n",
        "\n",
        "#                        elif isinstance(content, str):\n",
        "#                            for line in content.splitlines():\n",
        "#                                doc.add_paragraph(line)\n",
        "\n",
        "#                        else:\n",
        "#                            msg = f\"[WARNING] Tipo de contenido no reconocido en seccin '{clean_title}': {type(content)}\"\n",
        "#                            print(msg)\n",
        "#                            errores.append(msg)\n",
        "#                            doc.add_paragraph(f\" Contenido no reconocido o vaco: {type(content)}\")\n",
        "\n",
        "#                        doc.add_page_break()\n",
        "#                        secciones_exportadas += 1\n",
        "\n",
        "#                    except Exception as e_sec:\n",
        "#                        msg = f\"[ERROR] Fallo exportando seccin '{title}': {e_sec}\"\n",
        "#                        print(msg)\n",
        "#                        errores.append(msg)\n",
        "#                        doc.add_paragraph(f\" Error en seccin '{title}': {e_sec}\")\n",
        "#                        traceback.print_exc()\n",
        "\n",
        "#                # Aadir advertencia si no se aadi nada\n",
        "#                if len(doc.paragraphs) == 0:\n",
        "#                    doc.add_paragraph(\" El informe no contiene contenido vlido para exportar.\")#\n",
        "\n",
        "#                # 2) Guardar documento\n",
        "#                output_path = \"informe_generado.docx\"\n",
        "#                doc.save(output_path)\n",
        "#                print(f\"[DEBUG] Documento guardado en {output_path}\")\n",
        "#                print(f\"[DEBUG] Secciones exportadas correctamente: {secciones_exportadas}\")\n",
        "#                if errores:\n",
        "#                    print(\"[DEBUG] Secciones con errores:\", len(errores))\n",
        "#                    for err in errores:\n",
        "#                        print(\"[DEBUG]\", err)\n",
        "\n",
        "#                # 3) Convertir a base64 para enlace de descarga\n",
        "#                with open(output_path, \"rb\") as f:\n",
        "#                    b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "#                href = (\n",
        "#                    f'<a download=\"{output_path}\" '\n",
        "#                    f'href=\"data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,{b64}\">'\n",
        "#                    \" Descargar informe Word</a>\"\n",
        "#                )\n",
        "#                display(HTML(href))\n",
        "\n",
        "#            except Exception as e:\n",
        "#                print(f\"[ERROR] Fallo en la exportacin a Word: {e}\")\n",
        "#                traceback.print_exc()\n",
        "#                display(HTML(\"<b style='color:red;'> Error al generar el documento Word.</b>\"))\n",
        "\n",
        "\n",
        "#        def _export_to_word(b):\n",
        "#            # Import local para no chocar con variables externas\n",
        "#            from IPython.display import clear_output, display, HTML\n",
        "#            from docx import Document\n",
        "#            from docx.shared import Inches\n",
        "#            import ipywidgets as widgets\n",
        "#            import pandas as _pd\n",
        "\n",
        "#            clear_output(wait=True)\n",
        "#            print(\"[DEBUG] Iniciando exportacin a Word\")\n",
        "\n",
        "#            # 1) Crear documento y volcar todas las secciones\n",
        "#            doc = Document()\n",
        "#            doc.add_heading('Informe Generado', level=0)\n",
        "\n",
        "#            for title, content in builder.sections:\n",
        "#                clean_title = title.lstrip('# ').strip()\n",
        "#                doc.add_heading(clean_title, level=1)\n",
        "\n",
        "#                if isinstance(content, _pd.DataFrame):\n",
        "#                    df = content\n",
        "#                    table = doc.add_table(rows=1, cols=len(df.columns))\n",
        "#                    hdr = table.rows[0].cells\n",
        "#                    for i, col in enumerate(df.columns):\n",
        "#                        hdr[i].text = str(col)\n",
        "#                    for row in df.itertuples(index=False):\n",
        "#                        cells = table.add_row().cells\n",
        "#                        for i, val in enumerate(row):\n",
        "#                            cells[i].text = str(val)\n",
        "\n",
        "#                #elif hasattr(content, \"savefig\"):\n",
        "#                #    img_stream = io.BytesIO()\n",
        "#                #    content.savefig(img_stream, format='png', bbox_inches='tight')\n",
        "#                #    img_stream.seek(0)\n",
        "#                #    doc.add_picture(img_stream, width=Inches(6))\n",
        "\n",
        "#                elif hasattr(content, \"savefig\"):\n",
        "#                    try:\n",
        "#                        img_stream = io.BytesIO()\n",
        "#                        content.savefig(img_stream, format='png', bbox_inches='tight')\n",
        "#                        img_stream.seek(0)\n",
        "#                        if img_stream.getbuffer().nbytes > 0:\n",
        "#                            doc.add_picture(img_stream, width=Inches(6))\n",
        "#                        else:\n",
        "#                            doc.add_paragraph(\"[ERROR: La imagen generada est vaca o corrupta]\")\n",
        "#                    except Exception as e:\n",
        "#                        doc.add_paragraph(f\"[ERROR al guardar imagen: {type(e).__name__}: {e}]\")\n",
        "\n",
        "#                else:\n",
        "#                    for line in str(content).splitlines():\n",
        "#                        doc.add_paragraph(line)\n",
        "\n",
        "#                doc.add_page_break()\n",
        "\n",
        "#            if len(doc.paragraphs) == 0:\n",
        "#                doc.add_paragraph(\" El informe no contiene contenido vlido para exportar.\")\n",
        "\n",
        "#            # 2) Guardar a disco\n",
        "#            output_path = \"informe_generado.docx\"\n",
        "#            doc.save(output_path)\n",
        "#            print(f\"[DEBUG] Documento guardado en {output_path}\")\n",
        "\n",
        "#            # 3) Convertir a Base64 para Data URI\n",
        "#            with open(output_path, \"rb\") as f:\n",
        "#                b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "#            href = (\n",
        "#                f'<a download=\"{output_path}\" '\n",
        "#                f'href=\"data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,{b64}\">'\n",
        "#                \" Descargar informe Word</a>\"\n",
        "#            )\n",
        "#            display(HTML(href))\n",
        "\n",
        "\n",
        "\n",
        "#        #  En tu funcin mostrar_informe, justo DESPUS de builder.render() pon:\n",
        "\n",
        "#        btn_export = widgets.Button(\n",
        "#            description=\"Exportar a Word\",\n",
        "#            button_style=\"info\",\n",
        "#            layout=widgets.Layout(margin=\"10px 0 0 0\")\n",
        "#        )\n",
        "#        btn_export.on_click(_export_to_word)\n",
        "#        display(btn_export)\n",
        "\n",
        "#    btn_generate.on_click(_on_generate_clicked)\n",
        "\n",
        "#def mostrar_informe():\n",
        "#    \"\"\"Men: Generar Informe.\"\"\"\n",
        "#    clear_output(wait=True)\n",
        "#    print(\"[DEBUG] mostrar_informe start\")\n",
        "#    builder = ReportBuilder(globals())\n",
        "#    builder.build_sections()\n",
        "#    builder.render()\n",
        "#    print(\"[DEBUG] mostrar_informe end\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 13. MEN PRINCIPAL  FUNCIONA\n",
        "# ===============================================================\n",
        "#    NIVEL-1                            NIVEL-2                       NIVEL-3  (sin cambios)\n",
        "# \n",
        "# 1) Bienvenida                      1. Bienvenida\n",
        "# 2) Ayuda General                   8. Ayuda Global\n",
        "# 3) Bloque 1  Carga  Seleccin    2. Carga/Segmentacin       2.1 Carga de Datos\n",
        "#                                                                  2.2 Segmentacin Datos\n",
        "#                                     3. Seleccin variables X   (sin sub-nivel, invoca directo)\n",
        "# 4) Bloque 2  Entrenamiento         4. Entrenamiento Modelos   4.1 SVR  4.2 NN  4.3 XGB \n",
        "#                                     5. Prediccin/Visualiz.    5.1 Pred.SVR  5.2 Pred.NN \n",
        "# 5) Bloque 3  Optimizacin          6. Optimizacin modelos    6.1 SVR  6.2 NN \n",
        "# 6) Bloque 4  Interpretabilidad     7. Interpretacin xIA      (sin sub-nivel, invoca directo)\n",
        "# 7) Generar Informe Final            8. Generar Informe Final   (sin sub-nivel, invoca directo)\n",
        "# \n",
        "from ipywidgets import Dropdown, Button, VBox, HBox, Output\n",
        "from IPython.display import HTML as dHTML, clear_output\n",
        "\n",
        "# \n",
        "# 0  Diccionario HOJAS    funcin\n",
        "# \n",
        "menu_funcs = {\n",
        "    # Bienvenida + Ayuda\n",
        "    \"1. Bienvenida\":            mostrar_bienvenida,\n",
        "    \"2. Ayuda Global\":          mostrar_ayuda_completa,\n",
        "\n",
        "    # Bloque 1\n",
        "    \"B1.1.1 Carga de Datos\":       mostrar_carga,\n",
        "    \"B1.1.2 Segmentacin Datos\":   mostrar_split,\n",
        "    \"B1.2.1 Seleccin variables X\": mostrar_seleccion_variables,\n",
        "\n",
        "    # Bloque 2  Entrenamiento\n",
        "    \"B2.1.1 Entrenamiento SVR\":            mostrar_svr,\n",
        "    \"B2.1.2 Entrenamiento NN\":             mostrar_nn,\n",
        "    \"B2.1.3 Entrenamiento XGBoost\":        mostrar_xgb,\n",
        "    \"B2.1.4 Entrenamiento Random Forest\":  mostrar_rf,\n",
        "    \"B2.1.5 Entrenamiento RNN\":            mostrar_rnn,\n",
        "    \"B2.1.6 Comparador Modelos\":           mostrar_comparador_modelos,\n",
        "\n",
        "    # Bloque 2  Prediccin\n",
        "    \"B2.2.1 Prediccin SVR\":               mostrar_prediccion_svr,\n",
        "    \"B2.2.2 Prediccin NN\":                mostrar_prediccion_nn,\n",
        "    \"B2.2.3 Prediccin XGBoost\":           mostrar_prediccion_xgboost,\n",
        "    \"B2.2.4 Prediccin Random Forest\":     mostrar_prediccion_rf,\n",
        "    \"B2.2.5 Prediccin RNN\":               mostrar_prediccion_rnn,\n",
        "    \"B2.2.6 Visualizacin resultados\":     mostrar_grafico_y_vs_x,\n",
        "\n",
        "    # Bloque 3  Optimizacin\n",
        "    \"B3.1.1 Optimizacin SVR\":             mostrar_optimizacion_svr,\n",
        "    \"B3.1.2 Optimizacin NN\":              mostrar_optimizacion_nn,\n",
        "    \"B3.1.3 Optimizacin XGBoost\":         mostrar_optimizacion_xgb,\n",
        "    \"B3.1.4 Optimizacin Random Forest\":   mostrar_optimizacion_rf,\n",
        "    \"B3.1.5 Optimizacin RNN\":             mostrar_optimizacion_rnn,\n",
        "\n",
        "    # Bloque 4  xIA\n",
        "    \"B4.1 Interpretacin xIA\":            mostrar_xai,\n",
        "\n",
        "    # Informe Final\n",
        "    \"Generar Informe Final\":              mostrar_informe,\n",
        "}\n",
        "\n",
        "# \n",
        "# 1  rbol multinivel\n",
        "# \n",
        "menu_tree = {\n",
        "    \"Bienvenida\": {\n",
        "        \"1. Bienvenida\": None\n",
        "    },\n",
        "    \"Ayuda General\": {\n",
        "        \"2. Ayuda Global\": None\n",
        "    },\n",
        "    \"Bloque 1  Carga y segmentacin de datos y Seleccin Variables\": {\n",
        "        \"B1.1 Carga y Segmentacin de Datos\": {\n",
        "            \"B1.1.1 Carga de Datos\": None,\n",
        "            \"B1.1.2 Segmentacin Datos\": None,\n",
        "        },\n",
        "        \"B1.2 Seleccin variables X\": {\n",
        "            \"B1.2.1 Seleccin variables X\": None,\n",
        "        },\n",
        "    },\n",
        "    \"Bloque 2  Entrenamiento de modelos IA y Prediccin de Salidas\": {\n",
        "        \"B2.1 Entrenamiento Modelos IA\": {\n",
        "            \"B2.1.1 Entrenamiento SVR\": None,\n",
        "            \"B2.1.2 Entrenamiento NN\": None,\n",
        "            \"B2.1.3 Entrenamiento XGBoost\": None,\n",
        "            \"B2.1.4 Entrenamiento Random Forest\": None,\n",
        "            \"B2.1.5 Entrenamiento RNN\": None,\n",
        "            \"B2.1.6 Comparador Modelos\": None,\n",
        "        },\n",
        "        \"B2.2 Prediccin y visualizacin de datos de salida\": {\n",
        "            \"B2.2.1 Prediccin SVR\": None,\n",
        "            \"B2.2.2 Prediccin NN\": None,\n",
        "            \"B2.2.3 Prediccin XGBoost\": None,\n",
        "            \"B2.2.4 Prediccin Random Forest\": None,\n",
        "            \"B2.2.5 Prediccin RNN\": None,\n",
        "            \"B2.2.6 Visualizacin resultados\": None,\n",
        "        },\n",
        "    },\n",
        "    \"Bloque 3  Optimizacin de Modelos IA\": {\n",
        "        \"B3.1 Optimizacin de modelos IA\": {\n",
        "            \"B3.1.1 Optimizacin SVR\": None,\n",
        "            \"B3.1.2 Optimizacin NN\": None,\n",
        "            \"B3.1.3 Optimizacin XGBoost\": None,\n",
        "            \"B3.1.4 Optimizacin Random Forest\": None,\n",
        "            \"B3.1.5 Optimizacin RNN\": None,\n",
        "        },\n",
        "    },\n",
        "    \"Bloque 4  Inteligencia Artificial Explicativa xIA\": {\n",
        "         \"B4.1 Interpretacin xIA\": None,\n",
        "#        \"B4.1 Interpretacin xIA\": {\n",
        "#            \"B4.1.1 Interpretacin xIA\": None,\n",
        "#        },\n",
        "    },\n",
        "    \"Generar Informe Final\": {\n",
        "         \"Generar Informe Final\": None,\n",
        "#        \"Generar Informe Final\": {\n",
        "#            \"Generar Informe Final\": None,\n",
        "#        },\n",
        "    },\n",
        "}\n",
        "\n",
        "# \n",
        "# 2  Helpers\n",
        "# \n",
        "def _crear_dropdown(options, nivel):\n",
        "    return widgets.Dropdown(\n",
        "        options=options,\n",
        "        description=f\"Nivel-{nivel}:\",\n",
        "        layout={\n",
        "            'width': '100%'          #   ancho fluido\n",
        "            # o '1100px', '80%', etc.\n",
        "        },\n",
        "        style={\n",
        "            'description_width': '200px'  # ajusta si hiciera falta\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def _subtree_for(path):\n",
        "    node = menu_tree\n",
        "    for key in path:\n",
        "        node = node[key]\n",
        "    return node  # None  hoja\n",
        "\n",
        "# \n",
        "# 3  Widgets contenedores\n",
        "# \n",
        "levels_box = VBox(layout={'width': 'auto'})  #  ancho ajustado dinmicamente\n",
        "\n",
        "btn_next = Button(\n",
        "    description=\"Seleccionar Siguiente\",\n",
        "    button_style=\"info\",\n",
        "    layout={'width': '190px'}\n",
        ")\n",
        "\n",
        "btn_run = Button(\n",
        "    description=\"Ejecutar\",\n",
        "    button_style=\"success\",\n",
        "    layout={'width': '140px'}\n",
        ")\n",
        "\n",
        "out_panel = widgets.Output(\n",
        "    layout={\n",
        "        'border': '1px solid #ccc',\n",
        "        'padding': '12px',\n",
        "        'width': '100%',\n",
        "        'max_height': '600px',\n",
        "        'overflow_y': 'auto',\n",
        "        'overflow_x': 'hidden',      #  evita scroll horizontal\n",
        "        'margin_top': '12px'\n",
        "    }\n",
        ")\n",
        "\n",
        "# \n",
        "# 4  Lgica de navegacin\n",
        "# \n",
        "def _crear_dropdown(options, nivel):\n",
        "    return Dropdown(\n",
        "        options=options,\n",
        "        description=f\"Nivel-{nivel}:\",\n",
        "        layout={\n",
        "            'width': 'auto',               #  ancho solo segn texto\n",
        "            'min_width': '400px',          #  margen mnimo visual aceptable\n",
        "            'max_width': '700px'\n",
        "        },\n",
        "        style={'description_width': '160px'}\n",
        "    )\n",
        "\n",
        "def _update_buttons():\n",
        "    path = [d.value for d in levels_box.children]\n",
        "    leaf = (_subtree_for(path) is None)\n",
        "    btn_next.disabled = leaf\n",
        "    btn_run.disabled = not leaf\n",
        "\n",
        "def _on_change(ch):\n",
        "    dd_list = list(levels_box.children)\n",
        "    idx = dd_list.index(ch['owner'])\n",
        "    levels_box.children = tuple(dd_list[:idx+1])\n",
        "    _update_buttons()\n",
        "\n",
        "def _reset():\n",
        "    dd0 = _crear_dropdown(list(menu_tree.keys()), 1)\n",
        "    dd0.observe(_on_change, names='value')\n",
        "    levels_box.children = (dd0,)\n",
        "    _update_buttons()\n",
        "\n",
        "def _next(_):\n",
        "    path = [d.value for d in levels_box.children]\n",
        "    branch = _subtree_for(path)\n",
        "    if branch:\n",
        "        nivel = len(levels_box.children) + 1\n",
        "        new_dd = _crear_dropdown(list(branch.keys()), nivel)\n",
        "        new_dd.observe(_on_change, names='value')\n",
        "        levels_box.children = (*levels_box.children, new_dd)\n",
        "    _update_buttons()\n",
        "\n",
        "def _run(_):\n",
        "    nodo = levels_box.children[-1].value\n",
        "    func = menu_funcs.get(nodo)\n",
        "    out_panel.clear_output()\n",
        "    with out_panel:\n",
        "        if func is None:\n",
        "            print(f\"  No se ha implementado {nodo}.\")\n",
        "        else:\n",
        "            clear_output(wait=True)\n",
        "            func()\n",
        "\n",
        "btn_next.on_click(_next)\n",
        "btn_run.on_click(_run)\n",
        "_reset()\n",
        "\n",
        "# \n",
        "# 5  Mostrar UI\n",
        "# \n",
        "display(\n",
        "    VBox([\n",
        "        widgets.HTML(\"<h3 style='font-size:1.3rem;margin-bottom:5px;'> Men Principal</h3>\"),\n",
        "        levels_box,\n",
        "        HBox([btn_next, btn_run], layout={'gap': '30px', 'margin_top': '5px'}),\n",
        "        out_panel\n",
        "    ])\n",
        ")\n",
        "from ipywidgets import Button\n",
        "from IPython.display import display\n",
        "\n",
        "#  BLOQUE DE LIMPIEZA \n",
        "btn_limpiar = Button(\n",
        "    description=\" Limpiar pantalla\",\n",
        "    button_style=\"warning\",\n",
        "    layout={'width': '150px'}\n",
        ")\n",
        "\n",
        "def _on_limpiar(_):\n",
        "    # Borra solo el contenido del panel de resultados\n",
        "    out_panel.clear_output()\n",
        "\n",
        "btn_limpiar.on_click(_on_limpiar)\n",
        "\n",
        "# Lo mostramos justo debajo del men:\n",
        "display(btn_limpiar)\n",
        "\n"
      ]
    }
  ]
}