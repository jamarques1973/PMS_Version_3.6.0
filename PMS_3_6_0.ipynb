{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxHRSP0urTVWTp1eM1WRf6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7fc0afe58bdf453b9fc9a8d253d9b15e": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2876e80d1e6443f294166860dc38ac7f",
            "msg_id": "",
            "outputs": []
          }
        },
        "2876e80d1e6443f294166860dc38ac7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5842eef7f7f44ac2a75bbdb06d2d7587": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edd237a2675c4e02b880078db1cf977a": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_5842eef7f7f44ac2a75bbdb06d2d7587",
            "msg_id": "",
            "outputs": []
          }
        },
        "a7a278021a7c487c8439839997017fd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2e2646a3891471b8807094aac79a861": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a7a278021a7c487c8439839997017fd1",
            "msg_id": "",
            "outputs": []
          }
        },
        "c8a60fd4d33740afbffc2aa693697f85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f430a6c173b4747a76e9507b7de9304": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c8a60fd4d33740afbffc2aa693697f85",
            "msg_id": "",
            "outputs": []
          }
        },
        "8c83ecff325b4735bf4fc5ff385b4e81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "670dc6aeae984c3b86a690f03651ad34": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_8c83ecff325b4735bf4fc5ff385b4e81",
            "msg_id": "",
            "outputs": []
          }
        },
        "f829dc46033642c9adfc93e550750301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4276874e205f4773a23b128ecf1d0251": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f829dc46033642c9adfc93e550750301",
            "msg_id": "",
            "outputs": []
          }
        },
        "919e3eff833541f9ae00a2c385b32a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3eb0a55f8044831aa45e116d4a6730e": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_919e3eff833541f9ae00a2c385b32a0e",
            "msg_id": "",
            "outputs": []
          }
        },
        "0f5817d01b0f40318504805491728c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98241a67dd0e4d9592eff31baf72ef30": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_0f5817d01b0f40318504805491728c5e",
            "msg_id": "",
            "outputs": []
          }
        },
        "3cbc1394f08e4127ab3393b7165417fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f7f7b60ab3406a8455a1ca982c88c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fee1b1861c8442ebb2243e5de847f2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "ba296edc19694bd195af5396c36787ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33c1a4964ec045218f32535f0ce9968b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c61e73ba88ba426cbbed8cfed13269be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62694866e04b47928a2722c9b29bc07e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "40%"
          }
        },
        "2c8478edfaf94d7f912022b6d1185ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7cd40e9f8284f459d592efde209ec53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "593a5f8c069e4061995b8a0fbeddc33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "092e670f9b834b239aa0bf5ef521cb53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3222d4fb5c1e4138a594cf9edbaa0748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "d409974836ee4c2a965e721ad347c460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de1a11fa427646d3becc2e8c99ab2812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "847b518cb96948009cb97e0ae3681e4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d6a5470f567411289ce5def9c347abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "51dbdea738db4b688df8d4d2971babfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e928878511c742c2989a9e888e99797f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "b6fceaefb91545068c9be746981e9cf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6c793e725154a2298631adf6d807ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "a7332fe1349248c0bb14222040716e88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2afeee701b564b6eaec77ebe5c06e34d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "24d2633945b14ff0932130d6dc6b5171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bed4efd79e543049a04c1df3c72a50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "green",
            "description_width": ""
          }
        },
        "6e91c4f5807241a09441a1f7c1b1bf84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bac08d2ef8849fbbba1549429f404a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494affada79849c4a974652fe7a070c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9584861d1474450da2aac186b1a4481c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SelectMultipleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SelectMultipleModel",
            "_options_labels": [
              "Pearson",
              "Spearman",
              "Mutualinfo",
              "Boruta",
              "UMAP",
              "Todos"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "SelectMultipleView",
            "description": "Métodos Selección Variables X:",
            "description_tooltip": null,
            "disabled": false,
            "index": [],
            "layout": "IPY_MODEL_0fee1b1861c8442ebb2243e5de847f2b",
            "rows": 5,
            "style": "IPY_MODEL_ba296edc19694bd195af5396c36787ae"
          }
        },
        "04784ca9825a4257a788a76835bd7493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SelectMultipleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SelectMultipleModel",
            "_options_labels": [
              "RandomSearch",
              "BayesianOptimization",
              "Hyperband",
              "Optuna",
              "Todos"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "SelectMultipleView",
            "description": "Motores:",
            "description_tooltip": null,
            "disabled": false,
            "index": [],
            "layout": "IPY_MODEL_33c1a4964ec045218f32535f0ce9968b",
            "rows": 5,
            "style": "IPY_MODEL_c61e73ba88ba426cbbed8cfed13269be"
          }
        },
        "32d0722e22284d2999579e244303881e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4465fd38a444c68b0f67fe7ab67457a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "Épocas:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_092e670f9b834b239aa0bf5ef521cb53",
            "max": 200,
            "min": 1,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 10,
            "style": "IPY_MODEL_3222d4fb5c1e4138a594cf9edbaa0748",
            "value": 200
          }
        },
        "dc3686da512447a6ac66663c11e39abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "Capas:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_d409974836ee4c2a965e721ad347c460",
            "max": 6,
            "min": 1,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_de1a11fa427646d3becc2e8c99ab2812",
            "value": 3
          }
        },
        "2786b5a4367e4213a765e3963b277f51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "097e842afc9e4bc8bab29ca22ba6c07b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "Neuronas/capa:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_847b518cb96948009cb97e0ae3681e4b",
            "max": 512,
            "min": 256,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 8,
            "style": "IPY_MODEL_6d6a5470f567411289ce5def9c347abd",
            "value": 256
          }
        },
        "4ef6c8c5c1c64d25b3252f9f509bf7a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "Dropout:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_51dbdea738db4b688df8d4d2971babfc",
            "max": 0.7,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 0.05,
            "style": "IPY_MODEL_e928878511c742c2989a9e888e99797f",
            "value": 0.2
          }
        },
        "f006cb096af84ce5b75adbd83043108f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "L2 Reg:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b6fceaefb91545068c9be746981e9cf7",
            "max": 0.01,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 0.0005,
            "style": "IPY_MODEL_f6c793e725154a2298631adf6d807ce7",
            "value": 0.001
          }
        },
        "5ba19ca33a174db68883aa8712d71578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d22b5d395913418aa608ea277ceafa3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bac08d2ef8849fbbba1549429f404a0",
            "placeholder": "​",
            "style": "IPY_MODEL_494affada79849c4a974652fe7a070c6",
            "value": "<h3>🔧 Configuración Optimización NN</h3>"
          }
        },
        "9e4ab1aaac1e44e290d27f59d9be0bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cbc1394f08e4127ab3393b7165417fd",
            "placeholder": "​",
            "style": "IPY_MODEL_92f7f7b60ab3406a8455a1ca982c88c1",
            "value": "\n<h4>📘 Explicación de Parámetros</h4>\n<ul>\n  <li><b>Métodos X:</b> Métodos de selección de variables predictoras. Usan correlaciones estadísticas o algoritmos de reducción de dimensión. <br>\n      <i>Pearson</i> y <i>Spearman</i>: correlaciones lineales y monótonas.<br>\n      <i>MutualInfo</i>: mide dependencia informacional. <br>\n      <i>Boruta</i>: selección envolvente basada en árboles. <br>\n      <i>UMAP</i>: reducción no lineal de dimensiones. <br>\n      <b>Todos</b> ejecuta cada uno secuencialmente.</li>\n  <li><b>Motores:</b> Algoritmos de optimización de hiperparámetros. <br>\n      <i>RandomSearch</i>: búsqueda aleatoria. <br>\n      <i>BayesianOptimization</i>: estima regiones óptimas. <br>\n      <i>Hyperband</i>: eficiente para grandes espacios de búsqueda. <br>\n      <i>Optuna</i>: flexible y potente. <br>\n      <b>Todos</b> ejecuta todos los motores.</li>\n  <li><b>Función objetivo:</b> Métrica a maximizar o minimizar: <br>\n      <i>R2</i>: se desea maximizar. <i>MAE</i> y <i>MSE</i>: se minimizan.</li>\n  <li><b>Trials:</b> Número de combinaciones a evaluar en la búsqueda.</li>\n  <li><b>Épocas:</b> Iteraciones completas sobre el dataset de entrenamiento (100 a 2000 recomendado).</li>\n  <li><b>Capas:</b> Cantidad de capas ocultas en la red (1 a 20 habitual, máximo 100 para pruebas avanzadas).</li>\n  <li><b>Neuronas/capa:</b> Número de neuronas por capa (32 a 512 recomendado).</li>\n  <li><b>Dropout:</b> Fracción de neuronas descartadas en entrenamiento (0.1 a 0.4 recomendado).</li>\n  <li><b>L2 Reg:</b> Regularización L2 para evitar sobreajuste (0.001 a 0.01 habitual).</li>\n</ul>\n"
          }
        },
        "b87174a312154bb0b21f1fa9b4a10ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9584861d1474450da2aac186b1a4481c",
              "IPY_MODEL_04784ca9825a4257a788a76835bd7493"
            ],
            "layout": "IPY_MODEL_32d0722e22284d2999579e244303881e"
          }
        },
        "0b83efb382274762948ec1b1c2b21c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "R2",
              "MAE",
              "MSE"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Función objetivo:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_62694866e04b47928a2722c9b29bc07e",
            "style": "IPY_MODEL_2c8478edfaf94d7f912022b6d1185ad8"
          }
        },
        "2ea64f2fbd8644dea97dd6ee08fe22bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "Trials:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b7cd40e9f8284f459d592efde209ec53",
            "max": 50,
            "min": 1,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_593a5f8c069e4061995b8a0fbeddc33a",
            "value": 10
          }
        },
        "1157881e56804c99a5f1f8ef1b4d1850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4465fd38a444c68b0f67fe7ab67457a",
              "IPY_MODEL_dc3686da512447a6ac66663c11e39abf"
            ],
            "layout": "IPY_MODEL_2786b5a4367e4213a765e3963b277f51"
          }
        },
        "d26b4c62ddb04a9bb824b9ce2ce306ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_097e842afc9e4bc8bab29ca22ba6c07b",
              "IPY_MODEL_4ef6c8c5c1c64d25b3252f9f509bf7a3",
              "IPY_MODEL_f006cb096af84ce5b75adbd83043108f"
            ],
            "layout": "IPY_MODEL_5ba19ca33a174db68883aa8712d71578"
          }
        },
        "71d421482b4a4860b56eb229f708eb33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "🚀 Ejecutar Optimización NN",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_a7332fe1349248c0bb14222040716e88",
            "style": "IPY_MODEL_2afeee701b564b6eaec77ebe5c06e34d",
            "tooltip": ""
          }
        },
        "e5f7f55cd2414b5883aed40094410e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "Progreso:",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24d2633945b14ff0932130d6dc6b5171",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bed4efd79e543049a04c1df3c72a50f",
            "value": 0
          }
        },
        "adcf3930810b4875ac661551e21bc852": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_6e91c4f5807241a09441a1f7c1b1bf84",
            "msg_id": "",
            "outputs": []
          }
        },
        "e5106a7f20a845479749080ee3f6ffa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c15b5a7938b345e683e0c62506fa87b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce88efe7cbd04ce28e42da378097baf5": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c15b5a7938b345e683e0c62506fa87b5",
            "msg_id": "",
            "outputs": []
          }
        },
        "e82419e2f78c4765a900e8f7a2ec581f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c35f63e727cf43ce9472b8dde6be478d": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e82419e2f78c4765a900e8f7a2ec581f",
            "msg_id": "",
            "outputs": []
          }
        },
        "afd56917323c48ddbd8659d5a5153fd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d3da4f4034427f82d2a7c1a14c6fdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "21f23b86578b43029711ab0c738bc1da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "190px"
          }
        },
        "86c337421d204c7dbd1e7b5d3ea6b6ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f1fcb04c6cd643b3b68973c983929a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "140px"
          }
        },
        "70e1c0808ecd4e418817eba1b2ffc1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "98915d7e2be14836a5e833ee30db1402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid #ccc",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": "600px",
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": "hidden",
            "overflow_y": "auto",
            "padding": "12px",
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "292cdd96db474a32b72c0d7d19865225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": "700px",
            "min_height": null,
            "min_width": "400px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "1051c382444545a499c6c163113d973e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "160px"
          }
        },
        "6b6a1b2ed63f417a8712d79c54ba09e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36102fdd4844493b8c06ce22f319fac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "946d76c552b448279f2c0f2a2f372387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "Seleccionar Siguiente",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_21f23b86578b43029711ab0c738bc1da",
            "style": "IPY_MODEL_86c337421d204c7dbd1e7b5d3ea6b6ea",
            "tooltip": ""
          }
        },
        "584346a65568423da688e3063e5fb840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Ejecutar",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f1fcb04c6cd643b3b68973c983929a4e",
            "style": "IPY_MODEL_70e1c0808ecd4e418817eba1b2ffc1dd",
            "tooltip": ""
          }
        },
        "cdfca94dd0254e92a87f2a0ba02b706b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f03d3c0e8f3a47c5a836b9ecd1f16bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b6a1b2ed63f417a8712d79c54ba09e0",
            "placeholder": "​",
            "style": "IPY_MODEL_36102fdd4844493b8c06ce22f319fac2",
            "value": "<h3 style='font-size:1.3rem;margin-bottom:5px;'>📋 Menú Principal</h3>"
          }
        },
        "45661577f69949c0a69dac54bd7383cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9c0b3a5334f40508989952c82e744fb",
              "IPY_MODEL_d16186e75a0c4b4fb69fa013c0f42eb9"
            ],
            "layout": "IPY_MODEL_d4d3da4f4034427f82d2a7c1a14c6fdd"
          }
        },
        "b87506a2d19943709a4a31d60f13feda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_946d76c552b448279f2c0f2a2f372387",
              "IPY_MODEL_584346a65568423da688e3063e5fb840"
            ],
            "layout": "IPY_MODEL_cdfca94dd0254e92a87f2a0ba02b706b"
          }
        },
        "81f71060f06a467f938037fdd945a03b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_98915d7e2be14836a5e833ee30db1402",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "VBox(children=(HTML(value=\"<h2 style='color:#1E90FF;'>📘 AYUDA GLOBAL — Guía Completa de la Aplicación</h2>\"), …",
                  "application/vnd.jupyter.widget-view+json": {
                    "version_major": 2,
                    "version_minor": 0,
                    "model_id": "9cd63676a2f84d8a883b0261c17ada56"
                  }
                },
                "metadata": {
                  "application/vnd.jupyter.widget-view+json": {
                    "colab": {
                      "custom_widget_manager": {
                        "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                      }
                    }
                  }
                }
              }
            ]
          }
        },
        "0c513d02f1914314a848683355ef1c6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cabff7d374b4ad7b4467c8baa6b23c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f03d3c0e8f3a47c5a836b9ecd1f16bd1",
              "IPY_MODEL_45661577f69949c0a69dac54bd7383cf",
              "IPY_MODEL_b87506a2d19943709a4a31d60f13feda",
              "IPY_MODEL_81f71060f06a467f938037fdd945a03b"
            ],
            "layout": "IPY_MODEL_0c513d02f1914314a848683355ef1c6d"
          }
        },
        "d9c0b3a5334f40508989952c82e744fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Bienvenida",
              "Ayuda General",
              "Bloque 1 – Carga y segmentación de datos y Selección Variables",
              "Bloque 2 – Entrenamiento de modelos IA y Predicción de Salidas",
              "Bloque 3 – Optimización de Modelos IA",
              "Bloque 4 – Inteligencia Artificial Explicativa xIA",
              "Generar Informe Final"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Nivel-1:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_292cdd96db474a32b72c0d7d19865225",
            "style": "IPY_MODEL_1051c382444545a499c6c163113d973e"
          }
        },
        "5db0ca6dc6d64f2b9461c2fa7ac88c36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "150px"
          }
        },
        "7ffe7c666e02400f82a48375ecc0109c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1a141e8ccd144bf7bbd16a39863e4064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "warning",
            "description": "🧹 Limpiar pantalla",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5db0ca6dc6d64f2b9461c2fa7ac88c36",
            "style": "IPY_MODEL_7ffe7c666e02400f82a48375ecc0109c",
            "tooltip": ""
          }
        },
        "ccbaf9630780425ea9e222053bd354c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": "700px",
            "min_height": null,
            "min_width": "400px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "385259253b7f4fd0aa376c07a5c7969a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "160px"
          }
        },
        "d16186e75a0c4b4fb69fa013c0f42eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "2. Ayuda Global"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Nivel-2:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_ccbaf9630780425ea9e222053bd354c6",
            "style": "IPY_MODEL_385259253b7f4fd0aa376c07a5c7969a"
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamarques1973/PMS_Version_3.6.0/blob/main/PMS_3_6_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "BBBS6j7M2b6b",
        "outputId": "f01a8a7e-eeac-49f1-b527-b25a4cbd7436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=4dec1d320ec659800d563746a1606dfbe70ebcff702a0ab2d1341b9cdf4d5ce4\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n",
            "Collecting interpret\n",
            "  Downloading interpret-0.7.1-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting interpret-core==0.7.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading interpret_core-0.7.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.5.1)\n",
            "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.9.5)\n",
            "Requirement already satisfied: ipykernel>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.17.1)\n",
            "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (7.34.0)\n",
            "Requirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.24.1)\n",
            "Collecting SALib>=1.3.3 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading salib-1.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: shap>=0.28.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.48.0)\n",
            "Requirement already satisfied: dill>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.3.8)\n",
            "Collecting aplr>=10.6.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading aplr-10.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting dash<3.0.0,>=2.0.0 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting dash-cytoscape>=0.1.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading dash_cytoscape-1.0.2.tar.gz (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gevent>=1.3.6 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading gevent-25.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.32.3)\n",
            "Collecting Flask<3.1,>=1.0.4 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting Werkzeug<3.1 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting dash-html-components==2.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.14.1)\n",
            "Collecting retrying (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading retrying-1.4.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (75.2.0)\n",
            "Requirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Collecting zope.event (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading zope_event-5.1.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting zope.interface (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.1.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (25.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.7.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=3.8.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.7.14)\n",
            "Requirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.70.16)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18.1->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.1)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.2.1)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.8.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.23.0)\n",
            "Collecting setuptools (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.3.8)\n",
            "Downloading interpret-0.7.1-py3-none-any.whl (1.4 kB)\n",
            "Downloading interpret_core-0.7.1-py3-none-any.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aplr-10.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading gevent-25.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading salib-1.5.1-py3-none-any.whl (778 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.9/778.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.1-py3-none-any.whl (12 kB)\n",
            "Downloading zope_event-5.1.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: dash-cytoscape\n",
            "  Building wheel for dash-cytoscape (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-cytoscape: filename=dash_cytoscape-1.0.2-py3-none-any.whl size=4010717 sha256=97e38ca810edc348bb62dcd429dc7c30404a5b90420bb55a4210eccd4dd965ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/b1/ab/6c999ab288b4849d372e23c0a8f6ece7edb7ffeb8c97959ab0\n",
            "Successfully built dash-cytoscape\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, Werkzeug, setuptools, retrying, aplr, zope.interface, zope.event, Flask, SALib, interpret-core, gevent, dash, dash-cytoscape, interpret\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 3.1.1\n",
            "    Uninstalling Flask-3.1.1:\n",
            "      Successfully uninstalled Flask-3.1.1\n",
            "Successfully installed Flask-3.0.3 SALib-1.5.1 Werkzeug-3.0.6 aplr-10.9.0 dash-2.18.2 dash-core-components-2.0.0 dash-cytoscape-1.0.2 dash-html-components-2.0.0 dash-table-5.0.0 gevent-25.5.1 interpret-0.7.1 interpret-core-0.7.1 retrying-1.4.1 setuptools-80.9.0 zope.event-5.1.1 zope.interface-7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "34003bbd2f48468887caa423bbb19295"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boruta\n",
            "  Downloading Boruta-0.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from boruta) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (3.6.0)\n",
            "Downloading Boruta-0.4.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: boruta\n",
            "Successfully installed boruta-0.4.3\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.9.post2)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn>=1.6 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6->umap-learn) (3.6.0)\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "Paths used for configuration of notebook: \n",
            "    \t\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (80.9.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "Paths used for configuration of notebook: \n",
            "    \t\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\n",
            "Collecting optuna-integration[sklearn]\n",
            "  Downloading optuna_integration-4.4.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.4)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from optuna-integration[sklearn]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from optuna-integration[sklearn]) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from optuna-integration[sklearn]) (1.16.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->optuna-integration[sklearn]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->optuna-integration[sklearn]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->optuna-integration[sklearn]) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->optuna-integration[sklearn]) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->optuna-integration[sklearn]) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->optuna-integration[sklearn]) (1.17.0)\n",
            "Downloading optuna_integration-4.4.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: optuna-integration\n",
            "Successfully installed optuna-integration-4.4.0\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (25.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.14.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n",
            "Requirement already satisfied: interpret in /usr/local/lib/python3.11/dist-packages (0.7.1)\n",
            "Requirement already satisfied: interpret-core==0.7.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.5.1)\n",
            "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.9.5)\n",
            "Requirement already satisfied: ipykernel>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.17.1)\n",
            "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (7.34.0)\n",
            "Requirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.24.1)\n",
            "Requirement already satisfied: SALib>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.5.1)\n",
            "Requirement already satisfied: shap>=0.28.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.48.0)\n",
            "Requirement already satisfied: dill>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.3.8)\n",
            "Requirement already satisfied: aplr>=10.6.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (10.9.0)\n",
            "Requirement already satisfied: dash<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.18.2)\n",
            "Requirement already satisfied: dash-cytoscape>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.0.2)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (25.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.32.3)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.3)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.6)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.14.1)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.4.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (80.9.0)\n",
            "Requirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.1.1)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (7.2)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.1.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (25.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.7.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=3.8.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.7.14)\n",
            "Requirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.70.16)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18.1->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.1)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.2.1)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.8.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.23.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.3.8)\n",
            "Found existing installation: interpret 0.7.1\n",
            "Uninstalling interpret-0.7.1:\n",
            "  Successfully uninstalled interpret-0.7.1\n",
            "Found existing installation: interpret-core 0.7.1\n",
            "Uninstalling interpret-core-0.7.1:\n",
            "  Successfully uninstalled interpret-core-0.7.1\n",
            "\u001b[33mWARNING: Skipping interpret as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting interpret\n",
            "  Downloading interpret-0.7.1-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting interpret-core==0.7.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret)\n",
            "  Downloading interpret_core-0.7.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.5.1)\n",
            "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.9.5)\n",
            "Requirement already satisfied: ipykernel>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.17.1)\n",
            "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (7.34.0)\n",
            "Requirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.24.1)\n",
            "Requirement already satisfied: SALib>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.5.1)\n",
            "Requirement already satisfied: shap>=0.28.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.48.0)\n",
            "Requirement already satisfied: dill>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.3.8)\n",
            "Requirement already satisfied: aplr>=10.6.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (10.9.0)\n",
            "Requirement already satisfied: dash<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.18.2)\n",
            "Requirement already satisfied: dash-cytoscape>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.0.2)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (25.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.32.3)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.3)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.6)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.14.1)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.4.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (80.9.0)\n",
            "Requirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.1.1)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (7.2)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.1.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (25.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.7.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=3.8.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2025.7.14)\n",
            "Requirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.10.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.70.16)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18.1->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.1)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (8.2.1)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (5.8.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.2.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19.2->interpret-core==0.7.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (3.23.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.1->interpret) (4.3.8)\n",
            "Downloading interpret-0.7.1-py3-none-any.whl (1.4 kB)\n",
            "Downloading interpret_core-0.7.1-py3-none-any.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m201.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: interpret-core, interpret\n",
            "Successfully installed interpret-0.7.1 interpret-core-0.7.1\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40758 sha256=e3ca32b95ef75330a52225b36d8bd39da526fab0eeb31e139675a7990229d7ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "#CELDA DE INSTALACIONES - EJECUTAR ANTES QUE LA CELDA DEL PROGRAMA PMS\n",
        "!pip install ipywidgets\n",
        "!pip install keras-tuner scikit-learn openpyxl -q\n",
        "!pip install keras-tuner -q\n",
        "!pip install shap\n",
        "!pip install lime\n",
        "!pip install interpret\n",
        "!pip install shap lime xgboost -q\n",
        "!pip install boruta\n",
        "!pip install umap-learn\n",
        "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
        "!pip install ipywidgets openpyxl\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "!pip install --quiet ipywidgets openpyxl\n",
        "!pip install -q ipywidgets openpyxl\n",
        "!pip install optuna scikit-optimize --quiet\n",
        "!pip install optuna optuna-integration[sklearn]\n",
        "!pip install --upgrade scikeras\n",
        "!pip install -U interpret\n",
        "!pip uninstall -y interpret interpret-core\n",
        "!pip uninstall -y interpret\n",
        "!pip install --no-cache-dir interpret\n",
        "!pip install PyPDF2\n",
        "!pip install python-docx\n",
        "!pip install fpdf\n",
        "!pip install pypandoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4lPa1MnT-fiB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239,
          "referenced_widgets": [
            "7fc0afe58bdf453b9fc9a8d253d9b15e",
            "2876e80d1e6443f294166860dc38ac7f",
            "5842eef7f7f44ac2a75bbdb06d2d7587",
            "edd237a2675c4e02b880078db1cf977a",
            "a7a278021a7c487c8439839997017fd1",
            "c2e2646a3891471b8807094aac79a861",
            "c8a60fd4d33740afbffc2aa693697f85",
            "8f430a6c173b4747a76e9507b7de9304",
            "8c83ecff325b4735bf4fc5ff385b4e81",
            "670dc6aeae984c3b86a690f03651ad34",
            "f829dc46033642c9adfc93e550750301",
            "4276874e205f4773a23b128ecf1d0251",
            "919e3eff833541f9ae00a2c385b32a0e",
            "f3eb0a55f8044831aa45e116d4a6730e",
            "0f5817d01b0f40318504805491728c5e",
            "98241a67dd0e4d9592eff31baf72ef30",
            "3cbc1394f08e4127ab3393b7165417fd",
            "92f7f7b60ab3406a8455a1ca982c88c1",
            "0fee1b1861c8442ebb2243e5de847f2b",
            "ba296edc19694bd195af5396c36787ae",
            "33c1a4964ec045218f32535f0ce9968b",
            "c61e73ba88ba426cbbed8cfed13269be",
            "62694866e04b47928a2722c9b29bc07e",
            "2c8478edfaf94d7f912022b6d1185ad8",
            "b7cd40e9f8284f459d592efde209ec53",
            "593a5f8c069e4061995b8a0fbeddc33a",
            "092e670f9b834b239aa0bf5ef521cb53",
            "3222d4fb5c1e4138a594cf9edbaa0748",
            "d409974836ee4c2a965e721ad347c460",
            "de1a11fa427646d3becc2e8c99ab2812",
            "847b518cb96948009cb97e0ae3681e4b",
            "6d6a5470f567411289ce5def9c347abd",
            "51dbdea738db4b688df8d4d2971babfc",
            "e928878511c742c2989a9e888e99797f",
            "b6fceaefb91545068c9be746981e9cf7",
            "f6c793e725154a2298631adf6d807ce7",
            "a7332fe1349248c0bb14222040716e88",
            "2afeee701b564b6eaec77ebe5c06e34d",
            "24d2633945b14ff0932130d6dc6b5171",
            "8bed4efd79e543049a04c1df3c72a50f",
            "6e91c4f5807241a09441a1f7c1b1bf84",
            "9bac08d2ef8849fbbba1549429f404a0",
            "494affada79849c4a974652fe7a070c6",
            "9584861d1474450da2aac186b1a4481c",
            "04784ca9825a4257a788a76835bd7493",
            "32d0722e22284d2999579e244303881e",
            "b4465fd38a444c68b0f67fe7ab67457a",
            "dc3686da512447a6ac66663c11e39abf",
            "2786b5a4367e4213a765e3963b277f51",
            "097e842afc9e4bc8bab29ca22ba6c07b",
            "4ef6c8c5c1c64d25b3252f9f509bf7a3",
            "f006cb096af84ce5b75adbd83043108f",
            "5ba19ca33a174db68883aa8712d71578",
            "d22b5d395913418aa608ea277ceafa3a",
            "9e4ab1aaac1e44e290d27f59d9be0bc2",
            "b87174a312154bb0b21f1fa9b4a10ca3",
            "0b83efb382274762948ec1b1c2b21c7f",
            "2ea64f2fbd8644dea97dd6ee08fe22bb",
            "1157881e56804c99a5f1f8ef1b4d1850",
            "d26b4c62ddb04a9bb824b9ce2ce306ce",
            "71d421482b4a4860b56eb229f708eb33",
            "e5f7f55cd2414b5883aed40094410e01",
            "adcf3930810b4875ac661551e21bc852",
            "e5106a7f20a845479749080ee3f6ffa4",
            "c15b5a7938b345e683e0c62506fa87b5",
            "ce88efe7cbd04ce28e42da378097baf5",
            "e82419e2f78c4765a900e8f7a2ec581f",
            "c35f63e727cf43ce9472b8dde6be478d",
            "afd56917323c48ddbd8659d5a5153fd6",
            "d4d3da4f4034427f82d2a7c1a14c6fdd",
            "21f23b86578b43029711ab0c738bc1da",
            "86c337421d204c7dbd1e7b5d3ea6b6ea",
            "f1fcb04c6cd643b3b68973c983929a4e",
            "70e1c0808ecd4e418817eba1b2ffc1dd",
            "98915d7e2be14836a5e833ee30db1402",
            "292cdd96db474a32b72c0d7d19865225",
            "1051c382444545a499c6c163113d973e",
            "6b6a1b2ed63f417a8712d79c54ba09e0",
            "36102fdd4844493b8c06ce22f319fac2",
            "946d76c552b448279f2c0f2a2f372387",
            "584346a65568423da688e3063e5fb840",
            "cdfca94dd0254e92a87f2a0ba02b706b",
            "f03d3c0e8f3a47c5a836b9ecd1f16bd1",
            "45661577f69949c0a69dac54bd7383cf",
            "b87506a2d19943709a4a31d60f13feda",
            "81f71060f06a467f938037fdd945a03b",
            "0c513d02f1914314a848683355ef1c6d",
            "8cabff7d374b4ad7b4467c8baa6b23c8",
            "d9c0b3a5334f40508989952c82e744fb",
            "5db0ca6dc6d64f2b9461c2fa7ac88c36",
            "7ffe7c666e02400f82a48375ecc0109c",
            "1a141e8ccd144bf7bbd16a39863e4064",
            "ccbaf9630780425ea9e222053bd354c6",
            "385259253b7f4fd0aa376c07a5c7969a",
            "d16186e75a0c4b4fb69fa013c0f42eb9",
            "1104c93b8cc346f5ab9173529259151e",
            "106bd52209224d9d9ec6d5f8dcf96772",
            "4f06057bd64346f98593a9a0623cc66f",
            "a41d2abb301e48ba8df8da670e53e977",
            "2ac66f097ee84257bd60ca0914f7bffe",
            "7da971c7dd484944bc560e4506a42bcd",
            "6076b3b8c10940ce996f1b2c7b67f73f",
            "602fc543709348fba1b4f5099c9fa036",
            "37176ba8f8124ad6b29ac35c461306e3",
            "717ab25606834e3397916968e02b4ab8",
            "9e2d9c45ecc94212be84ffbea37d7831",
            "0646db9a4ab042c483e275c931196059",
            "32005eb156cd43fe9459d2f966a2210b",
            "1f0a26b01f2e4e9fa4b7dc7eebce1f57",
            "2225a8db84734179bcb20dc56c0a61d2",
            "93468cc808a24690be54fbf39b707f60",
            "7d7dae87ed09499f9d6b270d65c64fbf",
            "bd838c8b1b7b4a2682d27ee36a83e838",
            "ae9a0274cbcd4e8388a3793d2cf59ce0",
            "856dbf2c27954e1bb3f243c6611c46bc",
            "6dda1545ab3b4f2e9a2ee2df2bed6f9a",
            "82e8bd61836e4bd0b0cf941a31838c42",
            "22ba85f34e1841379299bcd675bedfd3",
            "9cd63676a2f84d8a883b0261c17ada56"
          ]
        },
        "outputId": "2b04b260-1fd0-440b-a4fe-0f2f9263488a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fc0afe58bdf453b9fc9a8d253d9b15e"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edd237a2675c4e02b880078db1cf977a"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2e2646a3891471b8807094aac79a861"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f430a6c173b4747a76e9507b7de9304"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "670dc6aeae984c3b86a690f03651ad34"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4276874e205f4773a23b128ecf1d0251"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3eb0a55f8044831aa45e116d4a6730e"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98241a67dd0e4d9592eff31baf72ef30"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce88efe7cbd04ce28e42da378097baf5"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c35f63e727cf43ce9472b8dde6be478d"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "/* Ancho y tipografía de los widgets */\n",
              ".widget-dropdown, .widget-select-multiple, .widget-button {\n",
              "  width: 400px !important;       /* cajas más anchas */\n",
              "  font-size: 14px !important;    /* texto de 14px */\n",
              "}\n",
              ".widget-dropdown > label, .widget-select-multiple > label {\n",
              "  font-size: 14px !important;    /* etiquetas también grandes */\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value=\"<h3 style='font-size:1.3rem;margin-bottom:5px;'>📋 Menú Principal</h3>\"), VBox(child…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cabff7d374b4ad7b4467c8baa6b23c8"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='warning', description='🧹 Limpiar pantalla', layout=Layout(width='150px'), style=ButtonSty…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a141e8ccd144bf7bbd16a39863e4064"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================================\n",
        "# PROGRAMA COMPLETO PMS\n",
        "# Programa para el entrenamiento, comparación y optimización de modelos matemáticos para llevar a cabo el Gemelo Digital del Proceso de Fabricación de Tableros MDF\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# 0. INSTALACION DE COMPONENTES PYTHON NECESARIOS PARA LA EJECUCIÓN DEL PROGRAMA\n",
        "# ============================================================================\n",
        "#Antes de ejecutar el programa asegurar que están instaladas las siguientes librerias:\n",
        "#!pip install ipywidgets\n",
        "#!pip install keras-tuner scikit-learn openpyxl -q\n",
        "#!pip install keras-tuner -q\n",
        "#!pip install shap\n",
        "#!pip install lime\n",
        "#!pip install interpret\n",
        "#!pip install shap lime xgboost -q\n",
        "#!pip install boruta\n",
        "#!pip install umap-learn\n",
        "#!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
        "#!pip install ipywidgets openpyxl\n",
        "#!jupyter nbextension enable --py widgetsnbextension\n",
        "#!pip install --quiet ipywidgets openpyxl\n",
        "#!pip install optuna scikit-optimize --quiet\n",
        "#!pip install optuna optuna-integration[sklearn]\n",
        "#!pip install scikeras\n",
        "#!pip install --upgrade scikeras\n",
        "#!pip install -U interpret\n",
        "#!pip install PyPDF2\n",
        "#!pip install python-docx\n",
        "#!pip install fpdf\n",
        "#!pip install pypandoc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 1. LIBRERÍAS\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle, os\n",
        "from io import StringIO\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from keras_tuner import BayesianOptimization\n",
        "from sklearn.model_selection import KFold\n",
        "import pickle, os\n",
        "import shutil\n",
        "import time\n",
        "import threading\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import keras_tuner as kt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "import threading\n",
        "import pickle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Ftrl\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import time\n",
        "import random\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost.callback import EarlyStopping as XgbEarlyStopping\n",
        "import pkgutil\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "import threading, time, pickle, os\n",
        "from ipywidgets import Output\n",
        "import io\n",
        "from google.colab import output\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "import re\n",
        "\n",
        "def sanitize_name(s):\n",
        "    \"\"\"\n",
        "    Unifica la sanitización de cualquier string de columna:\n",
        "    Reemplaza espacios, puntos, comas, punto y coma, dos puntos,\n",
        "    barras (/ \\\\), paréntesis (), corchetes [], llaves {},\n",
        "    signo %, +, -, *, &, ^, $, #, @, !, ?, =, <, >, |, `, ~\n",
        "    por guión bajo y colapsa múltiples guiones bajos.\n",
        "    \"\"\"\n",
        "    # Reemplaza todo carácter no alfanumérico o guión bajo por '_'\n",
        "    t = re.sub(r\"[^\\w]\", \"_\", str(s))\n",
        "    # Colapsa múltiples guiones bajos consecutivos\n",
        "    t = re.sub(r\"_+\", \"_\", t)\n",
        "    return t.strip(\"_\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 2. VARIABLES DE SALIDA\n",
        "# ===============================================================\n",
        "out_carga = widgets.Output()\n",
        "out_svr = widgets.Output()\n",
        "out_nn = widgets.Output()\n",
        "out_xgb = widgets.Output()\n",
        "out_pred = widgets.Output()\n",
        "out_graf = widgets.Output()\n",
        "out_xai = widgets.Output()\n",
        "out_bienvenida = widgets.Output()\n",
        "out_ayuda = widgets.Output()\n",
        "out_nn_opt = widgets.Output()\n",
        "stop_flag = threading.Event()\n",
        "stop_flag_nn = threading.Event()\n",
        "out_svr_opt = widgets.Output()\n",
        "stop_flag_svr = threading.Event()\n",
        "out_xgb_opt = widgets.Output()\n",
        "stop_flag_xgb = threading.Event()\n",
        "out_rf = widgets.Output()\n",
        "rf_timer_stop_event = threading.Event()\n",
        "\n",
        "if 'out_rnn' not in globals():\n",
        "    out_rnn = widgets.Output()\n",
        "\n",
        "ayuda_visible = [False]  # Flag global para mostrar/ocultar ayuda\n",
        "\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 3. AYUDA GLOBAL\n",
        "# Este módulo sirve para proporcionar al usuario del programa toda la ayuda que podria necesitar para entender el funcionamiento del programa y sus diferentes parámetros.\n",
        "# Crea un “Centro de Ayuda” todo-en-uno:\n",
        "# • Un Accordion con 4 paneles (uno por Bloque)\n",
        "# • Cada panel contiene HTML muy detallado\n",
        "# • Se añaden imágenes/figuras ilustrativas (base-64 embebidas)\n",
        "# • Incluye un botón flotante “¿Necesitas ayuda?” para abrir/cerrar\n",
        "# ===============================================================\n",
        "def mostrar_ayuda_completa() -> None:\n",
        "    \"\"\"\n",
        "    Despliega la ayuda global, dividida en 4 bloques:\n",
        "\n",
        "    Bloque 1 → Carga, Segmentación y Selección de Variables\n",
        "    Bloque 2 → Entrenamiento y Visualización de Modelos\n",
        "    Bloque 3 → Optimización Multicapa (SVR, NN, XGB, RF, RNN)\n",
        "    Bloque 4 → Interpretabilidad xIA, Navegación y Menú Principal\n",
        "\n",
        "    Cada bloque se explica en profundidad con texto, tablas e\n",
        "    imágenes incrustadas (generadas al vuelo).  El usuario puede\n",
        "    cambiar de bloque pulsando los botones de navegación.\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    #  util_img_blocks.py  ·  4 generadores de imagen\n",
        "    # -----------------------------------------------\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.patches import FancyArrowPatch, Rectangle\n",
        "    import numpy as np, io, base64, textwrap, itertools, random\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "\n",
        "    # ──────────────────────────────────────────\n",
        "    def _fig_to_b64(fig) -> str:\n",
        "        \"Convierte una figura matplotlib en cadena base64\"\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format=\"png\", bbox_inches=\"tight\", dpi=140)\n",
        "        plt.close(fig); buf.seek(0)\n",
        "        return base64.b64encode(buf.read()).decode()\n",
        "\n",
        "    # ──────────────────────────────────────────\n",
        "    # BLOQUE 1  ·  Tubo de carga → split → selección\n",
        "    def _img_block1() -> str:\n",
        "        fig, ax = plt.subplots(figsize=(6, 2))\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        # 1) CSV/Excel\n",
        "        ax.add_patch(Rectangle((0.05, .25), .18, .5, fc=\"#f0f8ff\", ec=\"#4682b4\", lw=1.5))\n",
        "        ax.text(.14, .5, \"Excel/CSV\", ha=\"center\", va=\"center\", weight=\"bold\")\n",
        "\n",
        "        # 2) DataFrame\n",
        "        ax.add_patch(Rectangle((.30, .25), .18, .5, fc=\"#e6ffe6\", ec=\"#2e8b57\", lw=1.5))\n",
        "        ax.text(.39, .5, \"DataFrame\\n(pandas)\", ha=\"center\", va=\"center\")\n",
        "\n",
        "        # 3) Train/Test\n",
        "        ax.add_patch(Rectangle((.55, .45), .18, .3, fc=\"#fff4e6\", ec=\"#ff8c00\", lw=1.5))\n",
        "        ax.text(.64, .60, \"Train\", ha=\"center\", va=\"center\", size=8)\n",
        "        ax.add_patch(Rectangle((.55, .25), .18, .15, fc=\"#ffe6e6\", ec=\"#d80027\", lw=1.5))\n",
        "        ax.text(.64, .325, \"Test\", ha=\"center\", va=\"center\", size=8)\n",
        "\n",
        "        # 4) Selección de variables (nodos pequeños)\n",
        "        methods = [\"Pearson\", \"Mutual\\nInfo\", \"Boruta\", \"UMAP\"]\n",
        "        for i, m in enumerate(methods):\n",
        "            x = .82; y = .6 - i*0.15\n",
        "            ax.add_patch(Rectangle((x, y), .13, .1, fc=\"#fafafa\", ec=\"#555\", lw=1))\n",
        "            ax.text(x+.065, y+.05, m, ha=\"center\", va=\"center\", size=7)\n",
        "\n",
        "        # Flechas\n",
        "        def arrow(xy1, xy2):\n",
        "            ax.add_patch(FancyArrowPatch(xy1, xy2, arrowstyle=\"->\", lw=1, color=\"#444\"))\n",
        "        arrow((.23, .5), (.30, .5))\n",
        "        arrow((.48, .5), (.55, .5))\n",
        "        arrow((.73, .5), (.82, .55))\n",
        "        arrow((.73, .5), (.82, .4))\n",
        "        arrow((.73, .5), (.82, .25))\n",
        "        fig.suptitle(\"Pipeline Bloque 1\", fontweight=\"bold\")\n",
        "        return _fig_to_b64(fig)\n",
        "\n",
        "    # ──────────────────────────────────────────\n",
        "    # BLOQUE 2  ·  Comparativa de desempeño de modelos\n",
        "    def _img_block2() -> str:\n",
        "        models = [\"SVR\", \"NN\", \"XGB\", \"RF\", \"RNN\"]\n",
        "        scores = [0.82, 0.88, 0.91, 0.86, 0.84]  # ejemplo R²\n",
        "        fig, ax = plt.subplots(figsize=(5, 3))\n",
        "        bars = ax.bar(models, scores, color=\"#4c9be8\")\n",
        "        ax.set_ylim(0, 1.0)\n",
        "        ax.set_ylabel(\"R² en Test\")\n",
        "        ax.set_title(\"Rendimiento modelos (ejemplo)\")\n",
        "        for b, s in zip(bars, scores):\n",
        "            ax.text(b.get_x() + b.get_width()/2, s+0.02, f\"{s:.2f}\", ha=\"center\", va=\"bottom\", size=8)\n",
        "        return _fig_to_b64(fig)\n",
        "\n",
        "    # ──────────────────────────────────────────\n",
        "    # BLOQUE 3  ·  Convergencia de búsqueda HPO\n",
        "    def _img_block3() -> str:\n",
        "        fig, ax = plt.subplots(figsize=(5.5, 3))\n",
        "        n_iter = 30\n",
        "        # curva “score” ficticia para 3 técnicas\n",
        "        rng = np.random.RandomState(0)\n",
        "        for label, c in zip((\"GridSearch\", \"BayesSearch\", \"Optuna\"), (\"#999\", \"#2e8b57\", \"#d62728\")):\n",
        "            best_so_far = np.maximum.accumulate(rng.uniform(.5, .9, n_iter))\n",
        "            ax.plot(range(1, n_iter+1), best_so_far, label=label, lw=1.8, color=c)\n",
        "        ax.set_xlabel(\"Iteración\")\n",
        "        ax.set_ylabel(\"Score acumulado (R²)\")\n",
        "        ax.set_title(\"Evolución búsqueda de hiperparámetros\")\n",
        "        ax.legend(frameon=False, fontsize=8)\n",
        "        return _fig_to_b64(fig)\n",
        "\n",
        "    # ──────────────────────────────────────────\n",
        "    # BLOQUE 4  ·  Mini-summary de interpretabilidad (SHAP simulado)\n",
        "    def _img_block4() -> str:\n",
        "        feats = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"]\n",
        "        shap_vals = np.array([0.4, -0.35, 0.25, -0.15, 0.05])\n",
        "        colors = ['#d62728' if v<0 else '#2ca02c' for v in shap_vals]\n",
        "        fig, ax = plt.subplots(figsize=(4.5, 3))\n",
        "        ax.barh(feats, shap_vals, color=colors)\n",
        "        ax.axvline(0, color=\"#444\", lw=0.8)\n",
        "        ax.set_xlabel(\"Valor SHAP (impacto en la predicción)\")\n",
        "        ax.set_title(\"Contribución de variables (ejemplo)\")\n",
        "        plt.tight_layout()\n",
        "        return _fig_to_b64(fig)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # 1️⃣  TEXTOS DE AYUDA  (puedes ampliarlos todo lo que quieras)\n",
        "    #     — Cada bloque es un gran HTML con títulos, listas,\n",
        "    #       tablas <table>, imágenes <img>…\n",
        "    # ----------------------------------------------------------\n",
        "    bloque1_html = HTML(f\"\"\"\n",
        "    <h3 style='color:#2E8B57;'>🟢 Bloque 1 – Carga, segmentación y selección de variables</h3>\n",
        "    <p><b>Objetivo:</b> transformar hojas Excel o .csv en matrices <code>X</code> (predictoras) y\n",
        "    <code>Y</code> (objetivo).</p>\n",
        "\n",
        "    <h4>📥 Carga de datos</h4>\n",
        "    <ul>\n",
        "      <li>Importación directa desde área de transferencia (<i>copy–paste</i> de Excel) o desde fichero.</li>\n",
        "      <li>Validación de encabezados, tipos y <code>NaN</code>.</li>\n",
        "      <li>Ejemplo rápido:<br>\n",
        "      <code>X, Y = cargar_desde_clipboard(sep='\\\\t')</code></li>\n",
        "    </ul>\n",
        "\n",
        "    <h4>🧩 Segmentación Train/Test</h4>\n",
        "    <ul>\n",
        "      <li>División estratificada opcional (<i>stratify=Y</i> cuando <code>Y</code> es discreta).</li>\n",
        "      <li>Seed reproducible (<code>random_state=42</code>).</li>\n",
        "      <li>Visualización: tabla de tamaños y gráfico “barra apilada” para comparar distribución de\n",
        "          objetivos.</li>\n",
        "    </ul>\n",
        "\n",
        "    <h4>🔍 Selección de variables <i>X</i></h4>\n",
        "    <table>\n",
        "    <thead><tr><th>Método</th><th>Descripción resumida</th><th>Métrica interna</th></tr></thead>\n",
        "    <tbody>\n",
        "    <tr><td>Pearson / Spearman</td><td>Descarta colinealidad lineal / monótona</td><td>|ρ| &gt; τ</td></tr>\n",
        "    <tr><td>Mutual Info</td><td>Información mutua no-lineal</td><td>MI &gt; τ</td></tr>\n",
        "    <tr><td>Boruta</td><td>Selección envolvente basada en RF</td><td>Importancia &gt; shadow</td></tr>\n",
        "    <tr><td>UMAP</td><td>Reducción de dimensión no lineal (embedding)</td><td>Varianza retenida</td></tr>\n",
        "    </tbody></table>\n",
        "\n",
        "    <p><i>Resultado:</i> diccionario <code>RESUMEN_METODOS</code> con el subconjunto de columnas\n",
        "    aprobado por cada técnica.</p>\n",
        "    <<img src=\"data:image/png;base64,{_img_block1()}\"  ></td></tr>>\n",
        "    \"\"\")\n",
        "\n",
        "    bloque2_html = HTML(f\"\"\"\n",
        "    <h3 style='color:#1E90FF;'>🔵 Bloque 2 – Entrenamiento y visualización de modelos</h3>\n",
        "    <p>Incluye:</p>\n",
        "    <ul>\n",
        "      <li><b>SVR</b> (kernels lineal / RBF)</li>\n",
        "      <li><b>Red Neuronal densa</b> (Keras/TensorFlow)</li>\n",
        "      <li><b>XGBoost</b> (regresión)</li>\n",
        "      <li><b>Random Forest</b> (sklearn)</li>\n",
        "      <li><b>RNN</b> / LSTM para series temporales</li>\n",
        "    </ul>\n",
        "\n",
        "    <h4>Flujo de trabajo general</h4>\n",
        "    <ol>\n",
        "      <li>Escalado (<code>StandardScaler</code>) de <code>X</code> y <code>Y</code>.</li>\n",
        "      <li>Entrenamiento con <code>X_train</code>, evaluación con <code>X_test</code>.</li>\n",
        "      <li>Métricas trazadas: R², MSE, RMSE, MAE, MedAE.</li>\n",
        "      <li>Gráficos:\n",
        "        <ul>\n",
        "            <li>Y real vs Y predicho (scatter y línea)</li>\n",
        "            <li>Residuos: histograma + Q–Q + <i>residual vs fitted</i></li>\n",
        "        </ul>\n",
        "      </li>\n",
        "    </ol>\n",
        "\n",
        "    <h4>Ejemplo mínimo – SVR RBF</h4>\n",
        "    <pre>\n",
        "    svr = SVR(kernel='rbf', C=10, epsilon=0.1, gamma='scale')\n",
        "    svr.fit(X_train_scaled, y_train_scaled)\n",
        "    pred = scaler_y.inverse_transform(svr.predict(X_test_scaled).reshape(-1,1))\n",
        "    </pre>\n",
        "    <img src=\"data:image/png;base64,{_img_block2()}\"  ></td></tr>\n",
        "    \"\"\")\n",
        "\n",
        "    bloque3_html = HTML(f\"\"\"\n",
        "    <h3 style='color:#FFA500;'>🟠 Bloque 3 – Optimización multicapa (HPO)</h3>\n",
        "    <p>Se soportan cinco motores por tipo de modelo:</p>\n",
        "    <ul>\n",
        "      <li><b>GridSearchCV</b></li>\n",
        "      <li><b>RandomizedSearchCV</b></li>\n",
        "      <li><b>BayesSearchCV</b> (scikit-optimize)</li>\n",
        "      <li><b>Optuna</b></li>\n",
        "      <li><b>Hyperband / HalvingSearchCV</b></li>\n",
        "    </ul>\n",
        "\n",
        "    <table>\n",
        "    <thead><tr><th>Modelo</th><th>Espacio de búsqueda ⊂ ℝⁿ</th><th>Trials por defecto</th></tr></thead>\n",
        "    <tbody>\n",
        "    <tr><td>SVR</td><td>C, ε, kernel</td><td>30</td></tr>\n",
        "    <tr><td>Neural Net</td><td>#capas, neuronas, LR, dropout, ℓ₂</td><td>50</td></tr>\n",
        "    <tr><td>XGBoost</td><td>depth, lr, n_estim, γ, subsample…</td><td>100</td></tr>\n",
        "    <tr><td>Random Forest</td><td>n_estim, depth, mtry, bootstrap…</td><td>70</td></tr>\n",
        "    <tr><td>RNN</td><td>units, batch, epochs, LR</td><td>50</td></tr>\n",
        "    </tbody></table>\n",
        "\n",
        "    <p>Cada ejecución devuelve:</p>\n",
        "    <ul>\n",
        "      <li>TOP-5 configuraciones (tabla ordenada)</li>\n",
        "      <li>Mejor curva de predicción y residuos</li>\n",
        "      <li>Heat-map de métricas normalizadas + Radar chart</li>\n",
        "    </ul>\n",
        "    <img src=\"data:image/png;base64,{_img_block3()}\"  ></td></tr>\n",
        "    \"\"\")\n",
        "\n",
        "    bloque4_html = HTML(f\"\"\"\n",
        "    <h3 style='color:#8A2BE2;'>🟣 Bloque 4 – Interpretabilidad xIA &amp; Navegación</h3>\n",
        "    <p>El módulo integra <b>14</b> técnicas:</p>\n",
        "    <ol>\n",
        "      <li>SHAP (Tree / Kernel / Deep)</li>\n",
        "      <li>LIME (tabular)</li>\n",
        "      <li>KernelExplainer (SHAP caja negra)</li>\n",
        "      <li>Integrated Gradients</li>\n",
        "      <li>DeepLIFT / LRP</li>\n",
        "      <li>Permutation Feature Importance</li>\n",
        "      <li>Partial Dependence Plots (PDP)</li>\n",
        "      <li>Accumulated Local Effects (ALE)</li>\n",
        "      <li>ICE plots</li>\n",
        "      <li>Counterfactual Explainer</li>\n",
        "      <li>Anchors (árboles locales)</li>\n",
        "      <li>Modelos sustitutos (árbol global + líneas locales)</li>\n",
        "      <li>Explainable Boosting Machine (EBM)</li>\n",
        "      <li>Optuna Hyper-parameter Importance</li>\n",
        "    </ol>\n",
        "\n",
        "    <p>Para cada técnica se generan:</p>\n",
        "    <ul>\n",
        "      <li>Gráfico principal (summary, barras, scatter…)</li>\n",
        "      <li>Tabla local (primeras 10 muestras)</li>\n",
        "      <li>Tabla de importancia global</li>\n",
        "      <li>Bloque de interpretación con <i>tips</i> y lectura guiada</li>\n",
        "    </ul>\n",
        "    <img src=\"data:image/png;base64,{_img_block4()}\"  ></td></tr>\n",
        "    <p><i>Consejo:</i> combina SHAP + PDP + Permutation para tener una visión 360°: explicaciones locales,\n",
        "    efecto medio y robustez de cada variable.</p>\n",
        "    \"\"\")\n",
        "\n",
        "    # ───────────────────────────────────────────────────────────\n",
        "    # Botones + enlace a callback\n",
        "    # ————————————————————————————————————————\n",
        "    # ----------------------------------------------------------\n",
        "    # 2️⃣  ÁREA DE SALIDA (visible a todas las funciones)\n",
        "    # ----------------------------------------------------------\n",
        "    _help_output = widgets.Output()\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # 3️⃣  RENDER FUNCTIONS (deben existir ANTES de .on_click)\n",
        "    # ----------------------------------------------------------\n",
        "    def _render_bloque_1(*_):\n",
        "        _help_output.clear_output()\n",
        "        with _help_output:\n",
        "            display(bloque1_html)\n",
        "\n",
        "    def _render_bloque_2(*_):\n",
        "        _help_output.clear_output()\n",
        "        with _help_output:\n",
        "            display(bloque2_html)\n",
        "\n",
        "    def _render_bloque_3(*_):\n",
        "        _help_output.clear_output()\n",
        "        with _help_output:\n",
        "            display(bloque3_html)\n",
        "\n",
        "    def _render_bloque_4(*_):\n",
        "        _help_output.clear_output()\n",
        "        with _help_output:\n",
        "            display(bloque4_html)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # 4️⃣  BOTONES  (creados DESPUÉS de las funciones)\n",
        "    # ----------------------------------------------------------\n",
        "    btn_b1 = widgets.Button(description=\"ℹ️ Bloque 1 – DATOS\",           button_style='info')\n",
        "    btn_b2 = widgets.Button(description=\"ℹ️ Bloque 2 – ENTRENAMIENTO\",   button_style='info')\n",
        "    btn_b3 = widgets.Button(description=\"ℹ️ Bloque 3 – OPTIMIZACIÓN\",    button_style='info')\n",
        "    btn_b4 = widgets.Button(description=\"ℹ️ Bloque 4 – INTERPRETABILIDAD\", button_style='info')\n",
        "\n",
        "    # Enlazar callbacks  (ya no produce NameError)\n",
        "    btn_b1.on_click(_render_bloque_1)\n",
        "    btn_b2.on_click(_render_bloque_2)\n",
        "    btn_b3.on_click(_render_bloque_3)\n",
        "    btn_b4.on_click(_render_bloque_4)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # 5️⃣  INTERFAZ – se muestra la cabecera + botones + área\n",
        "    # ----------------------------------------------------------\n",
        "    display(\n",
        "        widgets.VBox([\n",
        "            widgets.HTML(\"<h2 style='color:#1E90FF;'>📘 AYUDA GLOBAL — Guía Completa de la Aplicación</h2>\"),\n",
        "            widgets.HTML(\"<p>Selecciona el bloque sobre el que necesitas información detallada.</p>\"),\n",
        "            widgets.HBox([btn_b1, btn_b2, btn_b3, btn_b4]),\n",
        "            _help_output\n",
        "        ])\n",
        "    )\n",
        "    #btn.on_click(mostrar_ayuda_completa)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 4. BIENVENIDA\n",
        "# Este módulo sirve como apoyo al usuario en la entrada al programa proporcionando una descripción bastante detallada de todo lo que hace el programa.\n",
        "# ===============================================================\n",
        "import io, base64, numpy as np, matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# ─────────────────────────────\n",
        "# UTILIDADES DE IMAGEN EMBEBIDA\n",
        "# ─────────────────────────────\n",
        "def _fig_to_b64(fig):\n",
        "    \"\"\"Devuelve la figura matplotlib codificada en base-64 (PNG).\"\"\"\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\", dpi=130)\n",
        "    plt.close(fig)\n",
        "    buf.seek(0)\n",
        "    return base64.b64encode(buf.read()).decode()\n",
        "\n",
        "# ⬇️  Mini-diagrama Bloque 2: ENTRENAMIENTO → VALIDACIÓN → PREDICCIÓN\n",
        "def _block2_img_training_prediction():\n",
        "    fig, ax = plt.subplots(figsize=(6.2, 1.0))\n",
        "    ax.axis(\"off\")\n",
        "    boxes = [\n",
        "        (\"ENTRENAMIENTO\", \"#e6ffe6\", \"#2e8b57\"),\n",
        "        (\"VALIDACIÓN\",    \"#fffbd5\", \"#ff8c00\"),\n",
        "        (\"PREDICCIÓN\",    \"#d6e0ff\", \"#4169e1\")\n",
        "    ]\n",
        "    xs = [.02, .36, .70]\n",
        "    for x, (txt, fc, ec) in zip(xs, boxes):\n",
        "        ax.annotate(\n",
        "            txt, (x, .5), ha=\"left\", va=\"center\",\n",
        "            bbox=dict(fc=fc, ec=ec, lw=1.4, boxstyle=\"round,pad=0.28\"))\n",
        "    for x in [.28, .62]:\n",
        "        ax.annotate(\n",
        "            \"\", xy=(x+.03, .5), xytext=(x-.03,.5),\n",
        "            arrowprops=dict(arrowstyle=\"->\", lw=1.5))\n",
        "    ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
        "    return _fig_to_b64(fig)\n",
        "\n",
        "# ⬇️  Flujos “Entrenar”  -vs-  “Optimizar”  -vs-  “Explicar”\n",
        "def _img_flow_trainpred():\n",
        "    fig, ax = plt.subplots(figsize=(4.5, .9)); ax.axis(\"off\")\n",
        "    ax.annotate(\"Bloque 1\\nCarga, \\nSegmentacion y \\nSelección\", (.05,.5), va=\"center\",\n",
        "                bbox=dict(fc=\"#f0f8ff\", ec=\"#4682b4\"))\n",
        "    ax.annotate(\"Bloque 2\\nEntrenamiento, \\nPredicción y \\nComparación IA\", (.55,.5), va=\"center\",\n",
        "                bbox=dict(fc=\"#e6ffe6\", ec=\"#2e8b57\"))\n",
        "    ax.annotate(\"\", xy=(.43,.5), xytext=(.33,.5),\n",
        "                arrowprops=dict(arrowstyle=\"->\", lw=1.6))\n",
        "    ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
        "    return _fig_to_b64(fig)\n",
        "\n",
        "def _img_flow_opt():\n",
        "    fig, ax = plt.subplots(figsize=(5.2, .9)); ax.axis(\"off\")\n",
        "    ax.annotate(\"Bloque 1\\nCarga, \\nSegmentacion y \\nSelección\", (.05,.5), va=\"center\",\n",
        "                bbox=dict(fc=\"#f0f8ff\", ec=\"#4682b4\"))\n",
        "    ax.annotate(\"Bloque 3\\nOptimización \\nModelos \\nde IA\", (.55,.5), va=\"center\",\n",
        "                bbox=dict(fc=\"#fff4e6\", ec=\"#ff8c00\"))\n",
        "    ax.annotate(\"\", xy=(.43,.5), xytext=(.33,.5),\n",
        "                arrowprops=dict(arrowstyle=\"->\", lw=1.6))\n",
        "    ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
        "    return _fig_to_b64(fig)\n",
        "\n",
        "def _img_flow_xai():\n",
        "    fig, ax = plt.subplots(figsize=(6.8, .9)); ax.axis(\"off\")\n",
        "    xs = [.03,.29,.55,.80]\n",
        "    labels  = [\"Bloque 1\\nCarga, \\nSegmentacion y \\nSelección\",\"Bloque 2\\nEntrenamiento, \\nPredicción y \\nComparación IA\",\"Bloque 3\\nOptimización \\nModelos \\nde IA\",\"Bloque 4\\nExplicación \\nModelos \\nde IA\"]\n",
        "    fcs     = [\"#f0f8ff\",\"#e6ffe6\",\"#fff4e6\",\"#fde2ff\"]\n",
        "    frames  = [\"#4682b4\",\"#2e8b57\",\"#ff8c00\",\"#ba55d3\"]\n",
        "    for x,l,fc,ec in zip(xs, labels, fcs, frames):\n",
        "        ax.annotate(l, (x,.5), va=\"center\",\n",
        "                    bbox=dict(fc=fc, ec=ec))\n",
        "    for x in [.21,.47,.72]:\n",
        "        ax.annotate(\"\", xy=(x+.05,.5), xytext=(x-.05,.5),\n",
        "                    arrowprops=dict(arrowstyle=\"->\", lw=1.6))\n",
        "    ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
        "    return _fig_to_b64(fig)\n",
        "\n",
        "# ─────────────────────────────\n",
        "#  FUNCIÓN PRINCIPAL DE CELDA\n",
        "# ─────────────────────────────\n",
        "def mostrar_bienvenida():\n",
        "    \"\"\"Pantalla de bienvenida profesional con itinerarios y accesos rápidos.\"\"\"\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    html_intro = HTML(f\"\"\"\n",
        "    <style>\n",
        "      .pms h1   {{font-family:'Segoe UI',Roboto,sans-serif;font-size:1.95rem;margin:.2em 0}}\n",
        "      .pms h2   {{font-size:1.15rem;color:#2E8B57;margin:.8em 0 .3em}}\n",
        "      .pms h3   {{font-size:1.05rem;margin:1.0em 0 .5em}}\n",
        "      .pms p, .pms li {{font-size:.95rem;line-height:1.55em}}\n",
        "      .pms .flow img {{border:1px solid #d0d0d0;border-radius:6px}}\n",
        "      .pms ul   {{margin-top:.2em;margin-bottom:1em}}\n",
        "    </style>\n",
        "\n",
        "    <div class='pms'>\n",
        "\n",
        "      <h1>🚀 Pipeline Modeling Suite (PMS)</h1>\n",
        "      <p><em>Entorno interactivo «pasta-y-ejecuta» para ciencia de datos end-to-end:\n",
        "         desde la preparación de datos hasta la explicación xAI.</em></p>\n",
        "\n",
        "      <!-- ▸▸ ITINERARIO 1: ENTRENAR + PREDICCIONES -->\n",
        "      <h2>Itinerarios sugeridos de uso</h2>\n",
        "\n",
        "      <h3>🧪 Entrenar <b>y predecir</b> &nbsp;(<i>Bloques 1 → 2</i>)</h3>\n",
        "      <div class='flow'><img src=\"data:image/png;base64,{_img_flow_trainpred()}\" /></div>\n",
        "      <ul>\n",
        "        <li><b>Funcionalidades Bloque 1 - Carga, segmentación y selección:</b></li>\n",
        "        <li><b>📥 Cargar Datos:</b> pegar o cargar X&nbsp;/ Y desde Excel/CSV.</li>\n",
        "        <li><b>🔀 Segmentar:</b> Train / Test (<code>stratify</code> opcional).</li>\n",
        "        <li><b>🔍 Seleccionar Variables:</b> Pearson, Spearman, Mutual Info, Boruta, UMAP.</li>\n",
        "      </ul>\n",
        "        <li><b>Funcionalidades Bloque 2 - Entrenamiento, predicción y comparación de modelos de IA:</b></li>\n",
        "        <li><b>🤖 Entrenar SVR:</b> SVR (Support Vector Regression) es un algoritmo de aprendizaje supervisado que busca predecir valores continuos ajustando una función que mantenga los errores dentro de un margen tolerable (épsilon) y maximizando la generalización del modelo.</li>\n",
        "        <li><b>🧠 Entrenar NN:</b> Una red neuronal (NN) es un modelo de aprendizaje automático inspirado en el cerebro humano, compuesto por capas de nodos interconectados que procesan datos para reconocer patrones y hacer predicciones.</li>\n",
        "        <li><b>🌲 Entrenar XGBoost:</b> Algoritmo de aprendizaje automático basado en árboles de decisión que utiliza técnicas de gradiente boosting para lograr alta precisión y eficiencia en tareas de clasificación y regresión. </li>\n",
        "        <li><b>🌳 Entrenar Random Forest.</b> Algoritmo de aprendizaje automático basado en conjuntos que construye múltiples árboles de decisión y combina sus predicciones para mejorar la precisión y reducir el sobreajuste. </li>\n",
        "        <li><b>🔁 Entrenar RNN:</b> La Red Neuronal Recurrente (RNN)) es un tipo de red neuronal diseñada para procesar secuencias de datos, donde cada salida depende no solo de la entrada actual sino también del estado anterior, lo que la hace ideal para tareas como series temporales, texto o audio</li>\n",
        "        <li><b>📈 Predicción con modelos SVR, NN, XGBoostm RandomForest y RNN:</b> compara Y-real vs Y-predicho, residuos y curvas.</li>\n",
        "        <li><b>📊 Comparador:</b> ranking métrico y visual entre modelos.</li>\n",
        "      </ul>\n",
        "      <div style=\"margin-bottom:1em;text-align:center\">\n",
        "        <img src=\"data:image/png;base64,{_block2_img_training_prediction()}\" />\n",
        "        <div style=\"font-size:.86rem;margin-top:.3em;color:#555\">\n",
        "          Flujo interno de Bloque 2: entrenamiento → validación → <b>predicción</b>\n",
        "        </div>\n",
        "      </div>\n",
        "\n",
        "      <!-- ▸▸ ITINERARIO 2: OPTIMIZAR -->\n",
        "      <h3>⚙️ Optimizar modelos &nbsp;(<i>Bloques 1 → 3</i>)</h3>\n",
        "      <div class='flow'><img src=\"data:image/png;base64,{_img_flow_opt()}\" /></div>\n",
        "      <ul>\n",
        "        <li><b>Funcionalidades Bloque 3 - Optimización de Modelos de Inteligencia Artificial:</b></li>\n",
        "        <li><b>🔧 Optimizar SVR:</b> Grid, Random, Optuna, BayesSearch.</li>\n",
        "        <li><b>⚙️ Optimizar NN:</b> RandomSearch, Bayesian, Hyperband, Optuna.</li>\n",
        "        <li><b>🔩 Optimizar XGB:</b> RandomSearch, Bayesian, Hyperband, Optuna.</li>\n",
        "        <li><b>🌳⚙️ Optimizar RF:</b> RandomSearch, Bayesian, Hyperband, Optuna.</li>\n",
        "        <li><b>🔁 Optimizar RNN:</b> RandomSearch, Bayesian, Hyperband, Optuna.</li>\n",
        "      </ul>\n",
        "\n",
        "      <!-- ▸▸ ITINERARIO 3: EXPLICAR -->\n",
        "      <h3>🧠 Explicar modelos &nbsp;(<i>Bloques 1 → 2 → 3 → 4</i>)</h3>\n",
        "      <div class='flow'><img src=\"data:image/png;base64,{_img_flow_xai()}\" /></div>\n",
        "      <ul>\n",
        "        <li><b>Funcionalidades Bloque 4 - Inteligencia Artificial Explicativa xIA:</b></li>\n",
        "        <li><b>📝 SHAP, LIME, KernelExplainer.</b></li>\n",
        "        <li><b>📈 PDP, ALE, ICE.</b></li>\n",
        "        <li><b>🧭 Surrogate Models &amp; Anchors.</b></li>\n",
        "        <li><b>💠 EBM, Contrafactuales.</b></li>\n",
        "      </ul>\n",
        "    \"\"\")\n",
        "\n",
        "    # Muestra el HTML + botones\n",
        "    display(html_intro)\n",
        "\n",
        "    #btn.on_click(mostrar_bienvenida)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 5. CARGA Y SEGMENTACION DE DATOS\n",
        "# Este módulo sirve para cargar los datos a modelar y para segmentar dichos datos en el grupo de Entrenamiento y de Test\n",
        "# ===============================================================\n",
        "# ===============================================================\n",
        "# 5.1. CARGA DE DATOS (X, Y y fechas) - FUNCIONA CORRECTAMENTE\n",
        "# Permite pegar desde Excel, procesar con validaciones y mostrar estadísticas.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import io\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "out_carga = widgets.Output()\n",
        "\n",
        "# Variables globales\n",
        "X_data, Y_data, FECHAS, y_variable_name = None, None, None, None\n",
        "\n",
        "def mostrar_carga(b=None):\n",
        "    out_carga.clear_output()\n",
        "\n",
        "    text_X = widgets.Textarea(\n",
        "        placeholder='Pega aquí la matriz X desde Excel (sin índice).',\n",
        "        layout=widgets.Layout(width='100%', height='150px')\n",
        "    )\n",
        "    text_Y = widgets.Textarea(\n",
        "        placeholder='Pega aquí la matriz Y (una o más columnas).',\n",
        "        layout=widgets.Layout(width='100%', height='100px')\n",
        "    )\n",
        "    text_F = widgets.Textarea(\n",
        "        placeholder='Pega aquí la columna de fechas (una sola columna con encabezado).',\n",
        "        layout=widgets.Layout(width='100%', height='100px')\n",
        "    )\n",
        "\n",
        "    boton_importar = widgets.Button(description=\"📥 Importar Datos\", button_style='success')\n",
        "    salida = widgets.Output()\n",
        "\n",
        "    def importar_datos(_):\n",
        "        salida.clear_output()\n",
        "        global X_data, Y_data, FECHAS, y_variable_name\n",
        "\n",
        "        try:\n",
        "            # ------------------ Matriz X ------------------\n",
        "            X = pd.read_csv(io.StringIO(text_X.value.strip()), sep='\\t', header=0)\n",
        "            X = X.apply(lambda col: col.str.replace(',', '.', regex=False) if col.dtype == 'object' else col)\n",
        "            X = X.apply(pd.to_numeric, errors='raise')\n",
        "\n",
        "            # ------------------ Matriz Y ------------------\n",
        "            Y = pd.read_csv(io.StringIO(text_Y.value.strip()), sep='\\t', header=0)\n",
        "            Y = Y.apply(lambda col: col.str.replace(',', '.', regex=False) if col.dtype == 'object' else col)\n",
        "            Y = Y.apply(pd.to_numeric, errors='raise')\n",
        "            y_variable_name = ', '.join(Y.columns) if Y.shape[1] > 1 else Y.columns[0]\n",
        "\n",
        "            # ------------------ Columna de Fechas ------------------\n",
        "            raw_text = text_F.value.strip()\n",
        "            F = pd.read_csv(io.StringIO(raw_text), sep='\\t', header=0)\n",
        "            if F.shape[1] != 1:\n",
        "                raise ValueError(\"❌ La columna de fechas debe tener solo una columna.\")\n",
        "\n",
        "            fecha_raw = F.iloc[:, 0].astype(str).str.strip()\n",
        "            FECHAS = pd.to_datetime(fecha_raw, errors='coerce', format=\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "            if FECHAS.isna().sum() > 0:\n",
        "                display(HTML(\"<b style='color:red;'>❌ Estas son las fechas no reconocidas:</b>\"))\n",
        "                display(pd.DataFrame({\"Fecha original\": fecha_raw[FECHAS.isna()]}))\n",
        "                raise ValueError(\"❌ Algunas fechas no se pudieron interpretar. Revisa el formato o caracteres ocultos.\")\n",
        "\n",
        "            # ------------------ Validar dimensiones ------------------\n",
        "            if not (len(X) == len(Y) == len(FECHAS)):\n",
        "                raise ValueError(f\"❌ Dimensiones incompatibles: X({len(X)}), Y({len(Y)}), Fechas({len(FECHAS)})\")\n",
        "\n",
        "            # ------------------ Guardar ------------------\n",
        "            X_data = X.copy()\n",
        "            Y_data = Y.copy()\n",
        "            mostrar_estadisticas_datos()  # ← Añadir esta línea al final\n",
        "\n",
        "            display(HTML(\"<b style='color:green;'>✅ Datos cargados correctamente.</b>\"))\n",
        "            mostrar_estadisticas_datos()\n",
        "\n",
        "        except Exception as e:\n",
        "            display(HTML(f\"<span style='color:red;'>❌ Error al importar: {str(e)}</span>\"))\n",
        "\n",
        "    boton_importar.on_click(importar_datos)\n",
        "\n",
        "    with out_carga:\n",
        "        display(HTML(\"<h3>📥 Carga de Datos</h3>\"))\n",
        "        display(widgets.VBox([\n",
        "            widgets.Label(\"🔷 Matriz X (variables predictoras):\"), text_X,\n",
        "            widgets.Label(\"🔶 Matriz Y (variable/s a predecir):\"), text_Y,\n",
        "            widgets.Label(\"🕒 Columna de Fechas (una sola columna):\"), text_F,\n",
        "            boton_importar, salida\n",
        "        ]))\n",
        "\n",
        "    # ✅ Mostrar el output\n",
        "    display(out_carga)\n",
        "\n",
        "def mostrar_estadisticas_datos():\n",
        "    with out_carga:\n",
        "        try:\n",
        "            display(HTML(\"<h3>📊 Estadísticas de los Datos Cargados</h3>\"))\n",
        "\n",
        "            # Estadísticas de X\n",
        "            if X_data is not None:\n",
        "                display(HTML(\"<h4>📈 Estadísticas de X (Variables Independientes):</h4>\"))\n",
        "                display(X_data.describe(include='all').T.style.set_caption(\"Resumen Estadístico de X\").format(precision=3))\n",
        "\n",
        "            # Estadísticas de Y\n",
        "            if Y_data is not None:\n",
        "                display(HTML(\"<h4>🎯 Estadísticas de Y (Variable Objetivo):</h4>\"))\n",
        "                display(Y_data.describe(include='all').T.style.set_caption(\"Resumen Estadístico de Y\").format(precision=3))\n",
        "\n",
        "            # Estadísticas de Fechas\n",
        "            if FECHAS is not None and not FECHAS.isna().all():\n",
        "                display(HTML(\"<h4>📅 Estadísticas de Fechas:</h4>\"))\n",
        "                display(pd.DataFrame({\n",
        "                    'Primera fecha': [FECHAS.min()],\n",
        "                    'Última fecha': [FECHAS.max()],\n",
        "                    'Total registros': [len(FECHAS)],\n",
        "                    'Frecuencia media (días)': [FECHAS.diff().dt.total_seconds().dropna().mean() / 86400]\n",
        "                }).T.rename(columns={0: 'Valor'}))\n",
        "\n",
        "        except Exception as e:\n",
        "            display(HTML(f\"<span style='color:red;'>❌ Error al mostrar estadísticas: {str(e)}</span>\"))\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5.2. SEGMENTACION DE DATOS PARA ENTRENAMIENTO Y TEST - FUNCIONA CORRECTAMENTE\n",
        "# Split %entrenamiento / % pruebas estándar o estratificado según distribución de Y\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, Javascript, clear_output\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Widget de salida\n",
        "out_split = widgets.Output()\n",
        "\n",
        "def mostrar_split(event=None):\n",
        "    clear_output(wait=True)           # 🔁 Borra todo lo visible antes de pintar de nuevo\n",
        "#    out_split.clear_output()          # 🔁 Borra lo que había dentro del widget\n",
        "    with out_split:\n",
        "        #out_split.clear_output()\n",
        "        # Validar que los datos estén cargados\n",
        "        if 'X_data' not in globals() or 'Y_data' not in globals():\n",
        "            print(\"❌ Debes cargar primero los datos (X_data, Y_data)\")\n",
        "            return\n",
        "        X = X_data.copy()\n",
        "        Y = Y_data.copy()\n",
        "\n",
        "        # Título e instrucciones\n",
        "        display(HTML(\"<h3 style='color:#2E8B57;'>🔀 Filtrado de Datos para Train/Test</h3>\"))\n",
        "        print(\"Configura el split de tu dataset (recomendado test_size=0.2, random_state=42)\")\n",
        "\n",
        "        # Controles de usuario\n",
        "        test_size = widgets.FloatSlider(\n",
        "            value=0.2, min=0.05, max=0.5, step=0.05,\n",
        "            description='test_size:', tooltip='reserva el % de filas para test'\n",
        "        )\n",
        "        random_state = widgets.BoundedIntText(\n",
        "            value=42, min=0, description='random_state:'\n",
        "        )\n",
        "        stratify_chk = widgets.Checkbox(\n",
        "            value=False, description='Estratificar según Y'\n",
        "        )\n",
        "        bin_method = widgets.Dropdown(\n",
        "            options=['qcut','cut'], value='qcut', description='Método bins:'\n",
        "        )\n",
        "        q_bins = widgets.BoundedIntText(\n",
        "            value=5, min=2, max=20, description='q (bins):'\n",
        "        )\n",
        "        btn_split = widgets.Button(\n",
        "            description='Aplicar Split', button_style='success'\n",
        "        )\n",
        "\n",
        "        display(widgets.VBox([test_size, random_state,\n",
        "                              stratify_chk, bin_method, q_bins, btn_split]))\n",
        "\n",
        "        def run_split(b):\n",
        "            out_split.clear_output()      # <-- CAMBIO: borra cualquier contenido previo en out_split cuando se pulsa de nuevo\n",
        "            global X_train, X_test, Y_train, Y_test, FECHAS_train, FECHAS_test\n",
        "            # Preparar stratify\n",
        "            stratify = None\n",
        "            if stratify_chk.value:\n",
        "                try:\n",
        "                    # Asegurar que usamos una sola columna para la estratificación\n",
        "                    if isinstance(Y, pd.DataFrame):\n",
        "                        Y_strat = Y.iloc[:, 0]\n",
        "                    else:\n",
        "                        Y_strat = pd.Series(Y)\n",
        "\n",
        "                    if bin_method.value == 'qcut':\n",
        "                        bins = pd.qcut(Y_strat, q=q_bins.value, duplicates='drop')\n",
        "                    else:\n",
        "                        bins = pd.cut(Y_strat, bins=q_bins.value)\n",
        "                    stratify = bins\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error al crear bins: {e}\")\n",
        "                    return\n",
        "\n",
        "            # Realizar split\n",
        "            try:\n",
        "                X_train, X_test, Y_train, Y_test, FECHAS_train, FECHAS_test = train_test_split(\n",
        "                    X, Y, FECHAS,\n",
        "                    test_size=test_size.value,\n",
        "                    random_state=random_state.value,\n",
        "                    stratify=stratify if stratify_chk.value else None\n",
        "                )\n",
        "                # justo aquí, guardo los parámetros en globals\n",
        "                global SPLIT_PARAMS                           # Nuevo desde aqui: Creado para poder generar el informe final\n",
        "                SPLIT_PARAMS = {\n",
        "                    \"test_size\": test_size.value,\n",
        "                    \"random_state\": random_state.value,\n",
        "                    \"stratify\": stratify_chk.value,\n",
        "                    \"bin_method\": bin_method.value,\n",
        "                    \"q_bins\": q_bins.value\n",
        "                }                                             # Nuevo hasta aqui\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error en train_test_split: {e}\")\n",
        "                return\n",
        "\n",
        "            y_col = Y.columns[0]\n",
        "            df_train = X_train.copy()\n",
        "            df_train[y_col] = Y_train.values\n",
        "            df_train['fecha'] = FECHAS_train.values\n",
        "            df_train['set'] = 'train'\n",
        "\n",
        "            df_test = X_test.copy()\n",
        "            df_test[y_col] = Y_test.values\n",
        "            df_test['fecha'] = FECHAS_test.values\n",
        "            df_test['set'] = 'test'\n",
        "\n",
        "            df_out = pd.concat([df_train, df_test], axis=0)\n",
        "\n",
        "            # Mostrar resumen de partición\n",
        "            print(f\"🔹 Total registros: {len(X)}\")\n",
        "            print(f\"🔹 Registros en Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "            print(f\"🔹 Registros en Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "            # Mostrar DataFrame final\n",
        "            print(\"📊 Resultado del split (train/test) para copiar y pegar:\")\n",
        "            display(df_out)\n",
        "\n",
        "            # Botones de copia X e Y\n",
        "            # Entrenamiento Y\n",
        "            btn_copy_y_train = widgets.Button(\n",
        "                description='📋 Copiar Y Train', icon='clipboard', button_style='info'\n",
        "            )\n",
        "            # Entrenamiento X\n",
        "            btn_copy_x_train = widgets.Button(\n",
        "                description='📋 Copiar X Train', icon='clipboard', button_style='info'\n",
        "            )\n",
        "            # Entrenamiento Fechas\n",
        "            btn_copy_f_train = widgets.Button(\n",
        "                description='📋 Copiar Fechas Train', icon='calendar', button_style='info'\n",
        "            )\n",
        "            # Test Y\n",
        "            btn_copy_y_test = widgets.Button(\n",
        "                description='📋 Copiar Y Test', icon='clipboard', button_style='info'\n",
        "            )\n",
        "            # Test X\n",
        "            btn_copy_x_test = widgets.Button(\n",
        "                description='📋 Copiar X Test', icon='clipboard', button_style='info'\n",
        "            )\n",
        "            # Test Fechas\n",
        "            btn_copy_f_test = widgets.Button(\n",
        "                description='📋 Copiar Fechas Test', icon='calendar', button_style='info'\n",
        "            )\n",
        "            msg_train_y = widgets.Output()\n",
        "            msg_train_x = widgets.Output()\n",
        "            msg_test_y = widgets.Output()\n",
        "            msg_test_x = widgets.Output()\n",
        "            msg_f_train = widgets.Output()\n",
        "            msg_f_test = widgets.Output()\n",
        "\n",
        "            def copy_y_train(_):\n",
        "                try:\n",
        "                    Y_train.to_frame() .to_clipboard(index=False)\n",
        "                    with msg_train_y:\n",
        "                        display(HTML(\"<span style='color:green;'>✅ Y Train copiada</span>\"))\n",
        "                except Exception:\n",
        "                    csv = Y_train.to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_train_y:\n",
        "                        display(HTML(\"<span style='color:green;'>✅ Y Train copiada via JS</span>\"))\n",
        "            def copy_x_train(_):\n",
        "                try:\n",
        "                    df_train.drop(columns=[Y.name,'set']).to_clipboard(index=False)\n",
        "                    with msg_train_x:\n",
        "                        display(HTML(\"<span style='color:green;'>✅ X Train copiada</span>\"))\n",
        "                except Exception:\n",
        "                    csv = df_train.drop(columns=[Y.name,'set']).to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_train_x:\n",
        "                        display(HTML(\"<span style='color:green;'>✅ X Train copiada via JS</span>\"))\n",
        "            def copy_f_train(_):\n",
        "                try:\n",
        "                    FECHAS.iloc[X_train.index].to_frame().to_clipboard(index=False)\n",
        "                    with msg_f_train:\n",
        "                        msg_f_train.clear_output()\n",
        "                        display(HTML(\"<span style='color:green;'>✅ Fechas Train copiadas</span>\"))\n",
        "                except Exception:\n",
        "                    csv = FECHAS.iloc[X_train.index].to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_f_train:\n",
        "                        msg_f_train.clear_output()\n",
        "                        display(HTML(\"<span style='color:green;'>✅ Fechas Train copiadas vía JS</span>\"))\n",
        "\n",
        "            def copy_y_test(_):\n",
        "                try:\n",
        "                    Y_test.to_frame().to_clipboard(index=False)\n",
        "                    with msg_test_y:\n",
        "                        display(HTML(\"<span style='color:green;'>✅ Y Test copiada</span>\"))\n",
        "                except Exception:\n",
        "                    csv = Y_test.to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_test_y:\n",
        "                        display(HTML(\"<span style='color:green;'>✅ Y Test copiada via JS</span>\"))\n",
        "            def copy_x_test(_):\n",
        "                try:\n",
        "                    df_test.drop(columns=[Y.name,'set']).to_clipboard(index=False)\n",
        "                    with msg_test_x:\n",
        "                        display(HTML(\"<span style='color:green;'>✅ X Test copiada</span>\"))\n",
        "                except Exception:\n",
        "                    csv = df_test.drop(columns=[Y.name,'set']).to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_test_x:\n",
        "                        display(HTML(\"<span style='color:green;'>✅ X Test copiada via JS</span>\"))\n",
        "            def copy_f_test(_):\n",
        "                try:\n",
        "                    FECHAS.iloc[X_test.index].to_frame().to_clipboard(index=False)\n",
        "                    with msg_f_test:\n",
        "                        msg_f_test.clear_output()\n",
        "                        display(HTML(\"<span style='color:green;'>✅ Fechas Test copiadas</span>\"))\n",
        "                except Exception:\n",
        "                    csv = FECHAS.iloc[X_test.index].to_csv(index=False)\n",
        "                    display(Javascript(f\"navigator.clipboard.writeText(`{csv}`)\"))\n",
        "                    with msg_f_test:\n",
        "                        msg_f_test.clear_output()\n",
        "                        display(HTML(\"<span style='color:green;'>✅ Fechas Test copiadas vía JS</span>\"))\n",
        "\n",
        "            btn_copy_y_train.on_click(copy_y_train)\n",
        "            btn_copy_x_train.on_click(copy_x_train)\n",
        "            btn_copy_f_train.on_click(copy_f_train)\n",
        "            btn_copy_y_test.on_click(copy_y_test)\n",
        "            btn_copy_x_test.on_click(copy_x_test)\n",
        "            btn_copy_f_test.on_click(copy_f_test)\n",
        "\n",
        "            #display(widgets.HBox([btn_copy_y_train, btn_copy_x_train, btn_copy_y_test, btn_copy_x_test]))\n",
        "            display(widgets.HBox([\n",
        "                btn_copy_y_train, btn_copy_x_train, btn_copy_y_test, btn_copy_x_test,\n",
        "                btn_copy_f_train, btn_copy_f_test\n",
        "            ]))\n",
        "            #display(widgets.HBox([msg_train_y, msg_train_x, msg_test_y, msg_test_x]))\n",
        "            display(widgets.HBox([\n",
        "                msg_train_y, msg_train_x, msg_test_y, msg_test_x, msg_f_train, msg_f_test\n",
        "            ]))\n",
        "\n",
        "        btn_split.on_click(run_split)\n",
        "\n",
        "    display(out_split)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 6. SELECCION DE VARIABLES X QUE CORRELAN CON LA VARIABLE Y - FUNCIONA CORRECTAMENTE\n",
        "# Este módulo permite seleccionar las variables X que tienen influencia en Y, usando los datos cargados, previa segmentación entre Datos para Entrenamiento y datos para test\n",
        "# ===============================================================================\n",
        "import pkgutil\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import threading\n",
        "import time\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "# Librerías opcionales\n",
        "BORUTA_AVAILABLE = pkgutil.find_loader('boruta') is not None\n",
        "UMAP_AVAILABLE   = pkgutil.find_loader('umap')   is not None\n",
        "if BORUTA_AVAILABLE:\n",
        "    from boruta import BorutaPy\n",
        "if UMAP_AVAILABLE:\n",
        "    import umap\n",
        "\n",
        "# Temporizador\n",
        "_timer_stop_event = threading.Event()\n",
        "def _update_timer(event, widget):\n",
        "    start = time.time()\n",
        "    while not event.is_set():\n",
        "        widget.value = f\"⏱️ {time.time() - start:.1f}s\"\n",
        "        time.sleep(0.5)\n",
        "\n",
        "# Salidas globales\n",
        "dlg_out = widgets.Output()\n",
        "res_out = widgets.Output()\n",
        "ayuda_out = widgets.Output()\n",
        "\n",
        "# Variables globales\n",
        "VARIABLES_SELECCIONADAS = {}\n",
        "METODO_SELECCION = \"\"\n",
        "VALORES_CORRELACION = pd.Series(dtype=float)\n",
        "RESUMEN_METODOS = {}\n",
        "\n",
        "\n",
        "def mostrar_seleccion_variables(event=None):\n",
        "    global X_train, Y_train\n",
        "    dlg_out.clear_output()\n",
        "    res_out.clear_output()\n",
        "    display(dlg_out, res_out)\n",
        "\n",
        "    with dlg_out:\n",
        "        clear_output()\n",
        "        if 'X_train' not in globals() or 'Y_train' not in globals():\n",
        "            print(\"❌ Debes segmentar antes los datos (X_train, Y_train).\")\n",
        "            return\n",
        "\n",
        "        X = X_train.copy()\n",
        "        Y = Y_train.copy()\n",
        "        if isinstance(Y, pd.DataFrame):\n",
        "            Y_corr = Y.iloc[:, 0]\n",
        "        else:\n",
        "            Y_corr = pd.Series(Y)\n",
        "\n",
        "        print(f\"🔍 Variables disponibles: {X.shape[1]}\")\n",
        "        print(f\"🔍 Observaciones en entrenamiento: {X.shape[0]}\")\n",
        "\n",
        "        display(HTML(\"<h3 style='color:#2E8B57;'>🔎 Selección de Variables</h3>\"))\n",
        "\n",
        "        ayuda_pearson = widgets.HTML(\"\"\"\n",
        "        <ul>\n",
        "            <li><b>Umbral X–Y:</b> Selecciona las variables cuya correlación absoluta con Y supera un valor determinado.</li>\n",
        "            <li><b>Valores recomendados:</b> entre 0.3 y 0.6 para evitar colinealidad excesiva.</li>\n",
        "            <li><i>Pearson mide relaciones lineales.</i></li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "        ayuda_spearman = widgets.HTML(\"\"\"\n",
        "        <ul>\n",
        "            <li><b>Umbral X–Y:</b> Selecciona variables con alta correlación de rangos con Y.</li>\n",
        "            <li><b>Recomendado:</b> igual a Pearson (0.3 – 0.6).</li>\n",
        "            <li><i>Spearman es útil para relaciones no lineales monótonas.</i></li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "        ayuda_mi = widgets.HTML(\"\"\"\n",
        "        <ul>\n",
        "            <li><b>Top k MI:</b> Número de variables con mayor información mutua respecto a Y.</li>\n",
        "            <li><b>Valores recomendados:</b> entre 5 y 10 para datasets medianos.</li>\n",
        "            <li><i>Captura relaciones no lineales.</i></li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "        ayuda_boruta = widgets.HTML(\"\"\"\n",
        "        <ul>\n",
        "            <li><b>RF est:</b> Número de árboles del bosque aleatorio. (Recomendado: ≥ 100).</li>\n",
        "            <li><b>Iter BOR:</b> Iteraciones del algoritmo Boruta. (Recomendado: 50–100).</li>\n",
        "            <li><b>Alpha:</b> Nivel de significancia estadística. (Recomendado: 0.01 – 0.1).</li>\n",
        "            <li><i>Selecciona solo variables relevantes con base en importancia del modelo.</i></li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "        ayuda_umap = widgets.HTML(\"\"\"\n",
        "        <ul>\n",
        "            <li><b>Dims UMAP:</b> Dimensiones latentes en la proyección (típicamente 2–5).</li>\n",
        "            <li><b>Top k UMAP:</b> Variables con mayor correlación a las dimensiones proyectadas.</li>\n",
        "            <li><i>UMAP revela estructuras no lineales complejas en los datos.</i></li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        metodo = widgets.Dropdown(\n",
        "            options=[\"Pearson\", \"Spearman\", \"MutualInfo\"]\n",
        "                    + ([\"Boruta\"] if BORUTA_AVAILABLE else [])\n",
        "                    + ([\"UMAP\"] if UMAP_AVAILABLE else []),\n",
        "            description='Método:'\n",
        "        )\n",
        "        th_xy = widgets.FloatSlider(\n",
        "            value=0.1, min=0.0, max=1.0, step=0.01, description='Umbral X–Y:', readout=False\n",
        "        )\n",
        "        th_xy_lbl = widgets.Label(value=f\"{th_xy.value:.2f}\")\n",
        "        def actualizar_umbral(change):\n",
        "            th_xy_lbl.value = f\"{change['new']:.2f}\"\n",
        "        th_xy.observe(actualizar_umbral, names='value')\n",
        "        fila_th_xy = widgets.HBox([th_xy, th_xy_lbl])\n",
        "\n",
        "        mi_k     = widgets.BoundedIntText(value=5, min=1, max=X.shape[1], description='Top k MI:')\n",
        "        bor_n    = widgets.BoundedIntText(value=100, min=1, max=1000, description='RF est:')\n",
        "        bor_iter = widgets.BoundedIntText(value=50, min=1, max=500, description='Iter BOR:')\n",
        "        bor_a    = widgets.BoundedFloatText(value=0.05, min=0.0, max=1.0, description='Alpha:')\n",
        "        umap_d   = widgets.BoundedIntText(value=2, min=1, max=min(10, X.shape[1]), description='Dims UMAP:')\n",
        "        umap_k   = widgets.BoundedIntText(value=5, min=1, max=X.shape[1], description='Top k UMAP:')\n",
        "        btn_run  = widgets.Button(description='Ejecutar', button_style='success')\n",
        "        btn_resumen = widgets.Button(description='📋 Ver Resumen', button_style='info')\n",
        "        timer_lbl= widgets.Label('⏱️ 0.0s')\n",
        "\n",
        "        ayudas = {\n",
        "            'Pearson': ayuda_pearson,\n",
        "            'Spearman': ayuda_spearman,\n",
        "            'MutualInfo': ayuda_mi,\n",
        "            'Boruta': ayuda_boruta,\n",
        "            'UMAP': ayuda_umap\n",
        "        }\n",
        "\n",
        "        fila_th_xy.layout.display = 'none'\n",
        "        mi_k.layout.display = 'none'\n",
        "        bor_n.layout.display = 'none'\n",
        "        bor_iter.layout.display = 'none'\n",
        "        bor_a.layout.display = 'none'\n",
        "        umap_d.layout.display = 'none'\n",
        "        umap_k.layout.display = 'none'\n",
        "\n",
        "        def toggle_params(_=None):\n",
        "            m = metodo.value\n",
        "            corr_visible = m in ['Pearson', 'Spearman']\n",
        "            fila_th_xy.layout.display = 'flex' if corr_visible else 'none'\n",
        "            mi_k.layout.display     = 'block' if m == 'MutualInfo' else 'none'\n",
        "            bor_n.layout.display    = 'block' if m == 'Boruta' else 'none'\n",
        "            bor_iter.layout.display = 'block' if m == 'Boruta' else 'none'\n",
        "            bor_a.layout.display    = 'block' if m == 'Boruta' else 'none'\n",
        "            umap_d.layout.display   = 'block' if m == 'UMAP' else 'none'\n",
        "            umap_k.layout.display   = 'block' if m == 'UMAP' else 'none'\n",
        "            with ayuda_out:\n",
        "                clear_output()\n",
        "                if m in ayudas:\n",
        "                    display(ayudas[m])\n",
        "\n",
        "        metodo.observe(toggle_params, names='value')\n",
        "        toggle_params()\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo, fila_th_xy, mi_k, bor_n, bor_iter, bor_a, umap_d, umap_k, ayuda_out, btn_run, btn_resumen, timer_lbl\n",
        "        ]))\n",
        "\n",
        "    def run_selection(_):\n",
        "        global VARIABLES_SELECCIONADAS, METODO_SELECCION, VALORES_CORRELACION, RESUMEN_METODOS\n",
        "        res_out.clear_output()\n",
        "        _timer_stop_event.clear()\n",
        "        threading.Thread(target=_update_timer, args=(_timer_stop_event, timer_lbl), daemon=True).start()\n",
        "\n",
        "        with res_out:\n",
        "            X = X_train.copy()\n",
        "            Y = Y_train.copy()\n",
        "            Y_corr = Y.iloc[:, 0] if isinstance(Y, pd.DataFrame) else pd.Series(Y)\n",
        "            method = metodo.value\n",
        "            selected = []\n",
        "            correlaciones = pd.Series(dtype=float)\n",
        "\n",
        "            if method in ['Pearson', 'Spearman']:\n",
        "                correlaciones = X.apply(lambda col: col.corr(Y_corr, method=method.lower()))\n",
        "                correlaciones_abs = correlaciones.abs()\n",
        "                display(HTML(f\"<h4>📈 Correlaciones X–Y (abs) usando <u>{method}</u>:</h4>\"))\n",
        "                display(correlaciones_abs.to_frame(name=f'|correlación {method}|'))\n",
        "                selected = correlaciones_abs[correlaciones_abs >= th_xy.value].index.tolist()\n",
        "                not_selected = correlaciones_abs[~correlaciones_abs.index.isin(selected)].index.tolist()\n",
        "\n",
        "                if selected:\n",
        "                    display(HTML(f\"<h4 style='color:green;'>✅ Variables Seleccionadas ({method}, umbral ≥ {th_xy.value}):</h4>\"))\n",
        "                    display(correlaciones[selected].to_frame(name=f'correlación {method}').sort_values(by=f'correlación {method}', ascending=False))\n",
        "                else:\n",
        "                    display(HTML(f\"<h4 style='color:red;'>⚠️ No se seleccionaron variables con {method} ≥ {th_xy.value}</h4>\"))\n",
        "\n",
        "                if not_selected:\n",
        "                    display(HTML(f\"<h4>📌 Variables No Seleccionadas ({method}):</h4>\"))\n",
        "                    display(correlaciones[not_selected].to_frame(name=f'correlación {method}').sort_values(by=f'correlación {method}', ascending=False))\n",
        "\n",
        "            elif method == 'MutualInfo':\n",
        "                mi = mutual_info_regression(X, Y_corr)\n",
        "                correlaciones = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
        "                display(HTML('<b>Top MutualInfo:</b>'))\n",
        "                display(correlaciones.head(mi_k.value).to_frame('MI'))\n",
        "                selected = correlaciones.head(mi_k.value).index.tolist()\n",
        "\n",
        "            elif method == 'Boruta' and BORUTA_AVAILABLE:\n",
        "                #rf = RandomForestRegressor(n_estimators=bor_n.value, random_state=42)\n",
        "                #bor = BorutaPy(rf, alpha=bor_a.value, max_iter=bor_iter.value, random_state=42)\n",
        "                # ─── AÑADIDO: usar todos los cores ───\n",
        "                rf = RandomForestRegressor(\n",
        "                    n_estimators=bor_n.value,\n",
        "                    random_state=42,\n",
        "                    n_jobs=-1              # Paraleliza el entrenamiento del RF\n",
        "                )\n",
        "                bor = BorutaPy(\n",
        "                    estimator=rf,\n",
        "                    alpha=bor_a.value,\n",
        "                    max_iter=bor_iter.value,\n",
        "                    random_state=42,\n",
        "                    verbose=2          # <–– 1 para resumen, 2 o 3 para detalle completo\n",
        "                )\n",
        "                bor.fit(X.values, Y_corr.values)\n",
        "                support_mask = bor.support_\n",
        "                correlaciones = pd.Series(bor.ranking_, index=X.columns)\n",
        "                dfb = pd.DataFrame({'Feature': X.columns, 'Rank': bor.ranking_, 'Support': support_mask})\n",
        "                display(HTML('<b>Ranking Boruta Top10:</b>'))\n",
        "                display(dfb.sort_values('Rank').head(10))\n",
        "                selected = list(X.columns[support_mask])\n",
        "\n",
        "            elif method == 'UMAP' and UMAP_AVAILABLE:\n",
        "                reducer = umap.UMAP(n_components=umap_d.value, random_state=42)\n",
        "                emb = reducer.fit_transform(X)\n",
        "                dfemb = pd.DataFrame(emb)\n",
        "                corr = X.apply(lambda c: np.mean([abs(np.corrcoef(c, dfemb[i])[0,1]) for i in range(dfemb.shape[1])]), axis=0)\n",
        "                correlaciones = corr\n",
        "                display(HTML('<b>Correlación media UMAP:</b>'))\n",
        "                display(correlaciones.to_frame('mean_corr'))\n",
        "                selected = correlaciones.sort_values(ascending=False).head(umap_k.value).index.tolist()\n",
        "\n",
        "            if not selected:\n",
        "                print(\"❌ No se seleccionaron variables. Ajusta el umbral o método.\")\n",
        "            else:\n",
        "                VARIABLES_SELECCIONADAS = selected\n",
        "                METODO_SELECCION = method\n",
        "                VALORES_CORRELACION = correlaciones\n",
        "                RESUMEN_METODOS[method] = selected\n",
        "\n",
        "                # ——— AÑADIDO: sanitizar nombres para que no haya corchetes, % ni espacios raros ———\n",
        "                #import re\n",
        "                #clean = lambda c: re.sub(r'[\\[\\]<>%]', '_', str(c))\n",
        "                #selected = [ clean(c) for c in selected ]\n",
        "                # ——— FIN AÑADIDO ———\n",
        "                #RESUMEN_METODOS[method] = selected\n",
        "\n",
        "                dfout = X[selected].copy()\n",
        "                dfout[Y_corr.name] = Y_corr.values\n",
        "                display(HTML('<b>Variables Seleccionadas:</b>'))\n",
        "                display(dfout)\n",
        "                txt = widgets.Textarea(value=dfout.drop(columns=[Y_corr.name]).to_csv(index=False),\n",
        "                                       layout=widgets.Layout(width='100%', height='150px'))\n",
        "                display(HTML('<b>CSV X:</b>'))\n",
        "                display(txt)\n",
        "\n",
        "        _timer_stop_event.set()\n",
        "\n",
        "    def mostrar_resumen(b):\n",
        "        with res_out:\n",
        "            clear_output()\n",
        "            if not RESUMEN_METODOS:\n",
        "                display(HTML(\"<b>⚠️ No hay métodos ejecutados aún.</b>\"))\n",
        "            else:\n",
        "                for metodo, variables in RESUMEN_METODOS.items():\n",
        "                    display(HTML(f\"<h4>📌 {metodo}:</h4>\"))\n",
        "                    display(pd.DataFrame(variables, columns=[\"Variables Seleccionadas\"]))\n",
        "\n",
        "    btn_run.on_click(run_selection, remove=True)\n",
        "    btn_run.on_click(run_selection)\n",
        "    btn_resumen.on_click(mostrar_resumen)\n",
        "\n",
        "# Mostrar panel en ejecución directa\n",
        "#display(dlg_out, res_out)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 7. ENTRENAMIENTO MODELOS Y VALIDACION DE ENTRENAMIENTO - FUNCIONA CORRECTAMENTE\n",
        "# Este módulo se descompone en varios sub-modulos para el entrenamiento de cada uno de los modelos con los datos cargados (segmento entrenamiento) y para las variables que correlan.\n",
        "# Posteriormente se valida el entrenamiento con el segmento de datos de test, para la prueba de los entrenamientos.\n",
        "# ===============================================================\n",
        "# ===============================================================\n",
        "# 7.1 ENTRENAMIENTO SVR - FUNCIONA CORRECTAMENTE\n",
        "# Este módulo se usa para entrenar el modelo SVR, permitiendo al usuario ajustar los principales parámetros de diseño.\n",
        "# ===============================================================\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Widget principal de salida (si no existe)\n",
        "if 'out_svr' not in globals():\n",
        "    out_svr = widgets.Output()\n",
        "\n",
        "def mostrar_svr(b=None):\n",
        "    if b is None:\n",
        "        display(out_svr)\n",
        "\n",
        "    with out_svr:\n",
        "        clear_output()\n",
        "\n",
        "        if 'X_train' not in globals() or 'X_test' not in globals():\n",
        "            display(widgets.HTML(\"\"\"<span style='color:red;'>❌ Primero debes segmentar los datos en train/test.</span>\"\"\"))\n",
        "            return\n",
        "\n",
        "        # Bloque para sincronizar variables individuales a partir del resumen RESUMEN_METODOS\n",
        "        if 'RESUMEN_METODOS' in globals():\n",
        "            for metodo, variables in RESUMEN_METODOS.items():\n",
        "                globals()[f\"selected_vars_{metodo.lower()}\"] = variables\n",
        "\n",
        "        # Widgets para configuración\n",
        "        metodo_sel = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selector:'\n",
        "        )\n",
        "        C_val = widgets.FloatLogSlider(value=1.0, base=10, min=-2, max=2, step=0.1, description='C:')\n",
        "        epsilon_val = widgets.FloatLogSlider(value=0.1, base=10, min=-3, max=0, step=0.1, description='Epsilon:')\n",
        "        kernel_val = widgets.Dropdown(options=['rbf', 'linear', 'poly', 'sigmoid'], value='rbf', description='Kernel:')\n",
        "        gamma_val = widgets.Dropdown(options=['scale', 'auto'], description='Gamma:')\n",
        "\n",
        "        btn_train = widgets.Button(description=\"🚀 Entrenar SVR\", button_style='success')\n",
        "        output_area = widgets.Output()\n",
        "        tiempo_lbl = widgets.Label()\n",
        "\n",
        "        def entrenar_svr(_):\n",
        "            output_area.clear_output()\n",
        "            resumen_modelos = []\n",
        "            inicio = time.time()\n",
        "\n",
        "            metodos = [metodo_sel.value] if metodo_sel.value != 'Todos' else ['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "            colores = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "            fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "            with output_area:\n",
        "                for i, metodo in enumerate(metodos):\n",
        "                    var_key = f\"selected_vars_{metodo.lower()}\"\n",
        "                    if var_key not in globals():\n",
        "                        print(f\"⚠️ No hay variables seleccionadas para el método: {metodo}\")\n",
        "                        continue\n",
        "                    selected_vars = globals()[var_key]\n",
        "\n",
        "                    Xtr, Xts = X_train[selected_vars], X_test[selected_vars]\n",
        "                    ytr, yts = Y_train.values.ravel(), Y_test.values.ravel()\n",
        "\n",
        "                    sx, sy = StandardScaler(), StandardScaler()\n",
        "                    Xtr_scaled = sx.fit_transform(Xtr)\n",
        "                    Xts_scaled = sx.transform(Xts)\n",
        "                    ytr_scaled = sy.fit_transform(ytr.reshape(-1,1)).ravel()\n",
        "\n",
        "                    model = SVR(C=C_val.value, epsilon=epsilon_val.value, kernel=kernel_val.value, gamma=gamma_val.value)\n",
        "                    model.fit(Xtr_scaled, ytr_scaled)\n",
        "\n",
        "                    y_pred_scaled = model.predict(Xts_scaled)\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "\n",
        "                    r2 = r2_score(yts, y_pred)\n",
        "                    mse = mean_squared_error(yts, y_pred)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    mae = mean_absolute_error(yts, y_pred)\n",
        "\n",
        "                    resumen_modelos.append({\n",
        "                        'Método': metodo,\n",
        "                        'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae\n",
        "                    })\n",
        "\n",
        "                    # Guardar modelo\n",
        "                    nombre_archivo = f\"modelo_svr_{metodo.lower()}.pkl\"\n",
        "                    with open(nombre_archivo, 'wb') as f:\n",
        "                        pickle.dump({'model': model, 'sx': sx, 'sy': sy, 'cols': selected_vars, 'yname': y_variable_name}, f)\n",
        "\n",
        "                    ax.plot(y_pred, label=f'{metodo}', alpha=0.7, linestyle='--', color=colores[i])\n",
        "                    print(f\"✅ Modelo {metodo} entrenado. R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "                if resumen_modelos:\n",
        "                    ax.plot(yts, label='Y Real', color='black', linewidth=2)\n",
        "                    ax.set_title('Comparación Y Real vs Predicciones SVR por Método')\n",
        "                    ax.grid(); ax.legend()\n",
        "                    plt.show()\n",
        "\n",
        "                    # Tabla resumen\n",
        "                    print(\"\\n📊 Resumen comparativo de métricas:\")\n",
        "                    df = pd.DataFrame(resumen_modelos)\n",
        "                    display(df.set_index('Método'))\n",
        "\n",
        "            tiempo_lbl.value = f\"⏱️ Duración total: {time.time()-inicio:.2f} segundos\"\n",
        "\n",
        "        btn_train.on_click(entrenar_svr)\n",
        "\n",
        "        # Ayuda extendida\n",
        "        ayuda = widgets.HTML(\"\"\"\n",
        "        <h4>ℹ️ Ayuda - Parámetros del modelo SVR</h4>\n",
        "        <ul>\n",
        "            <li><b>C:</b> Penalización al error. Valores altos = bajo sesgo, alto sobreajuste. Recomendado: 0.1 a 100</li>\n",
        "            <li><b>Epsilon:</b> Margen de tolerancia para el error. Cuanto mayor, más simple el modelo. Recomendado: 0.001 a 0.1</li>\n",
        "            <li><b>Kernel:</b> Función para proyectar los datos. rbf es el más común. Otras: linear, poly, sigmoid</li>\n",
        "            <li><b>Gamma:</b> Influencia de un punto de entrenamiento. 'scale' es recomendado.</li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            widgets.HBox([metodo_sel]),\n",
        "            widgets.HBox([C_val, epsilon_val]),\n",
        "            widgets.HBox([kernel_val, gamma_val]),\n",
        "            btn_train, tiempo_lbl,\n",
        "            ayuda, output_area\n",
        "        ]))\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 7.2. ENTRENAMIENTO RED NEURONAL - FUNCIONA CORRECTAMENTE\n",
        "# Este módulo permite entrenar una red neuronal con las variables seleccionadas\n",
        "# por diferentes métodos (Pearson, Spearman, Mutual Info, Boruta, UMAP, o Todos)\n",
        "# sobre los datos previamente segmentados como Train y evaluar sobre Test.\n",
        "# ===============================================================\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "if 'out_nn' not in globals():\n",
        "    out_nn = widgets.Output()\n",
        "\n",
        "def mostrar_nn(b=None):\n",
        "    if b is None:\n",
        "        display(out_nn)\n",
        "\n",
        "    with out_nn:\n",
        "        clear_output()\n",
        "\n",
        "        if 'X_train' not in globals() or 'X_test' not in globals():\n",
        "            display(widgets.HTML(\"<span style='color:red;'>❌ Primero debes segmentar los datos en train/test.</span>\"))\n",
        "            return\n",
        "\n",
        "        if 'RESUMEN_METODOS' in globals():\n",
        "            for metodo, variables in RESUMEN_METODOS.items():\n",
        "                globals()[f\"selected_vars_{metodo.lower()}\"] = variables\n",
        "\n",
        "        metodo_sel = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selector:'\n",
        "        )\n",
        "\n",
        "        capas = widgets.BoundedIntText(value=2, min=1, max=10, description='Capas ocultas:')\n",
        "        neuronas = widgets.Text(value='64,32', description='Neuronas por capa:', placeholder='Ej: 64,32')\n",
        "        activacion = widgets.Dropdown(options=['relu','tanh','sigmoid'], value='relu', description='Activación:')\n",
        "        loss_fn = widgets.Dropdown(options=[('MSE','mse'),('MAE','mae'),('Huber','huber')], value='mse', description='Pérdida:')\n",
        "        tasa = widgets.FloatText(value=0.001, description='Learning Rate:')\n",
        "        epocas = widgets.BoundedIntText(value=100, min=1, description='Epocas:')\n",
        "        batch = widgets.BoundedIntText(value=32, min=1, description='Batch size:')\n",
        "        opt = widgets.Dropdown(options=['adam','sgd','rmsprop'], value='adam', description='Optimizador:')\n",
        "\n",
        "        btn_train = widgets.Button(description='🚀 Entrenar NN', button_style='success')\n",
        "        output_area = widgets.Output()\n",
        "        tiempo_lbl = widgets.Label()\n",
        "\n",
        "        def entrenar_nn(_):\n",
        "            output_area.clear_output()\n",
        "            resumen_modelos = []\n",
        "            inicio = time.time()\n",
        "\n",
        "            metodos = [metodo_sel.value] if metodo_sel.value != 'Todos' else ['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "            colores = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "            fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "            with output_area:\n",
        "                for i, metodo in enumerate(metodos):\n",
        "                    var_key = f\"selected_vars_{metodo.lower()}\"\n",
        "                    if var_key not in globals():\n",
        "                        print(f\"⚠️ No hay variables seleccionadas para el método: {metodo}\")\n",
        "                        continue\n",
        "\n",
        "                    selected_vars = globals()[var_key]\n",
        "                    Xtr, Xts = X_train[selected_vars], X_test[selected_vars]\n",
        "                    ytr, yts = Y_train.values.ravel(), Y_test.values.ravel()\n",
        "\n",
        "                    sx, sy = StandardScaler(), StandardScaler()\n",
        "                    Xtr_scaled = sx.fit_transform(Xtr)\n",
        "                    Xts_scaled = sx.transform(Xts)\n",
        "                    ytr_scaled = sy.fit_transform(ytr.reshape(-1,1)).ravel()\n",
        "\n",
        "                    model = Sequential()\n",
        "                    try:\n",
        "                        layers = [int(n) for n in neuronas.value.split(',')]\n",
        "                        if len(layers) != capas.value:\n",
        "                            raise ValueError(\"❌ Especifica tantas capas como valores de neuronas.\")\n",
        "                        model.add(Dense(layers[0], activation=activacion.value, input_shape=(Xtr_scaled.shape[1],)))\n",
        "                        for units in layers[1:]:\n",
        "                            model.add(Dense(units, activation=activacion.value))\n",
        "                        model.add(Dense(1))\n",
        "                    except Exception as e:\n",
        "                        print(f\"❌ Error en la definición de la arquitectura: {e}\")\n",
        "                        return\n",
        "\n",
        "                    opt_dict = {'adam': Adam, 'sgd': SGD, 'rmsprop': RMSprop}\n",
        "                    optimizer = opt_dict[opt.value](learning_rate=tasa.value)\n",
        "\n",
        "                    loss = {'mse':'mean_squared_error','mae':'mean_absolute_error','huber':tf.keras.losses.Huber()}[loss_fn.value]\n",
        "                    model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n",
        "\n",
        "                    # === Aquí guardamos los hiperparámetros antes de entrenar ===\n",
        "                    hp = {\n",
        "                        'capas_ocultas': capas.value,\n",
        "                        'neuronas_por_capa': neuronas.value,  # e.g. '64,32'\n",
        "                        'activacion': activacion.value,\n",
        "                        'loss_fn': loss_fn.value,\n",
        "                        'learning_rate': tasa.value,\n",
        "                        'epocas': epocas.value,\n",
        "                        'batch_size': batch.value,\n",
        "                        'optimizador': opt.value\n",
        "                    }\n",
        "                    import pickle\n",
        "                    hp_fname = f\"hyperparams_nn_{metodo.lower()}.pkl\"\n",
        "                    try:\n",
        "                        with open(hp_fname, \"wb\") as f_hp:\n",
        "                            pickle.dump(hp, f_hp)\n",
        "                        print(f\"✅ Hiperparámetros guardados en {hp_fname}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"❌ No se pudo guardar hiperparámetros en {hp_fname}: {e}\")\n",
        "\n",
        "                    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "                    history = model.fit(Xtr_scaled, ytr_scaled, validation_split=0.2, epochs=epocas.value, batch_size=batch.value, verbose=0, callbacks=[es])\n",
        "\n",
        "                    y_pred_scaled = model.predict(Xts_scaled).ravel()\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "\n",
        "                    r2 = r2_score(yts, y_pred)\n",
        "                    mse = mean_squared_error(yts, y_pred)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    mae = mean_absolute_error(yts, y_pred)\n",
        "\n",
        "                    resumen_modelos.append({'Método': metodo, 'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae})\n",
        "\n",
        "                    nombre_archivo = f\"modelo_nn_{metodo.lower()}.h5\"\n",
        "                    model.save(nombre_archivo)\n",
        "\n",
        "                    with open(f\"escaladores_nn_{metodo.lower()}.pkl\", \"wb\") as f:\n",
        "                        pickle.dump({\n",
        "                            'scaler_X': sx,\n",
        "                            'scaler_Y': sy,\n",
        "                            'cols': selected_vars,\n",
        "                            'yname': y_variable_name\n",
        "                        }, f)\n",
        "\n",
        "                    ax.plot(y_pred, label=f'{metodo}', alpha=0.7, linestyle='--', color=colores[i])\n",
        "                    print(f\"✅ Modelo {metodo} entrenado. R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "\n",
        "                if resumen_modelos:\n",
        "                    ax.plot(yts, label='Y Real', color='black', linewidth=2)\n",
        "                    ax.set_title('Comparación Y Real vs Predicciones NN por Método')\n",
        "                    ax.grid(); ax.legend()\n",
        "                    plt.show()\n",
        "\n",
        "                    print(\"\\n📊 Resumen comparativo de métricas:\")\n",
        "                    df = pd.DataFrame(resumen_modelos)\n",
        "                    display(df.set_index('Método'))\n",
        "\n",
        "            tiempo_lbl.value = f\"⏱️ Duración total: {time.time()-inicio:.2f} segundos\"\n",
        "\n",
        "        btn_train.on_click(entrenar_nn)\n",
        "\n",
        "        ayuda = widgets.HTML(\"\"\"\n",
        "        <h4>ℹ️ Ayuda - Parámetros de la Red Neuronal</h4>\n",
        "        <ul>\n",
        "            <li><b>Capas ocultas:</b> Número de capas intermedias. Más capas pueden mejorar la expresividad.</li>\n",
        "            <li><b>Neuronas por capa:</b> Lista separada por comas. Cada valor representa una capa. Ej: 64,32</li>\n",
        "            <li><b>Activación:</b> Funciones como relu (recomendado), tanh, sigmoid. Afectan la no linealidad.</li>\n",
        "            <li><b>Learning Rate:</b> Tamaño del paso. Valores típicos: 0.001, 0.01</li>\n",
        "            <li><b>Epocas:</b> Número de iteraciones sobre el dataset. Demasiadas pueden sobreajustar.</li>\n",
        "            <li><b>Batch size:</b> Tamaño de lote en cada actualización de gradiente.</li>\n",
        "            <li><b>Optimizador:</b> Algoritmo de ajuste. Adam es general. SGD es más simple. RMSprop es bueno en secuencias.</li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            widgets.HBox([metodo_sel]),\n",
        "            widgets.HBox([capas, neuronas]),\n",
        "            widgets.HBox([activacion, opt]),\n",
        "            widgets.HBox([tasa, epocas, batch]),\n",
        "            widgets.HBox([loss_fn]),\n",
        "            btn_train, tiempo_lbl,\n",
        "            ayuda, output_area\n",
        "        ]))\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 7.3. ENTRENAMIENTO XGBOOST - FUNCIONA CORRECTAMENTE\n",
        "# Este módulo permite entrenar el modelo XGBoost con las variables seleccionadas\n",
        "# por diferentes métodos (Pearson, Spearman, Mutual Info, Boruta, UMAP, o Todos)\n",
        "# sobre los datos previamente segmentados como Train y evaluar sobre Test.\n",
        "# ===============================================================\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Widget principal de salida (si no existe)\n",
        "if 'out_xgb' not in globals():\n",
        "    out_xgb = widgets.Output()\n",
        "\n",
        "def mostrar_xgb(b=None):\n",
        "    if b is None:\n",
        "        display(out_xgb)\n",
        "\n",
        "    with out_xgb:\n",
        "        clear_output()\n",
        "\n",
        "        if 'X_train' not in globals() or 'X_test' not in globals():\n",
        "            display(widgets.HTML(\"\"\"<span style='color:red;'>❌ Primero debes segmentar los datos en train/test.</span>\"\"\"))\n",
        "            return\n",
        "\n",
        "        # Bloque para sincronizar variables individuales a partir del resumen RESUMEN_METODOS\n",
        "        if 'RESUMEN_METODOS' in globals():\n",
        "            for metodo, variables in RESUMEN_METODOS.items():\n",
        "                globals()[f\"selected_vars_{metodo.lower()}\"] = variables\n",
        "\n",
        "        metodo_sel = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selector:'\n",
        "        )\n",
        "        n_estimators = widgets.IntSlider(value=100, min=10, max=1000, step=10, description='Árboles:')\n",
        "        learning_rate = widgets.FloatLogSlider(value=0.1, base=10, min=-3, max=0, step=0.01, description='Learning Rate:')\n",
        "        max_depth = widgets.IntSlider(value=3, min=1, max=20, step=1, description='Profundidad:')\n",
        "        subsample = widgets.FloatSlider(value=1.0, min=0.1, max=1.0, step=0.1, description='Subsample:')\n",
        "\n",
        "        btn_train = widgets.Button(description=\"🚀 Entrenar XGBoost\", button_style='success')\n",
        "        output_area = widgets.Output()\n",
        "        tiempo_lbl = widgets.Label()\n",
        "\n",
        "        def entrenar_xgb(_):\n",
        "            output_area.clear_output()\n",
        "            resumen_modelos = []\n",
        "            inicio = time.time()\n",
        "\n",
        "            metodos = [metodo_sel.value] if metodo_sel.value != 'Todos' else ['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "            colores = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "            fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "            with output_area:\n",
        "                for i, metodo in enumerate(metodos):\n",
        "                    var_key = f\"selected_vars_{metodo.lower()}\"\n",
        "                    if var_key not in globals():\n",
        "                        print(f\"⚠️ No hay variables seleccionadas para el método: {metodo}\")\n",
        "                        continue\n",
        "                    selected_vars = globals()[var_key]\n",
        "\n",
        "                    Xtr, Xts = X_train[selected_vars], X_test[selected_vars]\n",
        "                    ytr, yts = Y_train.values.ravel(), Y_test.values.ravel()\n",
        "\n",
        "                    sx, sy = StandardScaler(), StandardScaler()\n",
        "                    Xtr_scaled = sx.fit_transform(Xtr)\n",
        "                    Xts_scaled = sx.transform(Xts)\n",
        "                    ytr_scaled = sy.fit_transform(ytr.reshape(-1,1)).ravel()\n",
        "\n",
        "                    model = XGBRegressor(\n",
        "                        n_estimators=n_estimators.value,\n",
        "                        learning_rate=learning_rate.value,\n",
        "                        max_depth=max_depth.value,\n",
        "                        subsample=subsample.value,\n",
        "                        verbosity=0\n",
        "                    )\n",
        "                    model.fit(Xtr_scaled, ytr_scaled)\n",
        "\n",
        "                    y_pred_scaled = model.predict(Xts_scaled)\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "\n",
        "                    r2 = r2_score(yts, y_pred)\n",
        "                    mse = mean_squared_error(yts, y_pred)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    mae = mean_absolute_error(yts, y_pred)\n",
        "\n",
        "                    resumen_modelos.append({\n",
        "                        'Método': metodo,\n",
        "                        'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae\n",
        "                    })\n",
        "\n",
        "                    # Guardar modelo\n",
        "                    nombre_archivo = f\"modelo_xgb_{metodo.lower()}.pkl\"\n",
        "                    with open(nombre_archivo, 'wb') as f:\n",
        "                        pickle.dump({'model': model, 'sx': sx, 'sy': sy, 'cols': selected_vars, 'yname': y_variable_name}, f)\n",
        "\n",
        "                    ax.plot(y_pred, label=f'{metodo}', alpha=0.7, linestyle='--', color=colores[i])\n",
        "                    print(f\"✅ Modelo {metodo} entrenado. R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "                if resumen_modelos:\n",
        "                    ax.plot(yts, label='Y Real', color='black', linewidth=2)\n",
        "                    ax.set_title('Comparación Y Real vs Predicciones XGBoost por Método')\n",
        "                    ax.grid(); ax.legend()\n",
        "                    plt.show()\n",
        "\n",
        "                    print(\"\\n📊 Resumen comparativo de métricas:\")\n",
        "                    df = pd.DataFrame(resumen_modelos)\n",
        "                    display(df.set_index('Método'))\n",
        "\n",
        "            tiempo_lbl.value = f\"⏱️ Duración total: {time.time()-inicio:.2f} segundos\"\n",
        "\n",
        "        btn_train.on_click(entrenar_xgb)\n",
        "\n",
        "        ayuda = widgets.HTML(\"\"\"\n",
        "        <h4>ℹ️ Ayuda - Parámetros del modelo XGBoost</h4>\n",
        "        <ul>\n",
        "            <li><b>n_estimators:</b> número de árboles. Mayor número = mayor precisión pero más tiempo.</li>\n",
        "            <li><b>learning_rate:</b> tasa de aprendizaje. Pequeños valores mejoran precisión, pero requieren más árboles.</li>\n",
        "            <li><b>max_depth:</b> profundidad de árboles. Mayor profundidad permite más complejidad, pero riesgo de sobreajuste.</li>\n",
        "            <li><b>subsample:</b> fracción de muestras usadas por árbol. Menor valor ayuda a regularización.</li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            widgets.HBox([metodo_sel]),\n",
        "            widgets.HBox([n_estimators, learning_rate]),\n",
        "            widgets.HBox([max_depth, subsample]),\n",
        "            btn_train, tiempo_lbl,\n",
        "            ayuda, output_area\n",
        "        ]))\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 7.4. ENTRENAMIENTO RANDOM FOREST - FUNCIONA CORRECTAMENTE\n",
        "# Este módulo permite entrenar Random Forest con variables seleccionadas\n",
        "# por diferentes métodos (Pearson, Spearman, Mutual Info, Boruta, UMAP, o Todos),\n",
        "# usando datos de entrenamiento y validando sobre test.\n",
        "# ===============================================================\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Widget principal de salida (si no existe)\n",
        "if 'out_rf' not in globals():\n",
        "    out_rf = widgets.Output()\n",
        "\n",
        "def mostrar_rf(b=None):\n",
        "    if b is None:\n",
        "        display(out_rf)\n",
        "\n",
        "    with out_rf:\n",
        "        clear_output()\n",
        "\n",
        "        if 'X_train' not in globals() or 'X_test' not in globals():\n",
        "            display(widgets.HTML(\"\"\"<span style='color:red;'>❌ Primero debes segmentar los datos en train/test.</span>\"\"\"))\n",
        "            return\n",
        "\n",
        "        if 'RESUMEN_METODOS' in globals():\n",
        "            for metodo, variables in RESUMEN_METODOS.items():\n",
        "                globals()[f\"selected_vars_{metodo.lower()}\"] = variables\n",
        "\n",
        "        metodo_sel = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selector:'\n",
        "        )\n",
        "\n",
        "        n_estimators = widgets.IntSlider(value=100, min=10, max=500, step=10, description='Árboles:')\n",
        "        max_depth = widgets.IntSlider(value=5, min=1, max=50, step=1, description='Profundidad:')\n",
        "        min_samples_split = widgets.IntSlider(value=2, min=2, max=20, step=1, description='Min Split:')\n",
        "        min_samples_leaf = widgets.IntSlider(value=1, min=1, max=20, step=1, description='Min Leaf:')\n",
        "        max_features = widgets.Dropdown(options=['auto', 'sqrt', 'log2'], value='sqrt', description='Max Features:')\n",
        "        bootstrap = widgets.Checkbox(value=True, description='Bootstrap:')\n",
        "\n",
        "        btn_train = widgets.Button(description=\"🚀 Entrenar RF\", button_style='success')\n",
        "        output_area = widgets.Output()\n",
        "        tiempo_lbl = widgets.Label()\n",
        "\n",
        "        def entrenar_rf(_):\n",
        "            output_area.clear_output()\n",
        "            resumen_modelos = []\n",
        "            inicio = time.time()\n",
        "\n",
        "            metodos = [metodo_sel.value] if metodo_sel.value != 'Todos' else ['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "            colores = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "            fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "            with output_area:\n",
        "                for i, metodo in enumerate(metodos):\n",
        "                    var_key = f\"selected_vars_{metodo.lower()}\"\n",
        "                    if var_key not in globals():\n",
        "                        print(f\"⚠️ No hay variables seleccionadas para el método: {metodo}\")\n",
        "                        continue\n",
        "                    selected_vars = globals()[var_key]\n",
        "\n",
        "                    Xtr, Xts = X_train[selected_vars], X_test[selected_vars]\n",
        "                    ytr, yts = Y_train.values.ravel(), Y_test.values.ravel()\n",
        "\n",
        "                    sx, sy = StandardScaler(), StandardScaler()\n",
        "                    Xtr_scaled = sx.fit_transform(Xtr)\n",
        "                    Xts_scaled = sx.transform(Xts)\n",
        "                    ytr_scaled = sy.fit_transform(ytr.reshape(-1,1)).ravel()\n",
        "\n",
        "                    model = RandomForestRegressor(\n",
        "                        n_estimators=n_estimators.value,\n",
        "                        max_depth=max_depth.value,\n",
        "                        min_samples_split=min_samples_split.value,\n",
        "                        min_samples_leaf=min_samples_leaf.value,\n",
        "                        max_features=max_features.value,\n",
        "                        bootstrap=bootstrap.value,\n",
        "                        random_state=42,\n",
        "                        n_jobs=-1\n",
        "                    )\n",
        "                    model.fit(Xtr_scaled, ytr_scaled)\n",
        "                    y_pred_scaled = model.predict(Xts_scaled)\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "\n",
        "                    r2 = r2_score(yts, y_pred)\n",
        "                    mse = mean_squared_error(yts, y_pred)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    mae = mean_absolute_error(yts, y_pred)\n",
        "\n",
        "                    resumen_modelos.append({\n",
        "                        'Método': metodo,\n",
        "                        'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae\n",
        "                    })\n",
        "\n",
        "                    nombre_archivo = f\"modelo_rf_{metodo.lower()}.pkl\"\n",
        "                    with open(nombre_archivo, 'wb') as f:\n",
        "                        pickle.dump({'model': model, 'sx': sx, 'sy': sy, 'cols': selected_vars, 'yname': y_variable_name}, f)\n",
        "\n",
        "                    ax.plot(y_pred, label=f'{metodo}', alpha=0.7, linestyle='--', color=colores[i])\n",
        "                    print(f\"✅ Modelo {metodo} entrenado. R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "                if resumen_modelos:\n",
        "                    ax.plot(yts, label='Y Real', color='black', linewidth=2)\n",
        "                    ax.set_title('Comparación Y Real vs Predicciones Random Forest por Método')\n",
        "                    ax.grid(); ax.legend()\n",
        "                    plt.show()\n",
        "\n",
        "                    print(\"\\n📊 Resumen comparativo de métricas:\")\n",
        "                    df = pd.DataFrame(resumen_modelos)\n",
        "                    display(df.set_index('Método'))\n",
        "\n",
        "            tiempo_lbl.value = f\"⏱️ Duración total: {time.time()-inicio:.2f} segundos\"\n",
        "\n",
        "        btn_train.on_click(entrenar_rf)\n",
        "\n",
        "        ayuda = widgets.HTML(\"\"\"\n",
        "        <h4>ℹ️ Ayuda - Parámetros del modelo Random Forest</h4>\n",
        "        <ul>\n",
        "            <li><b>n_estimators:</b> número de árboles. Mayor número = mejor precisión, mayor tiempo.</li>\n",
        "            <li><b>max_depth:</b> profundidad máxima. Limita la complejidad del modelo.</li>\n",
        "            <li><b>min_samples_split:</b> tamaño mínimo para dividir nodo. Mayor = menos sobreajuste.</li>\n",
        "            <li><b>min_samples_leaf:</b> mínimo de muestras en hoja. Controla profundidad mínima.</li>\n",
        "            <li><b>max_features:</b> nº de variables evaluadas por división. 'sqrt' es típico para regresión.</li>\n",
        "            <li><b>bootstrap:</b> si se usan muestras con reemplazo. True mejora diversidad.</li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            widgets.HBox([metodo_sel]),\n",
        "            widgets.HBox([n_estimators, max_depth]),\n",
        "            widgets.HBox([min_samples_split, min_samples_leaf]),\n",
        "            widgets.HBox([max_features, bootstrap]),\n",
        "            btn_train, tiempo_lbl,\n",
        "            ayuda, output_area\n",
        "        ]))\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 7.5. ENTRENAMIENTO REDES NEURONALES RECURRENTES (RNN) - FUNCIONA CORRECTAMENTE\n",
        "# Este módulo permite entrenar distintos tipos de RNN sobre los datos\n",
        "# segmentados (X_train, X_test, Y_train, Y_test, FECHAS_train, FECHAS_test),\n",
        "# con variables seleccionadas por cada método (Pearson, Spearman, etc.).\n",
        "# ===============================================================\n",
        "import time, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Bidirectional, TimeDistributed, RepeatVector\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widget de salida global\n",
        "if 'out_rnn' not in globals():\n",
        "    out_rnn = widgets.Output()\n",
        "\n",
        "# Output de resultados (asegura persistencia)\n",
        "if 'output_area_rnn' not in globals():\n",
        "    output_area_rnn = widgets.Output()\n",
        "\n",
        "def mostrar_rnn(b=None):\n",
        "    if b is None:\n",
        "        display(out_rnn)\n",
        "\n",
        "    with out_rnn:\n",
        "        clear_output()\n",
        "\n",
        "        if not all(k in globals() for k in ['X_train', 'X_test', 'Y_train', 'Y_test', 'FECHAS_train', 'FECHAS_test']):\n",
        "            display(widgets.HTML(\"\"\"<span style='color:red;'>❌ Primero debes segmentar los datos en train/test incluyendo fechas.</span>\"\"\"))\n",
        "            return\n",
        "\n",
        "        # Actualizar variables seleccionadas si existen\n",
        "        if 'RESUMEN_METODOS' in globals():\n",
        "            for metodo, variables in RESUMEN_METODOS.items():\n",
        "                globals()[f\"selected_vars_{metodo.lower()}\"] = variables\n",
        "\n",
        "        metodo_sel = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selector:'\n",
        "        )\n",
        "        tipo_rnn = widgets.Dropdown(\n",
        "            options=['Vanilla RNN', 'LSTM', 'GRU', 'BiLSTM', 'BiGRU', 'Deep RNN', 'Encoder-Decoder'],\n",
        "            description='Tipo RNN:'\n",
        "        )\n",
        "        window_size = widgets.IntSlider(value=10, min=5, max=100, description='Ventana:')\n",
        "        units = widgets.IntSlider(value=50, min=10, max=200, step=10, description='Unidades:')\n",
        "        layers = widgets.IntSlider(value=1, min=1, max=5, description='Capas:')\n",
        "        batch = widgets.IntSlider(value=32, min=8, max=128, step=8, description='Batch:')\n",
        "        epochs = widgets.IntSlider(value=30, min=10, max=500, step=10, description='Épocas:')\n",
        "        learning_rate = widgets.FloatLogSlider(value=0.001, base=10, min=-5, max=-1, step=0.1, description='LR:')\n",
        "        optimizer = widgets.Dropdown(options=['adam', 'sgd', 'rmsprop', 'adagrad'], description='Optimizador:')\n",
        "        loss_fn = widgets.Dropdown(options=['mse', 'mae', 'huber'], description='Pérdida:')\n",
        "        activation = widgets.Dropdown(options=['tanh', 'relu', 'sigmoid'], description='Activación:')\n",
        "        drop = widgets.FloatSlider(value=0.0, min=0.0, max=0.5, step=0.05, description='Dropout:')\n",
        "        boton_entrenar = widgets.Button(description='🚀 Entrenar RNN', button_style='success')\n",
        "        tiempo_lbl = widgets.Label()\n",
        "\n",
        "        def entrenar_rnn(_):\n",
        "            output_area_rnn.clear_output()\n",
        "            resumen_modelos = []\n",
        "            inicio = time.time()\n",
        "\n",
        "            metodos = [metodo_sel.value] if metodo_sel.value != 'Todos' else ['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "            colores = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "            fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "            for i, metodo in enumerate(metodos):\n",
        "                var_key = f\"selected_vars_{metodo.lower()}\"\n",
        "                if var_key not in globals():\n",
        "                    with output_area_rnn:\n",
        "                        print(f\"⚠️ No hay variables seleccionadas para el método: {metodo}\")\n",
        "                    continue\n",
        "                selected_vars = globals()[var_key]\n",
        "\n",
        "                df_train = X_train[selected_vars].copy()\n",
        "                df_test = X_test[selected_vars].copy()\n",
        "                y_train = Y_train.values.ravel()\n",
        "                y_test = Y_test.values.ravel()\n",
        "\n",
        "                sx, sy = StandardScaler(), StandardScaler()\n",
        "                Xtr = sx.fit_transform(df_train)\n",
        "                Xts = sx.transform(df_test)\n",
        "                ytr = sy.fit_transform(y_train.reshape(-1,1)).ravel()\n",
        "\n",
        "                def create_sequences(X, Y, window):\n",
        "                    X_seq, Y_seq = [], []\n",
        "                    for j in range(len(X) - window):\n",
        "                        X_seq.append(X[j:j+window])\n",
        "                        Y_seq.append(Y[j+window])\n",
        "                    return np.array(X_seq), np.array(Y_seq)\n",
        "\n",
        "                X_seq, Y_seq = create_sequences(Xtr, ytr, window_size.value)\n",
        "                Xts_seq, Yts_seq = create_sequences(Xts, y_test, window_size.value)\n",
        "                input_shape = (X_seq.shape[1], X_seq.shape[2])\n",
        "\n",
        "                # ——————————————————————————————————————————————————————\n",
        "                # Exportar las secuencias al namespace global para el motor IG\n",
        "                globals()['X_seq'] = X_seq.copy()\n",
        "                globals()['Y_seq'] = Y_seq.copy()\n",
        "                # ——————————————————————————————————————————————————————\n",
        "\n",
        "                model = Sequential()\n",
        "                RNNLayer = {\n",
        "                    'Vanilla RNN': SimpleRNN,\n",
        "                    'LSTM': LSTM,\n",
        "                    'GRU': GRU\n",
        "                }.get(tipo_rnn.value.split()[0], LSTM)\n",
        "\n",
        "                if 'Bi' in tipo_rnn.value:\n",
        "                    for _ in range(layers.value - 1):\n",
        "                        model.add(Bidirectional(RNNLayer(units.value, activation=activation.value, return_sequences=True), input_shape=input_shape))\n",
        "                    model.add(Bidirectional(RNNLayer(units.value, activation=activation.value)))\n",
        "                elif 'Deep' in tipo_rnn.value:\n",
        "                    for _ in range(layers.value - 1):\n",
        "                        model.add(RNNLayer(units.value, activation=activation.value, return_sequences=True))\n",
        "                    model.add(RNNLayer(units.value, activation=activation.value))\n",
        "                elif 'Encoder' in tipo_rnn.value:\n",
        "                    model.add(LSTM(units.value, activation=activation.value, input_shape=input_shape))\n",
        "                    model.add(RepeatVector(1))\n",
        "                    model.add(LSTM(units.value, activation=activation.value, return_sequences=True))\n",
        "                    model.add(TimeDistributed(Dense(1)))\n",
        "                else:\n",
        "                    for _ in range(layers.value - 1):\n",
        "                        model.add(RNNLayer(units.value, activation=activation.value, return_sequences=True, input_shape=input_shape))\n",
        "                    model.add(RNNLayer(units.value, activation=activation.value))\n",
        "\n",
        "                if 'Encoder' not in tipo_rnn.value:\n",
        "                    model.add(Dense(1))\n",
        "\n",
        "                opt_dict = {\n",
        "                    'adam': Adam(learning_rate=learning_rate.value),\n",
        "                    'sgd': SGD(learning_rate=learning_rate.value),\n",
        "                    'rmsprop': RMSprop(learning_rate=learning_rate.value),\n",
        "                    'adagrad': Adagrad(learning_rate=learning_rate.value)\n",
        "                }\n",
        "\n",
        "                #model.compile(loss=loss_fn.value, optimizer=opt_dict[optimizer.value])\n",
        "                from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, Huber\n",
        "\n",
        "                loss_function = {\n",
        "                    'mse': MeanSquaredError(),\n",
        "                    'mae': MeanAbsoluteError(),\n",
        "                    'huber': Huber()\n",
        "                }[loss_fn.value]\n",
        "\n",
        "                model.compile(loss=loss_function, optimizer=opt_dict[optimizer.value])\n",
        "\n",
        "                model.fit(X_seq, Y_seq, epochs=epochs.value, batch_size=batch.value, verbose=0)\n",
        "                Y_pred_scaled = model.predict(Xts_seq).ravel()\n",
        "                Y_pred = sy.inverse_transform(Y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                Y_real = Yts_seq\n",
        "\n",
        "                r2 = r2_score(Y_real, Y_pred)\n",
        "                mse = mean_squared_error(Y_real, Y_pred)\n",
        "                rmse = np.sqrt(mse)\n",
        "                mae = mean_absolute_error(Y_real, Y_pred)\n",
        "\n",
        "                resumen_modelos.append({\n",
        "                    'Método': metodo,\n",
        "                    'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae\n",
        "                })\n",
        "\n",
        "                # Dentro de la función entrenar_rnn, después de model.fit(...) y antes de model.save(...):\n",
        "                hp = {\n",
        "                    'tipo_rnn': tipo_rnn.value,\n",
        "                    'window_size': window_size.value,\n",
        "                    'units': units.value,\n",
        "                    'layers': layers.value,\n",
        "                    'batch_size': batch.value,\n",
        "                    'epochs': epochs.value,\n",
        "                    'learning_rate': learning_rate.value,\n",
        "                    'optimizer': optimizer.value,\n",
        "                    'loss_fn': loss_fn.value,\n",
        "                    'activation': activation.value,\n",
        "                    'dropout': drop.value\n",
        "                }\n",
        "                # Serializar en pickle:\n",
        "                import pickle\n",
        "                hp_fname = f\"hyperparams_rnn_{metodo.lower()}.pkl\"\n",
        "                with open(hp_fname, \"wb\") as f_hp:\n",
        "                    pickle.dump(hp, f_hp)\n",
        "                print(f\"✅ Hiperparámetros RNN guardados en {hp_fname}\")\n",
        "\n",
        "                nombre_archivo = f\"modelo_rnn_{metodo.lower()}.h5\"\n",
        "                model.save(nombre_archivo)\n",
        "\n",
        "                with open(f\"escaladores_rnn_{metodo.lower()}.pkl\", \"wb\") as f:\n",
        "                    pickle.dump({\n",
        "                        'scaler_X': sx,\n",
        "                        'scaler_Y': sy,\n",
        "                        'cols': selected_vars,\n",
        "                        'yname': y_variable_name\n",
        "                    }, f)\n",
        "\n",
        "                ax.plot(Y_pred, label=f'{metodo}', alpha=0.7, linestyle='--', color=colores[i])\n",
        "\n",
        "                with output_area_rnn:\n",
        "                    print(f\"✅ Modelo {metodo} entrenado. R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "            if resumen_modelos:\n",
        "                ax.plot(Y_real, label='Y Real', color='black', linewidth=2)\n",
        "                ax.set_title('Comparación Y Real vs Predicciones RNN por Método')\n",
        "                ax.grid(); ax.legend()\n",
        "\n",
        "                with output_area_rnn:\n",
        "                    plt.show()\n",
        "                    print(\"\\n📊 Resumen comparativo de métricas:\")\n",
        "                    df = pd.DataFrame(resumen_modelos)\n",
        "                    display(df.set_index('Método'))\n",
        "\n",
        "            tiempo_lbl.value = f\"⏱️ Duración total: {time.time()-inicio:.2f} segundos\"\n",
        "\n",
        "        boton_entrenar.on_click(entrenar_rnn)\n",
        "\n",
        "        ayuda = widgets.HTML(\"\"\"\n",
        "        <h4>ℹ️ Ayuda - Parámetros del modelo RNN</h4>\n",
        "        <ul>\n",
        "            <li><b>Tipo RNN:</b> Define la arquitectura de red. LSTM y GRU son recomendados para series con dependencia larga.</li>\n",
        "            <li><b>Ventana:</b> Número de pasos de tiempo usados para predecir el siguiente valor.</li>\n",
        "            <li><b>Capas / Unidades:</b> Más capas y unidades aumentan capacidad, pero también el riesgo de sobreajuste.</li>\n",
        "            <li><b>Batch, Epochs:</b> Controlan tamaño del lote y número de iteraciones de entrenamiento.</li>\n",
        "            <li><b>LR:</b> Tasa de aprendizaje. Valores muy altos o bajos pueden afectar la convergencia.</li>\n",
        "            <li><b>Dropout:</b> Regulariza y previene sobreajuste. 0.1–0.3 común.</li>\n",
        "        </ul>\n",
        "        \"\"\")\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_sel,\n",
        "            widgets.HBox([tipo_rnn, window_size]),\n",
        "            widgets.HBox([units, layers, batch]),\n",
        "            widgets.HBox([epochs, learning_rate, optimizer]),\n",
        "            widgets.HBox([activation, drop, loss_fn]),\n",
        "            boton_entrenar, tiempo_lbl,\n",
        "            ayuda, output_area_rnn\n",
        "        ]))\n",
        "\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# 7.6. COMPARADOR DE MODELOS ENTRENADOS - FUNCIONA CORRECTAMENTE\n",
        "# Esta cell compara los resultados obtenidos en la ejecución de los entrenamientos anteriores (SVR, NN, XGBoost, Random Forest y RNN)\n",
        "# ===========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle, os\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "out_model_comparator = widgets.Output()\n",
        "\n",
        "# Botón para mostrar el comparador\n",
        "btn_lanzar_comparador = widgets.Button(description='📊 Comparar Modelos Entrenados', button_style='success')\n",
        "\n",
        "def mostrar_comparador_modelos(b=None):\n",
        "    with out_model_comparator:\n",
        "        clear_output()\n",
        "        display(btn_lanzar_comparador)\n",
        "\n",
        "# Acción al pulsar botón de ejecutar comparador\n",
        "\n",
        "def ejecutar_comparador(b=None):\n",
        "    with out_model_comparator:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'>📊 Comparador de Modelos Entrenados</h3>\n",
        "        <p>Este comparador analiza los modelos entrenados con distintos algoritmos y métodos de selección de variables.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        ruta_modelos = '.'\n",
        "        modelos_validos = []\n",
        "\n",
        "        print(\"🔍 Buscando modelos guardados...\")\n",
        "        for archivo in os.listdir(ruta_modelos):\n",
        "            if archivo.startswith(\"modelo_\") and (archivo.endswith(\".pkl\") or archivo.endswith(\".h5\")):\n",
        "                nombre_modelo = archivo.replace(\"modelo_\", \"\").replace(\".pkl\", \"\").replace(\".h5\", \"\")\n",
        "                try:\n",
        "                    if archivo.endswith(\".pkl\"):\n",
        "                        with open(os.path.join(ruta_modelos, archivo), 'rb') as f:\n",
        "                            modelo_guardado = pickle.load(f)\n",
        "                        modelo = modelo_guardado.get('model')\n",
        "                        scaler_X = modelo_guardado.get('scaler_X', modelo_guardado.get('sx'))\n",
        "                        scaler_Y = modelo_guardado.get('scaler_Y', modelo_guardado.get('sy'))\n",
        "                        cols = modelo_guardado.get('cols')\n",
        "                        yname = modelo_guardado.get('yname')\n",
        "                        if scaler_X is None or scaler_Y is None:\n",
        "                            raise KeyError(\"Faltan escaladores\")\n",
        "                    else:\n",
        "                        modelo = load_model(os.path.join(ruta_modelos, archivo))\n",
        "                        escalador_path = f\"escaladores_{nombre_modelo}.pkl\"\n",
        "                        with open(escalador_path, 'rb') as f:\n",
        "                            escaladores = pickle.load(f)\n",
        "                        scaler_X = escaladores['scaler_X']\n",
        "                        scaler_Y = escaladores['scaler_Y']\n",
        "                        cols = escaladores['cols']\n",
        "                        yname = escaladores['yname']\n",
        "\n",
        "                    modelos_validos.append({\n",
        "                        'nombre': nombre_modelo,\n",
        "                        'modelo': modelo,\n",
        "                        'scaler_X': scaler_X,\n",
        "                        'scaler_Y': scaler_Y,\n",
        "                        'cols': cols,\n",
        "                        'yname': yname\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error al procesar {archivo}: {e}\")\n",
        "\n",
        "        if not modelos_validos:\n",
        "            display(HTML(\"<b style='color:red;'>⚠️ No se encontraron modelos entrenados válidos.</b>\"))\n",
        "            return\n",
        "\n",
        "        resultados = []\n",
        "        detalles_modelos = []\n",
        "\n",
        "        for entry in modelos_validos:\n",
        "            nombre = entry['nombre']\n",
        "            model = entry['modelo']\n",
        "            sx = entry['scaler_X']\n",
        "            sy = entry['scaler_Y']\n",
        "            cols = entry['cols']\n",
        "            yname = entry['yname']\n",
        "\n",
        "            Xtest = X_test[cols].copy()\n",
        "            ytest = Y_test.values.ravel()\n",
        "\n",
        "            # Detectar si es un modelo RNN por la forma esperada\n",
        "            try:\n",
        "                input_shape = model.input_shape\n",
        "                is_rnn = len(input_shape) == 3\n",
        "            except:\n",
        "                is_rnn = False\n",
        "\n",
        "            if is_rnn:\n",
        "                window = input_shape[1]\n",
        "                X_full = X_test[cols].copy()\n",
        "                y_full = Y_test.values.ravel()\n",
        "                X_scaled = sx.transform(X_full)\n",
        "                y_scaled = sy.transform(y_full.reshape(-1, 1)).ravel()\n",
        "\n",
        "                X_seq, y_seq = [], []\n",
        "                for i in range(len(X_scaled) - window):\n",
        "                    X_seq.append(X_scaled[i:i+window])\n",
        "                    y_seq.append(y_full[i+window])\n",
        "                X_seq = np.array(X_seq)\n",
        "                y_seq = np.array(y_seq)\n",
        "\n",
        "                pred_scaled = model.predict(X_seq).ravel()\n",
        "                pred = sy.inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
        "                ytest = y_seq  # ajustar Y real a misma longitud\n",
        "            else:\n",
        "                Xtest_scaled = sx.transform(Xtest)\n",
        "                pred_scaled = model.predict(Xtest_scaled)\n",
        "                if isinstance(pred_scaled, tuple): pred_scaled = pred_scaled[0]\n",
        "                pred = sy.inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
        "                ytest = ytest  # no cambia\n",
        "\n",
        "            r2 = r2_score(ytest, pred)\n",
        "            mse = mean_squared_error(ytest, pred)\n",
        "            rmse = np.sqrt(mse)\n",
        "            mae = mean_absolute_error(ytest, pred)\n",
        "\n",
        "            resultados.append({\n",
        "                'Modelo': nombre,\n",
        "                'R2': r2,\n",
        "                'MSE': mse,\n",
        "                'RMSE': rmse,\n",
        "                'MAE': mae,\n",
        "                'Pred': pred\n",
        "            })\n",
        "\n",
        "            detalles_modelos.append({\n",
        "                'Modelo': nombre,\n",
        "                'Variables X': ', '.join(cols),\n",
        "                'Variable Y': yname,\n",
        "                'Nº Variables': len(cols),\n",
        "                'Tipo Modelo': nombre.split('_')[0].upper(),\n",
        "                'Método Selección': nombre.split('_')[-1].capitalize()\n",
        "            })\n",
        "\n",
        "        df_resultados = pd.DataFrame(resultados)\n",
        "        df_detalles = pd.DataFrame(detalles_modelos)\n",
        "\n",
        "        display(HTML(\"<h4>📋 Comparativa de Resultados</h4>\"))\n",
        "        display(df_resultados[['Modelo', 'R2', 'RMSE', 'MAE', 'MSE']]\n",
        "                .sort_values(by='R2', ascending=False)\n",
        "                .style.set_caption(\"Ranking de Modelos por R²\")\n",
        "                .set_properties(**{'border': '1px solid gray', 'padding': '6px'})\n",
        "                .set_table_styles([\n",
        "                    {'selector': 'th', 'props': [('background-color', '#f0f0f0'), ('font-weight', 'bold')]},\n",
        "                ]))\n",
        "\n",
        "        df_sorted = df_resultados.sort_values(by='R2', ascending=False)\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        colors = ['gold', 'silver', 'peru'] + ['lightblue']*(len(df_sorted)-3)\n",
        "        ax.barh(df_sorted['Modelo'], df_sorted['R2'], color=colors[:len(df_sorted)])\n",
        "        ax.set_title(\"🏆 Ranking de Modelos por R²\")\n",
        "        ax.invert_yaxis()\n",
        "        for i, v in enumerate(df_sorted['R2']):\n",
        "            ax.text(v + 0.01, i, f\"{v:.3f}\", va='center')\n",
        "        plt.grid(axis='x')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        display(HTML(\"<h4>🔧 Parámetros de Entrenamiento de los Modelos</h4>\"))\n",
        "        display(df_detalles.style.set_caption(\"Resumen de Configuración de Modelos\")\n",
        "                .set_properties(**{'border': '1px solid gray', 'padding': '6px'})\n",
        "                .set_table_styles([\n",
        "                    {'selector': 'th', 'props': [('background-color', '#e0f7fa'), ('font-weight', 'bold')]},\n",
        "                ]))\n",
        "\n",
        "        # Mostrar tabla con parámetros técnicos (si existen)\n",
        "        tabla_parametros = []\n",
        "\n",
        "        for entry in modelos_validos:\n",
        "            nombre = entry['nombre']\n",
        "            modelo = entry['modelo']\n",
        "\n",
        "            fila = {'Modelo': nombre}\n",
        "            try:\n",
        "                if hasattr(modelo, 'get_params'):\n",
        "                    fila.update(modelo.get_params())\n",
        "                elif isinstance(modelo, tf.keras.Model):\n",
        "                    config = modelo.get_config()\n",
        "                    fila['Capas'] = len(config['layers'])\n",
        "                    fila['Optimizador'] = config.get('optimizer_config', {}).get('class_name', 'Desconocido')\n",
        "                    fila['Pérdida'] = config.get('loss', 'Desconocida')\n",
        "                else:\n",
        "                    fila['Info'] = '⚠️ Tipo de modelo no reconocido'\n",
        "            except Exception as e:\n",
        "                fila['Error'] = str(e)\n",
        "\n",
        "            tabla_parametros.append(fila)\n",
        "\n",
        "        df_parametros = pd.DataFrame(tabla_parametros)\n",
        "        display(HTML(\"<h4>🧾 Parámetros de Configuración (Detalles Técnicos)</h4>\"))\n",
        "        if not df_parametros.empty:\n",
        "            display(df_parametros.style.set_caption(\"Parámetros usados en cada modelo\")\n",
        "                    .set_properties(**{'border': '1px solid #ccc', 'padding': '4px'})\n",
        "                    .set_table_styles([{'selector': 'th', 'props': [('background-color', '#f8f8f8'), ('font-weight', 'bold')]}]))\n",
        "        else:\n",
        "            display(HTML(\"<i>No se pudieron recuperar parámetros para los modelos cargados.</i>\"))\n",
        "\n",
        "        display(HTML(\"<h4>📈 Gráficos Comparativos Y Real vs. Predicción</h4>\"))\n",
        "        for res in resultados:\n",
        "            nombre = res['Modelo']\n",
        "            pred = res['Pred']\n",
        "            fig, ax = plt.subplots(figsize=(10,4))\n",
        "            ax.plot(Y_test.values, label='Y Real', color='black')\n",
        "            ax.plot(pred, label=f'{nombre}', linestyle='--')\n",
        "            ax.set_title(f'{nombre}: Real vs Predicho')\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        # Tabla de parámetros completos si están disponibles\n",
        "        display(HTML(\"<h4>🧾 Parámetros de Configuración (Detalles Técnicos)</h4>\"))\n",
        "\n",
        "        tabla_parametros = []\n",
        "\n",
        "        for entry in modelos_validos:\n",
        "            modelo = entry['modelo']\n",
        "            nombre = entry['nombre']\n",
        "            tipo   = nombre.split('_')[0].upper()\n",
        "\n",
        "            if hasattr(modelo, 'get_params'):\n",
        "                try:\n",
        "                    params = modelo.get_params()\n",
        "                    tabla_parametros.append({\n",
        "                        'Modelo': nombre,\n",
        "                        **params\n",
        "                    })\n",
        "                except:\n",
        "                    tabla_parametros.append({'Modelo': nombre, 'Info': '⚠️ No se pudieron extraer los parámetros'})\n",
        "            elif isinstance(modelo, tf.keras.Model):\n",
        "                config = modelo.get_config()\n",
        "                tabla_parametros.append({\n",
        "                    'Modelo': nombre,\n",
        "                    'Capas': len(config['layers']),\n",
        "                    'Optim.': config.get('optimizer_config', {}).get('class_name', 'N/A'),\n",
        "                    'Loss': config.get('loss', 'N/A') if isinstance(config.get('loss'), str) else str(config.get('loss')),\n",
        "                    'Tipo': tipo\n",
        "                })\n",
        "            else:\n",
        "                tabla_parametros.append({'Modelo': nombre, 'Info': '⚠️ Modelo no compatible'})\n",
        "\n",
        "        df_params = pd.DataFrame(tabla_parametros)\n",
        "        display(df_params.style.set_caption(\"🛠️ Parámetros de Ajuste de Cada Modelo\")\n",
        "                .set_properties(**{'border': '1px solid #999', 'padding': '5px'})\n",
        "                .set_table_styles([{'selector': 'th', 'props': [('background-color', '#f9f9f9'), ('font-weight', 'bold')]}]))\n",
        "\n",
        "btn_lanzar_comparador.on_click(ejecutar_comparador)\n",
        "display(out_model_comparator)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8. PREDICCION Y VISUALIZACION DE MODELOS - PENDIENTE CORREGIR\n",
        "# Este módulo se descompone en varios sub-modulos para la realización de predicciones usando los diferentes modelos entrenados y visualizar los resultados de los modelos\n",
        "# ===============================================================\n",
        "# ===============================================================\n",
        "# 8.1. PREDICCIÓN SVR – FUNCIONA CORRECTAMENTE\n",
        "# Permite al usuario seleccionar método de selección, introducir datos\n",
        "# manualmente o generarlos automáticamente, y obtener predicciones SVR.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "import pickle, os\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "out_pred_svr = widgets.Output()\n",
        "\n",
        "def mostrar_prediccion_svr(b=None):\n",
        "    with out_pred_svr:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'>🔮 Predicción con Modelo SVR</h3>\n",
        "        <p>Este módulo permite realizar predicciones con modelos SVR entrenados previamente\n",
        "        utilizando variables seleccionadas automáticamente o introducidas manualmente.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selección de Variables:'\n",
        "        )\n",
        "\n",
        "        num_casos = widgets.BoundedIntText(value=5, min=1, max=100, description='Nº Casos:')\n",
        "\n",
        "        modo_datos = widgets.ToggleButtons(\n",
        "            options=['Automático', 'Manual'],\n",
        "            description='Modo de entrada:'\n",
        "        )\n",
        "\n",
        "        btn_generar = widgets.Button(description='➡️ Generar Variables X')\n",
        "        btn_predecir = widgets.Button(description='📈 Predecir Y', button_style='success')\n",
        "        btn_copiar = widgets.Button(description='📋 Copiar Tabla', button_style='info')\n",
        "\n",
        "        tabla_x = widgets.Output()\n",
        "        tabla_pred = widgets.Output()\n",
        "        grafico_pred = widgets.Output()\n",
        "        datos_generados = {}\n",
        "        resultados = {}\n",
        "\n",
        "        def generar_valores(_):\n",
        "            tabla_x.clear_output()\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            datos_generados.clear()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                nombre_modelo = f\"modelo_svr_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(nombre_modelo):\n",
        "                    continue\n",
        "                with open(nombre_modelo, 'rb') as f:\n",
        "                    modelo_dict = pickle.load(f)\n",
        "                cols = modelo_dict['cols']\n",
        "\n",
        "                df_x = pd.DataFrame()\n",
        "                for col in cols:\n",
        "                    if col not in X_data.columns or X_data[col].isnull().all():\n",
        "                        continue\n",
        "                    serie = X_data[col]\n",
        "                    minimo, maximo = serie.min(), serie.max()\n",
        "                    tendencia = 'asc' if serie.corr(Y_data.iloc[:, 0]) > 0 else 'desc'\n",
        "                    if modo_datos.value == 'Automático':\n",
        "                        vals = np.linspace(minimo, maximo, num_casos.value) if tendencia == 'asc' \\\n",
        "                            else np.linspace(maximo, minimo, num_casos.value)\n",
        "                        df_x[col] = vals\n",
        "                    else:\n",
        "                        df_x[col] = [widgets.FloatText(value=0.0, layout=widgets.Layout(width='80px')) for _ in range(num_casos.value)]\n",
        "                datos_generados[metodo] = df_x\n",
        "\n",
        "            with tabla_x:\n",
        "                clear_output()\n",
        "                display(HTML(f\"<h4>🧾 Variables X Generadas ({modo_datos.value})</h4>\"))\n",
        "                for metodo in metodos:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    df = datos_generados[metodo]\n",
        "                    display(HTML(f\"<b style='color:#2E8B57;'>{metodo}</b>\"))\n",
        "\n",
        "                    if isinstance(df, pd.DataFrame) and modo_datos.value == 'Automático':\n",
        "                        display(df)\n",
        "                    elif modo_datos.value == 'Manual':\n",
        "                        grid = widgets.GridspecLayout(num_casos.value + 1, len(df.columns))\n",
        "                        for j, col in enumerate(df.columns):\n",
        "                            grid[0, j] = widgets.Label(value=col)\n",
        "                            for i in range(num_casos.value):\n",
        "                                grid[i+1, j] = df[col][i]\n",
        "                        display(grid)\n",
        "\n",
        "        def realizar_prediccion(_):\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                if metodo not in datos_generados:\n",
        "                    continue\n",
        "                df = datos_generados[metodo]\n",
        "                nombre_modelo = f\"modelo_svr_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(nombre_modelo):\n",
        "                    continue\n",
        "                with open(nombre_modelo, 'rb') as f:\n",
        "                    modelo_dict = pickle.load(f)\n",
        "\n",
        "                sx, sy, model = modelo_dict['sx'], modelo_dict['sy'], modelo_dict['model']\n",
        "\n",
        "                if modo_datos.value == 'Manual':\n",
        "                    df_manual = pd.DataFrame()\n",
        "                    for col in df.columns:\n",
        "                        df_manual[col] = [w.value for w in df[col]]\n",
        "                    df_to_use = df_manual\n",
        "                else:\n",
        "                    df_to_use = df\n",
        "\n",
        "                x_scaled = sx.transform(df_to_use.values)\n",
        "                y_pred = sy.inverse_transform(model.predict(x_scaled).reshape(-1, 1)).ravel()\n",
        "                df_pred = df_to_use.copy()\n",
        "                df_pred['Y_pred'] = y_pred\n",
        "                resultados[metodo] = df_pred\n",
        "\n",
        "            tabla_pred.clear_output()\n",
        "            contenedor_tablas = []\n",
        "            for metodo in metodos:\n",
        "                if metodo in resultados:\n",
        "                    df = resultados[metodo]\n",
        "                    contenedor_tablas.append(HTML(f\"<h4 style='color:#2E8B57;'>🧮 {metodo}</h4>\"))\n",
        "                    contenedor_tablas.append(widgets.Output())\n",
        "                    with contenedor_tablas[-1]:\n",
        "                        display(df)\n",
        "\n",
        "            with tabla_pred:\n",
        "                clear_output()\n",
        "                display(HTML(\"<h3>📋 Resultados de la Predicción</h3>\"))\n",
        "                display(widgets.VBox(contenedor_tablas))\n",
        "\n",
        "            with grafico_pred:\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        y_vals = resultados[metodo]['Y_pred'].values\n",
        "                        if y_vals.size > 0:\n",
        "                            plt.plot(y_vals, label=str(metodo), linestyle='--')\n",
        "\n",
        "                plt.title(\"📊 Predicciones Y por Método\")\n",
        "                plt.xlabel(\"Caso\")\n",
        "                plt.ylabel(\"Y_predicho\")\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        def copiar_al_portapapeles(_):\n",
        "            from IPython.display import Javascript\n",
        "            contenido = \"\"\n",
        "            for metodo, df in resultados.items():\n",
        "                contenido += f\"\\n### {metodo}\\n\" + df.to_csv(index=False)\n",
        "            js_code = f\"navigator.clipboard.writeText(`{contenido}`)\"\n",
        "            display(Javascript(js_code))\n",
        "            print(\"✅ Tabla copiada al portapapeles.\")\n",
        "\n",
        "        btn_generar.on_click(generar_valores)\n",
        "        btn_predecir.on_click(realizar_prediccion)\n",
        "        btn_copiar.on_click(copiar_al_portapapeles)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            num_casos,\n",
        "            modo_datos,\n",
        "            btn_generar,\n",
        "            tabla_x,\n",
        "            btn_predecir,\n",
        "            tabla_pred,\n",
        "            grafico_pred,\n",
        "            btn_copiar\n",
        "        ]))\n",
        "\n",
        "# Mostrar en ejecución directa\n",
        "display(out_pred_svr)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8.2. PREDICCIÓN NN – FUNCIONA CORRECTAMENTE\n",
        "# Permite al usuario seleccionar método de selección, introducir datos\n",
        "# manualmente o generarlos automáticamente, y obtener predicciones NN.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "import pickle, os\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "out_pred_nn = widgets.Output()\n",
        "\n",
        "def mostrar_prediccion_nn(b=None):\n",
        "    with out_pred_nn:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'>🔮 Predicción con Red Neuronal</h3>\n",
        "        <p>Este módulo permite realizar predicciones con redes neuronales previamente entrenadas\n",
        "        utilizando variables seleccionadas automáticamente o introducidas manualmente.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selección de Variables:'\n",
        "        )\n",
        "\n",
        "        num_casos = widgets.BoundedIntText(value=5, min=1, max=100, description='Nº Casos:')\n",
        "\n",
        "        modo_datos = widgets.ToggleButtons(\n",
        "            options=['Automático', 'Manual'],\n",
        "            description='Modo de entrada:'\n",
        "        )\n",
        "\n",
        "        btn_generar = widgets.Button(description='➡️ Generar Variables X')\n",
        "        btn_predecir = widgets.Button(description='📈 Predecir Y', button_style='success')\n",
        "        btn_copiar = widgets.Button(description='📋 Copiar Tabla', button_style='info')\n",
        "\n",
        "        tabla_x = widgets.Output()\n",
        "        tabla_pred = widgets.Output()\n",
        "        grafico_pred = widgets.Output()\n",
        "        datos_generados = {}\n",
        "        resultados = {}\n",
        "\n",
        "        def generar_valores(_):\n",
        "            tabla_x.clear_output()\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            datos_generados.clear()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                modelo_path = f\"modelo_nn_{metodo.lower()}.h5\"\n",
        "                scaler_path = f\"escaladores_nn_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(modelo_path) or not os.path.exists(scaler_path):\n",
        "                    continue\n",
        "                with open(scaler_path, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                cols = datos['cols']\n",
        "\n",
        "                df_x = pd.DataFrame()\n",
        "                for col in cols:\n",
        "                    if col not in X_data.columns or X_data[col].isnull().all():\n",
        "                        continue\n",
        "                    serie = X_data[col]\n",
        "                    minimo, maximo = serie.min(), serie.max()\n",
        "                    tendencia = 'asc' if serie.corr(Y_data.iloc[:, 0]) > 0 else 'desc'\n",
        "                    if modo_datos.value == 'Automático':\n",
        "                        vals = np.linspace(minimo, maximo, num_casos.value) if tendencia == 'asc' \\\n",
        "                            else np.linspace(maximo, minimo, num_casos.value)\n",
        "                        df_x[col] = vals\n",
        "                    else:\n",
        "                        df_x[col] = [widgets.FloatText(value=0.0, layout=widgets.Layout(width='80px')) for _ in range(num_casos.value)]\n",
        "                datos_generados[metodo] = df_x\n",
        "\n",
        "            with tabla_x:\n",
        "                clear_output()\n",
        "                display(HTML(f\"<h4>🧾 Variables X Generadas ({modo_datos.value})</h4>\"))\n",
        "                for metodo in metodos:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    df = datos_generados[metodo]\n",
        "                    display(HTML(f\"<b style='color:#2E8B57;'>{metodo}</b>\"))\n",
        "                    if isinstance(df, pd.DataFrame) and modo_datos.value == 'Automático':\n",
        "                        display(df)\n",
        "                    elif modo_datos.value == 'Manual':\n",
        "                        grid = widgets.GridspecLayout(num_casos.value + 1, len(df.columns))\n",
        "                        for j, col in enumerate(df.columns):\n",
        "                            grid[0, j] = widgets.Label(value=col)\n",
        "                            for i in range(num_casos.value):\n",
        "                                grid[i+1, j] = df[col][i]\n",
        "                        display(grid)\n",
        "\n",
        "        def realizar_prediccion(_):\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                if metodo not in datos_generados:\n",
        "                    continue\n",
        "                df = datos_generados[metodo]\n",
        "                modelo_path = f\"modelo_nn_{metodo.lower()}.h5\"\n",
        "                scaler_path = f\"escaladores_nn_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(modelo_path) or not os.path.exists(scaler_path):\n",
        "                    continue\n",
        "\n",
        "                model = load_model(modelo_path)\n",
        "                with open(scaler_path, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                sx, sy = datos['scaler_X'], datos['scaler_Y']\n",
        "\n",
        "                if modo_datos.value == 'Manual':\n",
        "                    df_manual = pd.DataFrame()\n",
        "                    for col in df.columns:\n",
        "                        df_manual[col] = [w.value for w in df[col]]\n",
        "                    df_to_use = df_manual\n",
        "                else:\n",
        "                    df_to_use = df\n",
        "\n",
        "                x_scaled = sx.transform(df_to_use.values)\n",
        "                y_pred = sy.inverse_transform(model.predict(x_scaled)).ravel()\n",
        "                df_pred = df_to_use.copy()\n",
        "                df_pred['Y_pred'] = y_pred\n",
        "                resultados[metodo] = df_pred\n",
        "\n",
        "            tabla_pred.clear_output()\n",
        "            contenedor_tablas = []\n",
        "            for metodo in metodos:\n",
        "                if metodo in resultados:\n",
        "                    df = resultados[metodo]\n",
        "                    contenedor_tablas.append(HTML(f\"<h4 style='color:#2E8B57;'>🧮 {metodo}</h4>\"))\n",
        "                    contenedor_tablas.append(widgets.Output())\n",
        "                    with contenedor_tablas[-1]:\n",
        "                        display(df)\n",
        "\n",
        "            with tabla_pred:\n",
        "                clear_output()\n",
        "                display(HTML(\"<h3>📋 Resultados de la Predicción</h3>\"))\n",
        "                display(widgets.VBox(contenedor_tablas))\n",
        "\n",
        "            with grafico_pred:\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        plt.plot(resultados[metodo]['Y_pred'].values, label=metodo, linestyle='--')\n",
        "                plt.title(\"📊 Predicciones Y por Método (NN)\")\n",
        "                plt.xlabel(\"Caso\")\n",
        "                plt.ylabel(\"Y_predicho\")\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        def copiar_al_portapapeles(_):\n",
        "            from IPython.display import Javascript\n",
        "            contenido = \"\"\n",
        "            for metodo, df in resultados.items():\n",
        "                contenido += f\"\\n### {metodo}\\n\" + df.to_csv(index=False)\n",
        "            js_code = f\"navigator.clipboard.writeText(`{contenido}`)\"\n",
        "            display(Javascript(js_code))\n",
        "            print(\"✅ Tabla copiada al portapapeles.\")\n",
        "\n",
        "        btn_generar.on_click(generar_valores)\n",
        "        btn_predecir.on_click(realizar_prediccion)\n",
        "        btn_copiar.on_click(copiar_al_portapapeles)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            num_casos,\n",
        "            modo_datos,\n",
        "            btn_generar,\n",
        "            tabla_x,\n",
        "            btn_predecir,\n",
        "            tabla_pred,\n",
        "            grafico_pred,\n",
        "            btn_copiar\n",
        "        ]))\n",
        "\n",
        "# Mostrar en ejecución directa\n",
        "display(out_pred_nn)\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8.3. PREDICCIÓN XGBOOST – FUNCIONA CORRECTAMENTE\n",
        "# Permite al usuario seleccionar método de selección, introducir datos\n",
        "# manualmente o generarlos automáticamente, y obtener predicciones con XGBoost.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "import pickle, os\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "out_pred_xgb = widgets.Output()\n",
        "\n",
        "def mostrar_prediccion_xgboost(b=None):\n",
        "    with out_pred_xgb:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'>🔮 Predicción con XGBoost</h3>\n",
        "        <p>Este módulo permite realizar predicciones con modelos XGBoost previamente entrenados\n",
        "        utilizando variables seleccionadas automáticamente o introducidas manualmente.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selección de Variables:'\n",
        "        )\n",
        "\n",
        "        num_casos = widgets.BoundedIntText(value=5, min=1, max=100, description='Nº Casos:')\n",
        "\n",
        "        modo_datos = widgets.ToggleButtons(\n",
        "            options=['Automático', 'Manual'],\n",
        "            description='Modo de entrada:'\n",
        "        )\n",
        "\n",
        "        btn_generar = widgets.Button(description='➡️ Generar Variables X')\n",
        "        btn_predecir = widgets.Button(description='📈 Predecir Y', button_style='success')\n",
        "        btn_copiar = widgets.Button(description='📋 Copiar Tabla', button_style='info')\n",
        "\n",
        "        tabla_x = widgets.Output()\n",
        "        tabla_pred = widgets.Output()\n",
        "        grafico_pred = widgets.Output()\n",
        "        datos_generados = {}\n",
        "        resultados = {}\n",
        "\n",
        "        def generar_valores(_):\n",
        "            tabla_x.clear_output()\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            datos_generados.clear()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                path_model = f\"modelo_xgb_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(path_model):\n",
        "                    continue\n",
        "                with open(path_model, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                cols = datos['cols']\n",
        "\n",
        "                df_x = pd.DataFrame()\n",
        "                for col in cols:\n",
        "                    if col not in X_data.columns or X_data[col].isnull().all():\n",
        "                        continue\n",
        "                    serie = X_data[col]\n",
        "                    minimo, maximo = serie.min(), serie.max()\n",
        "                    tendencia = 'asc' if serie.corr(Y_data.iloc[:, 0]) > 0 else 'desc'\n",
        "                    if modo_datos.value == 'Automático':\n",
        "                        vals = np.linspace(minimo, maximo, num_casos.value) if tendencia == 'asc' \\\n",
        "                            else np.linspace(maximo, minimo, num_casos.value)\n",
        "                        df_x[col] = vals\n",
        "                    else:\n",
        "                        df_x[col] = [widgets.FloatText(value=0.0, layout=widgets.Layout(width='80px')) for _ in range(num_casos.value)]\n",
        "                datos_generados[metodo] = df_x\n",
        "\n",
        "            with tabla_x:\n",
        "                clear_output()\n",
        "                display(HTML(f\"<h4>🧾 Variables X Generadas ({modo_datos.value})</h4>\"))\n",
        "                for metodo in metodos:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    df = datos_generados[metodo]\n",
        "                    display(HTML(f\"<b style='color:#2E8B57;'>{metodo}</b>\"))\n",
        "                    if isinstance(df, pd.DataFrame) and modo_datos.value == 'Automático':\n",
        "                        display(df)\n",
        "                    elif modo_datos.value == 'Manual':\n",
        "                        grid = widgets.GridspecLayout(num_casos.value + 1, len(df.columns))\n",
        "                        for j, col in enumerate(df.columns):\n",
        "                            grid[0, j] = widgets.Label(value=col)\n",
        "                            for i in range(num_casos.value):\n",
        "                                grid[i+1, j] = df[col][i]\n",
        "                        display(grid)\n",
        "\n",
        "        def realizar_prediccion(_):\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                if metodo not in datos_generados:\n",
        "                    continue\n",
        "                df = datos_generados[metodo]\n",
        "                path_model = f\"modelo_xgb_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(path_model):\n",
        "                    continue\n",
        "\n",
        "                with open(path_model, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                model = datos['model']\n",
        "                sx, sy = datos['sx'], datos['sy']\n",
        "\n",
        "                if modo_datos.value == 'Manual':\n",
        "                    df_manual = pd.DataFrame()\n",
        "                    for col in df.columns:\n",
        "                        df_manual[col] = [w.value for w in df[col]]\n",
        "                    df_to_use = df_manual\n",
        "                else:\n",
        "                    df_to_use = df\n",
        "\n",
        "                x_scaled = sx.transform(df_to_use.values)\n",
        "                y_pred = sy.inverse_transform(model.predict(x_scaled).reshape(-1, 1)).ravel()\n",
        "                df_pred = df_to_use.copy()\n",
        "                df_pred['Y_pred'] = y_pred\n",
        "                resultados[metodo] = df_pred\n",
        "\n",
        "            contenedor_tablas = []\n",
        "            for metodo in metodos:\n",
        "                if metodo in resultados:\n",
        "                    df = resultados[metodo]\n",
        "                    contenedor_tablas.append(HTML(f\"<h4 style='color:#2E8B57;'>🧮 {metodo}</h4>\"))\n",
        "                    contenedor_tablas.append(widgets.Output())\n",
        "                    with contenedor_tablas[-1]:\n",
        "                        display(df)\n",
        "\n",
        "            with tabla_pred:\n",
        "                clear_output()\n",
        "                display(HTML(\"<h3>📋 Resultados de la Predicción (XGBoost)</h3>\"))\n",
        "                display(widgets.VBox(contenedor_tablas))\n",
        "\n",
        "            with grafico_pred:\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        plt.plot(resultados[metodo]['Y_pred'].values, label=metodo, linestyle='--')\n",
        "                plt.title(\"📊 Predicciones Y por Método (XGBoost)\")\n",
        "                plt.xlabel(\"Caso\")\n",
        "                plt.ylabel(\"Y_predicho\")\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        def copiar_al_portapapeles(_):\n",
        "            from IPython.display import Javascript\n",
        "            contenido = \"\"\n",
        "            for metodo, df in resultados.items():\n",
        "                contenido += f\"\\n### {metodo}\\n\" + df.to_csv(index=False)\n",
        "            js_code = f\"navigator.clipboard.writeText(`{contenido}`)\"\n",
        "            display(Javascript(js_code))\n",
        "            print(\"✅ Tabla copiada al portapapeles.\")\n",
        "\n",
        "        btn_generar.on_click(generar_valores)\n",
        "        btn_predecir.on_click(realizar_prediccion)\n",
        "        btn_copiar.on_click(copiar_al_portapapeles)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            num_casos,\n",
        "            modo_datos,\n",
        "            btn_generar,\n",
        "            tabla_x,\n",
        "            btn_predecir,\n",
        "            tabla_pred,\n",
        "            grafico_pred,\n",
        "            btn_copiar\n",
        "        ]))\n",
        "\n",
        "# Mostrar en ejecución directa\n",
        "display(out_pred_xgb)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8.4. PREDICCIÓN RANDOM FOREST – FUNCIONA CORRECTAMENTE\n",
        "# Permite al usuario seleccionar método de selección, introducir datos\n",
        "# manualmente o generarlos automáticamente, y obtener predicciones con RF.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "import pickle, os, traceback, time\n",
        "from IPython.display import display, HTML, clear_output, Javascript\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "out_pred_rf = widgets.Output()\n",
        "\n",
        "def mostrar_prediccion_rf(b=None):\n",
        "    with out_pred_rf:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'>🔮 Predicción con Random Forest</h3>\n",
        "        <p>Este módulo permite realizar predicciones con modelos Random Forest previamente entrenados\n",
        "        utilizando variables seleccionadas automáticamente o introducidas manualmente.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selección de Variables:'\n",
        "        )\n",
        "\n",
        "        num_casos = widgets.BoundedIntText(value=5, min=1, max=100, description='Nº Casos:')\n",
        "\n",
        "        modo_datos = widgets.ToggleButtons(\n",
        "            options=['Automático', 'Manual'],\n",
        "            description='Modo de entrada:'\n",
        "        )\n",
        "\n",
        "        btn_generar = widgets.Button(description='➡️ Generar Variables X')\n",
        "        btn_predecir = widgets.Button(description='📈 Predecir Y', button_style='success')\n",
        "        btn_copiar = widgets.Button(description='📋 Copiar Tabla', button_style='info')\n",
        "\n",
        "        tabla_x = widgets.Output()\n",
        "        tabla_pred = widgets.Output()\n",
        "        grafico_pred = widgets.Output()\n",
        "        progreso = widgets.Label()\n",
        "\n",
        "        datos_generados = {}\n",
        "        resultados = {}\n",
        "\n",
        "        def generar_valores(_):\n",
        "            tabla_x.clear_output()\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            datos_generados.clear()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                path_model = f\"modelo_rf_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(path_model):\n",
        "                    continue\n",
        "                try:\n",
        "                    with open(path_model, 'rb') as f:\n",
        "                        datos = pickle.load(f)\n",
        "                    cols = datos.get('cols', [])\n",
        "                    if not cols:\n",
        "                        continue\n",
        "\n",
        "                    df_x = pd.DataFrame()\n",
        "                    for col in cols:\n",
        "                        if col not in X_data.columns or X_data[col].isnull().all():\n",
        "                            continue\n",
        "                        serie = X_data[col]\n",
        "                        minimo, maximo = serie.min(), serie.max()\n",
        "                        tendencia = 'asc' if serie.corr(Y_data.iloc[:, 0]) > 0 else 'desc'\n",
        "                        if modo_datos.value == 'Automático':\n",
        "                            vals = np.linspace(minimo, maximo, num_casos.value) if tendencia == 'asc' \\\n",
        "                                else np.linspace(maximo, minimo, num_casos.value)\n",
        "                            df_x[col] = vals\n",
        "                        else:\n",
        "                            df_x[col] = [widgets.FloatText(value=0.0, layout=widgets.Layout(width='80px')) for _ in range(num_casos.value)]\n",
        "                    datos_generados[metodo] = df_x\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error al generar variables para {metodo}: {e}\")\n",
        "\n",
        "            with tabla_x:\n",
        "                clear_output()\n",
        "                display(HTML(f\"<h4>🧾 Variables X Generadas ({modo_datos.value})</h4>\"))\n",
        "                for metodo in metodos:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    df = datos_generados[metodo]\n",
        "                    display(HTML(f\"<b style='color:#2E8B57;'>{metodo}</b>\"))\n",
        "                    if isinstance(df, pd.DataFrame) and modo_datos.value == 'Automático':\n",
        "                        display(df)\n",
        "                    elif modo_datos.value == 'Manual':\n",
        "                        grid = widgets.GridspecLayout(num_casos.value + 1, len(df.columns))\n",
        "                        for j, col in enumerate(df.columns):\n",
        "                            grid[0, j] = widgets.Label(value=col)\n",
        "                            for i in range(num_casos.value):\n",
        "                                grid[i+1, j] = df[col][i]\n",
        "                        display(grid)\n",
        "\n",
        "        def realizar_prediccion(_):\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            progreso.value = \"⏳ Realizando predicciones...\"\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            t_ini = time.time()\n",
        "            errores = []  # ← para mostrar errores al final\n",
        "\n",
        "            for metodo in metodos:\n",
        "                if metodo not in datos_generados:\n",
        "                    continue\n",
        "                df = datos_generados[metodo]\n",
        "                path_model = f\"modelo_rf_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(path_model):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    with open(path_model, 'rb') as f:\n",
        "                        datos = pickle.load(f)\n",
        "                    model = datos['model']\n",
        "                    #sx, sy = datos['scaler_X'], datos['scaler_Y']\n",
        "                    sx = datos.get('scaler_X', None)\n",
        "                    sy = datos.get('scaler_Y', None)\n",
        "\n",
        "\n",
        "                    if modo_datos.value == 'Manual':\n",
        "                        df_manual = pd.DataFrame()\n",
        "                        for col in df.columns:\n",
        "                            df_manual[col] = [w.value for w in df[col]]\n",
        "                        df_to_use = df_manual\n",
        "                    else:\n",
        "                        df_to_use = df\n",
        "\n",
        "                    if set(df_to_use.columns) != set(datos['cols']):\n",
        "                        errores.append(f\"⚠️ Columnas incompatibles para {metodo}. Se omite.\")\n",
        "                        continue\n",
        "                    df_to_use = df_to_use[datos['cols']]\n",
        "\n",
        "                    if df_to_use.empty:\n",
        "                        errores.append(f\"⚠️ DataFrame vacío para {metodo}. Se omite.\")\n",
        "                        continue\n",
        "\n",
        "                    print(f\"[DEBUG] Prediciendo para método: {metodo}, df.shape = {df_to_use.shape}\")\n",
        "\n",
        "                    # === Predicción con o sin escalado ===\n",
        "                    if sx is not None:\n",
        "                        x_scaled = sx.transform(df_to_use.values)\n",
        "                    else:\n",
        "                        x_scaled = df_to_use.values\n",
        "\n",
        "                    y_pred_scaled = model.predict(x_scaled).reshape(-1, 1)\n",
        "\n",
        "                    if sy is not None:\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled).ravel()\n",
        "                    else:\n",
        "                        y_pred = y_pred_scaled.ravel()\n",
        "\n",
        "                    df_pred = df_to_use.copy()\n",
        "                    df_pred['Y_pred'] = y_pred\n",
        "                    resultados[metodo] = df_pred\n",
        "\n",
        "                except Exception as e:\n",
        "                    errores.append(f\"❌ Error al predecir para {metodo}:\\n{traceback.format_exc()}\")\n",
        "\n",
        "            progreso.value = f\"✅ Predicciones completadas en {time.time() - t_ini:.1f}s\"\n",
        "\n",
        "            contenedor_tablas = []\n",
        "            for metodo in metodos:\n",
        "                if metodo in resultados:\n",
        "                    df = resultados[metodo]\n",
        "                    contenedor_tablas.append(HTML(f\"<h4 style='color:#2E8B57;'>🧮 {metodo}</h4>\"))\n",
        "                    contenedor_tablas.append(widgets.Output())\n",
        "                    with contenedor_tablas[-1]:\n",
        "                        display(df)\n",
        "\n",
        "            with tabla_pred:\n",
        "                clear_output()\n",
        "                display(HTML(\"<h3>📋 Resultados de la Predicción (Random Forest)</h3>\"))\n",
        "                if contenedor_tablas:\n",
        "                    display(widgets.VBox(contenedor_tablas))\n",
        "                if errores:\n",
        "                    display(HTML(\"<h4 style='color:red;'>❌ Errores detectados:</h4>\"))\n",
        "                    for err in errores:\n",
        "                        display(HTML(f\"<pre style='color:darkred;'>{err}</pre>\"))\n",
        "\n",
        "            with grafico_pred:\n",
        "                clear_output()\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                hay_datos = False\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        plt.plot(resultados[metodo]['Y_pred'].values, label=metodo, linestyle='--')\n",
        "                        hay_datos = True\n",
        "                if hay_datos:\n",
        "                    plt.title(\"📊 Predicciones Y por Método (Random Forest)\")\n",
        "                    plt.xlabel(\"Caso\")\n",
        "                    plt.ylabel(\"Y_predicho\")\n",
        "                    plt.legend()\n",
        "                    plt.grid(True)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "        def copiar_al_portapapeles(_):\n",
        "            contenido = \"\"\n",
        "            for metodo, df in resultados.items():\n",
        "                contenido += f\"\\n### {metodo}\\n\" + df.to_csv(index=False)\n",
        "            js_code = f\"navigator.clipboard.writeText(`{contenido}`)\"\n",
        "            display(Javascript(js_code))\n",
        "            print(\"✅ Tabla copiada al portapapeles.\")\n",
        "\n",
        "        btn_generar.on_click(generar_valores)\n",
        "        btn_predecir.on_click(realizar_prediccion)\n",
        "        btn_copiar.on_click(copiar_al_portapapeles)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            num_casos,\n",
        "            modo_datos,\n",
        "            btn_generar,\n",
        "            tabla_x,\n",
        "            btn_predecir,\n",
        "            progreso,\n",
        "            tabla_pred,\n",
        "            grafico_pred,\n",
        "            btn_copiar\n",
        "        ]))\n",
        "\n",
        "\n",
        "# Mostrar en ejecución directa\n",
        "display(out_pred_rf)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8.5. PREDICCIÓN CON RNN – FUNCIONA CORRECTAMENTE\n",
        "# Permite al usuario seleccionar método de selección, introducir datos\n",
        "# manualmente o generarlos automáticamente, y obtener predicciones con RNN.\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "import pickle, os, traceback\n",
        "from IPython.display import display, HTML, clear_output, Javascript\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "out_pred_rnn = widgets.Output()\n",
        "resultados = {}\n",
        "datos_generados = {}\n",
        "\n",
        "def mostrar_prediccion_rnn(b=None):\n",
        "    with out_pred_rnn:\n",
        "        clear_output()\n",
        "\n",
        "        display(HTML(\"\"\"\n",
        "        <h3 style='color:#2E8B57;'>🔮 Predicción con RNN</h3>\n",
        "        <p>Este módulo permite realizar predicciones con modelos RNN previamente entrenados\n",
        "        utilizando variables seleccionadas automáticamente o introducidas manualmente.</p>\n",
        "        \"\"\"))\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "            description='Selección de Variables:'\n",
        "        )\n",
        "\n",
        "        num_casos = widgets.BoundedIntText(value=5, min=1, max=100, description='Nº Casos:')\n",
        "        modo_datos = widgets.ToggleButtons(options=['Automático', 'Manual'], description='Modo de entrada:')\n",
        "\n",
        "        btn_generar = widgets.Button(description='➡️ Generar Variables X')\n",
        "        btn_predecir = widgets.Button(description='📈 Predecir Y', button_style='success')\n",
        "        btn_copiar = widgets.Button(description='📋 Copiar Tabla', button_style='info')\n",
        "\n",
        "        tabla_x = widgets.Output()\n",
        "        tabla_pred = widgets.Output()\n",
        "        grafico_pred = widgets.Output()\n",
        "        progreso = widgets.Label()\n",
        "\n",
        "        def generar_valores(_):\n",
        "            tabla_x.clear_output()\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            progreso.value = \"\"\n",
        "            datos_generados.clear()\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            for metodo in metodos:\n",
        "                modelo_path = f\"modelo_rnn_{metodo.lower()}.h5\"\n",
        "                scaler_path = f\"escaladores_rnn_{metodo.lower()}.pkl\"\n",
        "                if not os.path.exists(modelo_path) or not os.path.exists(scaler_path):\n",
        "                    continue\n",
        "                with open(scaler_path, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                cols = datos.get('cols', [])\n",
        "                if not cols:\n",
        "                    continue\n",
        "\n",
        "                df_x = pd.DataFrame()\n",
        "                for col in cols:\n",
        "                    if col not in X_data.columns or X_data[col].isnull().all():\n",
        "                        continue\n",
        "                    serie = X_data[col]\n",
        "                    minimo, maximo = serie.min(), serie.max()\n",
        "                    tendencia = 'asc' if serie.corr(Y_data.iloc[:, 0]) > 0 else 'desc'\n",
        "                    if modo_datos.value == 'Automático':\n",
        "                        valores = np.linspace(minimo, maximo, num_casos.value) if tendencia == 'asc' else np.linspace(maximo, minimo, num_casos.value)\n",
        "                        df_x[col] = valores\n",
        "                    else:\n",
        "                        df_x[col] = [widgets.FloatText(value=0.0, layout=widgets.Layout(width='80px')) for _ in range(num_casos.value)]\n",
        "                datos_generados[metodo] = df_x\n",
        "\n",
        "            with tabla_x:\n",
        "                clear_output()\n",
        "                display(HTML(f\"<h4>🧾 Variables X Generadas ({modo_datos.value})</h4>\"))\n",
        "                for metodo in metodos:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    display(HTML(f\"<b style='color:#2E8B57;'>{metodo}</b>\"))\n",
        "                    df = datos_generados[metodo]\n",
        "                    if modo_datos.value == 'Automático':\n",
        "                        display(df)\n",
        "                    else:\n",
        "                        grid = widgets.GridspecLayout(num_casos.value + 1, len(df.columns))\n",
        "                        for j, col in enumerate(df.columns):\n",
        "                            grid[0, j] = widgets.Label(value=col)\n",
        "                            for i in range(num_casos.value):\n",
        "                                grid[i + 1, j] = df[col][i]\n",
        "                        display(grid)\n",
        "\n",
        "        def realizar_prediccion(_):\n",
        "            tabla_pred.clear_output()\n",
        "            grafico_pred.clear_output()\n",
        "            progreso.value = \"⏳ Realizando predicciones...\"\n",
        "            resultados.clear()\n",
        "\n",
        "            metodos = [metodo_selector.value] if metodo_selector.value != 'Todos' else [\n",
        "                'Pearson', 'Spearman', 'MutualInfo', 'Boruta', 'UMAP']\n",
        "\n",
        "            errores = []\n",
        "\n",
        "            for metodo in metodos:\n",
        "                try:\n",
        "                    if metodo not in datos_generados:\n",
        "                        continue\n",
        "                    df = datos_generados[metodo]\n",
        "\n",
        "                    modelo_path = f\"modelo_rnn_{metodo.lower()}.h5\"\n",
        "                    scaler_path = f\"escaladores_rnn_{metodo.lower()}.pkl\"\n",
        "                    if not os.path.exists(modelo_path) or not os.path.exists(scaler_path):\n",
        "                        errores.append(f\"❌ Archivos no encontrados para {metodo}\")\n",
        "                        continue\n",
        "\n",
        "                    with open(scaler_path, 'rb') as f:\n",
        "                        datos = pickle.load(f)\n",
        "                    sx, sy = datos.get('scaler_X'), datos.get('scaler_Y')\n",
        "                    model = load_model(modelo_path)\n",
        "\n",
        "                    if modo_datos.value == 'Manual':\n",
        "                        df_manual = pd.DataFrame()\n",
        "                        for col in df.columns:\n",
        "                            df_manual[col] = [w.value for w in df[col]]\n",
        "                        df_to_use = df_manual\n",
        "                    else:\n",
        "                        df_to_use = df\n",
        "\n",
        "                    if set(df_to_use.columns) != set(datos['cols']):\n",
        "                        errores.append(f\"⚠️ Columnas incompatibles para {metodo}. Se omite.\")\n",
        "                        continue\n",
        "                    df_to_use = df_to_use[datos['cols']]\n",
        "\n",
        "                    x_scaled = sx.transform(df_to_use.values)\n",
        "                    x_scaled_rnn = x_scaled.reshape((x_scaled.shape[0], 1, x_scaled.shape[1]))\n",
        "                    y_pred_scaled = model.predict(x_scaled_rnn).reshape(-1, 1)\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled).ravel()\n",
        "\n",
        "                    df_pred = df_to_use.copy()\n",
        "                    df_pred['Y_pred'] = y_pred\n",
        "                    resultados[metodo] = df_pred\n",
        "\n",
        "                except Exception as e:\n",
        "                    errores.append(f\"❌ Error al predecir para {metodo}:\\n{traceback.format_exc()}\")\n",
        "\n",
        "            progreso.value = \"✅ Predicciones completadas\"\n",
        "\n",
        "            with tabla_pred:\n",
        "                clear_output()\n",
        "                contenedor_tablas = []\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        df = resultados[metodo]\n",
        "                        contenedor_tablas.append(HTML(f\"<h4 style='color:#2E8B57;'>🧮 {metodo}</h4>\"))\n",
        "                        contenedor_tablas.append(widgets.Output())\n",
        "                        with contenedor_tablas[-1]:\n",
        "                            display(df)\n",
        "                if contenedor_tablas:\n",
        "                    display(HTML(\"<h3>📋 Resultados de la Predicción (RNN)</h3>\"))\n",
        "                    display(widgets.VBox(contenedor_tablas))\n",
        "                if errores:\n",
        "                    display(HTML(\"<h4 style='color:red;'>❌ Errores detectados:</h4>\"))\n",
        "                    for err in errores:\n",
        "                        display(HTML(f\"<pre style='color:darkred;'>{err}</pre>\"))\n",
        "\n",
        "            with grafico_pred:\n",
        "                clear_output()\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                for metodo in metodos:\n",
        "                    if metodo in resultados:\n",
        "                        plt.plot(resultados[metodo]['Y_pred'].values, label=metodo, linestyle='--')\n",
        "                plt.title(\"📊 Predicciones Y por Método (RNN)\")\n",
        "                plt.xlabel(\"Caso\")\n",
        "                plt.ylabel(\"Y_predicho\")\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        def copiar_al_portapapeles(_):\n",
        "            contenido = \"\"\n",
        "            for metodo, df in resultados.items():\n",
        "                contenido += f\"\\n### {metodo}\\n\" + df.to_csv(index=False)\n",
        "            js_code = f\"navigator.clipboard.writeText(`{contenido}`)\"\n",
        "            display(Javascript(js_code))\n",
        "            print(\"✅ Tabla copiada al portapapeles.\")\n",
        "\n",
        "        btn_generar.on_click(generar_valores)\n",
        "        btn_predecir.on_click(realizar_prediccion)\n",
        "        btn_copiar.on_click(copiar_al_portapapeles)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            num_casos,\n",
        "            modo_datos,\n",
        "            btn_generar,\n",
        "            tabla_x,\n",
        "            btn_predecir,\n",
        "            progreso,\n",
        "            tabla_pred,\n",
        "            grafico_pred,\n",
        "            btn_copiar\n",
        "        ]))\n",
        "\n",
        "# Mostrar en ejecución directa\n",
        "display(out_pred_rnn)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8.6. VISUALIZACION Y VS X – NUEVA LÓGICA IMPLEMENTADA\n",
        "# Permite al usuario seleccionar variables X que correlacionan o no con Y, ver las curvas Y_real vs Y_pred con SVR, NN, XGB, RF y RNN,\n",
        "# y copiar tabla de datos generada al portapapeles. Compatible con los métodos de selección (Pearson, Spearman, etc.)\n",
        "# ===============================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import glob\n",
        "from keras.models import load_model\n",
        "\n",
        "# Output global\n",
        "out_graf_86 = widgets.Output()\n",
        "\n",
        "def listar_modelos():\n",
        "    modelos = {}\n",
        "    for path in glob.glob(\"modelo_*.pkl\"):\n",
        "        nombre = os.path.splitext(os.path.basename(path))[0].replace(\"modelo_\", \"\").upper()\n",
        "        modelos[nombre] = path\n",
        "\n",
        "    for h5_path in glob.glob(\"modelo_*.h5\"):\n",
        "        nombre = os.path.splitext(os.path.basename(h5_path))[0].replace(\"modelo_\", \"\").upper()\n",
        "        pkl_path = f\"escaladores_{nombre.lower()}.pkl\"\n",
        "        if os.path.exists(pkl_path):\n",
        "            modelos[nombre] = (h5_path, pkl_path)\n",
        "        else:\n",
        "            modelos[nombre] = h5_path\n",
        "    return modelos\n",
        "\n",
        "def mostrar_grafico_y_vs_x():\n",
        "    with out_graf_86:\n",
        "        clear_output()\n",
        "\n",
        "        if 'X_data' not in globals() or 'Y_data' not in globals():\n",
        "            print(\"❌ Faltan datos cargados (X_data o Y_data).\")\n",
        "            return\n",
        "\n",
        "        modelos_disponibles = listar_modelos()\n",
        "\n",
        "        selector_variable = widgets.Dropdown(\n",
        "            options=X_data.columns.tolist(),\n",
        "            description='Variable X:',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        selector_dataset = widgets.ToggleButtons(\n",
        "            options=['Train', 'Test'],\n",
        "            description='Dataset:',\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        selector_modelos = widgets.SelectMultiple(\n",
        "            options=list(modelos_disponibles.keys()),\n",
        "            description='Modelos:',\n",
        "            layout=widgets.Layout(width='50%', height='150px')\n",
        "        )\n",
        "\n",
        "        boton_ver = widgets.Button(description='📊 Comparar Y', button_style='success')\n",
        "        resumen_out = widgets.Output()\n",
        "        tabla_out = widgets.Output()\n",
        "        debug_out = widgets.Output()\n",
        "        grafico_out = widgets.Output()\n",
        "\n",
        "        def graficar(_):\n",
        "            resumen_out.clear_output()\n",
        "            tabla_out.clear_output()\n",
        "            grafico_out.clear_output()\n",
        "            debug_out.clear_output()\n",
        "\n",
        "            var_x = selector_variable.value\n",
        "            if var_x is None:\n",
        "                print(\"⚠️ No se ha seleccionado variable X\")\n",
        "                return\n",
        "\n",
        "            dataset = selector_dataset.value\n",
        "            if dataset == 'Train':\n",
        "                X_base = X_train.copy()\n",
        "                Y_base = Y_train.copy()\n",
        "            else:\n",
        "                X_base = X_test.copy()\n",
        "                Y_base = Y_test.copy()\n",
        "\n",
        "            x_vals = X_base[var_x].values\n",
        "            y_vals = Y_base.values.ravel()\n",
        "            df = pd.DataFrame({\"X\": x_vals, \"Y_real\": y_vals})\n",
        "            metricas = {}\n",
        "\n",
        "            modelos_seleccionados = selector_modelos.value\n",
        "            if isinstance(modelos_seleccionados, str):\n",
        "                modelos_seleccionados = [modelos_seleccionados]\n",
        "            elif isinstance(modelos_seleccionados, tuple):\n",
        "                modelos_seleccionados = list(modelos_seleccionados)\n",
        "\n",
        "            for modelo_key in modelos_seleccionados:\n",
        "                modelo_path = modelos_disponibles.get(modelo_key)\n",
        "                if not modelo_path:\n",
        "                    with debug_out:\n",
        "                        print(f\"⚠️ Modelo {modelo_key} no encontrado.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    if isinstance(modelo_path, tuple):\n",
        "                        h5_file, pkl_file = modelo_path\n",
        "                        model = load_model(h5_file)\n",
        "                        with open(pkl_file, 'rb') as f:\n",
        "                            datos = pickle.load(f)\n",
        "                    else:\n",
        "                        with open(modelo_path, 'rb') as f:\n",
        "                            datos = pickle.load(f)\n",
        "                        model = datos.get('model')\n",
        "\n",
        "                    if model is None:\n",
        "                        raise ValueError(\"❌ No se encontró el modelo entrenado en el archivo.\")\n",
        "\n",
        "                    sx = datos.get('scaler_X', datos.get('sx'))\n",
        "                    sy = datos.get('scaler_Y', datos.get('sy'))\n",
        "\n",
        "                    if sx is None or sy is None:\n",
        "                        raise ValueError(\"❌ No se encontraron los escaladores (sx/sy o scaler_X/scaler_Y) en el modelo.\")\n",
        "\n",
        "                    cols = datos.get('cols', None)\n",
        "                    if cols is None:\n",
        "                        cols = list(set(X_base.columns) & set(sx.feature_names_in_))\n",
        "                        if not cols:\n",
        "                            raise ValueError(\"❌ No se pudo inferir columnas para aplicar scaler_X\")\n",
        "\n",
        "                    X_scaled = sx.transform(X_base[cols])\n",
        "\n",
        "                    # Si el modelo requiere entrada 3D (ej. RNN)\n",
        "                    if hasattr(model, 'input_shape') and len(model.input_shape) == 3:\n",
        "                        X_scaled = np.expand_dims(X_scaled, axis=1)  # Convertir a (batch_size, 1, features)\n",
        "\n",
        "                    y_pred_scaled = model.predict(X_scaled).reshape(-1, 1)\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled).ravel()\n",
        "\n",
        "                    df[f\"Y_{modelo_key}\"] = y_pred\n",
        "                    metricas[modelo_key] = {\n",
        "                        'R2': r2_score(y_vals, y_pred),\n",
        "                        'MSE': mean_squared_error(y_vals, y_pred),\n",
        "                        'MAE': mean_absolute_error(y_vals, y_pred)\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    with debug_out:\n",
        "                        print(f\"⚠️ Error al procesar {modelo_key}: {e}\")\n",
        "                        print(f\"📁 Contenido del modelo cargado: {list(datos.keys()) if 'datos' in locals() else '❌ No cargado'}\")\n",
        "\n",
        "            with resumen_out:\n",
        "                display(HTML(\"<h4>📌 Métricas comparativas:</h4>\"))\n",
        "                filas = [[m, f\"{v['R2']:.3f}\", f\"{v['MSE']:.3f}\", f\"{v['MAE']:.3f}\"] for m, v in metricas.items()]\n",
        "                display(pd.DataFrame(filas, columns=['Modelo', 'R²', 'MSE', 'MAE']))\n",
        "\n",
        "            with tabla_out:\n",
        "                display(HTML(\"<h4>📊 Tabla X, Y real y predicho (Top 20 casos):</h4>\"))\n",
        "                display(df.head(20))\n",
        "                try:\n",
        "                    import pyperclip\n",
        "                    pyperclip.copy(df.to_csv(sep=';', index=False))\n",
        "                    print(\"📋 Copiado al portapapeles\")\n",
        "                except:\n",
        "                    print(\"⚠️ pyperclip no disponible\")\n",
        "\n",
        "            with grafico_out:\n",
        "                plt.figure(figsize=(10,6))\n",
        "                plt.scatter(df['X'], df['Y_real'], label='Y Real', color='black', s=50, alpha=0.6)\n",
        "                for col in df.columns:\n",
        "                    if col.startswith(\"Y_\"):\n",
        "                        modelo = col[2:]\n",
        "                        if modelo in metricas:\n",
        "                            plt.scatter(df['X'], df[col], label=f\"{modelo} (R²={metricas[modelo]['R2']:.2f})\", alpha=0.6)\n",
        "                plt.xlabel(var_x)\n",
        "                plt.ylabel('Y')\n",
        "                plt.title(f\"Comparación Y real vs predicción - {var_x} ({dataset})\")\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        boton_ver.on_click(graficar)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            selector_variable,\n",
        "            selector_dataset,\n",
        "            selector_modelos,\n",
        "            boton_ver,\n",
        "            resumen_out,\n",
        "            tabla_out,\n",
        "            grafico_out,\n",
        "            debug_out\n",
        "        ]))\n",
        "\n",
        "# Mostrar salida\n",
        "display(out_graf_86)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 9. OPTIMIZACION - FUNCIONA CORRECTAMENTE\n",
        "# Este módulo es el responsable de la optimización completa de los diferentes modelos entrenados, que busca minimizar el Minimo Error Cuadrático (MAE) y la pérdida (LOSS) optimizando el HPO .\n",
        "# ===============================================================\n",
        "# ===============================================================\n",
        "# 9.1. OPTIMIZACIÓN SVR – FUNCIONA CORRECTAMENTE\n",
        "# Optimiza hiperparámetros del modelo SVR basándose en RESUMEN_METODOS\n",
        "# Incluye GridSearchCV, RandomizedSearchCV, Optuna y BayesSearchCV (scikit-optimize)\n",
        "# Ampliado con múltiples configuraciones de motores para búsqueda jerárquica\n",
        "# Añade trazabilidad visual detallada paso a paso\n",
        "# Incluye tabla comparativa de los 5 mejores modelos\n",
        "# ===============================================================\n",
        "# Importaciones\n",
        "import time\n",
        "import re       # ——— AÑADIDO ———\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, median_absolute_error\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML, Javascript\n",
        "import optuna\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "# Widget principal\n",
        "out_opt_svr = widgets.Output()\n",
        "\n",
        "def mostrar_optimizacion_svr():\n",
        "    with out_opt_svr:\n",
        "        clear_output()\n",
        "\n",
        "        # 1) Verifico que ya se haya segmentado X_train/X_test\n",
        "        if 'X_train' not in globals() or 'X_test' not in globals():\n",
        "            print(\"❌ Ejecute primero la segmentación para definir X_train y X_test.\")\n",
        "            return\n",
        "\n",
        "        # 2) Ahora sí puedo sanear columnas\n",
        "        # Usar sanitize_name para limpiar columnas en el payload\n",
        "        def clean_cols(col_list):\n",
        "            return [sanitize_name(c) for c in col_list]\n",
        "        # Ejemplo de sanitización de X_train antes de fit\n",
        "        X_train.columns = [sanitize_name(col) for col in X_train.columns]\n",
        "        X_test.columns = [sanitize_name(col) for col in X_test.columns]\n",
        "\n",
        "        display(HTML(\"<h3 style='color:#2E8B57;'>🔧 Optimización de Hiperparámetros - Modelo SVR</h3>\"))\n",
        "        display(HTML(\"\"\"\n",
        "            <h3 style='color:#2E8B57;'>🔧 Optimización de Hiperparámetros - Modelo SVR</h3>\n",
        "            <p>Este módulo permite encontrar la mejor configuración de hiperparámetros del modelo SVR\n",
        "            usando distintos motores de optimización. Cada motor aplica estrategias diferentes de búsqueda del óptimo:</p>\n",
        "            <ul>\n",
        "                <li><b>GridSearchCV</b>: búsqueda exhaustiva sobre combinaciones definidas. Garantiza el hallazgo del mejor resultado entre todas las combinaciones, pero puede ser computacionalmente costoso.</li>\n",
        "                <li><b>RandomizedSearchCV</b>: muestreo aleatorio sobre el espacio de búsqueda. Acelera el proceso seleccionando combinaciones al azar.</li>\n",
        "                <li><b>Optuna</b>: optimización bayesiana con estrategia de aprendizaje secuencial. Aprende de cada iteración para mejorar la siguiente.</li>\n",
        "                <li><b>BayesSearchCV</b>: búsqueda bayesiana usando scikit-optimize. Muy eficaz para espacios amplios con hiperparámetros complejos.</li>\n",
        "                <li><b>HalvingGridSearchCV</b>: búsqueda jerárquica que evalúa inicialmente muchas configuraciones con pocos recursos y reserva los recursos mayores solo a las mejores.</li>\n",
        "            </ul>\n",
        "\n",
        "            <h4 style=\"color:#1E90FF;\">📘 ¿Qué es la Validación Cruzada?</h4>\n",
        "            <p>La validación cruzada (CV) evalúa la capacidad de generalización de un modelo dividiendo los datos en varias particiones (\"folds\").\n",
        "            En cada iteración, uno de los folds se usa como conjunto de validación mientras los restantes se usan para entrenamiento.\n",
        "            El modelo se entrena y valida múltiples veces y luego se promedia la métrica para obtener una evaluación más robusta.</p>\n",
        "            <p>Esto reduce el riesgo de sobreajuste y asegura que el modelo funciona correctamente en datos que no ha visto.</p>\n",
        "\n",
        "            <h4 style=\"color:#1E90FF;\">📊 ¿Qué son los residuos?</h4>\n",
        "            <p>Los residuos son la diferencia entre los valores reales (observados) y los predichos por el modelo.\n",
        "            Se calculan como:</p>\n",
        "            <pre>residuo = valor_real - valor_predicho</pre>\n",
        "            <p>Interpretación:</p>\n",
        "            <ul>\n",
        "                <li>🔹 Residuos cercanos a cero indican buen ajuste.</li>\n",
        "                <li>🔹 Una distribución normal de los residuos es deseable: implica que los errores son aleatorios.</li>\n",
        "                <li>🔹 La presencia de sesgos, asimetrías o colas en los residuos puede indicar fallos estructurales del modelo.</li>\n",
        "            </ul>\n",
        "            <p>Además de los histogramas, se utiliza el test de Shapiro-Wilk para evaluar si los residuos siguen una distribución normal:</p>\n",
        "            <pre>p-value > 0.05 ➜ los residuos se consideran normales.</pre>\n",
        "\n",
        "            <h4 style=\"color:#1E90FF;\">📉 Comparativa Visual entre Modelos</h4>\n",
        "            <p>Una vez obtenidos los 5 mejores modelos, se genera una comparativa visual con las métricas R², MSE, MAE, RMSE y MedAE\n",
        "            para facilitar la selección del modelo más robusto en función de las prioridades del usuario.</p>\n",
        "            <p>También se generan histogramas de residuos para evaluar el comportamiento del error y gráficos Q-Q para validar la normalidad de dichos residuos.</p>\n",
        "            <p>Se incluirán gráficos de barras para comparar métricas entre modelos y residuos superpuestos para identificar el más preciso.</p>\n",
        "            <hr>\n",
        "            <p style=\"color:gray;\">Puedes lanzar la optimización seleccionando el método de selección de variables (Pearson, MutualInfo, etc.) y los motores deseados.</p>\n",
        "            \"\"\"))\n",
        "\n",
        "        metodos = list(RESUMEN_METODOS.keys()) if isinstance(RESUMEN_METODOS, dict) else []\n",
        "        if not metodos:\n",
        "            display(HTML(\"<span style='color:red;'>⚠️ No se encontraron variables seleccionadas por ningún método en RESUMEN_METODOS.</span>\"))\n",
        "            return\n",
        "\n",
        "        metodo_selector = widgets.Dropdown(\n",
        "            options=metodos + ['Todos'],\n",
        "            description='Variables X:',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        funcion_selector = widgets.Dropdown(\n",
        "            options=['R2', 'MSE', 'MAE', 'RMSE', 'MedAE'],\n",
        "            value='R2',\n",
        "            description='Función objetivo:',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        motor_selector = widgets.SelectMultiple(\n",
        "            options=['GridSearchCV', 'RandomizedSearchCV', 'Optuna', 'BayesSearchCV', 'Todos'],\n",
        "            value=['GridSearchCV'],\n",
        "            description='Motores de Optimización:',\n",
        "            layout=widgets.Layout(width='70%', height='100px')\n",
        "        )\n",
        "\n",
        "        btn_ejecutar = widgets.Button(description='🚀 Ejecutar Optimización', button_style='success')\n",
        "        progreso = widgets.HTML()\n",
        "        traza = widgets.Output()\n",
        "        salida_resultados = widgets.Output()\n",
        "\n",
        "        def ejecutar_optimizacion(_):\n",
        "            with salida_resultados:\n",
        "                clear_output()\n",
        "            with traza:\n",
        "                clear_output()\n",
        "                print(\"🟢 Iniciando optimización...\")\n",
        "\n",
        "            inicio = time.time()\n",
        "            metodos_a_probar = metodos if metodo_selector.value == 'Todos' else [metodo_selector.value]\n",
        "            motores = ['GridSearchCV', 'RandomizedSearchCV', 'Optuna', 'BayesSearchCV'] if 'Todos' in motor_selector.value else list(motor_selector.value)\n",
        "\n",
        "            mejores_modelos = []\n",
        "\n",
        "            for metodo in metodos_a_probar:\n",
        "                with traza:\n",
        "                    print(f\"\\n🔍 Optimizando para variables seleccionadas por: {metodo}\")\n",
        "                #vars_x = RESUMEN_METODOS.get(metodo, [])\n",
        "                #if not vars_x:\n",
        "                #    with traza:\n",
        "                #        print(f\"⚠️ No hay variables seleccionadas por {metodo}. Se omite.\")\n",
        "                #    continue\n",
        "                # ——— AÑADIDO: limpiar lista de variables antes de indexar ———\n",
        "                raw_vars = RESUMEN_METODOS.get(metodo, [])\n",
        "                if not raw_vars:\n",
        "                    with traza:\n",
        "                        print(f\"⚠️ No hay variables para {metodo}, omito.\")\n",
        "                    continue\n",
        "                vars_x = clean_cols(raw_vars)\n",
        "                # ——— FIN AÑADIDO ———\n",
        "                try:\n",
        "                    X_train_sel = X_train[vars_x].copy()\n",
        "                    X_test_sel = X_test[vars_x].copy()\n",
        "                    y_train_sel = Y_train.values.ravel()\n",
        "                    y_test_sel = Y_test.values.ravel()\n",
        "\n",
        "                    sx = StandardScaler()\n",
        "                    sy = StandardScaler()\n",
        "                    X_train_scaled = sx.fit_transform(X_train_sel)\n",
        "                    X_test_scaled = sx.transform(X_test_sel)\n",
        "                    y_train_scaled = sy.fit_transform(y_train_sel.reshape(-1, 1)).ravel()\n",
        "\n",
        "                    def calcular_score(y_real, y_pred):\n",
        "                        if funcion_selector.value == 'R2': return r2_score(y_real, y_pred)\n",
        "                        elif funcion_selector.value == 'MSE': return mean_squared_error(y_real, y_pred)\n",
        "                        elif funcion_selector.value == 'MAE': return mean_absolute_error(y_real, y_pred)\n",
        "                        elif funcion_selector.value == 'RMSE': return np.sqrt(mean_squared_error(y_real, y_pred))\n",
        "                        elif funcion_selector.value == 'MedAE': return median_absolute_error(y_real, y_pred)\n",
        "\n",
        "                    def objetivo_optuna(trial):\n",
        "                        C = trial.suggest_float('C', 1e-2, 1e3, log=True)\n",
        "                        epsilon = trial.suggest_float('epsilon', 1e-4, 0.5, log=True)\n",
        "                        kernel = trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly'])\n",
        "                        degree = trial.suggest_int('degree', 2, 4) if kernel == 'poly' else 3\n",
        "\n",
        "                        #C = trial.suggest_float('C', 0.1, 100, log=True)\n",
        "                        #epsilon = trial.suggest_float('epsilon', 0.01, 1.0, log=True)\n",
        "                        #kernel = trial.suggest_categorical('kernel', ['rbf', 'linear'])\n",
        "\n",
        "                        #svr = SVR(C=C, epsilon=epsilon, kernel=kernel)\n",
        "\n",
        "                        svr = SVR(C=C, epsilon=epsilon, kernel=kernel, degree=degree)\n",
        "                        svr.fit(X_train_scaled, y_train_scaled)\n",
        "                        y_pred = sy.inverse_transform(svr.predict(X_test_scaled).reshape(-1, 1)).ravel()\n",
        "                        score = calcular_score(y_test_sel, y_pred)\n",
        "                        return score if funcion_selector.value == 'R2' else -score\n",
        "\n",
        "                    for motor in motores:\n",
        "                        with traza:\n",
        "                            print(f\"⚙️ Motor: {motor} → Variables: {len(vars_x)} → Datos: {X_train_scaled.shape}\")\n",
        "\n",
        "                        best_model = None\n",
        "\n",
        "                        if motor == 'GridSearchCV':\n",
        "                            grid = GridSearchCV(\n",
        "                                SVR(),\n",
        "#                                param_grid={\n",
        "#                                    'C': [0.1, 1, 10, 100],\n",
        "#                                    'epsilon': [0.01, 0.1, 0.5, 1],\n",
        "#                                    'kernel': ['rbf', 'linear']\n",
        "#                                },\n",
        "                                param_grid= {\n",
        "                                    'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
        "                                    'epsilon': [0.001, 0.01, 0.1, 0.5],\n",
        "                                    'kernel': ['rbf', 'linear', 'poly'],\n",
        "                                    'degree': [2, 3]  # solo si kernel = poly\n",
        "                                },\n",
        "                                scoring='r2', cv=3, n_jobs=-1\n",
        "                            )\n",
        "                            grid.fit(X_train_scaled, y_train_scaled)\n",
        "                            best_model = grid.best_estimator_\n",
        "\n",
        "                        elif motor == 'RandomizedSearchCV':\n",
        "                            rand = RandomizedSearchCV(\n",
        "                                SVR(),\n",
        "#                                param_distributions={\n",
        "#                                    'C': np.logspace(-1, 2, 20),\n",
        "#                                    'epsilon': np.logspace(-2, 0, 20),\n",
        "#                                    'kernel': ['rbf', 'linear']\n",
        "#                                },\n",
        "                                param_distributions={\n",
        "                                    'C': stats.reciprocal(1e-2, 1e3),\n",
        "                                    'epsilon': stats.reciprocal(1e-4, 0.5),\n",
        "                                    'kernel': ['rbf', 'linear', 'poly'],\n",
        "                                    'degree': stats.randint(2, 4)\n",
        "                                },\n",
        "                                scoring='r2', n_iter=30, cv=3, n_jobs=-1, random_state=42\n",
        "                            )\n",
        "                            rand.fit(X_train_scaled, y_train_scaled)\n",
        "                            best_model = rand.best_estimator_\n",
        "\n",
        "                        elif motor == 'Optuna':\n",
        "                            study = optuna.create_study(direction='maximize' if funcion_selector.value == 'R2' else 'minimize')\n",
        "                            study.optimize(objetivo_optuna, n_trials=30)\n",
        "                            best_model = SVR(**study.best_params)\n",
        "                            best_model.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "                        elif motor == 'BayesSearchCV':\n",
        "                            bayes = BayesSearchCV(\n",
        "                                SVR(),\n",
        "#                                search_spaces={\n",
        "#                                    'C': Real(0.1, 100, prior='log-uniform'),\n",
        "#                                    'epsilon': Real(0.01, 1.0, prior='log-uniform'),\n",
        "#                                    'kernel': Categorical(['rbf', 'linear'])\n",
        "#                                },\n",
        "                                search_spaces={\n",
        "                                    'C': Real(1e-2, 1e3, prior='log-uniform'),\n",
        "                                    'epsilon': Real(1e-4, 0.5, prior='log-uniform'),\n",
        "                                    'kernel': Categorical(['rbf', 'linear', 'poly']),\n",
        "                                    'degree': (2, 4)\n",
        "                                },\n",
        "                                scoring='r2', cv=3, n_iter=30, n_jobs=-1, random_state=42\n",
        "                            )\n",
        "                            bayes.fit(X_train_scaled, y_train_scaled)\n",
        "                            best_model = bayes.best_estimator_\n",
        "\n",
        "                        y_pred = sy.inverse_transform(best_model.predict(X_test_scaled).reshape(-1, 1)).ravel()\n",
        "                        score = calcular_score(y_test_sel, y_pred)\n",
        "                        mejores_modelos.append((f\"{metodo} - {motor}\", best_model, score, y_test_sel, y_pred))\n",
        "\n",
        "                        with traza:\n",
        "                            print(f\"✅ {metodo} [{motor}] → {funcion_selector.value}: {score:.4f}\")\n",
        "\n",
        "                        # ──────────────────────────────────────────────────\n",
        "                        # ⬇️  Bloque de Grabación de resultados - persistencia\n",
        "                        # ──────────────────────────────────────────────────\n",
        "                        try:\n",
        "                            from pathlib import Path\n",
        "                            import pickle\n",
        "\n",
        "                            save_dir = Path(\"modelos_opt\")\n",
        "                            save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "                            modelo_fname = (\n",
        "                                save_dir / f\"modelo_svr_{metodo.lower()}_opt_{motor.lower()}.pkl\"\n",
        "                            )\n",
        "                            study_fname = (\n",
        "                                save_dir / f\"optuna_svr_{metodo.lower()}_opt_{motor.lower()}.pkl\"\n",
        "                            )\n",
        "\n",
        "                            # Obtener nombre Y de forma robusta\n",
        "                            if isinstance(Y_train, pd.Series):\n",
        "                                y_name = Y_train.name or \"target\"\n",
        "                            else:  # DataFrame (una sola columna)\n",
        "                                y_name = Y_train.columns[0] if Y_train.shape[1] == 1 else \"target\"\n",
        "\n",
        "                            payload = {\n",
        "                                \"model\":  best_model,\n",
        "                                \"sx\":     sx,\n",
        "                                \"sy\":     sy,\n",
        "                                \"cols\":   vars_x,\n",
        "                                \"yname\":  y_name,\n",
        "                                \"score\":  score,\n",
        "                                \"metric\": funcion_selector.value,\n",
        "                            }\n",
        "\n",
        "                            with open(modelo_fname, \"wb\") as f:\n",
        "                                pickle.dump(payload, f)\n",
        "\n",
        "                            study_fname = \"optuna_study.pkl\"   # nombre que espera la celda 10\n",
        "                            if motor.lower() == \"optuna\" and \"study\" in locals():\n",
        "                                with open(study_fname, \"wb\") as f:\n",
        "                                    pickle.dump(study, f)\n",
        "                                with traza: print(f\"📁 Estudio Optuna guardado en: {study_fname}\")\n",
        "\n",
        "                            with traza: print(f\"💾 Modelo guardado en: {modelo_fname}\")\n",
        "\n",
        "                            # Registrar en un diccionario global opcional\n",
        "                            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "                            OPT_MODELS[(\"svr\", metodo.lower(), motor.lower())] = payload\n",
        "                            if motor.lower() == \"optuna\" and \"study\" in locals():\n",
        "                                OPT_MODELS[(\"svr\", metodo.lower(), \"optuna_study\")] = study\n",
        "\n",
        "                        except Exception as e:\n",
        "                            with traza: print(f\"⚠️ No se pudo guardar el modelo o estudio: {e}\")\n",
        "                        # ──────────────────────────────────────────────\n",
        "                        # ⬆️  FIN DEL BLOQUE DE PERSISTENCIA\n",
        "                        # ──────────────────────────────────────────────\n",
        "                except Exception as e:\n",
        "                    with traza:\n",
        "                        print(f\"❌ Error al optimizar {metodo}: {e}\")\n",
        "\n",
        "            with salida_resultados:\n",
        "                if mejores_modelos:\n",
        "                    mejores_modelos.sort(key=lambda x: x[2], reverse=(funcion_selector.value == 'R2'))\n",
        "                    top5 = mejores_modelos[:5]\n",
        "\n",
        "                    df_top5 = pd.DataFrame([{\n",
        "                        'Método-Motor': m[0],\n",
        "                        funcion_selector.value: round(m[2], 4),\n",
        "                        'C': m[1].C,\n",
        "                        'Epsilon': m[1].epsilon,\n",
        "                        'Kernel': m[1].kernel\n",
        "                    } for m in top5])\n",
        "\n",
        "                    display(HTML(\"<h4 style='color:#2E8B57;'>📊 Top 5 Modelos Optimizado...</h4>\"))\n",
        "                    display(df_top5.style.set_caption(\"Top 5 configuraciones SVR\")\n",
        "                            .set_properties(**{'border': '1px solid gray', 'text-align': 'center'})\n",
        "                            .set_table_styles([{'selector': 'th', 'props': [('background-color', '#2E8B57'), ('color', 'white')]}]))\n",
        "\n",
        "                    mejor = top5[0]\n",
        "                    metodo_motor, model, score, y_real, y_pred = mejor\n",
        "\n",
        "                    display(HTML(f\"\"\"\n",
        "                    <h4 style='color:green;'>🎯 Mejor configuración encontrada:</h4>\n",
        "                    <b>Método:</b> {metodo_motor}<br>\n",
        "                    <b>{funcion_selector.value}:</b> {score:.4f}\n",
        "                    <hr>\n",
        "                    <h4>📋 Detalle de Hiperparámetros:</h4>\n",
        "                    \"\"\"))\n",
        "\n",
        "                    df_hp = pd.DataFrame({\n",
        "                        'Parámetro': ['C', 'epsilon', 'kernel'],\n",
        "                        'Valor Óptimo': [model.C, model.epsilon, model.kernel],\n",
        "                        'Descripción': [\n",
        "                            'Penalización del margen. Equilibra error y generalización',\n",
        "                            'Zona de tolerancia sin penalización en el error',\n",
        "                            'Función que transforma el espacio (lineal o no lineal)'\n",
        "                        ],\n",
        "                        'Rango Típico': ['0.1 – 100', '0.01 – 1.0', 'rbf / linear']\n",
        "                    })\n",
        "\n",
        "                    display(df_hp.style.set_table_styles([\n",
        "                        {'selector': 'th', 'props': [('background-color', '#2E8B57'), ('color', 'white')]},\n",
        "                        {'selector': 'td', 'props': [('text-align', 'center')]}\n",
        "                    ]).set_properties(**{'border': '1px solid gray', 'padding': '6px'}))\n",
        "\n",
        "                    plt.figure(figsize=(8, 5))\n",
        "                    plt.plot(y_real, label='Real', marker='o')\n",
        "                    plt.plot(y_pred, label='Predicción', marker='x')\n",
        "                    plt.title(f'Y Real vs Y Predicho (Mejor SVR - {metodo_motor})')\n",
        "                    plt.legend()\n",
        "                    plt.grid()\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                    # ===============================================================\n",
        "                    # Análisis de Residuos del Mejor Modelo\n",
        "                    # ===============================================================\n",
        "                    residuos = y_real - y_pred\n",
        "\n",
        "                    display(HTML(\"<h4 style='color:#2E8B57;'>📉 Análisis de Residuos del Mejor Modelo</h4>\"))\n",
        "                    display(HTML(\"\"\"\n",
        "                    <p>Los <b>residuos</b> representan la diferencia entre los valores reales (observados) y los predichos por el modelo.\n",
        "                    Evaluar su comportamiento ayuda a determinar si el modelo ha capturado correctamente la estructura de los datos.</p>\n",
        "                    <ul>\n",
        "                    <li><b>Residuos = Valor Real – Valor Predicho</b></li>\n",
        "                    <li>Distribución simétrica centrada en cero es señal de buen ajuste</li>\n",
        "                    <li>Asimetría, colas largas o concentraciones pueden indicar sobreajuste, variables omitidas u otros problemas.</li>\n",
        "                    </ul>\n",
        "                    \"\"\"))\n",
        "\n",
        "                    # Estadísticas básicas\n",
        "                    res_stats = pd.DataFrame({\n",
        "                        'Métrica': ['Media', 'Desviación estándar', 'Mínimo', 'Máximo'],\n",
        "                        'Valor': [np.mean(residuos), np.std(residuos), np.min(residuos), np.max(residuos)]\n",
        "                    })\n",
        "                    display(res_stats.style.set_caption(\"📌 Estadísticas de los Residuos\")\n",
        "                            .set_properties(**{'border': '1px solid gray', 'text-align': 'center'})\n",
        "                            .set_table_styles([{'selector': 'th', 'props': [('background-color', '#2E8B57'), ('color', 'white')]}]))\n",
        "\n",
        "                    # Histograma de residuos\n",
        "                    plt.figure(figsize=(8,4))\n",
        "                    plt.hist(residuos, bins=25, color='skyblue', edgecolor='black')\n",
        "                    plt.title('📊 Histograma de Residuos')\n",
        "                    plt.xlabel('Residuo')\n",
        "                    plt.ylabel('Frecuencia')\n",
        "                    plt.grid(True)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                    # Gráfico Q-Q\n",
        "                    plt.figure(figsize=(6, 6))\n",
        "                    stats.probplot(residuos, dist=\"norm\", plot=plt)\n",
        "                    plt.title('📈 Gráfico Q-Q de los Residuos')\n",
        "                    plt.grid(True)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                    # Test de normalidad de Shapiro-Wilk\n",
        "                    residuos_validos = residuos[~np.isnan(residuos) & ~np.isinf(residuos)]\n",
        "\n",
        "                    display(HTML(\"<h4>📐 Test de Normalidad (Shapiro-Wilk)</h4>\"))\n",
        "\n",
        "                    print(f\"Número de residuos válidos: {len(residuos_validos)}\")\n",
        "                    print(\"Primeros residuos válidos:\", residuos_validos[:10])\n",
        "\n",
        "                    if residuos_validos.size >= 3:\n",
        "                        try:\n",
        "                            stat, p = shapiro(residuos_validos)\n",
        "                            interpretacion = '✅ Residuos normales (p > 0.05)' if p > 0.05 else '⚠️ Residuos no normales (p ≤ 0.05)'\n",
        "                            display(HTML(f\"\"\"\n",
        "                                <ul>\n",
        "                                    <li><b>Estadístico:</b> {stat:.4f}</li>\n",
        "                                    <li><b>p-valor:</b> {p:.4f}</li>\n",
        "                                    <li>{interpretacion}</li>\n",
        "                                </ul>\n",
        "                            \"\"\"))\n",
        "                        except Exception as e:\n",
        "                            display(HTML(f\"<span style='color:red;'>❌ Error al ejecutar el test de Shapiro: {e}</span>\"))\n",
        "                    else:\n",
        "                        display(HTML(\"<span style='color:red;'>❌ No hay suficientes datos válidos para realizar el test de normalidad.</span>\"))\n",
        "\n",
        "                    # ==========================================================\n",
        "                    # 🔍 ANALISIS COMPARATIVO AVANZADO\n",
        "                    # ==========================================================\n",
        "                    # ============================================\n",
        "                    # 📊 CREACIÓN DE DATAFRAME DE MÉTRICAS PARA COMPARATIVA\n",
        "                    # ============================================\n",
        "                    metricas_df = pd.DataFrame([\n",
        "                        {\n",
        "                            'Modelo': nombre,\n",
        "                            'R2': r2_score(y_real, y_pred),\n",
        "                            'MSE': mean_squared_error(y_real, y_pred),\n",
        "                            'MAE': mean_absolute_error(y_real, y_pred),\n",
        "                            'RMSE': np.sqrt(mean_squared_error(y_real, y_pred)),\n",
        "                            'MedAE': median_absolute_error(y_real, y_pred)\n",
        "                        }\n",
        "                        for nombre, modelo, _, y_real, y_pred in top5\n",
        "                    ])\n",
        "\n",
        "                    # ============================================\n",
        "                    # 🔥 MAPA DE CALOR DE MÉTRICAS\n",
        "                    # ============================================\n",
        "                    metricas_norm = (metricas_df.drop('Modelo', axis=1) - metricas_df.drop('Modelo', axis=1).min()) / (\n",
        "                        metricas_df.drop('Modelo', axis=1).max() - metricas_df.drop('Modelo', axis=1).min())\n",
        "                    plt.figure(figsize=(10, 5))\n",
        "                    sns.heatmap(metricas_norm.T, annot=True, cmap='YlGnBu', xticklabels=metricas_df['Modelo'], fmt=\".2f\")\n",
        "                    plt.title(\"🌡️ Mapa de Calor de Métricas Normalizadas\")\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                    # ============================================\n",
        "                    # 🎯 RADAR CHART DE MÉTRICAS\n",
        "                    # ============================================\n",
        "                    #import matplotlib.pyplot as plt\n",
        "                    from math import pi\n",
        "\n",
        "                    # Preparar datos\n",
        "                    categorias = list(metricas_df.columns[1:])\n",
        "                    N = len(categorias)\n",
        "                    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "                    angles += angles[:1]\n",
        "\n",
        "                    plt.figure(figsize=(8, 6))\n",
        "                    for i in range(len(metricas_df)):\n",
        "                        valores = metricas_df.iloc[i, 1:].values.flatten().tolist()\n",
        "                        valores += valores[:1]\n",
        "                        plt.polar(angles, valores, label=metricas_df.iloc[i, 0], marker='o')\n",
        "\n",
        "                    plt.xticks(angles[:-1], categorias, color='grey', size=8)\n",
        "                    plt.title(\"🎯 Radar Chart - Comparativa Multimétrica\")\n",
        "                    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                    # ===============================================================\n",
        "                    # ACopiar resultados al portapapeles\n",
        "                    # ===============================================================\n",
        "                    btn_copiar = widgets.Button(description='📋 Copiar Hiperparámetros', button_style='info')\n",
        "                    def copiar(_):\n",
        "                        texto = str(model.get_params())\n",
        "                        js = f\"navigator.clipboard.writeText(`{texto}`)\"\n",
        "                        display(Javascript(js))\n",
        "                        print(\"📋 Copiados al portapapeles.\")\n",
        "                    btn_copiar.on_click(copiar)\n",
        "                    display(btn_copiar)\n",
        "\n",
        "                else:\n",
        "                    print(\"⚠️ No se encontró ninguna configuración óptima válida.\")\n",
        "\n",
        "            progreso.value = f\"⏱️ Tiempo total de optimización: {time.time() - inicio:.2f} segundos\"\n",
        "\n",
        "        btn_ejecutar.on_click(ejecutar_optimizacion)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            metodo_selector,\n",
        "            funcion_selector,\n",
        "            motor_selector,\n",
        "            btn_ejecutar,\n",
        "            progreso,\n",
        "            traza,\n",
        "            salida_resultados\n",
        "        ]))\n",
        "\n",
        "#mostrar_optimizacion_svr()\n",
        "display(out_opt_svr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 9.2. OPTIMIZACION NN - FUNCIONA CORRECTAMENTE\n",
        "# Este módulo es el responsable de la optimización completa de la red neuronal, que busca minimizar el Minimo Error Cuadrático (MAE) y la pérdida (LOSS) optimizando el HPO que incluye: capas ocultas,\n",
        "# neuronas por capa, función de activación, velocidad de aprendizaje, epocas, funación de optimización.\n",
        "# Incluye el grabado de datos\n",
        "# ===============================================================\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import shapiro\n",
        "from ipywidgets import VBox, HBox, Dropdown, IntSlider, IntText, IntProgress, FloatSlider, SelectMultiple, Button, Output, HTML, Accordion\n",
        "from IPython.display import display, clear_output\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "# ——— AÑADIDO: Mixed-Precision Training ———\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "# ——— FIN AÑADIDO ———\n",
        "\n",
        "# ——— AÑADIDO: import de EarlyStopping + definición de TimeStopping ———\n",
        "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
        "\n",
        "class TimeStopping(Callback):\n",
        "    \"\"\"Detiene el entrenamiento si supera un tiempo máximo (en segundos).\"\"\"\n",
        "    def __init__(self, max_seconds=600):\n",
        "        super().__init__()\n",
        "        self.max_seconds = max_seconds\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if time.time() - self.start_time > self.max_seconds:\n",
        "            self.model.stop_training = True\n",
        "# ——— FIN AÑADIDO ———\n",
        "\n",
        "import signal\n",
        "\n",
        "# ——— AÑADIDO: Timeout para tuner.search con signal.alarm ———\n",
        "class TimeoutException(Exception):\n",
        "    pass\n",
        "\n",
        "def _timeout_handler(signum, frame):\n",
        "    raise TimeoutException()\n",
        "\n",
        "# Registramos el manejador\n",
        "signal.signal(signal.SIGALRM, _timeout_handler)\n",
        "# ——— FIN AÑADIDO ———\n",
        "\n",
        "# ——— AÑADIDO: Custom R2 metric para Keras ———\n",
        "class R2(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='r2', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.sse = self.add_weight(name='sse', initializer='zeros')\n",
        "        self.sst = self.add_weight(name='sst', initializer='zeros')\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "        resid = tf.reduce_sum(tf.square(y_true - y_pred))\n",
        "        mean_true = tf.reduce_mean(y_true)\n",
        "        sst_value = tf.reduce_sum(tf.square(y_true - mean_true))\n",
        "        self.sse.assign_add(resid)\n",
        "        self.sst.assign_add(sst_value)\n",
        "    def result(self):\n",
        "        return 1.0 - (self.sse / (self.sst + tf.keras.backend.epsilon()))\n",
        "    def reset_states(self):\n",
        "        self.sse.assign(0.0)\n",
        "        self.sst.assign(0.0)\n",
        "# ——— FIN AÑADIDO ———\n",
        "\n",
        "import optuna\n",
        "from keras_tuner import RandomSearch, BayesianOptimization, Hyperband, Objective\n",
        "\n",
        "# ========== Widgets de configuración ==========\n",
        "\n",
        "# Ayuda extendida sobre los parámetros\n",
        "ayuda_parametros = HTML(\"\"\"\n",
        "<h4>📘 Explicación de Parámetros</h4>\n",
        "<ul>\n",
        "  <li><b>Métodos X:</b> Métodos de selección de variables predictoras. Usan correlaciones estadísticas o algoritmos de reducción de dimensión. <br>\n",
        "      <i>Pearson</i> y <i>Spearman</i>: correlaciones lineales y monótonas.<br>\n",
        "      <i>MutualInfo</i>: mide dependencia informacional. <br>\n",
        "      <i>Boruta</i>: selección envolvente basada en árboles. <br>\n",
        "      <i>UMAP</i>: reducción no lineal de dimensiones. <br>\n",
        "      <b>Todos</b> ejecuta cada uno secuencialmente.</li>\n",
        "  <li><b>Motores:</b> Algoritmos de optimización de hiperparámetros. <br>\n",
        "      <i>RandomSearch</i>: búsqueda aleatoria. <br>\n",
        "      <i>BayesianOptimization</i>: estima regiones óptimas. <br>\n",
        "      <i>Hyperband</i>: eficiente para grandes espacios de búsqueda. <br>\n",
        "      <i>Optuna</i>: flexible y potente. <br>\n",
        "      <b>Todos</b> ejecuta todos los motores.</li>\n",
        "  <li><b>Función objetivo:</b> Métrica a maximizar o minimizar: <br>\n",
        "      <i>R2</i>: se desea maximizar. <i>MAE</i> y <i>MSE</i>: se minimizan.</li>\n",
        "  <li><b>Trials:</b> Número de combinaciones a evaluar en la búsqueda.</li>\n",
        "  <li><b>Épocas:</b> Iteraciones completas sobre el dataset de entrenamiento (100 a 2000 recomendado).</li>\n",
        "  <li><b>Capas:</b> Cantidad de capas ocultas en la red (1 a 20 habitual, máximo 100 para pruebas avanzadas).</li>\n",
        "  <li><b>Neuronas/capa:</b> Número de neuronas por capa (32 a 512 recomendado).</li>\n",
        "  <li><b>Dropout:</b> Fracción de neuronas descartadas en entrenamiento (0.1 a 0.4 recomendado).</li>\n",
        "  <li><b>L2 Reg:</b> Regularización L2 para evitar sobreajuste (0.001 a 0.01 habitual).</li>\n",
        "</ul>\n",
        "\"\"\")\n",
        "\n",
        "select_metodos = SelectMultiple(\n",
        "    options=['Pearson', 'Spearman', 'Mutualinfo', 'Boruta', 'UMAP', 'Todos'],\n",
        "    description='Métodos Selección Variables X:',\n",
        "    layout={'width': '50%'}\n",
        ")\n",
        "\n",
        "select_motores = SelectMultiple(\n",
        "    options=['RandomSearch', 'BayesianOptimization', 'Hyperband', 'Optuna', 'Todos'],\n",
        "    description='Motores:',\n",
        "    layout={'width': '50%'}\n",
        ")\n",
        "\n",
        "func_objetivo = Dropdown(\n",
        "    options=['R2', 'MAE', 'MSE'],\n",
        "    value='R2',\n",
        "    description='Función objetivo:',\n",
        "    layout={'width': '40%'}\n",
        ")\n",
        "\n",
        "# Rango de hiperparámetros\n",
        "n_trials_slider = IntSlider(value=10, min=1, max=50, step=1, description='Trials:')\n",
        "rango_epocas = IntSlider(value=500, min=1, max=200, step=10, description='Épocas:')\n",
        "rango_capas = IntSlider(value=3, min=1, max=6, step=1, description='Capas:')\n",
        "rango_neuronas = IntSlider(value=64, min=256, max=512, step=8, description='Neuronas/capa:')\n",
        "dropout_rate = FloatSlider(value=0.2, min=0.0, max=0.7, step=0.05, description='Dropout:')\n",
        "l2_reg = FloatSlider(value=0.001, min=0.0, max=0.01, step=0.0005, description='L2 Reg:')\n",
        "\n",
        "btn_ejecutar = Button(description='🚀 Ejecutar Optimización NN', button_style='success')\n",
        "progreso_bar = IntProgress(min=0, max=1, description='Progreso:', style={'bar_color': 'green'})\n",
        "#progreso_label = Label(value=\"\")\n",
        "out_nn = Output()\n",
        "\n",
        "# ========== Función principal ==========\n",
        "def ejecutar_optimizacion(_):\n",
        "    with out_nn:\n",
        "        clear_output()\n",
        "\n",
        "        # 2) Ahora sí puedo sanear columnas\n",
        "        # Usar sanitize_name para limpiar columnas en el payload\n",
        "        def clean_cols(col_list):\n",
        "            return [sanitize_name(c) for c in col_list]\n",
        "        # Ejemplo de sanitización de X_train antes de fit\n",
        "        X_train.columns = [sanitize_name(col) for col in X_train.columns]\n",
        "        X_test.columns = [sanitize_name(col) for col in X_test.columns]\n",
        "\n",
        "        print(\"⏳ Iniciando optimización...\")\n",
        "        start_time = time.time()\n",
        "        max_total_time = 6000   # 100 minutos en total para TODO el tuning\n",
        "        resultados_modelos = []\n",
        "\n",
        "        for metodo in select_metodos.value:\n",
        "            print(f\"🔍 Método: {metodo}\")\n",
        "            # Simulación de filtrado de variables según el método seleccionado\n",
        "            X_train_sel, X_test_sel = X_train.copy(), X_test.copy()\n",
        "            y_train_sel, y_test_sel = Y_train.copy(), Y_test.copy()\n",
        "\n",
        "            # ──────────────────────────────────────────────────────────\n",
        "            # Si el usuario marca “Todos”, reemplazamos esa opción\n",
        "            motores = list(select_motores.value)\n",
        "            if \"Todos\" in motores:\n",
        "                motores = [\"RandomSearch\", \"BayesianOptimization\", \"Hyperband\", \"Optuna\"]\n",
        "            # ──────────────────────────────────────────────────────────\n",
        "            from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
        "            #from tensorflow_addons.metrics import RSquare  # si lo tienes instalado\n",
        "\n",
        "            #for motor in select_motores.value:\n",
        "            for motor in motores:\n",
        "                print(f\"⚙️ Motor: {motor}\")\n",
        "\n",
        "                # ——— AÑADIDO: tope global de tiempo ———\n",
        "                # 1. Chequeo de tiempo PARA ESTE motor\n",
        "                elapsed = time.time() - start_time\n",
        "                if elapsed > max_total_time:\n",
        "                    print(f\"⏹️ Tiempo agotado antes de {motor} en {metodo}; sigo con el siguiente método.\")\n",
        "                    break   # solo sale del bucle 'motor'\n",
        "                # ——— FIN AÑADIDO ———\n",
        "\n",
        "                metric_name = func_objetivo.value.lower()\n",
        "                direction = 'max' if metric_name == 'r2' else 'min'\n",
        "                # Ahora monitorizamos la métrica de validación correcta:\n",
        "                tuner_metric = f\"val_{metric_name}\"                 # 'val_r2', 'val_mae' o 'val_mse'\n",
        "\n",
        "                #tuner_metric = 'val_loss' if metric_name == 'r2' else metric_name\n",
        "                #tuner_metric = 'val_r2'   if metric_name=='r2' else f'val_{metric_name}'\n",
        "\n",
        "                def build_model(hp):\n",
        "                    model = keras.Sequential()\n",
        "                    model.add(layers.Input(shape=(X_train_sel.shape[1],)))\n",
        "                    for i in range(hp.Int('layers', 1, rango_capas.max)):\n",
        "                        model.add(layers.Dense(hp.Int(f'units_{i}', 8, rango_neuronas.max), activation='relu'))\n",
        "                        model.add(layers.Dropout(hp.Float(f'dropout_{i}', 0.0, dropout_rate.max)))\n",
        "                    model.add(layers.Dense(1))\n",
        "                    #model.compile(optimizer='adam', loss='mse')\n",
        "                    model.compile(\n",
        "                        optimizer='adam',\n",
        "                        loss='mse',\n",
        "                        metrics=[R2(name='r2'),\n",
        "                                tf.keras.metrics.MeanSquaredError(name='mse'),\n",
        "                                tf.keras.metrics.MeanAbsoluteError(name='mae')]\n",
        "                    )\n",
        "                    return model\n",
        "\n",
        "                #callbacks = [EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]\n",
        "                # ——— AÑADIDO: EarlyStopping con min_delta + TimeStopping ———\n",
        "                callbacks = [\n",
        "                    EarlyStopping(\n",
        "                        monitor=tuner_metric,      # mejorar según la métrica que toque\n",
        "                        mode = 'max' if metric_name == 'r2' else 'min',\n",
        "                        min_delta=1e-1,            # mejora mínima del 10%\n",
        "                        patience=3,               # si no mejora tras 3 épocas, cortar\n",
        "                        restore_best_weights=True\n",
        "                    ),\n",
        "                    TimeStopping(max_seconds=120)  # tope de 120 s (~2 min) por modelo\n",
        "                ]\n",
        "                # ——— FIN AÑADIDO ———\n",
        "\n",
        "                # Eliminar directorios previos si existen\n",
        "                if motor == 'RandomSearch':\n",
        "                    tuner_dir = f'randomsearch_dir/random_{metodo}'\n",
        "                    if os.path.exists(tuner_dir):\n",
        "                        shutil.rmtree(tuner_dir)\n",
        "                    tuner = RandomSearch(\n",
        "                        build_model,\n",
        "                        objective=Objective(tuner_metric, direction=direction),\n",
        "                        max_trials=n_trials_slider.value,\n",
        "                        executions_per_trial=1,\n",
        "                        directory='randomsearch_dir',\n",
        "                        project_name=f'random_{metodo}'\n",
        "                    )\n",
        "                    #tuner.search(X_train_sel, y_train_sel, epochs=rango_epocas.value, validation_split=0.2, verbose=1, callbacks=callbacks)\n",
        "                    # ——— AÑADIDO: timeout para este tuner ———\n",
        "                    remaining = max_total_time - (time.time() - start_time)\n",
        "                    # no menos de 1 segundo\n",
        "                    timeout_secs = int(max(1, remaining))\n",
        "                    signal.alarm(timeout_secs)\n",
        "                    try:\n",
        "                        tuner.search(\n",
        "                          X_train_sel, y_train_sel,\n",
        "                          epochs=rango_epocas.value,\n",
        "                          validation_split=0.2,\n",
        "                          verbose=1,\n",
        "                          callbacks=callbacks\n",
        "                        )\n",
        "                    except TimeoutException:\n",
        "                        print(f\"⏹️ RandomSearch ({metodo}) interrumpido tras {timeout_secs}s\")\n",
        "                    finally:\n",
        "                        signal.alarm(0)\n",
        "                  # ——— FIN AÑADIDO ———\n",
        "                    best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "                elif motor == 'BayesianOptimization':\n",
        "                    tuner_dir = f'bo_dir/bo_{metodo}'\n",
        "                    if os.path.exists(tuner_dir):\n",
        "                        shutil.rmtree(tuner_dir)\n",
        "                    tuner = BayesianOptimization(\n",
        "                        build_model,\n",
        "                        objective=Objective(tuner_metric, direction=direction),\n",
        "                        max_trials=n_trials_slider.value,\n",
        "                        directory='bo_dir',\n",
        "                        project_name=f'bo_{metodo}'\n",
        "                    )\n",
        "                    #tuner.search(X_train_sel, y_train_sel, epochs=rango_epocas.value, validation_split=0.2, verbose=1, callbacks=callbacks)\n",
        "                    # ——— AÑADIDO: timeout para este tuner ———\n",
        "                    remaining = max_total_time - (time.time() - start_time)\n",
        "                    # no menos de 1 segundo\n",
        "                    timeout_secs = int(max(1, remaining))\n",
        "                    signal.alarm(timeout_secs)\n",
        "                    try:\n",
        "                        tuner.search(\n",
        "                           X_train_sel, y_train_sel,\n",
        "                           epochs=rango_epocas.value,\n",
        "                           validation_split=0.2,\n",
        "                           verbose=1,\n",
        "                           callbacks=callbacks\n",
        "                        )\n",
        "                    except TimeoutException:\n",
        "                        print(f\"⏹️ RandomSearch ({metodo}) interrumpido tras {timeout_secs}s\")\n",
        "                    finally:\n",
        "                        signal.alarm(0)\n",
        "                    # ——— FIN AÑADIDO ———\n",
        "                    best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "                elif motor == 'Hyperband':\n",
        "                    tuner_dir = f'hyperband_dir/hyper_{metodo}'\n",
        "                    if os.path.exists(tuner_dir):\n",
        "                        shutil.rmtree(tuner_dir)\n",
        "                    tuner = Hyperband(\n",
        "                        build_model,\n",
        "                        objective=Objective(tuner_metric, direction=direction),\n",
        "                        #max_epochs=rango_epocas.max,\n",
        "                        max_epochs=rango_epocas.value,\n",
        "                        factor=4,                         # ← de 2 a 4 ⇒ menos brackets\n",
        "                        directory='hyperband_dir',\n",
        "                        project_name=f'hyper_{metodo}'\n",
        "                    )\n",
        "                    # ─── AÑADIDO AQUÍ: callbacks específicos para Hyperband ───\n",
        "                    callbacks = [\n",
        "                        EarlyStopping(\n",
        "                            monitor=tuner_metric,\n",
        "                            mode = 'max' if metric_name == 'r2' else 'min',  # <— aquí le decimos a Keras qué queremos\n",
        "                            min_delta=1e-2,\n",
        "                            patience=2,\n",
        "                            restore_best_weights=True\n",
        "                        ),\n",
        "                        TimeStopping(max_seconds=120)\n",
        "                    ]\n",
        "                    # ─── FIN AÑADIDO ───\n",
        "\n",
        "                    #tuner.search(X_train_sel, y_train_sel, validation_split=0.2, verbose=1, callbacks=callbacks)\n",
        "                    #tuner.search(X_train_sel, y_train_sel, epochs=rango_epocas.value, validation_split=0.2, verbose=1, callbacks=callbacks)\n",
        "\n",
        "                    # ——— AÑADIDO: timeout para este tuner ———\n",
        "                    remaining = max_total_time - (time.time() - start_time)\n",
        "                    # no menos de 1 segundo\n",
        "                    timeout_secs = int(max(1, remaining))\n",
        "                    signal.alarm(timeout_secs)\n",
        "                    try:\n",
        "                        tuner.search(\n",
        "                           X_train_sel, y_train_sel,\n",
        "                           epochs=rango_epocas.value,\n",
        "                           validation_split=0.2,\n",
        "                           verbose=1,\n",
        "                           callbacks=callbacks\n",
        "                        )\n",
        "                    except TimeoutException:\n",
        "                        print(f\"⏹️ RandomSearch ({metodo}) interrumpido tras {timeout_secs}s\")\n",
        "                    finally:\n",
        "                        signal.alarm(0)\n",
        "                    # ——— FIN AÑADIDO ———\n",
        "                    best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "                elif motor == 'Optuna':\n",
        "                    def objective(trial):\n",
        "                        model = keras.Sequential()\n",
        "                        model.add(layers.Input(shape=(X_train_sel.shape[1],)))\n",
        "                        for i in range(trial.suggest_int('n_layers', 1, rango_capas.max)):\n",
        "                            model.add(layers.Dense(trial.suggest_int(f'n_units_l{i}', 8, rango_neuronas.max), activation='relu'))\n",
        "                            model.add(layers.Dropout(trial.suggest_float(f'dropout_l{i}', 0.0, dropout_rate.max)))\n",
        "                        model.add(layers.Dense(1))\n",
        "                        model.compile(optimizer='adam', loss='mse')\n",
        "                        #model.fit(X_train_sel, y_train_sel, epochs=rango_epocas.value, batch_size=32, verbose=0, validation_split=0.2)\n",
        "                        model.fit(X_train_sel, y_train_sel, epochs=rango_epocas.value, batch_size=32, verbose=1, validation_split=0.2, callbacks=callbacks)\n",
        "                        preds = model.predict(X_test_sel).ravel()\n",
        "                        return -r2_score(y_test_sel, preds) if func_objetivo.value == 'R2' else mean_absolute_error(y_test_sel, preds)\n",
        "\n",
        "                    direction = 'maximize' if func_objetivo.value == 'R2' else 'minimize'\n",
        "\n",
        "                    #study = optuna.create_study(direction=direction)\n",
        "                    # ——— AÑADIDO: Pruner para cortar trials poco prometedores ———\n",
        "                    from optuna.pruners import MedianPruner\n",
        "                    remaining = max_total_time - (time.time() - start_time)\n",
        "                    if remaining <= 0:\n",
        "                        print(\"⏹️ Ya no queda tiempo para Optuna.\")\n",
        "                        continue\n",
        "\n",
        "                    study = optuna.create_study(direction=direction,\n",
        "                                              pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=10))\n",
        "                    # timeout detiene el optimize tras X segundos, sin esperar a n_trials\n",
        "                    study.optimize(objective,\n",
        "                                  n_trials=n_trials_slider.value,\n",
        "                                  timeout=remaining)\n",
        "                    # ——— FIN AÑADIDO ———\n",
        "                    #study.optimize(objective, n_trials=n_trials_slider.value)\n",
        "\n",
        "                    best_params = study.best_params\n",
        "\n",
        "                progreso_bar.value += 1\n",
        "                print(f\"✅ Completado: Método {metodo}, Motor {motor}\")\n",
        "                print(f\"✅ Optimización completada en {time.time() - start_time:.2f} segundos.\")\n",
        "\n",
        "                # Placeholder para evaluación final\n",
        "                model = keras.Sequential()\n",
        "                model.add(layers.Input(shape=(X_train_sel.shape[1],)))\n",
        "                for _ in range(rango_capas.value):\n",
        "                    model.add(layers.Dense(rango_neuronas.value, activation='relu'))\n",
        "                    model.add(layers.Dropout(dropout_rate.value))\n",
        "                model.add(layers.Dense(1))\n",
        "                model.compile(optimizer='adam', loss='mse')\n",
        "                model.fit(X_train_sel, y_train_sel, epochs=rango_epocas.value, batch_size=32, verbose=0, callbacks=callbacks)\n",
        "\n",
        "                y_pred = model.predict(X_test_sel).ravel()\n",
        "                r2 = r2_score(y_test_sel, y_pred)\n",
        "                mae = mean_absolute_error(y_test_sel, y_pred)\n",
        "                mse = mean_squared_error(y_test_sel, y_pred)\n",
        "\n",
        "                # *********************************************************\n",
        "                # Visualización del modelo optimizado\n",
        "                # *********************************************************\n",
        "                fig, ax = plt.subplots(figsize=(6, 4))\n",
        "                ax.scatter(range(len(y_pred)), y_test_sel.values.ravel(), label='Y Real')\n",
        "                ax.plot(range(len(y_pred)), y_pred, color='orange', label='Y Predicho')\n",
        "                ax.set_title(f\"XY-Y Real vs. Predicho: {metodo}-{motor}\")\n",
        "                ax.set_xlabel(\"Casos\")\n",
        "                ax.set_ylabel(\"Y\")\n",
        "                ax.legend()\n",
        "                ax.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # ──────────────────────────────────────────────────────────────\n",
        "                # 🔒  BLOQUE DE PERSISTENCIA  (NO ALTERA LA LÓGICA EXISTENTE)\n",
        "                #     · Guarda el mejor modelo de cada motor en /modelos_opt\n",
        "                #     · Guarda escaladores y columnas en un .pkl auxiliar\n",
        "                #     · Guarda el study de Optuna, si existe\n",
        "                # ──────────────────────────────────────────────────────────────\n",
        "                try:\n",
        "                    from pathlib import Path\n",
        "                    import pickle\n",
        "\n",
        "                    # 1️⃣  Métrica que queremos persistir como «score»\n",
        "                    if func_objetivo.value == \"R2\":\n",
        "                        score_val = r2\n",
        "                    elif func_objetivo.value == \"MAE\":\n",
        "                        score_val = mae\n",
        "                    else:                                 # \"MSE\"\n",
        "                        score_val = mse\n",
        "\n",
        "                    # ---------- rutas ----------\n",
        "                    save_dir = Path(\"modelos_opt\")\n",
        "                    save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "                    # nombre robusto variable-objetivo\n",
        "                    if isinstance(Y_train, pd.Series):\n",
        "                        y_name = Y_train.name or \"target\"\n",
        "                    else:                                # DataFrame\n",
        "                        y_name = Y_train.columns[0] if Y_train.shape[1] == 1 else \"target\"\n",
        "\n",
        "                    base_fname  = f\"nn_{metodo.lower()}_opt_{motor.lower()}\"\n",
        "                    model_fname = save_dir / f\"modelo_{base_fname}.h5\"          # modelo\n",
        "                    meta_fname  = save_dir / f\"meta_{base_fname}.pkl\"           # metadatos\n",
        "                    study_fname = save_dir / f\"optuna_{base_fname}.pkl\"         # estudio Optuna\n",
        "\n",
        "                    # ---------- modelo ----------\n",
        "                    model_to_save = model                # alias universal\n",
        "                    model_to_save.save(model_fname, include_optimizer=True)\n",
        "\n",
        "                    best_hps_dict = {}\n",
        "                    if motor in (\"RandomSearch\",\"BayesianOptimization\",\"Hyperband\"):\n",
        "                        # para Keras Tuner:\n",
        "                        best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "                        best_hps_dict = {\n",
        "                          \"layers\":  best_hps.get(\"layers\"),\n",
        "                          \"neurons\": best_hps.get(\"units_0\"),\n",
        "                          \"dropout\": best_hps.get(\"dropout_0\"),\n",
        "                          \"epochs\":  rango_epocas.value\n",
        "                        }\n",
        "                    elif motor == \"Optuna\":\n",
        "                        # Optuna:\n",
        "                        best_hps_dict = study.best_params.copy()\n",
        "                        best_hps_dict[\"epochs\"] = rango_epocas.value\n",
        "\n",
        "                    # Comprobación de scalers\n",
        "                    if 'sx' not in locals():\n",
        "                        sx = None\n",
        "                    if 'sy' not in locals():\n",
        "                        sy = None\n",
        "\n",
        "                    # Extrae los hiperparámetros relevantes asegurando ambos nombres\n",
        "                    # (esto funciona tanto para Optuna como KerasTuner)\n",
        "                    layers_ = best_hps_dict.get(\"layers\") or best_hps_dict.get(\"n_layers\")\n",
        "                    neurons_ = best_hps_dict.get(\"neurons\") or best_hps_dict.get(\"n_units_l0\")\n",
        "                    dropout_ = best_hps_dict.get(\"dropout\") or best_hps_dict.get(\"dropout_l0\")\n",
        "                    epochs_ = best_hps_dict.get(\"epochs\")\n",
        "\n",
        "                    meta_payload = {\n",
        "                        \"sx\":    locals().get(\"sx\", None),\n",
        "                        \"sy\":    locals().get(\"sy\", None),\n",
        "                        \"cols\":  X_train_sel.columns.tolist(),\n",
        "                        \"yname\": y_name,\n",
        "                        \"score\": float(score_val),\n",
        "                        \"metric\": func_objetivo.value,\n",
        "                        \"motor\": motor,\n",
        "                        \"metodo\": metodo,\n",
        "                        # Nombres duplicados para compatibilidad máxima\n",
        "                        \"layers\": layers_,\n",
        "                        \"n_layers\": layers_,\n",
        "                        \"neurons\": neurons_,\n",
        "                        \"n_units_l0\": neurons_,\n",
        "                        \"dropout\": dropout_,\n",
        "                        \"dropout_l0\": dropout_,\n",
        "                        \"epochs\": epochs_,\n",
        "                        **{k: v for k, v in best_hps_dict.items() if k not in [\"layers\", \"n_layers\", \"neurons\", \"n_units_l0\", \"dropout\", \"dropout_l0\", \"epochs\"]}\n",
        "                    }\n",
        "\n",
        "                    with open(meta_fname, \"wb\") as f_meta:\n",
        "                        pickle.dump(meta_payload, f_meta)\n",
        "\n",
        "                    # ---------- Optuna ----------\n",
        "                    msg_opt = \"\"\n",
        "                    if motor == \"Optuna\" and \"study\" in locals():\n",
        "                        with open(study_fname, \"wb\") as f_st:\n",
        "                            pickle.dump(study, f_st)\n",
        "                        msg_opt = f\" · Estudio guardado → {study_fname}\"\n",
        "\n",
        "                    # ---------- feedback ----------\n",
        "                    try:                                        # usa traza si existe\n",
        "                        with traza:\n",
        "                            print(f\"💾 Modelo guardado → {model_fname}\")\n",
        "                            print(f\"🗂️  Metadatos     → {meta_fname}{msg_opt}\")\n",
        "                    except NameError:\n",
        "                        print(f\"💾 Modelo guardado → {model_fname}\")\n",
        "                        print(f\"🗂️  Metadatos     → {meta_fname}{msg_opt}\")\n",
        "\n",
        "                    # ---------- registro global opcional ----------\n",
        "                    OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "                    OPT_MODELS[(\"nn\", metodo.lower(), motor.lower())] = {\n",
        "                        # 1️⃣ rutas de fichero obligatorias\n",
        "                        \"model_path\":  str(model_fname),\n",
        "                        \"meta_path\":   str(meta_fname),\n",
        "                        # guardamos el propio objeto (o su path si prefieres)\n",
        "                        \"model\":       model_to_save,\n",
        "                        # escaladores\n",
        "                        \"sx\":          sx,\n",
        "                        \"sy\":          sy,\n",
        "                        # columnas utilizadas\n",
        "                        \"cols\":        X_train_sel.columns.tolist(),\n",
        "                        # métrica y score\n",
        "                        \"metric\":      func_objetivo.value,\n",
        "                        \"score\":       float(score_val),\n",
        "                        # metadatos de optimización\n",
        "                        \"motor\":       motor,\n",
        "                        \"metodo\":      metodo,\n",
        "                        # hiperparámetros, duplicados para compat\n",
        "                        \"layers\":      layers_,\n",
        "                        \"n_layers\":    layers_,\n",
        "                        \"neurons\":     neurons_,\n",
        "                        \"n_units_l0\":  neurons_,\n",
        "                        \"dropout\":     dropout_,\n",
        "                        \"dropout_l0\":  dropout_,\n",
        "                        \"epochs\":      epochs_,\n",
        "                        # cualquier otro parámetro de best_hps_dict\n",
        "                        **{k: v for k, v in best_hps_dict.items()\n",
        "                            if k not in {\"layers\",\"neurons\",\"dropout\",\"epochs\"}}\n",
        "                    }\n",
        "\n",
        "                    if motor == \"Optuna\" and \"study\" in locals():\n",
        "                        OPT_MODELS[(\"nn\", metodo.lower(), \"optuna_study\")] = study\n",
        "\n",
        "                except Exception as e:\n",
        "                    # si algo falla, avisa pero NO interrumpe la optimización\n",
        "                    try:\n",
        "                        with traza:\n",
        "                            print(f\"⚠️  No se pudo guardar el modelo o estudio: {e}\")\n",
        "                    except NameError:\n",
        "                        print(f\"⚠️  No se pudo guardar el modelo o estudio: {e}\")\n",
        "                # ──────────────────────────────────────────────────────────────\n",
        "                # 🔚  FIN BLOQUE DE PERSISTENCIA\n",
        "                # ──────────────────────────────────────────────────────────────\n",
        "\n",
        "                resultados_modelos.append({\n",
        "                    'Motor': motor, 'Método': metodo,\n",
        "                    'R2': r2, 'MAE': mae, 'MSE': mse,\n",
        "                    'Épocas': rango_epocas.value,\n",
        "                    'Capas': rango_capas.value,\n",
        "                    'Neuronas': rango_neuronas.value,\n",
        "                    'Dropout': dropout_rate.value,\n",
        "                    'L2': l2_reg.value\n",
        "                })\n",
        "\n",
        "            # tras bucle motores, chequeo global de tiempo  # MODIFICADO\n",
        "            if time.time() - start_time > max_total_time:\n",
        "                print(\"⏹️ Tiempo total agotado; salgo de métodos.\")\n",
        "                break  # rompe bucle métodos\n",
        "\n",
        "        global df_results\n",
        "        df_results = pd.DataFrame(resultados_modelos)\n",
        "        df_top5 = df_results.sort_values(by=func_objetivo.value, ascending=(func_objetivo.value != 'R2')).head(5)\n",
        "        best = df_top5.iloc[0]\n",
        "\n",
        "        display(HTML(\"<h4>🏆 Top 5 Modelos Optim.</h4>\"))\n",
        "        display(df_top5.style.set_caption(\"Modelos Óptimos\").format(precision=4))\n",
        "\n",
        "        y_pred = model.predict(X_test_sel).ravel()\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        ax.scatter(y_test_sel, y_pred, alpha=0.6)\n",
        "        ax.plot([y_test_sel.min(), y_test_sel.max()], [y_test_sel.min(), y_test_sel.max()], 'r--')\n",
        "        ax.set_title(\"Y Real vs. Y Predicho\")\n",
        "        ax.set_xlabel(\"Y Real\")\n",
        "        ax.set_ylabel(\"Y Predicho\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        #residuos = y_test_sel - y_pred\n",
        "        residuos = y_test_sel.values.ravel() - y_pred\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.scatter(y_pred, residuos, alpha=0.6)\n",
        "        plt.axhline(0, color='red', linestyle='--')\n",
        "        plt.title(\"Residuos vs. Predicción\")\n",
        "        plt.xlabel(\"Y Predicho\")\n",
        "        plt.ylabel(\"Residuos\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        stat, p_value = shapiro(residuos)\n",
        "        display(HTML(f\"<b>📊 Test de Shapiro:</b> p = {p_value:.5f} → {'Normal' if p_value > 0.05 else 'No normal'}\"))\n",
        "\n",
        "        from math import pi\n",
        "        categorias = ['R2', 'MAE', 'MSE']\n",
        "        valores = [best['R2'], best['MAE'], best['MSE']]\n",
        "        valores += valores[:1]\n",
        "        angles = [n / float(len(categorias)) * 2 * pi for n in range(len(categorias))]\n",
        "        angles += angles[:1]\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        ax = plt.subplot(111, polar=True)\n",
        "        plt.xticks(angles[:-1], categorias)\n",
        "        ax.plot(angles, valores, linewidth=2)\n",
        "        ax.fill(angles, valores, alpha=0.3)\n",
        "        plt.title(\"Radar de Métricas\")\n",
        "        plt.show()\n",
        "\n",
        "        global NN_RESULTADOS_TOP5, NN_MEJOR_MODELO, NN_RESIDUOS, NN_METODO_MEJOR, NN_MOTOR_MEJOR\n",
        "        NN_RESULTADOS_TOP5 = df_top5\n",
        "        NN_MEJOR_MODELO = best\n",
        "        NN_RESIDUOS = residuos\n",
        "        NN_METODO_MEJOR = best['Método']\n",
        "        NN_MOTOR_MEJOR = best['Motor']\n",
        "\n",
        "# ***********************************************************************\n",
        "# Visualización de los mejores modelos optimizados\n",
        "# ***********************************************************************\n",
        "        for idx, row in df_top5.iterrows():\n",
        "            metodo, motor = row['Método'], row['Motor']\n",
        "            model = keras.Sequential()\n",
        "            model.add(layers.Input(shape=(X_train_sel.shape[1],)))\n",
        "            for _ in range(int(row['Capas'])):\n",
        "                model.add(layers.Dense(int(row['Neuronas']), activation='relu'))\n",
        "                model.add(layers.Dropout(row['Dropout']))\n",
        "            model.add(layers.Dense(1))\n",
        "            model.compile(optimizer='adam', loss='mse')\n",
        "            model.fit(X_train_sel, y_train_sel, epochs=rango_epocas.value, batch_size=32, verbose=0, callbacks=callbacks)\n",
        "\n",
        "            y_pred = model.predict(X_test_sel).ravel()\n",
        "            fig, ax = plt.subplots(figsize=(6, 4))\n",
        "            ax.scatter(range(len(y_pred)), y_test_sel.values.ravel(), label='Y Real')\n",
        "            ax.plot(range(len(y_pred)), y_pred, color='orange', label='Y Predicho')\n",
        "            ax.set_title(f\"XY-Y Real vs. Predicho: {metodo}-{motor}\")\n",
        "            ax.set_xlabel(\"Casos\")\n",
        "            ax.set_ylabel(\"Y\")\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        print(f\"✅ Optimización completada en {time.time() - start_time:.2f} segundos.\")\n",
        "\n",
        "# ========== Visualización y conexión ==========\n",
        "control_panel = VBox([\n",
        "    HTML(\"<h3>🔧 Configuración Optimización NN</h3>\"),\n",
        "    ayuda_parametros,\n",
        "    HBox([select_metodos, select_motores]),\n",
        "    func_objetivo,\n",
        "    n_trials_slider,\n",
        "    HBox([rango_epocas, rango_capas]),\n",
        "    HBox([rango_neuronas, dropout_rate, l2_reg]),\n",
        "    btn_ejecutar,\n",
        "    progreso_bar,\n",
        "    out_nn\n",
        "])\n",
        "\n",
        "def mostrar_optimizacion_nn():\n",
        "    display(control_panel)\n",
        "\n",
        "try:\n",
        "    btn_ejecutar._click_handlers.callbacks.clear()\n",
        "except:\n",
        "    pass\n",
        "btn_ejecutar.on_click(ejecutar_optimizacion)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================\n",
        "# Cell 9.3: OPTIMIZACIÓN XGBOOST - FUNCIONA CORRECTAMENTE\n",
        "# Optimización múltiple de hiperparámetros del modelo XGBoost\n",
        "# Compatible con motores: RandomSearch, Bayesian, Hyperband y Optuna\n",
        "# Variables: seleccionadas desde Cell 6 (Pearson, Spearman, MutualInfo, Boruta, UMAP, Todos)\n",
        "# Datos: segmentados desde Cell 5.2 (X_train, Y_train, X_test, Y_test)\n",
        "# ===============================================\n",
        "def mostrar_optimizacion_xgb():\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import pandas as pd\n",
        "    import ipywidgets as widgets\n",
        "    import traceback\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    from xgboost import XGBRegressor\n",
        "    import numpy as np\n",
        "    from skopt import BayesSearchCV  # Motor de optimización bayesiano\n",
        "    # Importar Hyperband\n",
        "    from sklearn.experimental import enable_halving_search_cv  # noqa\n",
        "    from sklearn.model_selection import HalvingRandomSearchCV\n",
        "    # Importar Optuna\n",
        "    from optuna.integration import OptunaSearchCV\n",
        "    import optuna\n",
        "    optuna.logging.set_verbosity(optuna.logging.INFO)\n",
        "    from optuna.distributions import IntDistribution, FloatDistribution\n",
        "\n",
        "    # Mostrar progreso en RandomSearch mediante logging\n",
        "    import logging\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    global out_opt_xgb, traza_xgb\n",
        "    out_opt_xgb   = widgets.Output()      # panel principal\n",
        "    traza_xgb     = widgets.Output()      # trazas de cada motor\n",
        "\n",
        "    # ========= AÑADIDOS: Funciones de saneamiento =========\n",
        "\n",
        "    # 2) Ahora sí puedo sanear columnas\n",
        "    # Usar sanitize_name para limpiar columnas en el payload\n",
        "    def clean_cols(col_list):\n",
        "        return [sanitize_name(c) for c in col_list]\n",
        "    # Ejemplo de sanitización de X_train antes de fit\n",
        "    X_train.columns = [sanitize_name(col) for col in X_train.columns]\n",
        "    X_test.columns = [sanitize_name(col) for col in X_test.columns]\n",
        "\n",
        "    # ——— SANITIZAR LISTAS DE RESUMEN_METODOS ———\n",
        "    for metodo, vars_ in RESUMEN_METODOS.items():\n",
        "        if isinstance(vars_, list):\n",
        "            RESUMEN_METODOS[metodo] = [sanitize_name(v) for v in vars_]\n",
        "        elif isinstance(vars_, pd.DataFrame) and not vars_.empty:\n",
        "            # asumimos que la columna de variable se llama \"Variable\" o la primera\n",
        "            col = 'Variable' if 'Variable' in vars_.columns else vars_.columns[0]\n",
        "            # saneamos esa columna in‑place\n",
        "            RESUMEN_METODOS[metodo][col] = vars_[col].astype(str).map(sanitize_name)\n",
        "    # ——— FIN SANITIZACIÓN RESUMEN_METODOS ———\n",
        "\n",
        "    #import re\n",
        "\n",
        "    #def clean_columns(df):\n",
        "    #    \"\"\"\n",
        "    #    Transforma todos los nombres de columna a str y sustituye\n",
        "    #    corchetes, %, <, > y espacios por guiones bajos.\n",
        "    #    \"\"\"\n",
        "    #    df = df.copy()\n",
        "    #    df.columns = (\n",
        "    #        df.columns\n",
        "    #          .astype(str)\n",
        "    #          .str.replace(r'[\\[\\]<>%]', '_', regex=True)\n",
        "    #          .str.replace(r'\\s+', '_', regex=True)\n",
        "    #          .str.strip('_')\n",
        "    #    )\n",
        "    #    return df\n",
        "\n",
        "    #def clean_cols(var_list):\n",
        "    #    \"\"\"\n",
        "    #    Limpia una lista de nombres de columna con las mismas reglas.\n",
        "    #    \"\"\"\n",
        "    #    return [\n",
        "    #        re.sub(r'[\\[\\]<>%]', '_', str(v))\n",
        "    #          .replace(' ', '_')\n",
        "    #          .strip('_')\n",
        "    #        for v in var_list\n",
        "    #    ]\n",
        "\n",
        "    # ======================================================\n",
        "\n",
        "    # 📌 Parámetros configurables del motor de optimización\n",
        "    slider_n_iter = widgets.IntSlider(value=50, min=10, max=300, step=10,\n",
        "                                      description='n_iter:', layout=widgets.Layout(width='45%'))\n",
        "    ayuda_n_iter = widgets.HTML(\"<small><b>n_iter:</b> número de combinaciones aleatorias a probar (mayor = más preciso, pero más lento). No aplica a  Hyperbrand.</small>\")\n",
        "\n",
        "    slider_cv = widgets.IntSlider(value=3, min=2, max=10, step=1,\n",
        "                                  description='cv:', layout=widgets.Layout(width='45%'))\n",
        "    ayuda_cv = widgets.HTML(\"<small><b>cv:</b> número de particiones para validación cruzada (mínimo 2)</small>\")\n",
        "\n",
        "    selector_funcion_objetivo = widgets.Dropdown(\n",
        "        options=['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
        "        value='r2',\n",
        "        description='Función:',\n",
        "        layout=widgets.Layout(width='45%')\n",
        "    )\n",
        "    ayuda_funcion = widgets.HTML(\"<small><b>Función:</b> métrica a optimizar. R2 para ajuste, MSE o MAE para error</small>\")\n",
        "\n",
        "    def seleccionar_variables_filtradas(metodo):\n",
        "        global X_train, Y_train, X_test, RESUMEN_METODOS\n",
        "\n",
        "        # --- AÑADIDO: sanear columnas globales antes de todo ---\n",
        "        #X_train = clean_columns(X_train)\n",
        "        #X_test  = clean_columns(X_test)\n",
        "        # --- FIN AÑADIDO -----------------------------------------\n",
        "\n",
        "        print(f\"\\n🔧 [seleccionar_variables_filtradas] Iniciando con método: '{metodo}'\")\n",
        "        try:\n",
        "            assert 'X_train' in globals(), \"❌ 'X_train' no está definido\"\n",
        "            assert 'Y_train' in globals(), \"❌ 'Y_train' no está definido\"\n",
        "            assert 'RESUMEN_METODOS' in globals(), \"❌ 'RESUMEN_METODOS' no está definido\"\n",
        "\n",
        "            vars_sel = []\n",
        "            if metodo.strip().lower() == \"todos\":\n",
        "                all_vars = []\n",
        "                for k, df in RESUMEN_METODOS.items():\n",
        "                    if isinstance(df, list):\n",
        "                        all_vars.extend(df)\n",
        "                    elif isinstance(df, pd.DataFrame) and not df.empty:\n",
        "                        col = 'Variable' if 'Variable' in df.columns else df.columns[0]\n",
        "                        all_vars.extend(df[col].dropna().tolist())\n",
        "                vars_sel = list(set(all_vars))\n",
        "            elif metodo in RESUMEN_METODOS:\n",
        "                df_vars = RESUMEN_METODOS[metodo]\n",
        "                if isinstance(df_vars, list):\n",
        "                    vars_sel = df_vars\n",
        "                elif isinstance(df_vars, pd.DataFrame):\n",
        "                    if not df_vars.empty:\n",
        "                        col = 'Variable' if 'Variable' in df_vars.columns else df_vars.columns[0]\n",
        "                        vars_sel = df_vars[col].dropna().tolist()\n",
        "                    else:\n",
        "                        return\n",
        "                else:\n",
        "                    return\n",
        "            else:\n",
        "                return\n",
        "            if not vars_sel:\n",
        "                return\n",
        "\n",
        "            columnas_faltantes = [col for col in vars_sel if col not in X_train.columns]\n",
        "            if columnas_faltantes:\n",
        "                print(f\"❌ Columnas no existentes en X_train: {columnas_faltantes}\")\n",
        "                return\n",
        "\n",
        "\n",
        "        #    # ——— AÑADIDO: obtener y limpiar lista raw_vars ———\n",
        "        #    raw_vars = []\n",
        "        #    if metodo.strip().lower() == \"todos\":\n",
        "        #        all_vars = []\n",
        "        #        for df in RESUMEN_METODOS.values():\n",
        "        #            if isinstance(df, list):\n",
        "        #                all_vars += df\n",
        "        #            elif isinstance(df, pd.DataFrame) and not df.empty:\n",
        "        #                col = 'Variable' if 'Variable' in df.columns else df.columns[0]\n",
        "        #                all_vars += df[col].dropna().tolist()\n",
        "        #        raw_vars = list(set(all_vars))\n",
        "        #    elif metodo in RESUMEN_METODOS:\n",
        "        #        df_vars = RESUMEN_METODOS[metodo]\n",
        "        #        if isinstance(df_vars, list):\n",
        "        #            raw_vars = df_vars\n",
        "        #        elif isinstance(df_vars, pd.DataFrame) and not df_vars.empty:\n",
        "        #            col = 'Variable' if 'Variable' in df_vars.columns else df_vars.columns[0]\n",
        "        #            raw_vars = df_vars[col].dropna().tolist()\n",
        "        #    else:\n",
        "        #        return\n",
        "\n",
        "        #    if not raw_vars:\n",
        "        #        with traza_xgb:\n",
        "        #            print(f\"⚠️ No hay variables para '{metodo}'.\")\n",
        "        #        return\n",
        "\n",
        "            # limpiar lista de nombres\n",
        "        #    vars_sel = clean_cols(raw_vars)\n",
        "\n",
        "        #    # comprobar que existen tras limpiar\n",
        "        #    faltantes = [c for c in vars_sel if c not in X_train.columns]\n",
        "        #    if faltantes:\n",
        "        #        with traza_xgb:\n",
        "        #            print(f\"❌ Columnas no existentes en X_train: {faltantes}\")\n",
        "        #        return\n",
        "        #    # ——— FIN AÑADIDO ———\n",
        "\n",
        "            X_sel = X_train[vars_sel].copy()\n",
        "            Y_sel = Y_train.copy()\n",
        "            globals()['X_train_filtrado'] = X_sel\n",
        "            globals()['Y_train_filtrado'] = Y_sel\n",
        "            globals()['metodo_usado_xgb'] = metodo\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"❌ Excepción atrapada desde consola principal:\")\n",
        "            print(traceback.format_exc())\n",
        "\n",
        "    def ejecutar_metricas_finales(modelo, nombre_motor=\"Desconocido\"):\n",
        "        try:\n",
        "            X_test_filtrado = X_test[X_train_filtrado.columns]\n",
        "            preds = best_model.predict(X_test_filtrado)\n",
        "\n",
        "            r2 = r2_score(Y_test, preds)\n",
        "            mse = mean_squared_error(Y_test, preds)\n",
        "            mae = mean_absolute_error(Y_test, preds)\n",
        "\n",
        "            df_metricas = pd.DataFrame({\n",
        "                'Métrica': ['R2', 'MSE', 'MAE'],\n",
        "                'Valor': [r2, mse, mae]\n",
        "            })\n",
        "            display(df_metricas.style.set_caption(\"📈 Rendimiento del Modelo Óptimo\").format(precision=4))\n",
        "\n",
        "            # Cálculo de residuos y análisis\n",
        "            residuos = Y_test.values.ravel() - preds.ravel()\n",
        "            df_residuos = pd.DataFrame({\n",
        "                'Índice': range(len(residuos)),\n",
        "                'Y_real': Y_test.values.ravel(),\n",
        "                'Y_predicho': preds.ravel(),\n",
        "                'Residuo': residuos\n",
        "            })\n",
        "            display(df_residuos.head().style.set_caption(\"🧮 Ejemplo de Cálculo de Residuos\"))\n",
        "\n",
        "            # Histograma de residuos\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            sns.histplot(residuos, bins=30, kde=True, color='skyblue')\n",
        "            plt.axvline(0, color='red', linestyle='--')\n",
        "            plt.title(\"Histograma de Residuos\")\n",
        "            plt.xlabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "            plt.ylabel(\"Frecuencia\")\n",
        "            plt.show()\n",
        "\n",
        "            # 📌 Prueba de normalidad de Shapiro-Wilk\n",
        "            stat, p_value = shapiro(residuos)\n",
        "            display(HTML(f\"<h4>🧪 Prueba de Normalidad (Shapiro-Wilk)</h4><ul><li>Estadístico: {stat:.4f}</li><li>p-valor: {p_value:.4f}</li><li>{'✅ Los residuos siguen una distribución normal (p > 0.05)' if p_value > 0.05 else '⚠️ Los residuos no siguen una distribución normal (p ≤ 0.05)'}</li></ul>\"))\n",
        "\n",
        "            # Explicación de los resultados\n",
        "            display(HTML(\"\"\"\n",
        "                <h4>🧾 Explicación de Resultados:</h4>\n",
        "                <ul>\n",
        "                    <li><b>R2:</b> mide el grado de ajuste del modelo. Valores cercanos a 1 indican buen ajuste.</li>\n",
        "                    <li><b>MSE:</b> error cuadrático medio. Penaliza más los errores grandes.</li>\n",
        "                    <li><b>MAE:</b> error absoluto medio. Más robusto ante valores atípicos.</li>\n",
        "                    <li><b>Residuos:</b> diferencia entre el valor real y el predicho. Deben estar centrados en 0.</li>\n",
        "                    <li><b>Histograma:</b> ayuda a evaluar si los residuos siguen una distribución normal.</li>\n",
        "                    <li><b>Shapiro-Wilk:</b> prueba estadística que indica si los residuos son normales. Se acepta normalidad si p > 0.05.</li>\n",
        "                </ul>\n",
        "            \"\"\"))\n",
        "\n",
        "            # Gráficas modelo optimo\n",
        "            plt.figure(figsize=(10, 4))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(Y_test.values, label='Real')\n",
        "            plt.plot(preds, label='Predicho')\n",
        "            plt.legend()\n",
        "            plt.title(\"X-Y Real vs X-Y Predicho\")\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.scatter(Y_test, preds, alpha=0.6)\n",
        "            min_val = min(Y_test.values.min(), preds.min())\n",
        "            max_val = max(Y_test.values.max(), preds.max())\n",
        "            plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal')\n",
        "            plt.xlabel(\"Y real\")\n",
        "            plt.ylabel(\"Y predicho\")\n",
        "            plt.title(\"Y Real vs Y Predicho\")\n",
        "            plt.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"❌ Error en métricas finales:\", traceback.format_exc())\n",
        "    # ────────────────────────────────────────────────────────────────\n",
        "    # 🔒 FUNCIÓN DE PERSISTENCIA  (XGBoost)\n",
        "    # ────────────────────────────────────────────────────────────────\n",
        "    import pickle, pathlib, datetime\n",
        "    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "    def guardar_xgb(best_model, best_params, nombre_motor,\n",
        "                    metodo_usado_xgb, selector_funcion_objetivo,\n",
        "                    X_train_filtrado, X_test, Y_train, Y_test,\n",
        "                    study=None, traza_out=None):\n",
        "        score_val = None\n",
        "        try:\n",
        "            # 1) carpeta\n",
        "            pathlib.Path(\"modelos_opt\").mkdir(exist_ok=True)\n",
        "\n",
        "            # 3) nombre robusto de variable-objetivo\n",
        "            y_name = getattr(Y_train, \"name\", None) or \\\n",
        "                    (Y_train.columns[0] if hasattr(Y_train, \"columns\") else \"Y\")\n",
        "\n",
        "            # 4) rutas\n",
        "            ts      = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            tag     = f\"xgb_{metodo_usado_xgb.lower()}_{nombre_motor.lower()}_opt_{ts}\"\n",
        "            model_f = f\"modelos_opt/{tag}.pkl\"\n",
        "            meta_f  = f\"modelos_opt/{tag}_meta.pkl\"\n",
        "            study_f = f\"modelos_opt/{tag}_study.pkl\"\n",
        "\n",
        "            # 5) modelo\n",
        "            with open(model_f, \"wb\") as f:\n",
        "                pickle.dump(best_model, f)\n",
        "\n",
        "            # 6) metadatos\n",
        "            meta = dict(\n",
        "                score       = float(score_val),\n",
        "                func_obj    = selector_funcion_objetivo.value,\n",
        "                motor       = nombre_motor,\n",
        "                metodo_x    = metodo_usado_xgb,\n",
        "                cols        = list(X_train_filtrado.columns),\n",
        "                yname       = y_name,\n",
        "                best_params = best_params,\n",
        "                fecha       = ts,\n",
        "            )\n",
        "            with open(meta_f, \"wb\") as f:\n",
        "                pickle.dump(meta, f)\n",
        "\n",
        "            # 7) estudio Optuna (si procede)\n",
        "            if nombre_motor.lower() == \"optuna\" and study is not None:\n",
        "                with open(study_f, \"wb\") as f:\n",
        "                    pickle.dump(study, f)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Si score_val aún no se ha calculado, no emitir warning\n",
        "            if score_val is None:\n",
        "                return\n",
        "            if traza_out is not None:\n",
        "                with traza_out:\n",
        "                    print(f\"⚠️  No se pudo guardar modelo/estudio: {e}\")\n",
        "            else:\n",
        "                print(f\"⚠️  No se pudo guardar modelo/estudio: {e}\")\n",
        "\n",
        "    # ===================================================\n",
        "    # Motor de Optimización RandomSearch CV\n",
        "    # ===================================================\n",
        "    def optimizar_randomsearch():\n",
        "        try:\n",
        "            assert 'X_train_filtrado' in globals()\n",
        "            assert 'Y_train_filtrado' in globals()\n",
        "\n",
        "            print(\"📌 Iniciando optimización con RandomSearch...\")\n",
        "            funcion_objetivo = selector_funcion_objetivo.value\n",
        "            n_iter_val = slider_n_iter.value\n",
        "            cv_val = slider_cv.value\n",
        "            print(f\"🎯 Función de optimización seleccionada: {funcion_objetivo} (n_iter={n_iter_val}, cv={cv_val})\")\n",
        "\n",
        "            param_dist = {\n",
        "                'n_estimators': list(range(50, 300)),\n",
        "                'max_depth': list(range(3, 15)),\n",
        "                'learning_rate': np.linspace(0.01, 0.3, 30),\n",
        "                'subsample': np.linspace(0.5, 1.0, 20),\n",
        "                'colsample_bytree': np.linspace(0.5, 1.0, 20),\n",
        "                'gamma': np.linspace(0, 5, 20)\n",
        "            }\n",
        "\n",
        "            print(f\"🔧 Hiperparámetros a optimizar: {list(param_dist.keys())}\")\n",
        "            print(f\"🔁 Número de iteraciones: {n_iter_val}, Validación cruzada (cv): {cv_val}\")\n",
        "\n",
        "            model = XGBRegressor(random_state=42, verbosity=0)\n",
        "            search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
        "                                        n_iter=n_iter_val, scoring=funcion_objetivo, cv=cv_val, random_state=42,\n",
        "                                        n_jobs=-1, verbose=3)\n",
        "\n",
        "            # ——— AÑADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TRAIN ———\n",
        "            # 1) Forzar que todos los nombres sean str\n",
        "            #X_train_filtrado.columns = X_train_filtrado.columns.astype(str)\n",
        "            ## 2) Reemplazar corchetes y '<', '>' por '_'\n",
        "            #X_train_filtrado.columns = (\n",
        "            #    X_train_filtrado\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            # ——— FIN AÑADIDO ———\n",
        "            # # ——— AÑADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TEST ———\n",
        "            #X_test.columns = X_test.columns.astype(str)\n",
        "            #X_test.columns = (\n",
        "            #    X_test\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            # ——— FIN AÑADIDOS ———\n",
        "\n",
        "            search.fit(X_train_filtrado, Y_train_filtrado)\n",
        "\n",
        "            global best_model, best_params\n",
        "            best_model = search.best_estimator_\n",
        "            best_params = search.best_params_\n",
        "            tabla_resultados = pd.DataFrame(best_params.items(), columns=['Hiperparámetro', 'Valor óptimo'])\n",
        "            display(tabla_resultados.style.set_caption(\"📋 Tabla de Hiperparámetros Óptimos\").format(precision=4))\n",
        "\n",
        "            ejecutar_metricas_finales(best_model, nombre_motor=\"RandomSearch\")\n",
        "\n",
        "            # ─── Tras best_model, best_params en optimizar_randomsearch() ───\n",
        "            # Calcular el score real con los datos de test\n",
        "            preds = best_model.predict(X_test[X_train_filtrado.columns])\n",
        "            if selector_funcion_objetivo.value == \"r2\":\n",
        "                score_val = r2_score(Y_test, preds)\n",
        "            elif selector_funcion_objetivo.value == \"neg_mean_absolute_error\":\n",
        "                score_val = mean_absolute_error(Y_test, preds)\n",
        "            else:\n",
        "                score_val = mean_squared_error(Y_test, preds)\n",
        "\n",
        "            # Guardar en OPT_MODELS\n",
        "            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "            OPT_MODELS[(\"xgb\", metodo_usado_xgb.lower(), \"randomsearch\")] = {\n",
        "                \"model\":       best_model,\n",
        "                \"score\":       float(score_val),\n",
        "                \"metric\":      selector_funcion_objetivo.value,\n",
        "                \"param_dist\":  param_dist,           # <- espacio RandomSearch\n",
        "                \"best_params\": best_params,\n",
        "                \"cols\":        list(X_train_filtrado.columns)\n",
        "            }\n",
        "            # ──────────────────────────────────────────────────────────────────\n",
        "\n",
        "            # --- llamada de persistencia ----\n",
        "            guardar_xgb(best_model, best_params, \"RandomSearch\",\n",
        "                        metodo_usado_xgb, selector_funcion_objetivo,\n",
        "                        X_train_filtrado, X_test, Y_train, Y_test,\n",
        "                        study=None, traza_out=traza_xgb)\n",
        "            print(\"✅ Optimización con RandomSearch completada.\")\n",
        "            # ─── 3) Imprime la confirmación aquí dentro ───\n",
        "            if traza_xgb is not None:\n",
        "                with traza_xgb:\n",
        "                    print(f\"💾 Modelo guardado     → modelos_opt/xgb_{metodo_usado_xgb.lower()}_randomsearch_opt_*.pkl\")\n",
        "                    print(f\"💾 Metadatos guardados → modelos_opt/xgb_{metodo_usado_xgb.lower()}_randomsearch_opt_*_meta.pkl\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"❌ Error en optimización RandomSearch:\", traceback.format_exc())\n",
        "\n",
        "    # ===================================================\n",
        "    # Motor de Optimización Bayesian\n",
        "    # ===================================================\n",
        "    def optimizar_bayesian():\n",
        "        try:\n",
        "            assert 'X_train_filtrado' in globals()\n",
        "            assert 'Y_train_filtrado' in globals()\n",
        "\n",
        "            print(\"📌 Iniciando optimización con Bayesian Optimization...\")\n",
        "            funcion_objetivo = selector_funcion_objetivo.value\n",
        "            n_iter_val = slider_n_iter.value\n",
        "            cv_val = slider_cv.value\n",
        "\n",
        "            param_spaces = {\n",
        "                'n_estimators': (50, 300),\n",
        "                'max_depth': (3, 15),\n",
        "                'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
        "                'subsample': (0.5, 1.0),\n",
        "                'colsample_bytree': (0.5, 1.0),\n",
        "                'gamma': (0.0, 5.0)\n",
        "            }\n",
        "\n",
        "            print(f\"🔧 Hiperparámetros a optimizar: {list(param_spaces.keys())}\")\n",
        "            print(f\"🔁 Número de iteraciones: {n_iter_val}, Validación cruzada (cv): {cv_val}\")\n",
        "\n",
        "            model = XGBRegressor(random_state=42, verbosity=0)\n",
        "\n",
        "            opt = BayesSearchCV(model, search_spaces=param_spaces,\n",
        "                n_iter=n_iter_val, scoring=funcion_objetivo, cv=cv_val,\n",
        "                n_jobs=-1, verbose=3, random_state=42)\n",
        "            opt.fit(X_train_filtrado, Y_train_filtrado)\n",
        "\n",
        "            global best_model, best_params\n",
        "            if opt.best_estimator_ is not None:\n",
        "                best_model = opt.best_estimator_\n",
        "                best_params = opt.best_params_\n",
        "\n",
        "            tabla_resultados = pd.DataFrame(best_params.items(), columns=['Hiperparámetro', 'Valor óptimo'])\n",
        "            display(tabla_resultados.style.set_caption(\"📋 Tabla de Hiperparámetros Óptimos (Bayesian)\").format(precision=4))\n",
        "\n",
        "            ejecutar_metricas_finales(best_model, nombre_motor=\"Bayesian\")\n",
        "\n",
        "            # ─── Tras best_model, best_params en optimizar_bayesian() ───\n",
        "            preds = best_model.predict(X_test[X_train_filtrado.columns])\n",
        "            if selector_funcion_objetivo.value == \"r2\":\n",
        "                score_val = r2_score(Y_test, preds)\n",
        "            elif selector_funcion_objetivo.value == \"neg_mean_absolute_error\":\n",
        "                score_val = mean_absolute_error(Y_test, preds)\n",
        "            else:\n",
        "                score_val = mean_squared_error(Y_test, preds)\n",
        "\n",
        "            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "            OPT_MODELS[(\"xgb\", metodo_usado_xgb.lower(), \"bayesian\")] = {\n",
        "                \"model\":       best_model,\n",
        "                \"score\":       float(score_val),\n",
        "                \"metric\":      selector_funcion_objetivo.value,\n",
        "                \"param_spaces\": param_spaces,      # <- espacio BayesSearchCV\n",
        "                \"best_params\": best_params,\n",
        "                \"cols\":        list(X_train_filtrado.columns)\n",
        "            }\n",
        "\n",
        "            # --- llamada de persistencia ----\n",
        "            guardar_xgb(best_model, best_params, \"Bayesian\",\n",
        "                        metodo_usado_xgb, selector_funcion_objetivo,\n",
        "                        X_train_filtrado, X_test, Y_train, Y_test,\n",
        "                        study=None, traza_out=traza_xgb)\n",
        "            print(\"✅ Optimización con Bayesian Optimization completada.\")\n",
        "            with traza_xgb:\n",
        "                print(f\"💾 Modelo guardado     → modelos_opt/xgb_{metodo_usado_xgb.lower()}_bayesian_opt_*.pkl\")\n",
        "                print(f\"💾 Metadatos guardados → modelos_opt/xgb_{metodo_usado_xgb.lower()}_bayesian_opt_*_meta.pkl\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"❌ Error en optimización Bayesian:\", traceback.format_exc())\n",
        "\n",
        "    # ===================================================\n",
        "    # Motor de Optimización HyperBrand\n",
        "    # ===================================================\n",
        "    def optimizar_hyperband():\n",
        "        try:\n",
        "            assert 'X_train_filtrado' in globals()\n",
        "            assert 'Y_train_filtrado' in globals()\n",
        "\n",
        "            print(\"📌 Iniciando optimización con Hyperband...\")\n",
        "            funcion_objetivo = selector_funcion_objetivo.value\n",
        "            cv_val = slider_cv.value\n",
        "\n",
        "            # 🔧 Corregido: se elimina 'n_estimators' de los hiperparámetros buscados\n",
        "            param_dist = {\n",
        "                'max_depth': list(range(3, 15)),\n",
        "                'learning_rate': np.linspace(0.01, 0.3, 30),\n",
        "                'subsample': np.linspace(0.5, 1.0, 20),\n",
        "                'colsample_bytree': np.linspace(0.5, 1.0, 20),\n",
        "                'gamma': np.linspace(0, 5, 20)\n",
        "            }\n",
        "\n",
        "            model = XGBRegressor(random_state=42, verbosity=0)\n",
        "            search = HalvingRandomSearchCV(model, param_dist,\n",
        "                                          scoring=funcion_objetivo, cv=cv_val,\n",
        "                                          factor=3, resource='n_estimators',\n",
        "                                          max_resources=300, random_state=42,\n",
        "                                          verbose=2, n_jobs=-1)\n",
        "\n",
        "            # ——— AÑADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TRAIN ———\n",
        "            # 1) Forzar que todos los nombres sean str\n",
        "            #X_train_filtrado.columns = X_train_filtrado.columns.astype(str)\n",
        "            # 2) Reemplazar corchetes y '<', '>' por '_'\n",
        "            #X_train_filtrado.columns = (\n",
        "            #    X_train_filtrado\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            # ——— FIN AÑADIDO ———\n",
        "            # # ——— AÑADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TEST ———\n",
        "            #X_test.columns = X_test.columns.astype(str)\n",
        "            #X_test.columns = (\n",
        "            #    X_test\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            # ——— FIN AÑADIDOS ———\n",
        "\n",
        "            search.fit(X_train_filtrado, Y_train_filtrado)\n",
        "\n",
        "            global best_model, best_params\n",
        "            best_model = search.best_estimator_\n",
        "            best_params = search.best_params_\n",
        "\n",
        "            tabla_resultados = pd.DataFrame(best_params.items(), columns=['Hiperparámetro', 'Valor óptimo'])\n",
        "            display(tabla_resultados.style.set_caption(\"📋 Tabla de Hiperparámetros Óptimos (Hyperband)\").format(precision=4))\n",
        "\n",
        "            ejecutar_metricas_finales(best_model, nombre_motor=\"Hyperband\")\n",
        "\n",
        "            # ─── Tras best_model, best_params en optimizar_hyperband() ───\n",
        "            preds = best_model.predict(X_test[X_train_filtrado.columns])\n",
        "            if selector_funcion_objetivo.value == \"r2\":\n",
        "                score_val = r2_score(Y_test, preds)\n",
        "            elif selector_funcion_objetivo.value == \"neg_mean_absolute_error\":\n",
        "                score_val = mean_absolute_error(Y_test, preds)\n",
        "            else:\n",
        "                score_val = mean_squared_error(Y_test, preds)\n",
        "\n",
        "            # Hyperband usaba param_dist también\n",
        "            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "            OPT_MODELS[(\"xgb\", metodo_usado_xgb.lower(), \"hyperband\")] = {\n",
        "                \"model\":       best_model,\n",
        "                \"score\":       float(score_val),\n",
        "                \"metric\":      selector_funcion_objetivo.value,\n",
        "                \"param_dist\":  param_dist,           # <- espacio Hyperband\n",
        "                \"best_params\": best_params,\n",
        "                \"cols\":        list(X_train_filtrado.columns)\n",
        "            }\n",
        "\n",
        "            # --- llamada de persistencia ----\n",
        "            guardar_xgb(best_model, best_params, \"Hyperband\",\n",
        "                        metodo_usado_xgb, selector_funcion_objetivo,\n",
        "                        X_train_filtrado, X_test, Y_train, Y_test,\n",
        "                        study=None, traza_out=traza_xgb)\n",
        "            print(\"✅ Optimización con Hyperband completada.\")\n",
        "            with traza_xgb:\n",
        "               print(f\"💾 Modelo guardado     → modelos_opt/xgb_{metodo_usado_xgb.lower()}_hyperband_opt_*.pkl\")\n",
        "               print(f\"💾 Metadatos guardados → modelos_opt/xgb_{metodo_usado_xgb.lower()}_hyperband_opt_*_meta.pkl\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"❌ Error en optimización Hyperband:\", traceback.format_exc())\n",
        "\n",
        "    # ===================================================\n",
        "    # Motor de Optimización Optuna\n",
        "    # ===================================================\n",
        "    def optimizar_optuna():\n",
        "        try:\n",
        "            print(\"\\n📌 Iniciando optimización con Optuna...\")\n",
        "            assert 'X_train_filtrado' in globals()\n",
        "            assert 'Y_train_filtrado' in globals()\n",
        "\n",
        "            # ——— AÑADIDO: Definir escaladores para Optuna ———\n",
        "            from sklearn.preprocessing import StandardScaler\n",
        "            # Ajustamos escalador de X sobre el train filtrado\n",
        "            X_scaler = StandardScaler().fit(X_train_filtrado)\n",
        "            # Ajustamos escalador de Y (reshape para vector columna)\n",
        "            y_scaler = StandardScaler().fit(\n",
        "                Y_train_filtrado.values.reshape(-1, 1)\n",
        "            )\n",
        "            # ——— FIN AÑADIDO ———\n",
        "\n",
        "            param_dist = {\n",
        "                'n_estimators': IntDistribution(50, 300),\n",
        "                'max_depth': IntDistribution(3, 15),\n",
        "                'learning_rate': FloatDistribution(0.01, 0.3),\n",
        "                'subsample': FloatDistribution(0.5, 1.0),\n",
        "                'colsample_bytree': FloatDistribution(0.5, 1.0),\n",
        "                'gamma': FloatDistribution(0, 5)\n",
        "            }\n",
        "\n",
        "            model = XGBRegressor(random_state=42, verbosity=0)\n",
        "            search = OptunaSearchCV(\n",
        "                estimator=model,\n",
        "                param_distributions=param_dist,\n",
        "                scoring=selector_funcion_objetivo.value,\n",
        "                n_trials=slider_n_iter.value,\n",
        "                cv=slider_cv.value,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            # ——— AÑADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TRAIN ———\n",
        "            # 1) Forzar que todos los nombres sean str\n",
        "            #X_train_filtrado.columns = X_train_filtrado.columns.astype(str)\n",
        "            # 2) Reemplazar corchetes y '<', '>' por '_'\n",
        "            #X_train_filtrado.columns = (\n",
        "            #    X_train_filtrado\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            # ——— FIN AÑADIDO ———\n",
        "            # # ——— AÑADIDO PARA ASEGURAR FUNCIONAMIENTO XGBOOST CON NONBRES DE FEATURES CON CARACTERES ESPECIALES - PARA TEST ———\n",
        "            #X_test.columns = X_test.columns.astype(str)\n",
        "            #X_test.columns = (\n",
        "            #    X_test\n",
        "            #    .columns\n",
        "            #    .str.replace(r'[\\[\\]<>]', '_', regex=True)\n",
        "            #)\n",
        "            # ——— FIN AÑADIDOS ———\n",
        "\n",
        "            search.fit(X_train_filtrado, Y_train_filtrado)\n",
        "            global best_model\n",
        "            best_model = search.best_estimator_\n",
        "\n",
        "            print(\"\\n✅ Hiperparámetros óptimos encontrados con Optuna:\")\n",
        "            global best_params\n",
        "            best_params = search.best_params_\n",
        "            display(pd.DataFrame([best_params]).T.rename(columns={0: 'Valor óptimo'}).style.set_caption(\"📋 Hiperparámetros Óptimos (Optuna)\").format(precision=4))\n",
        "\n",
        "            ejecutar_metricas_finales(best_model, nombre_motor=\"Optuna\")\n",
        "\n",
        "            # ─── Tras best_model, best_params en optimizar_optuna() ───\n",
        "            preds = best_model.predict(X_test[X_train_filtrado.columns])\n",
        "            if selector_funcion_objetivo.value == \"r2\":\n",
        "                score_val = r2_score(Y_test, preds)\n",
        "            elif selector_funcion_objetivo.value == \"neg_mean_absolute_error\":\n",
        "                score_val = mean_absolute_error(Y_test, preds)\n",
        "            else:\n",
        "                score_val = mean_squared_error(Y_test, preds)\n",
        "\n",
        "            # Capturamos el espacio usado por Optuna (param_dist) y el estudio\n",
        "            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "            OPT_MODELS[(\"xgb\", metodo_usado_xgb.lower(), \"optuna\")] = {\n",
        "                \"model\":       best_model,\n",
        "                'sx':          X_scaler,        # tu StandardScaler de X\n",
        "                'sy':          y_scaler,        # si existe\n",
        "                \"score\":       float(score_val),\n",
        "                \"metric\":      selector_funcion_objetivo.value,\n",
        "                \"param_dist\":  param_dist,           # <- espacio de Optuna\n",
        "                \"best_params\": best_params,\n",
        "                \"cols\":        list(X_train_filtrado.columns)\n",
        "            }\n",
        "            # Si quieres guardar el study:\n",
        "            if 'study' in locals():\n",
        "                OPT_MODELS[(\"xgb\", metodo_usado_xgb.lower(), \"optuna_study\")] = study\n",
        "\n",
        "            # --- llamada de persistencia ----\n",
        "            guardar_xgb(best_model, best_params, \"Optuna\",\n",
        "                        metodo_usado_xgb, selector_funcion_objetivo,\n",
        "                        X_train_filtrado, X_test, Y_train, Y_test,\n",
        "                        study=search.study_, traza_out=traza_xgb)\n",
        "            print(\"✅ Optimización con Hyperband completada.\")\n",
        "            with traza_xgb:\n",
        "                print(f\"💾 Modelo guardado     → modelos_opt/xgb_{metodo_usado_xgb.lower()}_optuna_opt_*.pkl\")\n",
        "                print(f\"💾 Metadatos guardados → modelos_opt/xgb_{metodo_usado_xgb.lower()}_optuna_opt_*_meta.pkl\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"❌ Error en optimización Optuna:\", traceback.format_exc())\n",
        "\n",
        "    opciones_metodos = sorted(list(RESUMEN_METODOS.keys()) + ['Todos'])\n",
        "    selector_metodo = widgets.Dropdown(\n",
        "        options=opciones_metodos,\n",
        "        description='Método:',\n",
        "        layout=widgets.Layout(width='50%')\n",
        "    )\n",
        "\n",
        "    boton_confirmar = widgets.Button(description=\"📥 Cargar variables seleccionadas\", button_style='primary')\n",
        "    boton_confirmar.on_click(lambda b: seleccionar_variables_filtradas(selector_metodo.value))\n",
        "\n",
        "    selector_motor = widgets.SelectMultiple(\n",
        "        options=['RandomSearch', 'Bayesian', 'Hyperband', 'Optuna', 'Todos'],\n",
        "        value=['RandomSearch'],\n",
        "        description='Motores:',\n",
        "        layout=widgets.Layout(width='50%', height='120px')\n",
        "    )\n",
        "\n",
        "    boton_opt = widgets.Button(description=\"🚀 Iniciar Optimización XGBoost\", button_style='success')\n",
        "    #boton_opt.on_click(lambda b: optimizar_randomsearch() if 'RandomSearch' in selector_motor.value or 'Todos' in selector_motor.value else None)\n",
        "\n",
        "    def lanzar_optimizaciones(_):\n",
        "        if 'RandomSearch' in selector_motor.value or 'Todos' in selector_motor.value:\n",
        "            optimizar_randomsearch()\n",
        "        if 'Bayesian' in selector_motor.value or 'Todos' in selector_motor.value:\n",
        "            optimizar_bayesian()\n",
        "        if 'Hyperband' in selector_motor.value or 'Todos' in selector_motor.value:\n",
        "            optimizar_hyperband()\n",
        "        if 'Optuna' in selector_motor.value or 'Todos' in selector_motor.value:\n",
        "            optimizar_optuna()\n",
        "\n",
        "    boton_opt.on_click(lanzar_optimizaciones)\n",
        "\n",
        "    out_opt_xgb.clear_output()\n",
        "    with out_opt_xgb:\n",
        "        display(HTML(\"<h3>🔧 Selección de Variables para Optimización XGBoost</h3>\"))\n",
        "        display(widgets.HBox([selector_metodo, boton_confirmar]))\n",
        "        display(HTML(\"<h3>⚙️ Parámetros de Optimización</h3>\"))\n",
        "        display(widgets.VBox([\n",
        "            widgets.HBox([slider_n_iter, slider_cv]),\n",
        "            widgets.HBox([ayuda_n_iter, ayuda_cv]),\n",
        "            widgets.HBox([selector_funcion_objetivo]),\n",
        "            ayuda_funcion\n",
        "        ]))\n",
        "        display(HTML(\"<h3>⚙️ Motores de Optimización</h3>\"))\n",
        "        display(widgets.VBox([selector_motor, boton_opt]))\n",
        "\n",
        "        display(traza_xgb)             # 🔹 se muestra el panel de trazas\n",
        "\n",
        "    display(out_opt_xgb)\n",
        "\n",
        "\n",
        "# ===============================================\n",
        "# Cell 9.4: OPTIMIZACIÓN RANDOM FOREST - AÑADIR GRABADO\n",
        "# Optimización múltiple de hiperparámetros del modelo RF\n",
        "# Compatible con motores: RandomSearch, Bayesian, Hyperband y Optuna\n",
        "# Variables: seleccionadas desde Cell 6 (Pearson, Spearman, MutualInfo, Boruta, UMAP, Todos)\n",
        "# Datos: segmentados desde Cell 5.2 (X_train, Y_train, X_test, Y_test)\n",
        "# ===============================================\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
        "from sklearn.model_selection import HalvingRandomSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.integration import OptunaSearchCV\n",
        "from skopt.space import Integer, Categorical\n",
        "from scipy.stats import shapiro\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# >>> INICIO DEL BLOQUE DE PERSISTENCIA  (Random-Forest) EN CELDA 9.4\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "import pathlib, datetime, pickle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "def guardar_rf(best_model,\n",
        "               optimizations_results,\n",
        "               scaler_X, scaler_Y,\n",
        "               cols,\n",
        "               X_test, Y_test,\n",
        "               func_objetivo,\n",
        "               study=None,\n",
        "               traza_out=None):\n",
        "    try:\n",
        "        # 1) Asegurar carpeta de destino\n",
        "        pathlib.Path(\"modelos_opt\").mkdir(exist_ok=True)\n",
        "\n",
        "        # 2) Preparar datos de prueba filtrados y escalados\n",
        "        X_test_sel = X_test[cols]\n",
        "        X_scaled   = scaler_X.transform(X_test_sel)\n",
        "        preds      = best_model.predict(X_scaled)\n",
        "        preds_inv  = scaler_Y.inverse_transform(preds.reshape(-1,1)).ravel()\n",
        "\n",
        "        # 3) Calcular métrica según func_objetivo\n",
        "        if func_objetivo == \"r2\":\n",
        "            score_val = r2_score(Y_test, preds_inv)\n",
        "        elif func_objetivo == \"neg_mean_absolute_error\":\n",
        "            score_val = mean_absolute_error(Y_test, preds_inv)\n",
        "        else:\n",
        "            score_val = mean_squared_error(Y_test, preds_inv)\n",
        "\n",
        "        # 4) Definir rutas de archivo\n",
        "        ts      = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        tag     = f\"rf_{func_objetivo}_opt_{ts}\"\n",
        "        model_f = f\"modelos_opt/{tag}.pkl\"\n",
        "        meta_f  = f\"modelos_opt/{tag}_meta.pkl\"\n",
        "        study_f = f\"modelos_opt/{tag}_study.pkl\"\n",
        "\n",
        "        # 5) Guardar modelo+escaladores+cols\n",
        "        with open(model_f, \"wb\") as f:\n",
        "            pickle.dump({\n",
        "                \"model\": best_model,\n",
        "                \"sx\":    scaler_X,\n",
        "                \"sy\":    scaler_Y,\n",
        "                \"cols\":  cols\n",
        "            }, f)\n",
        "\n",
        "        # 6) Guardar metadatos\n",
        "        meta = {\n",
        "            \"score\":  float(score_val),\n",
        "            \"metric\": func_objetivo,\n",
        "            \"cols\":   cols\n",
        "        }\n",
        "        with open(meta_f, \"wb\") as f:\n",
        "            pickle.dump(meta, f)\n",
        "\n",
        "        # 7) Guardar estudio Optuna si existe\n",
        "        if study is not None:\n",
        "            with open(study_f, \"wb\") as f:\n",
        "                pickle.dump(study, f)\n",
        "\n",
        "        # 8) Mensaje de confirmación\n",
        "        if traza_out is not None:\n",
        "            with traza_out:\n",
        "                print(f\"💾 Modelo guardado → {model_f}\")\n",
        "                print(f\"💾 Metadatos guardados → {meta_f}\")\n",
        "                if study is not None:\n",
        "                    print(f\"💾 Study guardado → {study_f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = f\"⚠️ No se pudo guardar modelo/estudio: {e}\"\n",
        "        if traza_out is not None:\n",
        "            with traza_out:\n",
        "                print(msg)\n",
        "        else:\n",
        "            print(msg)\n",
        "\n",
        "# Logger para depuración\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Salida global para la tabla y las gráficas\n",
        "out_rf_opt = widgets.Output()\n",
        "\n",
        "# Lista para almacenar los resultados de las optimizaciones\n",
        "optimizations_results = []\n",
        "\n",
        "def mostrar_optimizacion_rf():\n",
        "    with out_rf_opt:\n",
        "        clear_output()\n",
        "\n",
        "        # Aseguramos que las variables X e Y estén definidas globalmente\n",
        "        global X_train, Y_train, X_test, Y_test\n",
        "        if 'X_train' not in globals() or 'Y_train' not in globals() or 'X_test' not in globals() or 'Y_test' not in globals():\n",
        "            print(\"❌ Asegúrate de que las variables X_train, Y_train, X_test y Y_test estén correctamente definidas.\")\n",
        "            return\n",
        "\n",
        "        # ─── AÑADIDO: saneamiento global de nombres ───\n",
        "        #import re\n",
        "        #def clean_name(s):\n",
        "        #    t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))  # corchetes, %, /, . y espacios → _\n",
        "        #    t = re.sub(r'_+', '_', t)                     # colapsar guiones bajos repetidos\n",
        "        #    return t.strip('_')\n",
        "\n",
        "        # 1) Limpio las columnas de X_train y X_test\n",
        "        #X_train.columns = [clean_name(c) for c in X_train.columns]\n",
        "        #X_test.columns  = [clean_name(c) for c in X_test.columns]\n",
        "\n",
        "        # 2) Limpio todas las listas de RESUMEN_METODOS\n",
        "        #for m, lst in RESUMEN_METODOS.items():\n",
        "        #    if isinstance(lst, list):\n",
        "        #        RESUMEN_METODOS[m] = [clean_name(c) for c in lst]\n",
        "        # ─── FIN AÑADIDO ───\n",
        "\n",
        "        # 2) Ahora sí puedo sanear columnas\n",
        "        # Usar sanitize_name para limpiar columnas en el payload\n",
        "        def clean_cols(col_list):\n",
        "            return [sanitize_name(c) for c in col_list]\n",
        "        # Ejemplo de sanitización de X_train antes de fit\n",
        "        X_train.columns = [sanitize_name(col) for col in X_train.columns]\n",
        "        X_test.columns = [sanitize_name(col) for col in X_test.columns]\n",
        "\n",
        "        # Mostrar menú para seleccionar el motor de optimización, el método y la función de optimización\n",
        "        selector_motor = widgets.Dropdown(\n",
        "            options=['RandomSearch', 'BayesianOptimization', 'Hyperband', 'Optuna', 'Todos'],\n",
        "            description='Motor:',\n",
        "            value='RandomSearch',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        selector_metodo = widgets.Dropdown(\n",
        "            options=['Pearson', 'Spearman', 'Mutualinfo', 'Boruta', 'UAMP', 'Todos'],\n",
        "            description='Método:',\n",
        "            value='Todos',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        func_objetivo = widgets.Dropdown(\n",
        "            options=['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
        "            value='r2',\n",
        "            description='Función:',\n",
        "            layout=widgets.Layout(width='50%')\n",
        "        )\n",
        "\n",
        "        # Sliders para la configuración de la optimización\n",
        "        n_iter = widgets.IntSlider(value=50, min=10, max=300, step=10, description='n_iter:')\n",
        "        cv = widgets.IntSlider(value=5, min=2, max=10, step=1, description='cv:')\n",
        "\n",
        "        # Botón de ejecutar optimización\n",
        "        btn_ejecutar = widgets.Button(description=\"🚀 Ejecutar Optimización\", button_style='success')\n",
        "        barra_progreso = widgets.IntProgress(min=0, max=1, description='Progreso:')\n",
        "        salida_resultados = widgets.Output()\n",
        "\n",
        "        def ejecutar_optimización(_):\n",
        "            global OPT_MODELS\n",
        "            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "            salida_resultados.clear_output()\n",
        "            barra_progreso.bar_style = 'info'\n",
        "            barra_progreso.value = 0\n",
        "            tiempo_transcurrido = widgets.HTML('⏱️ Tiempo: 0.0s')\n",
        "            mensaje_estado = widgets.HTML(value=\"⏳ <b>Optimización en curso...</b>\")\n",
        "            inicio = time.time()\n",
        "\n",
        "            with salida_resultados:\n",
        "                print(\"⏳ Ejecutando optimización...\")\n",
        "\n",
        "                # Escaladores para entrenamiento/test\n",
        "                scaler_X = StandardScaler().fit(X_train)\n",
        "                scaler_Y = StandardScaler().fit(Y_train.values.reshape(-1,1))\n",
        "                X_train_scaled = scaler_X.transform(X_train)\n",
        "                X_test_scaled  = scaler_X.transform(X_test)\n",
        "                Y_train_scaled = scaler_Y.transform(Y_train.values.reshape(-1,1)).ravel()\n",
        "                Y_test_scaled  = scaler_Y.transform(Y_test.values.reshape(-1,1)).ravel()\n",
        "\n",
        "                # Inicializar motores de optimización\n",
        "                search_random = None\n",
        "                search_bayes = None\n",
        "                search_hyperband = None\n",
        "                best_rf_random = None\n",
        "                best_rf_bayes = None\n",
        "                best_rf_hyperband = None\n",
        "                best_rf_optuna = None\n",
        "                best_motor = None\n",
        "                best_metodo = None\n",
        "                best_rf = None\n",
        "                best_score = -np.inf\n",
        "                optimizations_results = []\n",
        "                best_score_random = best_score_bayes = best_score_hyperband = best_score_optuna = None\n",
        "\n",
        "                # Configurar lista de motores y métodos\n",
        "                motors = (['RandomSearch','BayesianOptimization','Hyperband','Optuna']\n",
        "                          if selector_motor.value=='Todos'\n",
        "                          else [selector_motor.value])\n",
        "                study = None          # placeholder que existirá en todos los motores\n",
        "                for motor in motors:\n",
        "                    # Para cada método de selección\n",
        "                    methods = list(RESUMEN_METODOS.keys()) if selector_metodo.value=='Todos' else [selector_metodo.value]\n",
        "                    for metodo in methods:\n",
        "                        print(f\"🔧 Motor: {motor} | Método: {metodo}\")\n",
        "                        #cols = RESUMEN_METODOS.get(metodo, X_train.columns.tolist())  # columnas de este método\n",
        "\n",
        "                        # ——— REEMPLAZO: obtengo directamente las columnas saneadas ———\n",
        "                        cols = RESUMEN_METODOS.get(metodo, [])\n",
        "                        if not cols:\n",
        "                            print(f\"⚠️ No hay variables para '{metodo}', omito.\")\n",
        "                            continue\n",
        "                        # ——— FIN REEMPLAZO ———\n",
        "\n",
        "                        X_train_sel = X_train[cols]\n",
        "                        X_test_sel  = X_test[cols]\n",
        "                        scaler_X = StandardScaler().fit(X_train_sel)\n",
        "                        X_train_scaled = scaler_X.transform(X_train_sel)\n",
        "                        X_test_scaled  = scaler_X.transform(X_test_sel)\n",
        "\n",
        "                        # Aquí nos aseguramos de que Y_train también se escala\n",
        "                        sy = StandardScaler()\n",
        "                        Y_train_scaled = sy.fit_transform(Y_train.values.reshape(-1, 1)).ravel()\n",
        "                        Y_test_scaled = sy.transform(Y_test.values.reshape(-1, 1)).ravel()\n",
        "\n",
        "                        # =================================================\n",
        "                        # Motor Random Search\n",
        "                        # =================================================\n",
        "                        if motor == 'RandomSearch':\n",
        "                            param_dist = {\n",
        "                                'n_estimators': np.arange(50, 501, 50),\n",
        "                                'max_depth': np.arange(3, 15, 1),\n",
        "                                'min_samples_split': np.arange(2, 21, 1),\n",
        "                                'min_samples_leaf': np.arange(1, 21, 1),\n",
        "                                'max_features': ['sqrt', 'log2', None],\n",
        "                                'bootstrap': [True, False]\n",
        "                            }\n",
        "                            search_random = RandomizedSearchCV(\n",
        "                                RandomForestRegressor(random_state=42),\n",
        "                                param_distributions=param_dist,\n",
        "                                n_iter=n_iter.value,\n",
        "                                cv=cv.value, n_jobs=-1, scoring=func_objetivo.value, random_state=42,\n",
        "                                return_train_score=True, verbose=3\n",
        "                            )\n",
        "                            search_random.fit(X_train_scaled, Y_train_scaled)\n",
        "                            best_rf_random = search_random.best_estimator_\n",
        "                            best_score_random = search_random.best_score_\n",
        "                            if best_score_random > best_score:\n",
        "                                best_score = best_score_random\n",
        "                                best_rf = best_rf_random\n",
        "                                best_metodo = metodo\n",
        "                                best_motor  = motor\n",
        "                            # Almacenar los resultados del motor RandomSearch\n",
        "                            # ─── Tras best_estimator_ y best_score de cada motor ───\n",
        "                            OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "                            # RandomSearch\n",
        "                            OPT_MODELS[(\"rf\", metodo.lower(), \"randomsearch\")] = {\n",
        "                                \"model\":       best_rf_random,\n",
        "                                \"sx\":          scaler_X,\n",
        "                                \"sy\":          sy,\n",
        "                                \"score\":       float(best_score_random),\n",
        "                                \"metric\":      func_objetivo.value,\n",
        "                                \"param_dist\":  param_dist,\n",
        "                                \"best_params\": search_random.best_params_,\n",
        "                                \"cols\":        cols\n",
        "                            }\n",
        "                            optimizations_results.append({\n",
        "                                'motor' : motor,\n",
        "                                'metodo': metodo,\n",
        "                                'puntuación': best_score_random,\n",
        "                                'R2': r2_score(Y_test_scaled, search_random.predict(X_test_scaled)),\n",
        "                                'MSE': mean_squared_error(Y_test_scaled, search_random.predict(X_test_scaled)),\n",
        "                                'MAE': mean_absolute_error(Y_test_scaled, search_random.predict(X_test_scaled)),\n",
        "                                'params': search_random.best_params_\n",
        "                            })\n",
        "\n",
        "                            pass\n",
        "                        # =================================================\n",
        "                        # Motor Bayesian\n",
        "                        # =================================================\n",
        "                        elif motor == 'BayesianOptimization':\n",
        "                            param_space = {\n",
        "                                'n_estimators': Integer(50, 500),\n",
        "                                'max_depth': Integer(3, 15),\n",
        "                                'min_samples_split': Integer(2, 20),\n",
        "                                'min_samples_leaf': Integer(1, 20),\n",
        "                                'max_features': Categorical(['sqrt', 'log2', None]),\n",
        "                                'bootstrap': Categorical([True, False])\n",
        "                            }\n",
        "                            search_bayes = BayesSearchCV(\n",
        "                                RandomForestRegressor(random_state=42),\n",
        "                                param_space,\n",
        "                                n_iter=n_iter.value,\n",
        "                                cv=cv.value, n_jobs=-1, scoring=func_objetivo.value, random_state=42,\n",
        "                                return_train_score=True, verbose=3\n",
        "                            )\n",
        "                            search_bayes.fit(X_train_scaled, Y_train_scaled)\n",
        "                            best_rf_bayes = search_bayes.best_estimator_\n",
        "                            best_score_bayes = search_bayes.best_score_\n",
        "                            if best_score_bayes > best_score:\n",
        "                                best_score = best_score_bayes\n",
        "                                best_rf = best_rf_bayes\n",
        "                                best_metodo = metodo\n",
        "                                best_motor  = motor\n",
        "                            # Almacenar los resultados del motor Bayesian\n",
        "                            OPT_MODELS[(\"rf\", metodo.lower(), \"bayesianoptimization\")] = {\n",
        "                                \"model\":       best_rf_bayes,\n",
        "                                \"sx\":          scaler_X,\n",
        "                                \"sy\":          sy,\n",
        "                                \"score\":       float(best_score_bayes),\n",
        "                                \"metric\":      func_objetivo.value,\n",
        "                                \"param_dist\":  param_space,\n",
        "                                \"best_params\": search_bayes.best_params_,\n",
        "                                \"cols\":        cols\n",
        "                            }\n",
        "                            optimizations_results.append({\n",
        "                                'motor' : motor,\n",
        "                                'metodo': metodo,\n",
        "                                'puntuación': best_score_bayes,\n",
        "                                'R2': r2_score(Y_test_scaled, search_bayes.predict(X_test_scaled)),\n",
        "                                'MSE': mean_squared_error(Y_test_scaled, search_bayes.predict(X_test_scaled)),\n",
        "                                'MAE': mean_absolute_error(Y_test_scaled, search_bayes.predict(X_test_scaled)),\n",
        "                                'params': search_bayes.best_params_\n",
        "                            })\n",
        "\n",
        "                            pass\n",
        "                        # =================================================\n",
        "                        # Motor Hyperband\n",
        "                        # =================================================\n",
        "                        elif motor == 'Hyperband':\n",
        "                            param_dist = {\n",
        "                                'n_estimators': np.arange(50, 501, 50),\n",
        "                                'max_depth': np.arange(3, 15, 1),\n",
        "                                'min_samples_split': np.arange(2, 21, 1),\n",
        "                                'min_samples_leaf': np.arange(1, 21, 1),\n",
        "                                'max_features': ['sqrt', 'log2', None],\n",
        "                                'bootstrap': [True, False]\n",
        "                            }\n",
        "                            search_hyperband = HalvingRandomSearchCV(\n",
        "                                RandomForestRegressor(random_state=42),\n",
        "                                param_distributions=param_dist,\n",
        "                                factor=3,  # Aumenta recursos a medida que mejora el modelo\n",
        "                                max_resources=300,  # Máximo número de recursos para la optimización\n",
        "                                min_resources=50,  # Número mínimo de recursos\n",
        "                                cv=cv.value, n_jobs=-1, scoring=func_objetivo.value, random_state=42,\n",
        "                                return_train_score=True, verbose=3\n",
        "                            )\n",
        "                            search_hyperband.fit(X_train_scaled, Y_train_scaled)\n",
        "                            best_rf_hyperband = search_hyperband.best_estimator_\n",
        "                            best_score_hyperband = search_hyperband.best_score_\n",
        "                            if best_score_hyperband > best_score:\n",
        "                                best_score = best_score_hyperband\n",
        "                                best_rf = best_rf_hyperband\n",
        "                                best_metodo = metodo\n",
        "                                best_motor  = motor\n",
        "                            # Almacenar los resultados del motor Hyperband\n",
        "                            OPT_MODELS[(\"rf\", metodo.lower(), \"hyperband\")] = {\n",
        "                                \"model\":       best_rf_hyperband,\n",
        "                                \"sx\":          scaler_X,\n",
        "                                \"sy\":          sy,\n",
        "                                \"score\":       float(best_score_hyperband),\n",
        "                                \"metric\":      func_objetivo.value,\n",
        "                                \"param_dist\":  param_dist,\n",
        "                                \"best_params\": search_hyperband.best_params_,\n",
        "                                \"cols\":        cols\n",
        "                            }\n",
        "                            optimizations_results.append({\n",
        "                                'motor' : motor,\n",
        "                                'metodo': metodo,\n",
        "                                'puntuación': best_score_hyperband,\n",
        "                                'R2': r2_score(Y_test_scaled, search_hyperband.predict(X_test_scaled)),\n",
        "                                'MSE': mean_squared_error(Y_test_scaled, search_hyperband.predict(X_test_scaled)),\n",
        "                                'MAE': mean_absolute_error(Y_test_scaled, search_hyperband.predict(X_test_scaled)),\n",
        "                                'params': search_hyperband.best_params_\n",
        "                            })\n",
        "\n",
        "                            pass\n",
        "                        # =================================================\n",
        "                        # Motor Optuna\n",
        "                        # =================================================\n",
        "                        elif motor == 'Optuna':\n",
        "                            def objective(trial: Trial):\n",
        "                                param = {\n",
        "                                    'n_estimators': trial.suggest_int('n_estimators', 50, 500, step=50),\n",
        "                                    'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "                                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "                                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
        "                                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
        "                                    'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
        "                                }\n",
        "                                model = RandomForestRegressor(**param, random_state=42)\n",
        "                                model.fit(X_train_scaled, Y_train_scaled)\n",
        "                                preds = model.predict(X_test_scaled)\n",
        "                                return mean_squared_error(Y_test_scaled, preds)\n",
        "\n",
        "                            #study = optuna.create_study(direction='maximize')  # Maximizar la puntuación\n",
        "                            direction = 'minimize' if func_objetivo.value != 'r2' else 'maximize'\n",
        "                            study = optuna.create_study(direction=direction)\n",
        "                            study.optimize(objective, n_trials=n_iter.value)\n",
        "\n",
        "                            best_rf_optuna = RandomForestRegressor(**study.best_params, random_state=42)\n",
        "                            best_rf_optuna.fit(X_train_scaled, Y_train_scaled)\n",
        "                            best_score_optuna = study.best_value\n",
        "                            if best_score_optuna > best_score:\n",
        "                                best_score = best_score_optuna\n",
        "                                best_rf = best_rf_optuna\n",
        "                                best_metodo = metodo\n",
        "                                best_motor  = motor\n",
        "\n",
        "                            # Almacenar los resultados del motor Optuna\n",
        "                            # … después de entrenar best_rf_optuna y antes de optimizations_results.append\n",
        "                            pred_opt = best_rf_optuna.predict(X_test_scaled)   # ← calcula una sola vez\n",
        "\n",
        "                            OPT_MODELS[(\"rf\", metodo.lower(), \"optuna\")] = {\n",
        "                                \"model\":       best_rf_optuna,\n",
        "                                \"sx\":          scaler_X,\n",
        "                                \"sy\":          sy,\n",
        "                                \"score\":       float(best_score_optuna),\n",
        "                                \"metric\":      func_objetivo.value,\n",
        "                                \"param_dist\":  study.best_params,\n",
        "                                \"best_params\": study.best_params,\n",
        "                                \"cols\":        cols\n",
        "                            }\n",
        "                            optimizations_results.append({\n",
        "                                'motor' : motor,\n",
        "                                'metodo': metodo,\n",
        "                                'puntuación': best_score_optuna,\n",
        "                                'R2'    : r2_score(Y_test_scaled, pred_opt),\n",
        "                                'MSE'   : mean_squared_error(Y_test_scaled, pred_opt),\n",
        "                                'MAE'   : mean_absolute_error(Y_test_scaled, pred_opt),\n",
        "                                'params': study.best_params\n",
        "                            })\n",
        "\n",
        "                            study = study\n",
        "\n",
        "                            pass\n",
        "\n",
        "                # Elegir el mejor modelo según el motor seleccionado\n",
        "                best_rf = best_rf_random if best_rf_random else best_rf_bayes if best_rf_bayes else best_rf_hyperband if best_rf_hyperband else best_rf_optuna\n",
        "\n",
        "                # Recopilamos sólo los scores que sí tenemos\n",
        "                scores = [\n",
        "                    best_score_random,\n",
        "                    best_score_bayes,\n",
        "                    best_score_hyperband,\n",
        "                    best_score_optuna\n",
        "                ]\n",
        "                # Filtramos los None\n",
        "                scores = [s for s in scores if s is not None]\n",
        "                # Si hay al menos uno, tomamos el máximo; si no, dejamos None o 0\n",
        "                best_score = max(scores) if scores else None\n",
        "\n",
        "                # Registrar en memoria el mejor RF en OPT_MODELS\n",
        "                OPT_MODELS = globals().setdefault(\"OPT_MODELS\", {})\n",
        "                OPT_MODELS[(\"rf\", best_metodo.lower(), best_motor.lower())] = {\n",
        "                    \"model\":       best_rf,\n",
        "                    \"sx\":          scaler_X,\n",
        "                    \"sy\":          sy,\n",
        "                    \"score\":       float(best_score),\n",
        "                    \"metric\":      func_objetivo.value,\n",
        "                    \"param_dist\":  optimizations_results[-1][\"params\"],\n",
        "                    \"best_params\": optimizations_results[-1][\"params\"],\n",
        "                    \"cols\":        RESUMEN_METODOS.get(best_metodo, X_train.columns.tolist())\n",
        "                }\n",
        "\n",
        "                #import re\n",
        "                # tras bucles, desescalamos y graficamos con el mejor de todos\n",
        "                # Ajuste de columnas según selección de variables\n",
        "                cols_rf = RESUMEN_METODOS.get(best_metodo, X_train.columns.tolist())\n",
        "                # ——— AÑADIDO: sanitizar nombres de cols_rf ———\n",
        "                # …después de determinar cols_rf original…\n",
        "                #raw_cols_rf = RESUMEN_METODOS.get(best_metodo, X_train.columns.tolist())\n",
        "\n",
        "                # 1) Sanitizamos:\n",
        "                #cols_rf = [ re.sub(r'[\\[\\]<>]', '_', str(c)) for c in raw_cols_rf ]\n",
        "\n",
        "                # 2) Filtramos para quedarnos solo con los que existen:\n",
        "                #cols_rf = [ c for c in cols_rf if c in X_train.columns ]\n",
        "                # ——— FIN AÑADIDO ———\n",
        "\n",
        "                scaler_X_final = StandardScaler().fit(X_train[cols_rf])\n",
        "                scaler_Y_final = StandardScaler().fit(Y_train.values.reshape(-1,1))\n",
        "                X_test_sel = X_test[cols_rf]\n",
        "                X_test_scaled_final = scaler_X_final.transform(X_test_sel)\n",
        "\n",
        "                # Predicción sobre columnas seleccionadas\n",
        "                preds_scaled = best_rf.predict(X_test_scaled_final)\n",
        "                preds_final = scaler_Y_final.inverse_transform(preds_scaled.reshape(-1,1)).ravel()\n",
        "\n",
        "                # >>> LLAMADA A PERSISTENCIA ───────────────────────────\n",
        "                guardar_rf(\n",
        "                    best_model=best_rf,\n",
        "                    optimizations_results=optimizations_results,\n",
        "                    scaler_X=scaler_X_final,\n",
        "                    scaler_Y=scaler_Y_final,\n",
        "                    cols=cols,\n",
        "                    X_test=X_test,\n",
        "                    Y_test=Y_test,\n",
        "                    func_objetivo=func_objetivo.value,\n",
        "                    study=study if study is not None else None,\n",
        "                    traza_out=salida_resultados\n",
        "                )\n",
        "                # ──────────────────────────────────────────────────────\n",
        "                # Mostrar el mejor modelo y los hiperparámetros\n",
        "                print(\"✅ Optimización completada.\")\n",
        "                print(f\"🔹 Mejor Modelo: {best_rf}\")\n",
        "                print(f\"🔹 Mejor Puntuación: {best_score:.4f}\")\n",
        "\n",
        "                plt.figure(figsize=(10,4))\n",
        "                plt.subplot(1,2,1)\n",
        "                plt.scatter(Y_test, preds_final, alpha=0.6)\n",
        "                plt.plot([Y_test.min(),Y_test.max()],[Y_test.min(),Y_test.max()],'r--', label='Ideal')\n",
        "                plt.title(f\"Real vs Predicho ({best_motor} - {best_metodo})\")\n",
        "                plt.xlabel(\"Y Real\"); plt.ylabel(\"Y Predicho\"); plt.legend(); plt.grid()\n",
        "\n",
        "                # Histograma residuos y curvas de distribución\n",
        "                residuos = Y_test.values.ravel() - preds_final\n",
        "                plt.figure(figsize=(6,4))\n",
        "                # Histograma\n",
        "                n,bins,patches=plt.hist(residuos,bins=20,density=True,alpha=0.6,edgecolor='black',label='Distribución Residuos')\n",
        "                # Curva normal\n",
        "                mu,sd=norm.fit(residuos)\n",
        "                x=np.linspace(bins.min(),bins.max(),100)\n",
        "                plt.plot(x,norm.pdf(x,mu,sd),linewidth=2, label='Curva normal')\n",
        "                # Curva KDE de la distribución real\n",
        "                kde = gaussian_kde(residuos)\n",
        "                x = np.linspace(bins.min(),bins.max(),100)\n",
        "                plt.plot(x, kde(x), lw=2, label='Densidad KDE')  # añadido KDE\n",
        "                plt.title('Histograma de residuos con curva normal y curva de densidad'); plt.tight_layout()\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "\n",
        "                # Test normalidad Shapiro\n",
        "                stat, p = shapiro(residuos)\n",
        "                display(HTML(f\"<h4>🧪 Prueba de Normalidad Shapiro-Wilk:</h4><ul><li>Estadístico: {stat:.4f}</li><li>p-valor: {p:.4f}</li><li>{'✅ Residuos normales' if p>0.05 else '⚠️ Residuos no normales'}</li></ul>\"))\n",
        "\n",
        "                # Eje de casos\n",
        "                n_casos = len(Y_test)\n",
        "                casos = range(n_casos)\n",
        "\n",
        "                # Extraer valores\n",
        "                y_real = Y_test.values if hasattr(Y_test, \"values\") else Y_test\n",
        "                y_pred = preds_final  # ajusta el nombre si usas otro\n",
        "\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                plt.scatter(casos, y_real, marker='o', label='Y real')\n",
        "                plt.scatter(casos, y_pred, marker='x', label='Y predicho')\n",
        "                plt.xlabel('Caso')\n",
        "                plt.ylabel('Y')\n",
        "                plt.title('Comparación de puntos: Y real vs. Y predicho')\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Mostrar los hiperparámetros óptimos\n",
        "                print(\"\\n⚙️ Mejores hiperparámetros encontrados:\")\n",
        "                optimal_params_df = pd.DataFrame.from_dict(best_rf.get_params(), orient='index', columns=['Valor óptimo'])\n",
        "                display(optimal_params_df.style.set_caption(\"📋 Parámetros Óptimos\").format(precision=4))\n",
        "\n",
        "                # Calcular y mostrar métricas\n",
        "                r2 = r2_score(Y_test, preds_final)\n",
        "                mse = mean_squared_error(Y_test, preds_final)\n",
        "                rmse = np.sqrt(mse)\n",
        "                mae = mean_absolute_error(Y_test, preds_final)\n",
        "\n",
        "                print(f\" Valores de ajuste para los datos de prueba\")\n",
        "                print(f\"🔹 R²: {r2:.4f}\")\n",
        "                print(f\"🔹 MSE: {mse:.4f}\")\n",
        "                print(f\"🔹 RMSE: {rmse:.4f}\")\n",
        "                print(f\"🔹 MAE: {mae:.4f}\")\n",
        "\n",
        "                # Tabla completa de optimizaciones\n",
        "                df_all=pd.DataFrame(optimizations_results).rename(columns={'motor':'Motor','metodo':'Método'})\n",
        "                display(df_all.style.set_caption(\"📊 Todos los resultados de optimización\").format({'R2':'{:.4f}','MSE':'{:.4f}','MAE':'{:.4f}'}))\n",
        "\n",
        "                # Ranking top 5\n",
        "                df_rank = pd.DataFrame(sorted(optimizations_results, key=lambda x: x['puntuación'], reverse=True)[:5])\n",
        "                display(df_rank.style.set_caption(\"🏅 Top 5 Optimización\").format({\"R2\": \"{:.4f}\", \"MSE\":\"{:.4f}\", \"MAE\":\"{:.4f}\"}))\n",
        "\n",
        "                barra_progreso.bar_style = 'success'\n",
        "                barra_progreso.value = 1\n",
        "                tiempo_transcurrido.value = f'⏱️ Tiempo total: {time.time() - inicio:.1f}s'\n",
        "                mensaje_estado = widgets.HTML(value=\"✅ Optimización completada\")\n",
        "\n",
        "        # Conectar la acción del botón con la ejecución de la optimización\n",
        "        btn_ejecutar.on_click(ejecutar_optimización)\n",
        "\n",
        "        # Mostrar widgets\n",
        "        display(HTML(\"<h3>🔧 Optimización</h3>\"))\n",
        "        display(widgets.VBox([\n",
        "            selector_motor,\n",
        "            selector_metodo,\n",
        "            func_objetivo,\n",
        "            n_iter,\n",
        "            cv,\n",
        "            btn_ejecutar,\n",
        "            barra_progreso,\n",
        "            salida_resultados\n",
        "        ]))\n",
        "\n",
        "# Llamar la función para que el menú se active desde el menú principal\n",
        "display(out_rf_opt)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================\n",
        "# Cell 9.5: OPTIMIZACIÓN REDES NEURONALES RECURRENTES - AÑADIR GRABADO\n",
        "# Optimización múltiple de hiperparámetros del modelo RNN\n",
        "# Compatible con motores: RandomSearch, Bayesian, Hyperband y Optuna\n",
        "# Variables: seleccionadas desde Cell 6 (Pearson, Spearman, MutualInfo, Boruta, UMAP, Todos)\n",
        "# Datos: segmentados desde Cell 5.2 (X_train, Y_train, X_test, Y_test)\n",
        "# ===============================================\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import threading, time\n",
        "\n",
        "# SKLEARN & KERAS imports\n",
        "from sklearn.base import BaseEstimator, RegressorMixin                        # wrapper base\n",
        "from sklearn.preprocessing import StandardScaler      # ← AÑADIDO\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, HalvingRandomSearchCV  # Import Hyperband\n",
        "# TensorFlow imports\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# skopt imports\n",
        "from skopt import BayesSearchCV  # Motor Bayesian\n",
        "from skopt.space import Real, Integer, Categorical  # Espacios de búsqueda para Bayesian\n",
        "\n",
        "# Salida global\n",
        "out_opt_rnn = widgets.Output()\n",
        "import re\n",
        "\n",
        "# === NEW: Wrapper para integrar RNN con la API de scikit-learn ===\n",
        "class RNNRegressor(BaseEstimator, RegressorMixin):                             # NEW\n",
        "    def __init__(self, units=32, dropout_rate=0.2, learning_rate=1e-3,\n",
        "        epochs=50, batch_size=32, verbose=0, sy=None):\n",
        "        self.units = units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.model_ = None\n",
        "        self.sy = sy\n",
        "\n",
        "    #def fit(self, X, y):\n",
        "    def fit(self, X, y, **kwargs):\n",
        "        # Si alguien pasa sy por fit, lo recogemos\n",
        "        sy = kwargs.pop(\"sy\", None)\n",
        "        if sy is not None:\n",
        "            self.sy = sy\n",
        "\n",
        "        # Absorber posibles argumentos de recurso (epochs) de Hyperband\n",
        "        kwargs.pop('epochs', None)\n",
        "        # Aseguramos tipos nativos\n",
        "        self.units = int(self.units)            # NOW ensures Python int\n",
        "        self.dropout_rate = float(self.dropout_rate)\n",
        "        self.learning_rate = float(self.learning_rate)\n",
        "        self.epochs = int(self.epochs)\n",
        "        self.batch_size = int(self.batch_size)\n",
        "\n",
        "        # Damos formato 3D para la capa recurrente: (n_samples, timesteps=1, n_features)\n",
        "        X3 = X.reshape((X.shape[0], 1, X.shape[1]))                             # NEW\n",
        "\n",
        "        # Construimos el RNN\n",
        "        self.model_ = Sequential()\n",
        "        self.model_.add(SimpleRNN(self.units, activation='tanh', input_shape=(1, X.shape[1])))\n",
        "        self.model_.add(Dropout(self.dropout_rate))\n",
        "        self.model_.add(Dense(1))\n",
        "        self.model_.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "\n",
        "        # Entrenamiento\n",
        "        self.model_.fit(X3, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Si vienen DataFrames o similares, extrae el ndarray:\n",
        "        X_arr = X.values if hasattr(X, \"values\") else X\n",
        "        # Ahora reshape en el ndarray:\n",
        "        X3 = X_arr.reshape((X_arr.shape[0], 1, X_arr.shape[1]))\n",
        "        y_scaled = self.model_.predict(X3, verbose=self.verbose).ravel()\n",
        "        if self.sy is not None:\n",
        "            return self.sy.inverse_transform(y_scaled.reshape(-1,1)).ravel()\n",
        "        return y_scaled\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 🔒 INICIO DE LA FUNCIÓN DE PERSISTENCIA  (RNN)\n",
        "#    · Guarda: modelo Keras (.keras)  +  metadatos (.pkl)\n",
        "#    · Si el motor es Optuna ➜ guarda también el objeto study\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "import pathlib, pickle, datetime\n",
        "\n",
        "def guardar_rnn(best_estimator, score, metodo, motor,\n",
        "                selector_scoring, X_cols, Y_ref,\n",
        "                sx, sy,                         # --- ADDED scaler args ---\n",
        "                study=None, traza_out=None):\n",
        "\n",
        "    \"\"\"\n",
        "    best_estimator  -> instancia RNNRegressor entrenada\n",
        "    score           -> métrica devuelta por el CV\n",
        "    metodo          -> método de selección (Pearson, …)\n",
        "    motor           -> motor de optimización (RandomSearch, …)\n",
        "    selector_scoring-> widget con la métrica elegida\n",
        "    X_cols          -> lista de columnas usadas en X\n",
        "    Y_ref           -> Y_train (Series o DataFrame) para extraer nombre\n",
        "    study           -> objeto optuna.study.Study   (solo motor Optuna)\n",
        "    traza_out       -> panel Output donde imprimir mensajes (opcional)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1) carpeta destino\n",
        "        pathlib.Path(\"modelos_opt\").mkdir(exist_ok=True)\n",
        "\n",
        "        # 2) nombre robusto\n",
        "        ts  = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        tag = f\"rnn_{metodo.lower()}_{motor.lower()}_opt_{ts}\"\n",
        "\n",
        "        # 3) guardar sub-modelo Keras (*.keras)\n",
        "        keras_f = f\"modelos_opt/{tag}.keras\"\n",
        "        best_estimator.model_.save(keras_f, include_optimizer=True)\n",
        "\n",
        "        # 4) metadatos\n",
        "        y_name = getattr(Y_ref, \"name\", None) \\\n",
        "                 or (Y_ref.columns[0] if hasattr(Y_ref, \"columns\") else \"Y\")\n",
        "\n",
        "        meta = dict(\n",
        "            params       = best_estimator.get_params(),\n",
        "            score        = float(score),\n",
        "            func_obj     = selector_scoring.value,\n",
        "            motor        = motor,\n",
        "            metodo_x     = metodo,\n",
        "            cols         = X_cols,\n",
        "            yname        = y_name,\n",
        "            fecha        = ts,\n",
        "            sx           = sx,                 # --- ADDED ---\n",
        "            sy           = sy                  # --- ADDED ---\n",
        "        )\n",
        "        with open(f\"modelos_opt/{tag}_meta.pkl\", \"wb\") as f:\n",
        "            pickle.dump(meta, f)\n",
        "\n",
        "        # 5) study Optuna (si procede)\n",
        "        if motor.lower() == \"optuna\" and study is not None:\n",
        "            with open(f\"modelos_opt/{tag}_study.pkl\", \"wb\") as f:\n",
        "                pickle.dump(study, f)\n",
        "\n",
        "        # 6) log\n",
        "        if traza_out is not None:\n",
        "            with traza_out:\n",
        "                print(f\"💾 Modelo guardado     → {keras_f}\")\n",
        "                print(f\"💾 Metadatos guardados → {tag}_meta.pkl\")\n",
        "                if motor.lower() == \"optuna\" and study is not None:\n",
        "                    print(f\"💾 Study Optuna       → {tag}_study.pkl\")\n",
        "\n",
        "    except Exception as e:\n",
        "        txt = f\"⚠️ No se pudo guardar modelo/estudio: {e}\"\n",
        "        if traza_out is not None:\n",
        "            with traza_out: print(txt)\n",
        "        else:\n",
        "            print(txt)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 🔒 FIN DE LA FUNCIÓN DE PERSISTENCIA  (RNN)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "# Función principal de la celda\n",
        "def mostrar_optimizacion_rnn():\n",
        "    \"\"\"\n",
        "    Carga las variables X, Y y FECHAS de los conjuntos Train/Test\n",
        "    según el método de selección escogido.\n",
        "    Permite preview, selección de motores, y ejecuta RandomSearchCV\n",
        "    sobre el wrapper RNNRegressor.\n",
        "    \"\"\"\n",
        "    global X_train_sel, Y_train_sel, FECHAS_train_sel\n",
        "    global X_test_sel, Y_test_sel, FECHAS_test_sel\n",
        "    global X_train, Y_train, FECHAS_train, X_test, Y_test, FECHAS_test\n",
        "    global RESUMEN_METODOS\n",
        "\n",
        "    global OPT_MODELS\n",
        "    if 'OPT_MODELS' not in globals() or not isinstance(OPT_MODELS, dict):\n",
        "        OPT_MODELS = {}\n",
        "    # ── ESCALERS GLOBALES PARA PASAR A guardar_rnn ──\n",
        "    global sx, sy\n",
        "    sx, sy = None, None\n",
        "\n",
        "    with out_opt_rnn:\n",
        "        clear_output(wait=True)\n",
        "        # Verificar que los conjuntos existan\n",
        "        required = ['X_train','Y_train','FECHAS_train','X_test','Y_test','FECHAS_test']\n",
        "        missing = [v for v in required if v not in globals()]\n",
        "        if missing:\n",
        "            print(f\"❌ Faltan datos: {', '.join(missing)}. Ejecuta la segmentación antes.\")\n",
        "            return\n",
        "\n",
        "        # 2) Ahora sí puedo sanear columnas\n",
        "        # Usar sanitize_name para limpiar columnas en el payload\n",
        "        def clean_cols(col_list):\n",
        "            return [sanitize_name(c) for c in col_list]\n",
        "        # Ejemplo de sanitización de X_train antes de fit\n",
        "        X_train.columns = [sanitize_name(col) for col in X_train.columns]\n",
        "        X_test.columns = [sanitize_name(col) for col in X_test.columns]\n",
        "\n",
        "        # Dropdown de métodos de selección\n",
        "        metodos = ['Pearson','Spearman','MutualInfo','Boruta','UMAP']\n",
        "        selector_metodo = widgets.Dropdown(\n",
        "            options=['Todos'] + metodos,\n",
        "            description='Método X:',\n",
        "            layout=widgets.Layout(width='300px')\n",
        "        )\n",
        "        # Botón de carga de variables y preview\n",
        "        btn_cargar = widgets.Button(description='📥 Cargar Variables', button_style='primary')\n",
        "        salida_carga = widgets.Output()\n",
        "\n",
        "        # Selector de motores (incluye Todos)\n",
        "        motores = ['RandomSearch','Bayesian','Hyperband','Optuna']\n",
        "        selector_motor = widgets.SelectMultiple(\n",
        "            options=['Todos'] + motores,\n",
        "            description='Motores:',\n",
        "            layout=widgets.Layout(width='300px', height='100px')\n",
        "        )\n",
        "        # Scoring, CV y Nº iteraciones\n",
        "        selector_scoring = widgets.Dropdown(\n",
        "            options=['r2','neg_mean_squared_error','neg_mean_absolute_error'],\n",
        "            value='r2',\n",
        "            description='Scoring:',\n",
        "            layout=widgets.Layout(width='300px')\n",
        "        )\n",
        "        cv = widgets.IntSlider(value=5, min=2, max=10, step=1, description='cv:')\n",
        "        n_iter = widgets.IntSlider(value=10, min=5, max=100, step=1, description='Nº iter:')\n",
        "        progreso_reloj = widgets.HTML('⏱️ 00:00:00')\n",
        "        # Botón de ejecución\n",
        "        btn_ejecutar = widgets.Button(description='🚀 Ejecutar Optimización', button_style='success')\n",
        "        salida_logs = widgets.Output()\n",
        "\n",
        "        # === cargar_variables: filtra y muestra preview ===\n",
        "        def cargar_variables(_):\n",
        "            global X_train_sel, Y_train_sel, FECHAS_train_sel\n",
        "            global X_test_sel, Y_test_sel, FECHAS_test_sel\n",
        "            salida_carga.clear_output(wait=True)\n",
        "\n",
        "            metodo = selector_metodo.value\n",
        "            with salida_carga:\n",
        "                # Determinar columnas X\n",
        "                if metodo == 'Todos':\n",
        "                    cols = sorted({c for m in metodos for c in RESUMEN_METODOS.get(m, [])})\n",
        "                else:\n",
        "                    cols = RESUMEN_METODOS.get(metodo, [])\n",
        "                if not cols:\n",
        "                    print(f\"❌ No hay variables para '{metodo}'.\")\n",
        "                    return\n",
        "\n",
        "                X_train_sel = X_train[cols].copy()\n",
        "                Y_train_sel = Y_train.copy()\n",
        "                FECHAS_train_sel = FECHAS_train.copy()\n",
        "                X_test_sel  = X_test[cols].copy()\n",
        "                Y_test_sel  = Y_test.copy()\n",
        "                FECHAS_test_sel  = FECHAS_test.copy()\n",
        "\n",
        "                # Preview robusto: concat por posición (reset_index) para evitar NaN\n",
        "                df_train = pd.concat([\n",
        "                    X_train_sel.head(5).reset_index(drop=True),\n",
        "                    Y_train_sel.head(5).reset_index(drop=True),\n",
        "                    FECHAS_train_sel.head(5).reset_index(drop=True).rename('Fecha')\n",
        "                ], axis=1)\n",
        "                df_train['Tipo'] = 'Train'\n",
        "\n",
        "                df_test = pd.concat([\n",
        "                    X_test_sel.head(5).reset_index(drop=True),\n",
        "                    Y_test_sel.head(5).reset_index(drop=True),\n",
        "                    FECHAS_test_sel.head(5).reset_index(drop=True).rename('Fecha')\n",
        "                ], axis=1)\n",
        "                df_test['Tipo'] = 'Test'\n",
        "\n",
        "                # --- ADDED: crear y guardar scalers ---\n",
        "                global sx, sy\n",
        "                sx = StandardScaler().fit(X_train_sel.values)\n",
        "                sy = StandardScaler().fit(Y_train_sel.values.reshape(-1,1))\n",
        "                with salida_carga:\n",
        "                    print(f\"✅ Cargadas {len(cols)} variables y escalers preparados para '{metodo}'\")\n",
        "\n",
        "                df_preview = pd.concat([df_train, df_test], ignore_index=True)\n",
        "                display(HTML(\"<h4>📋 Preview Train/Test (5 filas cada uno)</h4>\"))\n",
        "                display(df_preview)\n",
        "\n",
        "        # === ejecutar_optimizacion: RandomSearchCV con RNNRegressor ===\n",
        "        def ejecutar_optimizacion(_):\n",
        "            # Iniciar reloj\n",
        "            stop_event = threading.Event()\n",
        "            start_time = time.time()\n",
        "            def run_clock():\n",
        "                while not stop_event.is_set():\n",
        "                    elapsed = int(time.time() - start_time)\n",
        "                    h, rem = divmod(elapsed, 3600)\n",
        "                    m, s   = divmod(rem, 60)\n",
        "                    progreso_reloj.value = f\"⏱️ {h:02d}:{m:02d}:{s:02d}\"\n",
        "                    time.sleep(1)\n",
        "            t = threading.Thread(target=run_clock, daemon=True)\n",
        "            t.start()\n",
        "\n",
        "            salida_logs.clear_output(wait=True)\n",
        "            #start_time = time.time()  # inicio temporizador\n",
        "\n",
        "            with salida_logs:\n",
        "                resultados = []\n",
        "                # determinar métodos\n",
        "                sel_met = selector_metodo.value\n",
        "                methods = metodos.copy() if sel_met=='Todos' else [sel_met]\n",
        "                # determinar motores\n",
        "                sel_mot = list(selector_motor.value)\n",
        "                mot_sel = motores if 'Todos' in sel_mot else sel_mot\n",
        "\n",
        "                for metodo in methods:\n",
        "\n",
        "                    # (Aquí va la parte para obtener `cols` desde RESUMEN_METODOS)\n",
        "                    if metodo == 'Todos':\n",
        "                        cols = []\n",
        "                        for m_aux in metodos:\n",
        "                            cols += RESUMEN_METODOS.get(m_aux, [])\n",
        "                        cols = sorted(set(cols))\n",
        "                    else:\n",
        "                        cols = RESUMEN_METODOS.get(metodo, [])\n",
        "\n",
        "                    for motor in mot_sel:\n",
        "                        # ✅ AQUI VA el bloque que te he entregado\n",
        "                        print(f\"➡️ {motor} en '{metodo}'…\")\n",
        "\n",
        "                        # Validar columnas existentes\n",
        "                        effective_cols = [c for c in cols if c in X_test.columns]\n",
        "                        print(f\"[DEBUG] RNN método '{metodo}' - columnas esperadas: {len(cols)}, válidas: {len(effective_cols)} → {effective_cols}\")\n",
        "\n",
        "                        if len(effective_cols) < 2:\n",
        "                            print(f\"⚠️ Se omite '{metodo}' con motor '{motor}' porque solo seleccionó {len(effective_cols)} columnas válidas: {effective_cols}\")\n",
        "                            continue\n",
        "\n",
        "                        # Preparar subconjuntos\n",
        "                        X_train_sel = X_train[effective_cols].copy()\n",
        "                        Y_train_sel = Y_train.copy()\n",
        "                        FECHAS_train_sel = FECHAS_train.copy()\n",
        "\n",
        "                        X_test_sel  = X_test[effective_cols].copy()\n",
        "                        Y_test_sel  = Y_test.copy()\n",
        "                        FECHAS_test_sel  = FECHAS_test.copy()\n",
        "\n",
        "                        X_test_vals = X_test_sel.values\n",
        "                        y_test_vals = Y_test.values.ravel()\n",
        "\n",
        "                        kr = RNNRegressor()\n",
        "                        # ============================================\n",
        "                        # Motor de optimización RandomSearch\n",
        "                        # ============================================\n",
        "                        if motor == 'RandomSearch':\n",
        "                            # ——— AÑADIDO: re-ajustar escaladores para el filtrado actual ———\n",
        "                            sx = StandardScaler().fit(X_train_sel.values)\n",
        "                            sy = StandardScaler().fit(Y_train_sel.values.reshape(-1,1))\n",
        "                            # preparar datos (ahora con el scaler correcto)\n",
        "                            Xv = sx.transform(X_train_sel.values)\n",
        "                            yv = sy.transform(Y_train_sel.values.reshape(-1,1)).ravel()\n",
        "                            # Distribuciones de scipy.stats\n",
        "                            from scipy.stats import randint, uniform, loguniform\n",
        "                            # Usar nuestro wrapper\n",
        "                            kr = RNNRegressor(\n",
        "                                units=64, dropout_rate=0.2,\n",
        "                                learning_rate=1e-3, epochs=50,\n",
        "                                batch_size=32, verbose=3\n",
        "                            )  # NEW\n",
        "                            rs_params = {\n",
        "                                'units': randint(10, 61),\n",
        "                                'dropout_rate': uniform(0.0, 0.5),\n",
        "                                'learning_rate': loguniform(1e-4, 1e-2),\n",
        "                                'epochs': randint(50, 401),\n",
        "                                'batch_size': randint(16, 257)\n",
        "                            }\n",
        "                            rs = RandomizedSearchCV(\n",
        "                                estimator=kr,\n",
        "                                param_distributions=rs_params,\n",
        "                                n_iter=n_iter.value,\n",
        "                                scoring=selector_scoring.value,\n",
        "                                cv=cv.value,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1,\n",
        "                                verbose = 3\n",
        "                            )\n",
        "                            print(\"🔍 Ejecutando RandomizedSearchCV…\")\n",
        "                            rs.fit(Xv, yv)\n",
        "                            best = rs.best_estimator_\n",
        "                            score = rs.best_score_\n",
        "                            params = rs.best_params_\n",
        "\n",
        "                        # ============================================\n",
        "                        # Motor de optimización Bayesian\n",
        "                        # ============================================\n",
        "                        elif motor=='Bayesian':\n",
        "                            # preparar datos\n",
        "                            #Xv = X_train_sel.values\n",
        "                            #yv = Y_train_sel.values.ravel()\n",
        "                            sx = StandardScaler().fit(X_train_sel.values)\n",
        "                            sy = StandardScaler().fit(Y_train_sel.values.reshape(-1,1))\n",
        "                            Xv = sx.transform(X_train_sel.values)\n",
        "                            yv = sy.transform(Y_train_sel.values.reshape(-1,1)).ravel()\n",
        "                            # Usar nuestro wrapper\n",
        "                            kr = RNNRegressor()\n",
        "                            #param_space = {\n",
        "                            bay_params = {\n",
        "                                'units': Integer(10,60),\n",
        "                                'dropout_rate': Real(0.0,0.5),\n",
        "                                'learning_rate': Real(1e-4,1e-2,'log-uniform'),\n",
        "                                'epochs': Integer(50,400),\n",
        "                                'batch_size': Integer(16,256)\n",
        "                            }\n",
        "                            bs = BayesSearchCV(\n",
        "                                kr,\n",
        "                                #param_space,\n",
        "                                bay_params,\n",
        "                                n_iter=n_iter.value,\n",
        "                                scoring=selector_scoring.value,\n",
        "                                cv=cv.value,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1,\n",
        "                                verbose=3\n",
        "                            )\n",
        "                            print(\"🔍 Ejecutando Bayesian…\")\n",
        "                            bs.fit(Xv,yv)\n",
        "                            best, score, params = bs.best_estimator_, bs.best_score_, bs.best_params_\n",
        "\n",
        "                        # ============================================\n",
        "                        # Motor de optimización Hyperband\n",
        "                        # ============================================\n",
        "                        elif motor=='Hyperband':\n",
        "                            # preparar datos\n",
        "                            #Xv = X_train_sel.values\n",
        "                            #yv = Y_train_sel.values.ravel()\n",
        "                            sx = StandardScaler().fit(X_train_sel.values)\n",
        "                            sy = StandardScaler().fit(Y_train_sel.values.reshape(-1,1))\n",
        "                            Xv = sx.transform(X_train_sel.values)\n",
        "                            yv = sy.transform(Y_train_sel.values.reshape(-1,1)).ravel()\n",
        "                            # Usar nuestro wrapper\n",
        "                            kr = RNNRegressor()\n",
        "                            # Distribuciones scipy.stats para Hyperband\n",
        "                            from scipy.stats import randint, uniform, loguniform\n",
        "                            hb_params = {\n",
        "                                'units': randint(10, 61),              # enteros 10–60\n",
        "                                'dropout_rate': uniform(0.0, 0.5),      # continuo 0–0.5\n",
        "                                'learning_rate': loguniform(1e-4, 1e-2),# log-uniform 1e-4–1e-2\n",
        "                                'batch_size': randint(16, 257)         # enteros 16–256\n",
        "                            }\n",
        "                            hb = HalvingRandomSearchCV(\n",
        "                                estimator=kr,\n",
        "                                param_distributions=hb_params,\n",
        "                                factor=3,\n",
        "                                resource='epochs',\n",
        "                                #max_resources=400,\n",
        "                                max_resources=50,\n",
        "                                scoring=selector_scoring.value,\n",
        "                                cv=cv.value,\n",
        "                                #cv=2,\n",
        "                                random_state=42,\n",
        "                                #n_jobs=-1,\n",
        "                                n_jobs=1,\n",
        "                                verbose=3,\n",
        "                                error_score='raise'\n",
        "                            )\n",
        "                            print(\"🔍 Ejecutando Hyperband…\")\n",
        "                            hb.fit(Xv,yv)\n",
        "                            best, score, params = hb.best_estimator_, hb.best_score_, hb.best_params_\n",
        "\n",
        "                        # ============================================\n",
        "                        # Motor de optimización Optuna\n",
        "                        # ============================================\n",
        "                        elif motor=='Optuna':\n",
        "                            # Preparamos los datos\n",
        "                            #Xv = X_train_sel.values\n",
        "                            #yv = Y_train_sel.values.ravel()\n",
        "                            sx = StandardScaler().fit(X_train_sel.values)\n",
        "                            sy = StandardScaler().fit(Y_train_sel.values.reshape(-1,1))\n",
        "                            Xv = sx.transform(X_train_sel.values)\n",
        "                            yv = sy.transform(Y_train_sel.values.reshape(-1,1)).ravel()\n",
        "                            kr = RNNRegressor()  # wrapper limpio\n",
        "                            import optuna\n",
        "                            def objective(trial):\n",
        "                                # Sugerencia de hiperparámetros\n",
        "                                units = trial.suggest_int('units', 10, 20)\n",
        "                                dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
        "                                learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "                                epochs = trial.suggest_int('epochs', 50, 100)\n",
        "                                batch_size = trial.suggest_int('batch_size', 16, 256)\n",
        "                                # Instanciamos y entrenamos el modelo\n",
        "                                model = RNNRegressor(\n",
        "                                    units=units,\n",
        "                                    dropout_rate=dropout_rate,\n",
        "                                    learning_rate=learning_rate,\n",
        "                                    epochs=epochs,\n",
        "                                    batch_size=batch_size,\n",
        "                                    verbose=3\n",
        "                                )\n",
        "                                model.fit(Xv, yv)\n",
        "                                preds = model.predict(Xv)\n",
        "                                # Devolvemos la métrica a optimizar\n",
        "                                if selector_scoring.value == 'r2':\n",
        "                                    return r2_score(yv, preds)\n",
        "                                elif selector_scoring.value == 'neg_mean_squared_error':\n",
        "                                    return -mean_squared_error(yv, preds)\n",
        "                                else:  # neg_mean_absolute_error\n",
        "                                    return -mean_absolute_error(yv, preds)\n",
        "                            # Creamos y ejecutamos el estudio\n",
        "                            study = optuna.create_study(\n",
        "                                direction='maximize'\n",
        "                                if selector_scoring.value == 'r2'\n",
        "                                else 'minimize'\n",
        "                            )\n",
        "                            study.optimize(objective, n_trials=n_iter.value)\n",
        "                            # Recuperamos los mejores parámetros y entrenamos el modelo final\n",
        "                            best_params = study.best_params\n",
        "                            best = RNNRegressor(**best_params)\n",
        "                            best.fit(Xv, yv)\n",
        "                            # Interpretamos el score devuelto\n",
        "                            if selector_scoring.value == 'r2':\n",
        "                                score = study.best_value\n",
        "                            else:\n",
        "                                score = -study.best_value\n",
        "                            params = best_params\n",
        "\n",
        "                        else:\n",
        "                            best=None\n",
        "                            score = None\n",
        "                            params = {}\n",
        "\n",
        "                        # 🗄️ GUARDAR MODELO / METADATOS  ←―――――――――――――――――\n",
        "                        guardar_rnn(best, score, metodo, motor,\n",
        "                                    selector_scoring, cols, Y_train,\n",
        "                                    sx, sy,                    # <-- ahora son obligatorios\n",
        "                                    study if motor=='Optuna' else None,\n",
        "                                    traza_out=salida_logs)\n",
        "                        # ————————————————————————————————————————————————————\n",
        "\n",
        "                        # ── AÑADIR REGISTRO EN OPT_MODELS AQUÍ ─────────────────────────────\n",
        "                        # construimos el tag y las rutas (deberían coincidir con las usadas en guardar_rnn)\n",
        "                        ts       = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                        tag      = f\"rnn_{metodo.lower()}_{motor.lower()}_opt_{ts}\"\n",
        "                        model_f  = f\"modelos_opt/{tag}.keras\"\n",
        "                        meta_f   = f\"modelos_opt/{tag}_meta.pkl\"\n",
        "\n",
        "                        OPT_MODELS[('rnn', metodo, motor)] = {\n",
        "                            'model_path': model_f,\n",
        "                            'meta_path' : meta_f,\n",
        "                            'score'     : float(score),\n",
        "                            'metric'    : selector_scoring.value,   # ← aquí la métrica\n",
        "                            'params'    : params,\n",
        "                            'model'     : best,\n",
        "                            'cols'      : cols,\n",
        "                            'sx'        : sx,\n",
        "                            'sy'        : sy\n",
        "                        }\n",
        "                        # Si es Optuna, guarda también la ruta al study\n",
        "                        if motor.lower() == 'optuna' and study is not None:\n",
        "                            OPT_MODELS[('rnn', metodo, motor)]['study_path'] = f\"modelos_opt/{tag}_study.pkl\"\n",
        "\n",
        "                        # ────────────────────────────────────────────────────────────────\n",
        "\n",
        "                        resultados.append({\n",
        "                            'metodo':metodo,\n",
        "                            'motor':motor,\n",
        "                            'score':score,\n",
        "                            'params':params,\n",
        "                            'best':best\n",
        "                        })\n",
        "                # Detener reloj\n",
        "                stop_event.set()\n",
        "                t.join()\n",
        "                # mostrar mejor\n",
        "                valid = [r for r in resultados if r['score'] is not None]\n",
        "                if valid:\n",
        "                    best_r = max(valid, key=lambda x: x['score'])\n",
        "                    print(\"\\n🏆 Mejor optimización:\")\n",
        "                    print(f\"  Método: {best_r['metodo']}\")\n",
        "                    print(f\"  Motor: {best_r['motor']}\")\n",
        "                    print(f\"  Score: {best_r['score']:.4f}\")\n",
        "                    print(f\"  Params: {best_r['params']}\")\n",
        "\n",
        "                    # === Gráficas del mejor modelo ===\n",
        "                    modelo_best = best_r['best']\n",
        "                    metodo_best = best_r['metodo']\n",
        "                    cols_best   = [sanitize_name(c) for c in RESUMEN_METODOS.get(metodo_best, [])]\n",
        "\n",
        "                    # Filtramos columnas válidas que estén presentes\n",
        "                    effective_cols = [c for c in cols_best if c in X_test.columns]\n",
        "                    if len(effective_cols) < 2:\n",
        "                        print(f\"⚠️ El método '{metodo_best}' no tiene columnas válidas para el Test. Se omite predicción.\")\n",
        "                    else:\n",
        "                        X_test_vals = X_test[effective_cols].copy().values\n",
        "                        y_test_vals = Y_test.values.ravel()\n",
        "                        fechas      = FECHAS_test.values\n",
        "\n",
        "                        # 1) calcula la predicción escalada\n",
        "                        y_pred_scaled = modelo_best.predict(X_test_vals)\n",
        "                        # 2) inversa de sy para volver a la escala original\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                        residuos = y_test_vals - y_pred\n",
        "\n",
        "                        # Gráfico de puntos Y real. vs. Y predicho por fecha\n",
        "                        plt.figure(figsize=(8,3))\n",
        "                        plt.scatter(fechas, y_test_vals, label='Y real')\n",
        "                        plt.scatter(fechas, y_pred, label='Y predicha')\n",
        "                        plt.xlabel('Fecha')\n",
        "                        plt.ylabel('Y')\n",
        "                        plt.title('Y real vs predicha por Fecha')\n",
        "                        plt.legend()\n",
        "                        plt.tight_layout()\n",
        "                        plt.show()\n",
        "\n",
        "                        # Gráfico scatter Y real vs Y predicha\n",
        "                        plt.figure(figsize=(5,5))\n",
        "                        plt.scatter(y_test_vals, y_pred, alpha=0.6)\n",
        "                        plt.plot([y_test_vals.min(), y_test_vals.max()], [y_test_vals.min(), y_test_vals.max()], 'r--', label='Ideal')\n",
        "                        plt.xlabel('Y real'); plt.ylabel('Y predicha'); plt.title('Y real vs Y predicha')\n",
        "                        plt.tight_layout(); plt.legend(); plt.show()\n",
        "\n",
        "                        # Histograma residuos con Normal y KDE\n",
        "                        plt.figure(figsize=(6,4))\n",
        "                        from scipy.stats import norm, gaussian_kde, shapiro\n",
        "                        n,bins,_ = plt.hist(residuos, bins=20, density=True, alpha=0.6, label='Residuos')\n",
        "                        mu, sd = norm.fit(residuos)\n",
        "                        x = np.linspace(bins.min(), bins.max(), 100)\n",
        "                        plt.plot(x, norm.pdf(x, mu, sd), 'r--', label='Normal')\n",
        "                        kde = gaussian_kde(residuos)\n",
        "                        plt.plot(x, kde(x), 'g-', label='KDE')\n",
        "                        plt.xlabel('Residuo'); plt.ylabel('Densidad'); plt.title('Histograma de residuos')\n",
        "                        plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "                        # Test de normalidad Shapiro-Wilk\n",
        "                        stat, p_value = shapiro(residuos)\n",
        "                        print(f\"🧪 Shapiro-Wilk: estadístico={stat:.4f}, p-valor={p_value:.4f} -> {'Normal' if p_value > 0.05 else 'No normal'}\")\n",
        "\n",
        "                else:\n",
        "                    print(\"⚠️ No hay resultados con score definido.\")\n",
        "                # Tabla completa de optimizaciones\n",
        "                df_all=pd.DataFrame(resultados)\n",
        "                display(HTML(\"<h4>📊 Todos los resultados de optimización</h4>\"))\n",
        "                display(df_all)\n",
        "\n",
        "        # enlazar callbacks\n",
        "        btn_cargar.on_click(cargar_variables)\n",
        "        btn_ejecutar.on_click(ejecutar_optimizacion)\n",
        "\n",
        "        # mostrar interfaz\n",
        "        display(HTML(\"<h3>🔧 Optimización RNN - Carga y Motores</h3>\"))\n",
        "        display(widgets.VBox([\n",
        "            selector_metodo,\n",
        "            btn_cargar,\n",
        "            salida_carga,\n",
        "            selector_motor,\n",
        "            selector_scoring,\n",
        "            cv,\n",
        "            n_iter,\n",
        "            progreso_reloj,\n",
        "            btn_ejecutar,\n",
        "            salida_logs\n",
        "        ]))\n",
        "\n",
        "# Mostrar contenedor al cargar la celda\n",
        "display(out_opt_rnn)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 10. INTERPRETACION xIA - FUNCIONA CORRECTAMENTE - REQUIERE AJUSTES\n",
        "# Este módulo es el responsable de usar los modelos SHAP, LIME y KernelExplainer para explicar el comportamiento de los modelos de Modelado usados (SVR, NN y XGBoost)\n",
        "# ===============================================================\n",
        "import glob, io, base64, pickle, random, pprint\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "import os, re\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "# SHAP & LIME\n",
        "import shap\n",
        "from shap import TreeExplainer, KernelExplainer, GradientExplainer, DeepExplainer\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "# Scikit‑learn\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "try:\n",
        "    from interpret.glassbox import ExplainableBoostingRegressor\n",
        "except ImportError:\n",
        "    ExplainableBoostingRegressor = None  # placeholder\n",
        "\n",
        "if 'xai_results' not in globals():                        # Nuevo para celda 12\n",
        "    xai_results = {}                                      # Nuevo para celda 12\n",
        "\n",
        "# Mapeo de nombres para búsqueda de archivos\n",
        "MODEL_KEYS = {\n",
        "    'SVR': 'svr',\n",
        "    'NN': 'nn',\n",
        "    'XGBoost': 'xgb',\n",
        "    'Random Forest': 'rf',\n",
        "    'RNN': 'rnn'\n",
        "}\n",
        "\n",
        "# Lista maestra de motores que tienes disponibles\n",
        "ALL_MOTORES = [\n",
        "    \"SHAP\",\n",
        "    \"LIME\",\n",
        "    \"KernelExplainer\",\n",
        "    \"Integrated Gradients\",\n",
        "    \"DeepLIFT / LRP\",\n",
        "    \"Permutation Feature Importance\",\n",
        "    \"Partial Dependence Plots (PDP)\",\n",
        "    \"Accumulated Local Effects (ALE)\",\n",
        "    \"Individual Conditional Expectation (ICE) Plots\",\n",
        "    \"Counterfactual Explanations\",\n",
        "    \"Anchors\",\n",
        "    \"Surrogate Models (Global/Local)\",\n",
        "    \"Explainable Boosting Machine (EBM)\",\n",
        "    \"Optuna Hyperparameter Importance\"\n",
        "]\n",
        "\n",
        "# —— 0) Inyectamos CSS para todos los Dropdowns y SelectMultiple ——\n",
        "display(HTML(\"\"\"\n",
        "<style>\n",
        "/* Ancho y tipografía de los widgets */\n",
        ".widget-dropdown, .widget-select-multiple, .widget-button {\n",
        "  width: 400px !important;       /* cajas más anchas */\n",
        "  font-size: 14px !important;    /* texto de 14px */\n",
        "}\n",
        ".widget-dropdown > label, .widget-select-multiple > label {\n",
        "  font-size: 14px !important;    /* etiquetas también grandes */\n",
        "}\n",
        "</style>\n",
        "\"\"\"))\n",
        "\n",
        "# Listas de modelos entrenados y óptimos\n",
        "TRAINED_MODELS = [f\"{m}\" for m in MODEL_KEYS]\n",
        "OPTIMIZED_MODELS = [f\"{m}\" for m in MODEL_KEYS]\n",
        "# Métodos de selección\n",
        "SELECT_METHODS = ['Pearson', 'Spearman', 'Mutualinfo', 'Boruta', 'UMAP']\n",
        "\n",
        "# xIA methods\n",
        "XAI_METHODS = [\n",
        "    'SHAP', 'LIME', 'KernelExplainer', 'Integrated Gradients',\n",
        "    'DeepLIFT / LRP', 'Permutation Feature Importance',\n",
        "    'Partial Dependence Plots (PDP)', 'Accumulated Local Effects (ALE)',\n",
        "    'Individual Conditional Expectation (ICE) Plots',\n",
        "    'Counterfactual Explanations', 'Anchors',\n",
        "    'Surrogate Models (Global/Local)', 'Explainable Boosting Machine (EBM)',\n",
        "    'Optuna Hyperparameter Importance', 'Todos'\n",
        "]\n",
        "\n",
        "XAI_HELP = {\n",
        "    'SHAP': (\n",
        "        '<h4>SHAP</h4>'\n",
        "        '<p><b>SHAP</b> (SHapley Additive exPlanations) utiliza teoría de juegos para descomponer '\n",
        "        'la predicción de un modelo en aportes aditivos de cada característica.</p>'\n",
        "        '<p><b>Valores SHAP:</b> representan la contribución de cada variable a la predicción final.</p>'\n",
        "        '<ul>'\n",
        "        '<li><b>Empuje hacia arriba (positivo):</b> indica que la variable incrementa la predicción respecto al valor base.</li>'\n",
        "        '<li><b>Empuje hacia abajo (negativo):</b> indica que la variable decrementa la predicción respecto al valor base.</li>'\n",
        "        '</ul>'\n",
        "        '<p>El <i>valor base</i> es la predicción promedio del modelo sin conocer ninguna característica.</p>'\n",
        "        '<p>Para un punto de datos, la suma de los valores SHAP más el valor base equivale a la predicción del modelo.</p>'\n",
        "    ),\n",
        "    'LIME': (\n",
        "        '<h4>LIME</h4>'\n",
        "        '<p><b>LIME</b> (Local Interpretable Model-agnostic Explanations) explica la predicción '\n",
        "        'de cualquier modelo construyendo, en la vecindad de la instancia, un modelo lineal simple.</p>'\n",
        "        '<ul>'\n",
        "          '<li>Se perturban aleatoriamente las características de la muestra.</li>'\n",
        "          '<li>Se calcula la predicción del modelo “negro”.</li>'\n",
        "          '<li>Se ajusta una función lineal ponderada por proximidad al punto original.</li>'\n",
        "          '<li>Los coeficientes resultantes indican dirección y magnitud de influencia local.</li>'\n",
        "        '</ul>'\n",
        "        '<p><b>Interpretación:</b> coeficiente positivo ⇒ la característica empuja la predicción hacia arriba; '\n",
        "        'coeficiente negativo ⇒ la empuja hacia abajo.</p>'\n",
        "    ),\n",
        "    'KernelExplainer': (\n",
        "        '<h4>KernelExplainer</h4>'\n",
        "        '<p><b>KernelExplainer</b> es una extensión de SHAP para modelos de caja negra, '\n",
        "        'usando un núcleo de similitud para aproximar valores SHAP sin requerir acceso a gradientes.</p>'\n",
        "        '<ul>'\n",
        "        '<li><b>Fondo (background):</b> subconjunto de datos para estimar valor base.</li>'\n",
        "        '<li><b>Valor base:</b> predicción promedio del fondo.</li>'\n",
        "        '<li><b>Valores Kernel SHAP:</b> contribuciones de cada característica calculadas mediante ponderaciones del núcleo.</li>'\n",
        "        '<li><b>Empuje hacia arriba (positivo):</b> la característica aumenta la predicción con respecto al valor base.</li>'\n",
        "        '<li><b>Empuje hacia abajo (negativo):</b> la característica disminuye la predicción.</li>'\n",
        "        '</ul>'\n",
        "        '<p>El método pesa cada combinación de características según su similitud al punto de interés, '\n",
        "        'ofreciendo explicaciones globales y locales.</p>'\n",
        "        '<p><i>Interpretación de tablas:</i> cada fila es una muestra, columnas son características; '\n",
        "        'valores muestran su empuje.</p>'\n",
        "        '<p><i>Interpretación del gráfico summary:</i> distribución de valores Kernel SHAP, '\n",
        "        'el color indica magnitud de la característica.</p>'\n",
        "    ),\n",
        "    'Integrated Gradients': (\n",
        "        '<h4>Integrated Gradients</h4>'\n",
        "        '<p><b>Integrated Gradients</b> es un método de atribución para modelos diferenciables, '\n",
        "        'que integra gradientes desde una referencia (p.ej. vector cero) hasta la instancia objetivo.</p>'\n",
        "        '<ul>'\n",
        "        '<li><b>Ruta de integración:</b> línea recta desde referencia hasta punto de interés en el espacio de características.</li>'\n",
        "        '<li><b>Atributos IG:</b> promedio de gradientes a lo largo de la ruta, ponderando contribuciones.</li>'\n",
        "        '<li><b>Interpretación:</b> valores positivos ➔ aumento de predicción; valores negativos ➔ disminución.</li>'\n",
        "        '</ul>'\n",
        "        '<p><i>Interpretación de tablas:</i> cada fila es una muestra, columnas son características; '\n",
        "        'valores muestran la contribución integrada.</p>'\n",
        "        '<p><i>Interpretación de gráfico:</i> distribución de valores IG, destacando variables con mayores efectos acumulados.</p>'\n",
        "    ),\n",
        "    'DeepLIFT / LRP': (\n",
        "        '<h4>DeepLIFT / LRP</h4>'\n",
        "        '<p><b>DeepLIFT</b> y <b>Layer-wise Relevance Propagation (LRP)</b> son métodos de atribución que '\n",
        "        'propagan la relevancia de la salida de la red hacia atrás a cada neurona de entrada.</p>'\n",
        "        '<ul>'\n",
        "        '<li><b>Relevancia positiva:</b> indica que la característica contribuyó a aumentar la salida.</li>'\n",
        "        '<li><b>Relevancia negativa:</b> indica que la característica contribuyó a disminuir la salida.</li>'\n",
        "        '</ul>'\n",
        "        '<p>La suma de las relevancias de entrada equivale a la activación de salida menos la referencia.</p>'\n",
        "        '<p><b>Interpretación de tabla:</b> cada fila es una muestra, columnas son características y valores de relevancia.</p>'\n",
        "        '<p><b>Interpretación de gráfico:</b> barras muestran relevancia media global.</p>'\n",
        "    ),\n",
        "    'Permutation Feature Importance': (\n",
        "        '<h4>Permutation Feature Importance</h4>'\n",
        "        '<p>Mide la importancia de cada característica evaluando la caída en rendimiento '\n",
        "        'al permutar sus valores.</p>'\n",
        "        '<ul>'\n",
        "        '<li>Para cada variable, se permutan sus valores en el dataset de prueba.</li>'\n",
        "        '<li>Se mide la diferencia en la métrica (p.ej. R²).</li>'\n",
        "        '<li>Una caída mayor indica mayor importancia de esa variable.</li>'\n",
        "        '</ul>'\n",
        "        '<p>Interpretación de la tabla:</p>'\n",
        "        '<ul>'\n",
        "        '<li><b>Mean Importance:</b> promedio de las caídas de rendimiento tras permutar.</li>'\n",
        "        '<li><b>Std Importance:</b> variabilidad en esos descensos.</li>'\n",
        "        '</ul>'\n",
        "        '<p>Interpretación del gráfico:</p>'\n",
        "        '<ul>'\n",
        "        '<li>Puntos representan importancia media; barras de error, desviación estándar.</li>'\n",
        "        '</ul>'\n",
        "    ),\n",
        "    'Partial Dependence Plots (PDP)': (\n",
        "        '<h4>Partial Dependence Plots (PDP)</h4>'\n",
        "        '<p><b>Partial Dependence Plots</b> (PDP) permiten visualizar el efecto medio que tiene una variable (o par de variables) '\n",
        "        'sobre la predicción de un modelo, manteniendo fijas todas las demás. Es un método global y agnóstico al modelo, muy útil para '\n",
        "        'entender la dirección (positiva o negativa) y la magnitud del impacto de cada característica.</p>'\n",
        "\n",
        "        '<h5>¿Cómo se calcula?</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Se selecciona una variable y se construye una rejilla de valores representativos en su rango.</li>'\n",
        "        '<li>Para cada valor de la rejilla, se reemplaza dicha variable en todas las observaciones del conjunto de datos con ese valor.</li>'\n",
        "        '<li>Se predice el valor objetivo para este nuevo dataset y se calcula la media de las predicciones.</li>'\n",
        "        '<li>Estos valores promedio constituyen la <b>curva PDP</b> de la variable.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Interpretación:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>El <b>eje X</b> representa los valores posibles de la variable analizada.</li>'\n",
        "        '<li>El <b>eje Y</b> representa la predicción promedio del modelo cuando la variable toma esos valores.</li>'\n",
        "        '<li>Una <b>pendiente positiva</b> indica que un aumento de la variable incrementa la predicción media.</li>'\n",
        "        '<li>Una <b>pendiente negativa</b> indica que un aumento de la variable reduce la predicción media.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Importancia global con PDP:</h5>'\n",
        "        '<p>En este motor se calcula como el <b>rango</b> (máximo menos mínimo) de la curva PDP. '\n",
        "        'Esto representa cuánto puede cambiar la predicción promedio si se modifica la variable a lo largo de todo su dominio. '\n",
        "        'Cuanto mayor el rango, mayor es la importancia de esa variable en el comportamiento del modelo.</p>'\n",
        "\n",
        "        '<h5>Importancia local:</h5>'\n",
        "        '<p>También se calcula una tabla con los efectos PDP para las primeras muestras del conjunto de datos. '\n",
        "        'Para cada muestra y variable, se evalúa cuánto cambia la predicción promedio cuando se fija la variable al valor observado '\n",
        "        'y se mantiene el resto con su distribución real. Esto permite interpretar el efecto individual de cada característica '\n",
        "        'en una muestra específica, de forma análoga a SHAP o LIME.</p>'\n",
        "\n",
        "        '<h5>Limitaciones:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>No tiene en cuenta interacciones entre variables (a menos que se usen PDP bivariados).</li>'\n",
        "        '<li>Puede ser engañoso si hay fuerte correlación entre variables (reemplazar una variable puede generar combinaciones no plausibles).</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<p><i>En resumen:</i> PDP ayuda a responder: <b>¿Cómo varía la predicción promedio del modelo cuando cambio una variable concreta?</b></p>'\n",
        "    ),\n",
        "    'Accumulated Local Effects (ALE)': (\n",
        "        '<h4>Accumulated Local Effects (ALE)</h4>'\n",
        "        '<p><b>ALE</b> (Efectos Locales Acumulados) es un método de interpretabilidad global que muestra '\n",
        "        'cómo cambia la predicción promedio del modelo cuando una característica varía dentro de su dominio, '\n",
        "        'teniendo en cuenta las correlaciones entre variables.</p>'\n",
        "\n",
        "        '<h5>¿Cómo se calcula?</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Se divide el rango de una variable en <i>bins</i> (intervalos), usualmente basados en cuantiles para que contengan un número similar de observaciones.</li>'\n",
        "        '<li>En cada bin, se mide el efecto local de cambiar el valor de la variable desde el límite inferior al superior, manteniendo fijas las demás variables.</li>'\n",
        "        '<li>Estos efectos se acumulan a lo largo de los bins, produciendo una <b>curva ALE</b> que muestra cómo influye la variable sobre la predicción.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Ventajas frente a PDP:</h5>'\n",
        "        '<ul>'\n",
        "        '<li><b>Robusto ante correlaciones:</b> A diferencia de PDP, ALE no genera combinaciones irreales de variables, ya que respeta la distribución original de los datos.</li>'\n",
        "        '<li><b>Computacionalmente eficiente:</b> Sólo evalúa muestras dentro de cada bin, evitando duplicaciones masivas.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Interpretación:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>El eje <b>X</b> representa el valor de la variable (centrado por defecto).</li>'\n",
        "        '<li>El eje <b>Y</b> representa el efecto acumulado sobre la predicción del modelo.</li>'\n",
        "        '<li>Una pendiente positiva indica que aumentar esa variable tiende a aumentar la predicción.</li>'\n",
        "        '<li>Una pendiente negativa indica lo contrario.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Importancia global:</h5>'\n",
        "        '<p>En este motor, la <b>importancia global</b> de una variable se calcula como el <b>rango</b> '\n",
        "        'de su curva ALE (es decir, la diferencia entre el valor máximo y mínimo del efecto acumulado). '\n",
        "        'Un mayor rango implica mayor impacto medio de esa característica sobre la salida del modelo.</p>'\n",
        "\n",
        "        '<h5>Importancia local:</h5>'\n",
        "        '<p>Se muestran los valores ALE correspondientes a las primeras muestras del conjunto de datos. '\n",
        "        'Estos valores permiten entender el efecto individual de cada variable sobre la predicción de cada observación.</p>'\n",
        "\n",
        "        '<h5>Limitaciones:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Asume que los efectos son aditivos y no modela explícitamente interacciones (aunque puede ampliarse con ALE 2D).</li>'\n",
        "        '<li>Puede suavizar excesivamente relaciones no lineales muy complejas si se usan pocos bins.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<p><i>Resumen:</i> ALE es una técnica moderna, confiable y robusta para evaluar la importancia de cada variable '\n",
        "        'respetando la estructura estadística real del dataset.</p>'\n",
        "    ),\n",
        "    'Individual Conditional Expectation (ICE) Plots': (\n",
        "        '<h4>Individual Conditional Expectation (ICE) Plots</h4>'\n",
        "        '<p><b>ICE</b> es una técnica de interpretabilidad local que representa cómo varía la predicción de un modelo '\n",
        "        'para una observación concreta cuando se modifica una de sus características, manteniendo fijas las demás. '\n",
        "        'Es una generalización del método PDP (Partial Dependence Plots), pero a nivel individual.</p>'\n",
        "\n",
        "        '<h5>¿Cómo se calcula?</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Para una observación concreta, se generan múltiples versiones de ella cambiando solo una variable (por ejemplo, X1) a lo largo de un rango de valores.</li>'\n",
        "        '<li>Se evalúa el modelo sobre estas versiones y se obtienen las predicciones correspondientes.</li>'\n",
        "        '<li>La curva resultante muestra cómo cambia la salida del modelo solo por esa variable en esa observación concreta.</li>'\n",
        "        '<li>Repitiendo esto para varias muestras, se obtiene un conjunto de curvas ICE que capturan efectos individuales.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Interpretación:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Las curvas muestran cómo cambia la predicción de cada muestra cuando se varía una característica.</li>'\n",
        "        '<li>Permiten detectar <b>interacciones no lineales</b>, <b>heterogeneidad de efectos</b> o <b>inestabilidad</b> en el modelo.</li>'\n",
        "        '<li>Cuando todas las curvas son paralelas, la relación es globalmente estable (similar al PDP).</li>'\n",
        "        '<li>Cuando las curvas difieren fuertemente entre sí, la variable tiene un efecto que depende del resto del contexto (otras variables).</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Importancia global:</h5>'\n",
        "        '<p>En este motor, la importancia global de una variable se calcula como la <b>media del rango</b> de las curvas ICE sobre un subconjunto de muestras. '\n",
        "        'Este valor representa cuánto cambia, en promedio, la predicción individual cuando se varía esa característica. '\n",
        "        'Un mayor rango indica mayor influencia.</p>'\n",
        "\n",
        "        '<h5>Importancia local:</h5>'\n",
        "        '<p>Se muestra también una tabla con los <b>rangos individuales ICE</b> para las primeras observaciones del conjunto. '\n",
        "        'Esto permite ver cómo de sensible es cada muestra a cambios en una variable concreta.</p>'\n",
        "\n",
        "        '<h5>Ventajas:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Captura <b>efectos individuales</b>, no promedios, lo que permite diagnósticos precisos por observación.</li>'\n",
        "        '<li>Permite detectar comportamientos atípicos, interacciones complejas y sesgos locales.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Limitaciones:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Puede ser ruidoso si se muestran demasiadas curvas simultáneamente.</li>'\n",
        "        '<li>Como PDP, puede generar combinaciones irreales si las variables están fuertemente correlacionadas.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<p><i>Resumen:</i> ICE muestra <b>cómo cambia la predicción para cada observación</b> al modificar una variable específica, '\n",
        "        'ofreciendo una perspectiva individual que complementa la visión global de otros métodos.</p>'\n",
        "    ),\n",
        "    'Counterfactual Explanations' : (\n",
        "        '<h4>Counterfactual Explanations</h4>'\n",
        "        '<p>Las <b>explicaciones contrafactuales</b> muestran cómo debe modificarse una observación para que el modelo devuelva una predicción significativamente distinta, con el menor cambio posible en sus variables.</p>'\n",
        "\n",
        "        '<h5>¿Cómo funciona?</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Se parte de una predicción original y se busca un valor deseado que suponga un cambio relevante (por ejemplo, un +10%).</li>'\n",
        "        '<li>Se recorren los datos reales del conjunto de entrenamiento en busca de observaciones cuya predicción cumpla ese cambio.</li>'\n",
        "        '<li>De entre estas, se selecciona la más cercana (menor distancia) respecto a la observación original.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Interpretación:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>La <b>tabla local</b> muestra los contrafactuales más cercanos para las primeras observaciones.</li>'\n",
        "        '<li>La <b>importancia global</b> indica el cambio medio absoluto necesario en cada variable para alcanzar el objetivo deseado.</li>'\n",
        "        '<li>El <b>gráfico</b> ayuda a identificar qué variables son más influyentes a la hora de cambiar el resultado del modelo.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Ventajas:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Interpretabilidad muy intuitiva: responde a la pregunta \"¿qué tendría que cambiar para obtener otro resultado?\"</li>'\n",
        "        '<li>Utiliza ejemplos reales del dataset, evitando combinaciones irreales.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Limitaciones:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Puede no encontrar contrafactuales viables si el cambio deseado es muy ambicioso o los datos están muy restringidos.</li>'\n",
        "        '<li>Los resultados dependen de la densidad del dataset y de su cobertura del espacio de entrada.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<p><i>Resumen:</i> Las explicaciones contrafactuales ofrecen una herramienta poderosa y humana para entender qué modificaciones mínimas podrían generar resultados más deseables en un modelo de regresión.</p>'\n",
        "    ),\n",
        "    'Anchors' : (\n",
        "        '<h4>Anchors</h4>'\n",
        "        '<p><b>Anchors</b> es un método de explicabilidad local que identifica reglas sencillas tipo \"SI/ENTONCES\" que justifican una predicción concreta. Estas reglas actúan como <b>anclas</b>, es decir, condiciones que al cumplirse aseguran con alta probabilidad que la predicción del modelo se mantenga inalterada.</p>'\n",
        "\n",
        "        '<h5>¿Cómo se generan?</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Para cada muestra, se selecciona un conjunto de observaciones vecinas (por ejemplo, mediante muestreo aleatorio).</li>'\n",
        "        '<li>Se binariza la variable de salida en función del valor de la muestra objetivo.</li>'\n",
        "        '<li>Se entrena un árbol de decisión con profundidad limitada para detectar las reglas que mejor separan los datos según esa binarización.</li>'\n",
        "        '<li>Se extrae la regla correspondiente a la muestra objetivo (el camino en el árbol).</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Interpretación de resultados:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>La <b>tabla de reglas ancla</b> muestra, para cada muestra explicada, la regla encontrada, su <i>cobertura</i> (porcentaje de vecinos que la cumplen) y su <i>precisión</i> (porcentaje de vecinos cubiertos cuya predicción coincide con la de la muestra).</li>'\n",
        "        '<li>La <b>tabla de importancia global</b> refleja la frecuencia con la que cada variable aparece en las reglas generadas para las distintas muestras.</li>'\n",
        "        '<li>El <b>gráfico de dispersión</b> permite visualizar qué variables son más recurrentes en las reglas ancla, ayudando a detectar aquellas más influyentes en decisiones locales.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Ventajas:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Alta interpretabilidad, ya que genera explicaciones similares a reglas humanas simples.</li>'\n",
        "        '<li>Evalúa tanto precisión como cobertura, ofreciendo una visión balanceada de la fiabilidad de la explicación.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<h5>Limitaciones:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>No siempre se pueden encontrar reglas con buena cobertura y precisión.</li>'\n",
        "        '<li>Las explicaciones pueden ser sensibles a la selección de vecinos y a la profundidad del árbol.</li>'\n",
        "        '</ul>'\n",
        "\n",
        "        '<p><i>Resumen:</i> Anchors permite entender las predicciones de un modelo mediante reglas simples que fijan su comportamiento local. Es especialmente útil cuando se busca justificar decisiones modelo en términos comprensibles y accionables.</p>'\n",
        "    ),\n",
        "    'Surrogate Models (Global/Local)' : (\n",
        "        '<h4>Surrogate Models (Global/Local)</h4>'\n",
        "        '<p>Los modelos sustitutos permiten interpretar el comportamiento de un modelo complejo mediante un segundo modelo interpretable que lo imita. En este motor se utilizan dos enfoques complementarios:</p>'\n",
        "        '<h5>🔹 Surrogate Global</h5>'\n",
        "        '<p>Se construye un árbol de decisión de profundidad limitada que se entrena sobre el dataset completo, utilizando como variable dependiente la salida del modelo original. El árbol actúa como un \"modelo proxy\" que resume las reglas generales de decisión del modelo complejo. La tabla resultante muestra la importancia relativa de cada variable en el árbol (contribución a la reducción del error).</p>'\n",
        "        '<h5>🔸 Surrogate Local</h5>'\n",
        "        '<p>Para cada una de las primeras muestras se construye un modelo lineal ajustado en su vecindario más cercano. Esto permite identificar qué variables tienen mayor influencia en cada caso individual. Se calcula el valor medio absoluto de los coeficientes para todas las variables y se presenta como una medida de importancia local.</p>'\n",
        "        '<h5>📊 Comparación gráfica</h5>'\n",
        "        '<p>El gráfico compara visualmente la importancia de cada variable en el modelo global frente a su influencia local promedio. Las variables con alta importancia en ambos enfoques son especialmente robustas. En cambio, si una variable tiene alta influencia local pero no global (o viceversa), puede indicar comportamientos específicos o inconsistencias locales.</p>'\n",
        "        '<h5>✅ Utilidad</h5>'\n",
        "        '<p>Este enfoque resulta útil cuando se desea contrastar patrones globales con explicaciones locales, evaluar consistencia en la importancia de las variables o detectar sesgos o excepciones locales en el modelo.</p>'\n",
        "        '<h4>Surrogate Models (Global/Local)</h4>'\n",
        "        '<p>Un <b>modelo sustituto</b> imita el comportamiento del modelo complejo con un algoritmo interpretable.</p>'\n",
        "        '<h5>Global</h5><p>Árbol de decisión entrenado sobre todo el dataset.</p>'\n",
        "        '<h5>Local</h5><p>Regresiones lineales ajustadas en vecindarios de cada muestra.</p>'\n",
        "        '<p>Las tablas y el gráfico resumen la importancia de variables a ambas escalas.</p>'\n",
        "    ),\n",
        "    'Explainable Boosting Machine (EBM)' : (\n",
        "        '<h4>Explainable Boosting Machine (EBM)</h4>'\n",
        "        '<p>EBM (Explainable Boosting Machine) es un modelo de aprendizaje automático de tipo aditivo generalizado (GA²M) que combina interpretabilidad total con capacidad predictiva competitiva.</p>'\n",
        "        '<p>EBM se basa en boosting de árboles muy pequeños (stumps) que se agregan para aprender funciones univariantes (una por variable) o bivariantes (combinaciones seleccionadas automáticamente). Estas funciones se combinan de forma aditiva para producir la predicción.</p>'\n",
        "        '<h5>🧩 Funcionamiento:</h5>'\n",
        "        '<ul>'\n",
        "        '<li>Para cada variable, se ajusta una función parcial que explica su contribución a la predicción.</li>'\n",
        "        '<li>Estas funciones se aprenden de forma secuencial y se corrigen entre sí (boosting).</li>'\n",
        "        '<li>Al final, la predicción total es la suma de todas las contribuciones univariantes + un sesgo.</li>'\n",
        "        '</ul>'\n",
        "        '<h5>📊 Salidas del motor:</h5>'\n",
        "        '<ul>'\n",
        "        '<li><b>Importancia Global</b>: ganancia relativa de cada función parcial, ordenada de mayor a menor.</li>'\n",
        "        '<li><b>Tabla Local</b>: muestra para las primeras muestras cuánto contribuye cada variable (positiva o negativamente) al valor final predicho.</li>'\n",
        "        '<li><b>Gráfico de media absoluta</b>: la media de las contribuciones absolutas refleja la influencia promedio de cada variable.</li>'\n",
        "        '</ul>'\n",
        "        '<h5>✅ Interpretabilidad:</h5>'\n",
        "        '<p>EBM permite visualizar cada función de forma directa: cómo cambia la predicción según los valores de una variable. Además, se pueden explorar interacciones seleccionadas por el modelo.</p>'\n",
        "        '<h5>🧠 Utilidad:</h5>'\n",
        "        '<p>EBM es especialmente útil cuando se requiere una explicación precisa, reproducible y completamente interpretable del modelo, sin necesidad de técnicas post-hoc.</p>'\n",
        "    ),\n",
        "    'Optuna Hyperparameter Importance' : (\n",
        "        '<h4>Optuna Hyperparameter Importance</h4>'\n",
        "        '<p>Este motor de interpretabilidad analiza el impacto de cada hiperparámetro en la métrica objetivo utilizada durante la optimización automática con Optuna.</p>'\n",
        "        '<h5>⚙️ ¿Cómo funciona?</h5>'\n",
        "        '<p>El motor se basa en el módulo <code>optuna.importance</code>, que estima la importancia de cada hiperparámetro utilizando técnicas basadas en permutaciones o regresión de sustitución. Evalúa cómo varía la métrica objetivo (por ejemplo, el error) cuando se altera un hiperparámetro en particular, manteniendo los demás fijos.</p>'\n",
        "        '<ul>'\n",
        "        '<li>Se utiliza un <code>study</code> previamente entrenado (en memoria o desde un archivo).</li>'\n",
        "        '<li>Se extraen los <code>trials</code> y se aplica el método <code>get_param_importances()</code> para obtener las contribuciones relativas.</li>'\n",
        "        '</ul>'\n",
        "        '<h5>📊 Salidas interpretables</h5>'\n",
        "        '<ul>'\n",
        "        '<li><b>Tabla de Importancia Global</b>: muestra el porcentaje de influencia de cada hiperparámetro en la variabilidad del resultado. Cuanto mayor sea la contribución, más crítico es ese parámetro para mejorar el rendimiento del modelo.</li>'\n",
        "        '<li><b>Top 10 Trials</b>: recoge los 10 mejores ensayos (trials) con sus hiperparámetros y resultados, lo que permite identificar configuraciones óptimas.</li>'\n",
        "        '<li><b>Gráfico de barras</b>: visualiza la importancia relativa de los hiperparámetros, facilitando su comparación directa.</li>'\n",
        "        '</ul>'\n",
        "        '<h5>✅ Utilidad práctica</h5>'\n",
        "        '<p>Esta herramienta es especialmente útil para:</p>'\n",
        "        '<ul>'\n",
        "        '<li>Identificar qué hiperparámetros son verdaderamente influyentes y cuáles se pueden fijar o descartar.</li>'\n",
        "        '<li>Reducir el espacio de búsqueda para futuras optimizaciones.</li>'\n",
        "        '<li>Justificar decisiones sobre tuning del modelo de forma objetiva y visual.</li>'\n",
        "        '</ul>'\n",
        "    ),\n",
        "    'Todos': '<b>Todos</b>: Mostrar todas las explicaciones anteriores.'\n",
        "}\n",
        "\n",
        "# ---------------- Ajustes visuales ------------------------------\n",
        "N_SHAP_SAMPLES      = 50   # muestras para SHAP\n",
        "N_SHAP_BACKGROUND   = 50   # cuántas filas usar como “fondo” para KernelExplainer\n",
        "N_LIME_SAMPLES      = 50   # muestras para LIME\n",
        "N_KERNEL_SAMPLES    = 50   # muestras para KERNEL EXPLAINER\n",
        "N_KERNEL_BACKGROUND = 50   # cuántas filas usar como “fondo” para KernelExplainer\n",
        "N_DEEP_SAMPLES      = 50   # muestras para DeepLIFT\n",
        "N_PERM_SAMPLES      = 50   # muestras para Permutation\n",
        "N_PDP_SAMPLES       = 50   # muestras para PDP\n",
        "N_ALE_SAMPLES       = 50   # muestras para ALE\n",
        "N_ICE_SAMPLES       = 50   # muestras para ICE\n",
        "N_CF_SAMPLES        = 50   # muestras para Counterfactuales\n",
        "N_ANCHOR_SAMPLES    = 50   # muestras para Anchors\n",
        "N_SURR_SAMPLES      = 100  # muestras para Surrogate\n",
        "N_EBM_SAMPLES       = 200  # muestras para EBM\n",
        "GRID_RES            = 20   # puntos para PDP/ICE\n",
        "FIRST_SAMPLES       = 10   # filas tablas locales\n",
        "ALE_BINS            = 20   # bins para ALE\n",
        "ICE_SAMPLES         = 50   # nº máximo de curvas ICE para importancia global\n",
        "CF_SAMPLES          = 20   # nº muestras para contrafactuales\n",
        "CF_TARGET_DELTA     = 0.1  # cambio relativo % deseado en salida (10 %)\n",
        "ANC_NEIGHBORS       = 200  # vecinos para ajustar árbol local Anchors\n",
        "SURR_TREE_DEPTH     = 3    # profundidad del árbol sustituto global\n",
        "SURR_LOCAL_K        = 50   # nº de vecinos en cada regresión local\n",
        "SURR_COLOR_GLOBAL   = \"#1f77b4\"   # color puntos globales\n",
        "SURR_COLOR_LOCAL    = \"#d62728\"   # color aspas locales\n",
        "EBM_MAX_ITERS       = 500  # número de iteraciones boosting\n",
        "OPTUNA_STUDY_FILE = \"optuna_study.pkl\"  # ruta por defecto (puede cambiarse)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Diagramas de ejemplo para la ayuda dinámica\n",
        "# ----------------------------------------------------------------\n",
        "def _load_optuna_study(path: str):\n",
        "    if optuna is None:\n",
        "        return None\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "\n",
        "def _fig_to_base64(fig):\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\"); plt.close(fig)\n",
        "    buf.seek(0)\n",
        "    return base64.b64encode(buf.read()).decode()\n",
        "\n",
        "def _generate_simple_bar(title, feats, vals, ylabel):\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.bar(feats, vals, color=[\"green\" if v>0 else \"red\" for v in vals]); ax.axhline(0,c=\"k\")\n",
        "    ax.set_title(title); ax.set_ylabel(ylabel); plt.tight_layout();\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "\n",
        "# --- SHAP ------------------------------------------------------\n",
        "_generate_shap_diagram = lambda : _generate_simple_bar(\"Ejemplo Valores SHAP\", [\"X1\",\"X2\",\"X3\",\"X4\"], [0.4,-0.3,0.2,-0.1], \"SHAP value\")\n",
        "# --- LIME ------------------------------------------------------\n",
        "_generate_lime_diagram = lambda : _generate_simple_bar(\"Ejemplo Pesos LIME\", [\"X1\",\"X2\",\"X3\",\"X4\"], [0.7,-0.5,0.25,-0.15], \"Peso LIME\")\n",
        "# --- Integrated Gradients --------------------------------------\n",
        "_generate_ig_diagram   = lambda : _generate_simple_bar(\"Ejemplo Integrated Gradients\", [\"X1\",\"X2\",\"X3\",\"X4\"], [0.3,-0.1,0.4,-0.2], \"IG value\")\n",
        "# --- DeepLIFT/LRP ----------------------------------------------\n",
        "_generate_dl_diagram   = lambda : _generate_simple_bar(\"Ejemplo DeepLIFT / LRP\", [\"X1\",\"X2\",\"X3\",\"X4\"], [0.2,-0.05,0.1,-0.15], \"Relevancia\")\n",
        "# --- Permutation Importance ------------------------------------\n",
        "def _generate_perm_diagram():\n",
        "    feats = [\"X1\", \"X2\", \"X3\", \"X4\"]; means = [0.18, 0.07, 0.12, 0.25]; stds = [0.02, 0.01, 0.015, 0.03]\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.errorbar(range(len(feats)), means, yerr=stds, fmt='o', capsize=5)\n",
        "    ax.set_xticks(range(len(feats))); ax.set_xticklabels(feats)\n",
        "    ax.set_title(\"Ejemplo Permutation Importance\"); ax.set_ylabel(\"Importancia media\"); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "# --- PDP --------------------------------------------------------\n",
        "def _generate_pdp_diagram():\n",
        "    feats = [\"X1\", \"X2\", \"X3\", \"X4\"]; rng = [0.8, 0.3, 0.5, 1.0]\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.scatter(range(4), rng); ax.set_xticks(range(4)); ax.set_xticklabels(feats)\n",
        "    ax.set_ylabel(\"Rango PDP\"); ax.set_title(\"Ejemplo Importancia PDP\"); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "# --- ALE --------------------------------------------------------\n",
        "def _generate_ale_diagram():\n",
        "    feats = [\"X1\", \"X2\", \"X3\", \"X4\"]; rng = [1.1, 0.2, 0.6, 0.9]\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.scatter(range(4), rng); ax.set_xticks(range(4)); ax.set_xticklabels(feats)\n",
        "    ax.set_ylabel(\"Rango ALE\"); ax.set_title(\"Ejemplo Importancia ALE\"); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "# --- ICE --------------------------------------------------------\n",
        "def _generate_ice_diagram():\n",
        "    feats=[\"X1\",\"X2\",\"X3\",\"X4\"]; rng=[0.6,0.15,0.35,0.75]\n",
        "    fig,ax=plt.subplots(figsize=(4,3)); ax.scatter(range(4),rng); ax.set_xticks(range(4)); ax.set_xticklabels(feats)\n",
        "    ax.set_ylabel(\"Rango ICE\"); ax.set_title(\"Ejemplo Importancia ICE\"); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "# --- Counterfactual ----------------------------------------------\n",
        "def _generate_cf_diagram():\n",
        "    feats=[\"X1\",\"X2\",\"X3\",\"X4\"]; costs=[0.2,0.05,0.12,0.3]\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.bar(feats, costs, color=\"steelblue\"); ax.set_title(\"Ejemplo Coste Counterfactual\"); ax.set_ylabel(\"|Δfeature| medio\"); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_base64(fig)}' width='300px'>\"\n",
        "# --- Anchors ----------------------------------------------\n",
        "def _generate_anchor_diagram():\n",
        "    feats=[\"X1\",\"X2\",\"X3\",\"X4\"]; cover=[0.45,0.15,0.05,0.35]\n",
        "    fig,ax=plt.subplots(figsize=(4,3)); ax.bar(feats,cover,color=\"skyblue\"); ax.set_title(\"Cobertura Anchors ej.\"); ax.set_ylabel(\"Cobertura\"); plt.tight_layout();\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_b64(fig)}' width='300px'>\"\n",
        "# --- Surrogate Models (Global/Local) ----------------------------------------------\n",
        "def _generate_surr_diagram():\n",
        "    feats = [\"X1\", \"X2\", \"X3\", \"X4\"]; coef = [0.5, 0.1, 0.3, 0.2]\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.bar(feats, coef, color='goldenrod'); ax.set_title('Coeficientes Surrogate ej.'); ax.set_ylabel('|Coef|'); plt.tight_layout()\n",
        "    b64 = _fig_to_b64(fig)\n",
        "    return f\"<img src='data:image/png;base64,{b64}' width='300px'>\"\n",
        "# --- Explainable Boosting Machine (EBM) ----------------------------------------------\n",
        "def _generate_ebm_diagram():\n",
        "    feats=[\"X1\",\"X2\",\"X3\",\"X4\"]; gains=[0.25,0.1,0.15,0.35]\n",
        "    fig,ax=plt.subplots(figsize=(4,3)); ax.bar(feats,gains,color='seagreen');\n",
        "    ax.set_title('Importancia EBM ej.'); ax.set_ylabel('Ganancia relativa'); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_b64(fig)}' width='300px'>\"\n",
        "# --- Optuna Hyperparameter Importance ----------------------------------------------\n",
        "def _generate_optuna_diagram():\n",
        "    params=[\"lr\",\"depth\",\"n_estim\",\"subsample\"]; imp=[0.4,0.2,0.25,0.15]\n",
        "    fig,ax=plt.subplots(figsize=(4,3)); ax.bar(params,imp,color='mediumpurple'); ax.set_title('Importancia Optuna ej.'); ax.set_ylabel('Contribución'); plt.tight_layout()\n",
        "    return f\"<img src='data:image/png;base64,{_fig_to_b64(fig)}' width='300px'>\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. Motor SHAP\n",
        "# ============================================================\n",
        "explainer = None\n",
        "def _motor_shap(key, model_obj, X, predict_fn):\n",
        "    \"\"\"SHAP completo: valores, tabla y gráfico.\"\"\"\n",
        "    if key in ['xgb','rf']:\n",
        "        explainer = TreeExplainer(model_obj)\n",
        "        print(\"Verbose: Usando TreeExplainer\")\n",
        "    elif key=='svr':\n",
        "        background = shap.sample(X, min(N_SHAP_BACKGROUND, len(X)))\n",
        "        print(f\"Verbose: Background SVR sample shape: {background.shape}\")\n",
        "        explainer = KernelExplainer(predict_fn, background)\n",
        "        print(\"Verbose: Usando KernelExplainer para SVR\")\n",
        "    else:\n",
        "        background = shap.sample(X, min(N_SHAP_BACKGROUND, len(X)))\n",
        "        print(f\"Verbose: Background kernel sample shape: {background.shape}\")\n",
        "        explainer = KernelExplainer(predict_fn, background)\n",
        "        print(\"Verbose: Usando KernelExplainer\")\n",
        "\n",
        "    muestra = shap.sample(X, min(N_SHAP_SAMPLES, len(X)))\n",
        "    print(f\"Verbose: Muestra SHAP shape: {muestra.shape}\")\n",
        "    shap_vals = explainer.shap_values(muestra)\n",
        "    print(f\"Verbose: shap_vals shape: {np.array(shap_vals).shape}\")\n",
        "\n",
        "    # gráfica SHAP\n",
        "    print(\"Verbose: Generando summary_plot...\")\n",
        "    shap.summary_plot(shap_vals, muestra, plot_type=\"dot\", show=False)\n",
        "    fig = plt.gcf()      # Nuevo para la celda 12\n",
        "    plt.show()\n",
        "    # === Explicación de la gráfica SHAP ===\n",
        "    display(HTML(\"\"\"\n",
        "    <h4>Interpretación del gráfico SHAP</h4>\n",
        "    <p>En el gráfico summary_plot:<ul>\n",
        "    <li>Cada punto representa el efecto de una característica en una muestra.</li>\n",
        "    <li>El eje X muestra el valor SHAP (positivo empuja la predicción hacia arriba, negativo hacia abajo).</li>\n",
        "    <li>Los colores indican el valor de la característica (rojo = alto, azul = bajo).</li>\n",
        "    </ul></p>\n",
        "    \"\"\"))\n",
        "\n",
        "    # tabla de valores SHAP\n",
        "    print(\"Verbose: Creando DataFrame de valores SHAP...\")\n",
        "    shap_df = pd.DataFrame(shap_vals, columns=X.columns)\n",
        "    display(HTML(\"<h4>Valores SHAP (primeras 10 muestras)</h4>\"))\n",
        "    display(shap_df.head(10))\n",
        "\n",
        "    # === Explicación de la tabla de valores SHAP ===\n",
        "    display(HTML(\"\"\"\n",
        "    <h4>Interpretación de la tabla de valores SHAP</h4>\n",
        "    <ul>\n",
        "    <li>Cada fila corresponde a una muestra (observación).</li>\n",
        "    <li>Cada columna muestra el valor SHAP de esa característica.</li>\n",
        "    <li>Valores positivos empujan la predicción hacia arriba; negativos, hacia abajo.</li>\n",
        "    </ul>\n",
        "    \"\"\"))\n",
        "\n",
        "    # importancias globales\n",
        "    mean_abs = np.abs(shap_vals).mean(axis=0)\n",
        "    imp_df = pd.DataFrame({'feature':X.columns, 'mean_abs_shap':mean_abs})\n",
        "    imp_df = imp_df.sort_values('mean_abs_shap', ascending=False).reset_index(drop=True)\n",
        "    #imp_df = imp_df.sort_values('mean_abs_shap', ascending=False)\n",
        "    print(\"Verbose: Importancia global calculada\")\n",
        "    display(HTML(\"<h4>Importancia global (valor absoluto medio)</h4>\"))\n",
        "    display(imp_df)\n",
        "    # === Explicación de la importancia global ===\n",
        "    display(HTML(\"\"\"\n",
        "    <h4>Interpretación de la importancia global</h4>\n",
        "    <p>La importancia global ordena características por su valor absoluto medio de SHAP.</p>\n",
        "    <p>Valores más altos indican mayor contribución promedio al modelo.</p>\n",
        "    <p>Ejemplo: Una característica con <i>mean_abs_shap</i>=0.5 contribuye en promedio 0.5 unidades a la predicción.</p>\n",
        "    \"\"\"))\n",
        "    # Estadísticas adicionales  ---- Nuevo para la celda 12\n",
        "    stats = {\n",
        "        'shap_mean_abs_overall': float(mean_abs.mean()),\n",
        "        'shap_std_abs_overall': float(np.abs(shap_vals).std()),\n",
        "        'shap_imp_percentiles': imp_df['mean_abs_shap'].quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "    # Resultado  ---- Nuevo para la celda 12\n",
        "    resultado = {\n",
        "        'imp_df': imp_df.rename(columns={'mean_abs_shap':'value'}),\n",
        "        'df_local': shap_df,\n",
        "        'fig_summary': fig,\n",
        "        'stats': stats\n",
        "    }\n",
        "    return resultado\n",
        "pass\n",
        "\n",
        "# ============================================================\n",
        "# 2. Motor LIME\n",
        "# ============================================================\n",
        "explainer = None\n",
        "def _motor_lime(X, predict_fn):\n",
        "    \"\"\"LIME completo.\"\"\"\n",
        "    print(\"Verbose: Generando explicación con LIME para\", N_LIME_SAMPLES, \"muestras...\")\n",
        "    explainer = LimeTabularExplainer(\n",
        "        training_data=X.values,\n",
        "        feature_names=X.columns.tolist(),\n",
        "        mode='regression'\n",
        "    )\n",
        "    # Muestreamos sólo las primeras N_LIME_SAMPLES instancias\n",
        "    X_sample = X.iloc[:N_LIME_SAMPLES]\n",
        "\n",
        "    # calcular explicaciones para todas las muestras\n",
        "    local_weights = np.zeros((X_sample.shape[0], X_sample.shape[1]))\n",
        "    for i in range(X_sample.shape[0]):\n",
        "        exp = explainer.explain_instance(X_sample.values[i], predict_fn)\n",
        "        # usar índice_feat de exp.as_map()[0]\n",
        "        for feat_idx, weight in exp.as_map()[0]:\n",
        "            local_weights[i, feat_idx] = weight\n",
        "\n",
        "    # tabla primeras 10 muestras\n",
        "    lime_df = pd.DataFrame(local_weights, columns=X.columns)\n",
        "    display(HTML(\"<h4>Valores LIME (primeras 10 muestras)</h4>\"))\n",
        "    display(lime_df.head(10))\n",
        "    # Explicación tabla LIME\n",
        "    display(HTML(\n",
        "        '<p>En la tabla de LIME local, cada fila es una muestra y cada columna el peso asignado por LIME. '\n",
        "        'Valores positivos indican que la característica aumenta la predicción localmente, negativos la disminuyen.</p>'\n",
        "    ))\n",
        "\n",
        "    # importancia global media\n",
        "    mean_w = np.abs(local_weights).mean(axis=0)\n",
        "    #imp_df = pd.DataFrame({'feature':X.columns,'mean_abs_weight':mean_w}).sort_values('mean_abs_weight',ascending=False)\n",
        "    imp_df = pd.DataFrame({'feature': X.columns, 'mean_abs_weight': mean_w})\n",
        "    imp_df = imp_df.sort_values('mean_abs_weight', ascending=False).reset_index(drop=True)\n",
        "    display(HTML(\"<h4>Importancia global LIME</h4>\"))\n",
        "    display(imp_df)\n",
        "    # Explicación importancia global LIME\n",
        "    display(HTML(\n",
        "        '<p>La importancia global de LIME se calcula como el valor absoluto medio de los pesos '\n",
        "        'a través de todas las muestras. Característica con mayor valor afecta más la predicción de manera local.</p>'\n",
        "    ))\n",
        "\n",
        "    # gráfico LIME Value vs Feature Value\n",
        "    plt.figure(figsize=(8,6))\n",
        "    for idx, feat in enumerate(X.columns):\n",
        "        plt.scatter(X_sample[feat], local_weights[:,idx], label=feat, alpha=0.6)\n",
        "    plt.axhline(0,color='black',linewidth=0.8)\n",
        "    plt.xlabel('Valor de la característica')\n",
        "    plt.ylabel('Peso LIME')\n",
        "    plt.title('LIME: Peso vs Valor de la característica')\n",
        "    plt.legend(bbox_to_anchor=(1.05,1),loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()           # Nuevo para la celda 12\n",
        "    plt.show()\n",
        "    display(HTML(\n",
        "        '<p>En el gráfico LIME Value vs Feature Value, cada punto representa una muestra. '\n",
        "        'La posición vertical es el peso LIME, horizontal el valor original de la característica. '\n",
        "        'Permite ver cómo cambia la influencia de la variable según su valor.</p>'\n",
        "    ))\n",
        "\n",
        "    # Estadísticas adicionales  --- Nuevo para la celda 12\n",
        "    stats = {\n",
        "        'lime_imp_percentiles': imp_df['mean_abs_weight'].quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "\n",
        "    resultado = {\n",
        "        'imp_df': imp_df.rename(columns={'mean_abs_weight':'value'}),  # ['feature','value']\n",
        "        'df_local': lime_df,  # DataFrame de pesos locales\n",
        "        'fig_summary': fig,\n",
        "        'stats': stats\n",
        "    }\n",
        "    return resultado\n",
        "pass\n",
        "\n",
        "# ============================================================\n",
        "# 3. Motor KERNEL EXPLAINER\n",
        "# ============================================================\n",
        "#if 'KernelExplainer' in xai.value:\n",
        "def _motor_kernel(X, predict_fn):\n",
        "    \"\"\"KernelExplainer (SHAP).\"\"\"\n",
        "    print(\"Verbose: Calculando valores Kernel SHAP para\", N_KERNEL_SAMPLES, \" muestras...\")\n",
        "\n",
        "    # 1) Tomamos la muestra y el background de X\n",
        "    muestra     = shap.sample(X, min(N_KERNEL_SAMPLES, len(X)))\n",
        "    background  = shap.sample(X, min(N_KERNEL_BACKGROUND, len(X)))\n",
        "\n",
        "    # 2) Creamos el explainer y calculamos SHAP‐values solo para 'muestra'\n",
        "    ke_expl = KernelExplainer(predict_fn, background)\n",
        "    print(f\"[DEBUG] Calculando SHAP values para {len(muestra)} instancias…\")\n",
        "    ke_vals = ke_expl.shap_values(muestra.values)  # matriz (M, p)\n",
        "\n",
        "    # 3) Importancia local y global\n",
        "    ke_df = pd.DataFrame(ke_vals, columns=X.columns)\n",
        "    mean_ke = np.abs(ke_vals).mean(axis=0)\n",
        "    imp_df = pd.DataFrame({'feature': X.columns, 'mean_abs_kernel': mean_ke})\n",
        "    imp_df = imp_df.sort_values('mean_abs_kernel', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # 4) Summary plot — **usar 'muestra'**, no 'X' completo\n",
        "    shap.summary_plot(ke_vals, muestra, show=False)   # <<< aquí estaba el error\n",
        "    fig = plt.gcf()   # recupera la figura actual\n",
        "\n",
        "    # 5) Estadísticas adicionales\n",
        "    stats = {\n",
        "        'kernel_shap_imp_percentiles': imp_df['mean_abs_kernel'].quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "\n",
        "    display(HTML(\"<h4>Valores Kernel SHAP (primeras 10 muestras)</h4>\"))\n",
        "    display(ke_df.head(10))\n",
        "    display(HTML(\n",
        "        '<p>En la tabla anterior, cada fila corresponde a una muestra y cada columna al valor Kernel SHAP de esa característica. '\n",
        "        'Valores positivos indican empuje hacia arriba, negativos empuje hacia abajo.</p>'\n",
        "    ))\n",
        "    display(HTML(\"<h4>Importancia global Kernel SHAP</h4>\"))\n",
        "    display(imp_df)\n",
        "    display(HTML(\n",
        "        '<p>La tabla de importancia global muestra el valor absoluto medio del Kernel SHAP para cada característica. '\n",
        "        'Característica con valor más alto tiene mayor impacto medio en las predicciones.</p>'\n",
        "    ))\n",
        "    plt.show()\n",
        "    display(HTML(\n",
        "        '<p>El gráfico summary para Kernel SHAP funciona igual que SHAP: muestra distribución de valores, mostrando heterogeneidad e influencia de cada variable.</p>'\n",
        "    ))\n",
        "\n",
        "    # 6) Construir resultado\n",
        "    resultado = {\n",
        "        'imp_df': imp_df.rename(columns={'mean_abs_kernel':'value'}),\n",
        "        'df_local': ke_df,\n",
        "        'fig_summary': fig,\n",
        "        'stats': stats\n",
        "    }\n",
        "    return resultado\n",
        "pass\n",
        "\n",
        "# ============================================================\n",
        "# 4. Motor INTEGRATED GRADIENTS\n",
        "# ============================================================\n",
        "def _motor_ig(key, model_obj, X, cols, sx, predict_fn):\n",
        "    \"\"\"\n",
        "    Integrated Gradients:\n",
        "     - Para SVR/NN/XGBoost/RF: usamos SHAP GradientExplainer.\n",
        "     - Para RNN: implementamos IG manual sobre la secuencia.\n",
        "    \"\"\"\n",
        "    import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "    from IPython.display import display, HTML\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # parámetros IG\n",
        "    STEPS   = 50     # pasos de interpolación\n",
        "    TOP_N   = 5      # features globales a mostrar\n",
        "    N_LOCAL = 3      # muestras locales a mostrar\n",
        "\n",
        "    if key != 'rnn':\n",
        "        # —————— Camino original con SHAP GradientExplainer ——————\n",
        "        from shap import GradientExplainer\n",
        "\n",
        "        background = sx.transform(\n",
        "            X.sample(min(200, len(X)), random_state=0)\n",
        "        )\n",
        "        ig_expl = GradientExplainer(model_obj, background)\n",
        "        vals = ig_expl.shap_values(sx.transform(X))\n",
        "\n",
        "        # si devuelve shape (n, p, 1): comprímelo a 2D\n",
        "        if isinstance(vals, np.ndarray) and vals.ndim == 3 and vals.shape[2] == 1:\n",
        "            vals = np.squeeze(vals, -1)\n",
        "\n",
        "        ig_vals = vals  # (n, p)\n",
        "        # DataFrame de valores locales\n",
        "        ig_df = pd.DataFrame(ig_vals, columns=cols)\n",
        "        # importancia global\n",
        "        mean_abs = np.abs(ig_vals).mean(axis=0)\n",
        "        imp_df = (\n",
        "            pd.DataFrame({'feature': cols, 'value': mean_abs})\n",
        "              .sort_values('value', ascending=False)\n",
        "              .reset_index(drop=True)\n",
        "        )\n",
        "        # gráfico global\n",
        "        fig, ax = plt.subplots(figsize=(6,4))\n",
        "        ax.barh(imp_df['feature'].head(TOP_N)[::-1],\n",
        "                imp_df['value'].head(TOP_N)[::-1])\n",
        "        ax.set_title('IG: Top Global')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        display(HTML(\"<h4>IG: Importancia global (top features)</h4>\"))\n",
        "        display(imp_df.head(TOP_N))\n",
        "        display(fig)\n",
        "        display(HTML(\"<h4>IG: Valores locales (primeras muestras)</h4>\"))\n",
        "        display(ig_df.head(N_LOCAL))\n",
        "\n",
        "        return {'imp_df': imp_df, 'df_local': ig_df, 'fig_summary': fig}\n",
        "\n",
        "    else:\n",
        "        # —————— IG manual para RNN ——————\n",
        "        # necesitamos X_seq en globals(): array NumPy 3D (n_samples, timesteps, features)\n",
        "        X_seq = globals().get('X_seq')\n",
        "        if X_seq is None:\n",
        "            raise RuntimeError(\"Para RNN necesitas tener `X_seq` en globals().\")\n",
        "\n",
        "        # submuestra\n",
        "        n_sub = min(20, X_seq.shape[0])\n",
        "        idxs  = np.random.RandomState(0).choice(X_seq.shape[0], n_sub, replace=False)\n",
        "        seqs_np     = X_seq[idxs]                              # NumPy array (n_sub, t, f)\n",
        "        baseline_np = np.zeros_like(seqs_np[0:1])              # (1, t, f)\n",
        "\n",
        "        # convertimos a tensores solo para el tape\n",
        "        seqs_t     = tf.convert_to_tensor(seqs_np,     dtype=tf.float32)\n",
        "        baseline_t = tf.convert_to_tensor(baseline_np, dtype=tf.float32)\n",
        "\n",
        "        # acumulador en NumPy\n",
        "        all_grads = np.zeros_like(seqs_np, dtype=float)       # (n_sub, t, f)\n",
        "\n",
        "        # bucle de interpolación\n",
        "        for alpha in np.linspace(0, 1, STEPS):\n",
        "            interp = baseline_t + alpha * (seqs_t - baseline_t)\n",
        "            with tf.GradientTape() as tape:\n",
        "                tape.watch(interp)\n",
        "                preds = model_obj(interp)                      # (n_sub, 1)\n",
        "            grads_t = tape.gradient(preds, interp)             # tf.Tensor (n_sub, t, f)\n",
        "            grads_np = grads_t.numpy()                         # convertimos a NumPy\n",
        "            all_grads += grads_np\n",
        "\n",
        "        avg_grads = all_grads / STEPS                         # (n_sub, t, f)\n",
        "        ig_attribs = (seqs_np - baseline_np) * avg_grads      # (n_sub, t, f)\n",
        "\n",
        "        # importancia global: promedio absoluto sobre muestras y timesteps\n",
        "        global_imp = np.mean(np.abs(ig_attribs), axis=(0,1))  # (f,)\n",
        "        imp_df = (\n",
        "            pd.DataFrame({'feature': cols, 'value': global_imp})\n",
        "              .sort_values('value', ascending=False)\n",
        "              .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        timesteps = seqs_np.shape[1]\n",
        "\n",
        "        # — aquí vamos a dividir los locales por variable —\n",
        "        display(HTML(\"<h4>IG RNN: Valores locales por variable</h4>\"))\n",
        "        for feat_idx, feat_name in enumerate(cols):\n",
        "            # extraemos la matriz (N_LOCAL, timesteps) para esta variable\n",
        "            local_mat = ig_attribs[:N_LOCAL, :, feat_idx]\n",
        "            local_df   = pd.DataFrame(\n",
        "                local_mat,\n",
        "                index=[f\"muestra {i+1}\" for i in range(local_mat.shape[0])],\n",
        "                columns=[f\"timestep {t}\" for t in range(timesteps)]\n",
        "            )\n",
        "            display(HTML(f\"<h5>Variable: {feat_name}</h5>\"))\n",
        "            display(local_df)\n",
        "\n",
        "        # gráfico global RNN\n",
        "        fig, ax = plt.subplots(figsize=(6,4))\n",
        "        ax.barh(imp_df['feature'].head(TOP_N)[::-1],\n",
        "                imp_df['value'].head(TOP_N)[::-1])\n",
        "        ax.set_title('IG RNN: Top Global')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        display(HTML(\"<h4>IG RNN: Importancia global</h4>\"))\n",
        "        display(imp_df.head(TOP_N))\n",
        "        display(fig)\n",
        "\n",
        "        return {'imp_df': imp_df, 'df_local': local_df, 'fig_summary': fig}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. Motor DeepLIFT / LRP\n",
        "# ============================================================\n",
        "#if 'DeepLIFT / LRP' in xai.value:\n",
        "def _motor_dl(model_obj, key, X, cols, sx, sy):\n",
        "    \"\"\"DeepLIFT / LRP.\"\"\"\n",
        "    print(\"🔍 Calculando DeepLIFT/LRP para todas las muestras...\")\n",
        "    from shap import GradientExplainer\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # 1) Crear la submuestra y escalarla ─────────────────────────\n",
        "    X_sample = X.sample(min(N_DEEP_SAMPLES, len(X)), random_state=0)\n",
        "    X_scaled = sx.transform(X_sample)\n",
        "\n",
        "    # ── 2) Si es RNN, reshape 2D→3D según la forma de entrada del modelo ──\n",
        "    if key == 'rnn':\n",
        "        # model_obj.input_shape suele ser (None, timesteps, features)\n",
        "        _, timesteps, feat_dim = model_obj.input_shape\n",
        "        try:\n",
        "            X_scaled = X_scaled.reshape(-1, timesteps, feat_dim)\n",
        "            print(f\"[DEBUG] DeepLIFT RNN: reshaped a {X_scaled.shape}\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(\n",
        "                f\"No pude reshapear para RNN: esperaba (_, {timesteps}, {feat_dim}), \"\n",
        "                f\"pero sx.transform devolvió {sx.transform(X_sample).shape}. \"\n",
        "                f\"Error: {e}\"\n",
        "            )\n",
        "\n",
        "    # ── 3) Crear explainer y calcular valores ───────────────────────\n",
        "    explainer_dl = GradientExplainer(model_obj, X_scaled)\n",
        "    dl_vals = explainer_dl.shap_values(X_scaled)   # ya 3D → funciona OK\n",
        "\n",
        "    # 1.1) Squeeze si viene con dimensión extra\n",
        "    if isinstance(dl_vals, np.ndarray):\n",
        "        # eliminar ejes de longitud 1\n",
        "        dl_vals = np.squeeze(dl_vals)\n",
        "        print(f\"[DEBUG] tras squeeze: {dl_vals.shape}\")\n",
        "        # si sigue siendo 3D, asumimos (n_samples, time_steps, n_features)\n",
        "        if dl_vals.ndim == 3:\n",
        "            # colapsamos time_steps tomando la media\n",
        "            dl_vals = dl_vals.mean(axis=1)\n",
        "            print(f\"[DEBUG] tras mean over time axis: {dl_vals.shape}\")\n",
        "\n",
        "    # ── 5) DataFrame de relevancias locales ─────────────────────────\n",
        "    dl_df = pd.DataFrame(dl_vals, columns=cols)\n",
        "    display(HTML('<h4>DeepLIFT / LRP: Primeras 10 muestras</h4>'))\n",
        "    display(dl_df.head(10))\n",
        "    display(HTML(\"\"\"\n",
        "    <h4>📝 Cómo leer la tabla de las primeras 10 muestras</h4>\n",
        "    <ul>\n",
        "      <li>Cada fila corresponde a una de las primeras 10 observaciones de tu conjunto de datos.</li>\n",
        "      <li>Cada columna muestra la relevancia asignada por DeepLIFT/LRP a esa característica en esa muestra.</li>\n",
        "      <li>Un valor positivo indica que la característica empujó la predicción <b>hacia arriba</b> respecto al valor de referencia.</li>\n",
        "      <li>Un valor negativo indica que la característica empujó la predicción <b>hacia abajo</b>.</li>\n",
        "      <li>Por ejemplo, si para la muestra #3 el valor en la columna X2 es 0.15, quiere decir que X2 aumentó la salida del modelo en 0.15 unidades.</li>\n",
        "    </ul>\n",
        "    \"\"\"))\n",
        "\n",
        "    # ── 6) Importancia global (media absoluta) ───────────────────────\n",
        "    mean_abs = np.abs(dl_vals).mean(axis=0)\n",
        "    #imp_df = pd.DataFrame({'feature':cols,'mean_abs_dl':mean_abs}).sort_values('mean_abs_dl',ascending=False)\n",
        "    imp_df = pd.DataFrame({'feature': cols, 'mean_abs_dl': mean_abs})\n",
        "    imp_df = imp_df.sort_values('mean_abs_dl', ascending=False).reset_index(drop=True)\n",
        "    display(HTML('<h4>DeepLIFT / LRP: Importancia global</h4>'))\n",
        "    display(imp_df)\n",
        "    display(HTML(\"\"\"\n",
        "    <h4>📝 Cómo leer la tabla de importancia global</h4>\n",
        "    <ul>\n",
        "      <li>La “importancia global” es la media del valor absoluto de las relevancias en <b>todas</b> las muestras.</li>\n",
        "      <li>Se ordena de mayor a menor: las variables que aparecen arriba son las que, en promedio, más afectan la predicción.</li>\n",
        "      <li>Por ejemplo, si la media absoluta de X4 es 0.35, significa que X4 desvió la predicción en ±0.35 de media.</li>\n",
        "    </ul>\n",
        "    \"\"\"))\n",
        "\n",
        "    # ── 7) Gráfico de importancia global ────────────────────────────\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.scatter(range(len(imp_df)), imp_df['mean_abs_dl'], s=80)\n",
        "    plt.xticks(range(len(imp_df)), imp_df['feature'], rotation=45)\n",
        "    plt.title('Importancia DeepLIFT / LRP')\n",
        "    display(HTML(\"\"\"\n",
        "    <h4>📝 Cómo interpretar el gráfico de relevancias</h4>\n",
        "    <ul>\n",
        "      <li>Cada barra representa la relevancia media absoluta de una característica (la misma que en la tabla).</li>\n",
        "      <li>La altura de la barra indica cuán importante es esa variable en el conjunto completo.</li>\n",
        "      <li>Las barras verdes (si tuvieras colores) son relevancias positivas medias y las rojas negativas medias.</li>\n",
        "      <li>Una barra alta significa que, variando esa característica, la predicción del modelo cambia sustancialmente.</li>\n",
        "      <li>Este gráfico te ayuda a ver de un vistazo qué variables “mueven” más la predicción.</li>\n",
        "    </ul>\n",
        "    \"\"\"))\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    # ── 8) Estadísticas adicionales ─────────────────────────────────\n",
        "    stats = {'dlrp_imp_percentiles': imp_df['mean_abs_dl'].quantile([0.25, 0.5, 0.75]).to_dict()}\n",
        "\n",
        "    # ── 9) Devolver en el formato esperado ───────────────────────────\n",
        "    resultado = {\n",
        "        'imp_df': imp_df.rename(columns={'mean_abs_dl':'value'}),\n",
        "        'df_local': dl_df,\n",
        "        'fig_summary': fig,\n",
        "        'stats': stats\n",
        "    }\n",
        "    return resultado\n",
        "pass\n",
        "\n",
        "# ============================================================\n",
        "# 6. Motor Permutation Feature Importance\n",
        "# ============================================================\n",
        "def _motor_perm(model_obj, X, cols, sx, sy):\n",
        "    \"\"\"Permutation Feature Importance.\"\"\"\n",
        "    from sklearn.inspection import permutation_importance\n",
        "    import numpy as np, pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    print(\"🔍 Calculando Permutation Importance para un subconjunto de muestras...\")\n",
        "\n",
        "    # 1) Seleccionamos X_test/Y_test o fallback a X/Y\n",
        "    X_target = globals().get('X_test', X)\n",
        "    y_target = globals().get('Y_test', None)\n",
        "    if y_target is None:\n",
        "        raise ValueError(\"Y_test no definido para Permutation Importance\")\n",
        "\n",
        "    # ──────────────────────────────────────────────────────────\n",
        "    # 1.1) HARD–CODE: número de muestras para la permutación\n",
        "    #N_PERM_SAMPLES = 50\n",
        "    # 1.2) Muestreamos esas instancias\n",
        "    idxs = X_target.sample(\n",
        "        n=min(N_PERM_SAMPLES, len(X_target)),\n",
        "        random_state=0\n",
        "    ).index\n",
        "    X_target = X_target.loc[idxs, cols]\n",
        "    y_target = y_target.loc[idxs]\n",
        "    print(f\"[DEBUG] Usando {len(X_target)} muestras para Permutation Importance\")\n",
        "    # ──────────────────────────────────────────────────────────\n",
        "\n",
        "    # 2) Escalamos nuestro subset\n",
        "    X_scaled = sx.transform(X_target)\n",
        "\n",
        "    # 3) Calculamos la importancia por permutación\n",
        "    if hasattr(model_obj, 'predict'):\n",
        "        perm = permutation_importance(\n",
        "            model_obj,\n",
        "            X_scaled,\n",
        "            y_target.values.ravel(),\n",
        "            n_repeats=10,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    else:\n",
        "        raise TypeError(\"Modelo no soporta permutation_importance\")\n",
        "\n",
        "    # 4) Creamos el DataFrame ordenado\n",
        "    imp_df = pd.DataFrame({\n",
        "        'feature':         cols,\n",
        "        'mean_importance': perm.importances_mean,\n",
        "        'std_importance':  perm.importances_std\n",
        "    }).sort_values('mean_importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # 5) Mostramos la tabla global\n",
        "    display(HTML('<h4>Permutation Importance (subconjunto)</h4>'))\n",
        "    display(imp_df)\n",
        "\n",
        "    # 6) Gráfico con barras de error\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.errorbar(\n",
        "        range(len(imp_df)),\n",
        "        imp_df['mean_importance'],\n",
        "        yerr=imp_df['std_importance'],\n",
        "        fmt='o', capsize=5\n",
        "    )\n",
        "    plt.xticks(range(len(imp_df)), imp_df['feature'], rotation=45)\n",
        "    plt.title('Permutation Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    # 7) Estadísticas adicionales\n",
        "    stats = {\n",
        "        'perm_imp_percentiles': imp_df['mean_importance']\n",
        "                                  .quantile([0.25,0.5,0.75])\n",
        "                                  .to_dict()\n",
        "    }\n",
        "\n",
        "    # 8) Devolvemos el resultado en el formato esperado\n",
        "    return {\n",
        "        'imp_df':      imp_df.rename(columns={'mean_importance':'value'}),\n",
        "        'df_local':    imp_df.head(N_PERM_SAMPLES),  # top-N features\n",
        "        'fig_summary': fig,\n",
        "        'stats':       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "# ============================================================\n",
        "# 7. Motor Partial Dependence Plots (PDP)\n",
        "# ============================================================\n",
        "def _motor_pdp(X: pd.DataFrame, cols: list[str], predict_fn):\n",
        "    import numpy as np, pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor PDP\")\n",
        "\n",
        "    # ──────────────────────────────────────────────────────────\n",
        "    # HARD–CODE: número de muestras para la parte local de PDP\n",
        "    #N_PDP_SAMPLES = 50\n",
        "    print(f\"[DEBUG] Usando {N_PDP_SAMPLES} muestras aleatorias para PDP local\")\n",
        "    # ──────────────────────────────────────────────────────────\n",
        "\n",
        "    # ---------- PDP global -----------------------------------\n",
        "    pdp_ranges = []\n",
        "    pdp_curves = {}\n",
        "    for feat in cols:\n",
        "        print(f\"[DEBUG] Calculando PDP global para feature '{feat}'\")\n",
        "        grid = np.linspace(X[feat].min(), X[feat].max(), GRID_RES)\n",
        "        pd_vals = []\n",
        "        for g in grid:\n",
        "            X_temp = X.copy()\n",
        "            X_temp[feat] = g\n",
        "            preds = predict_fn(X_temp)\n",
        "            pd_vals.append(preds.mean())\n",
        "        pd_vals = np.array(pd_vals)\n",
        "        pdp_curves[feat] = pd_vals\n",
        "        pdp_ranges.append(pd_vals.max() - pd_vals.min())\n",
        "\n",
        "    imp_df = (\n",
        "        pd.DataFrame({\"feature\": cols, \"pdp_range\": pdp_ranges})\n",
        "          .sort_values(\"pdp_range\", ascending=False)\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "    print(\"[DEBUG] DataFrame de importancia global PDP creado\")\n",
        "\n",
        "    display(HTML(\"<h4>PDP: Importancia global (rango de la curva)</h4>\"))\n",
        "    display(imp_df)\n",
        "    display(HTML(\n",
        "        \"<p>La <b>importancia global</b> de cada característica se mide \"\n",
        "        \"como el rango (máx − mín) de su curva PDP.</p>\"\n",
        "    ))\n",
        "\n",
        "    # ---------- PDP local (subconjunto aleatorio) -------------\n",
        "    X_sample = X.sample(n=min(N_PDP_SAMPLES, len(X)), random_state=0)\n",
        "    n_samples = len(X_sample)\n",
        "    print(f\"[DEBUG] Calculando PDP local para {n_samples} muestras\")\n",
        "\n",
        "    base_pred_mean = predict_fn(X).mean()\n",
        "    pdp_local = np.zeros((n_samples, len(cols)))\n",
        "    for i, (_, row) in enumerate(X_sample.iterrows()):\n",
        "        for j, feat in enumerate(cols):\n",
        "            X_temp = X.copy()\n",
        "            X_temp[feat] = row[feat]\n",
        "            pdp_local[i, j] = predict_fn(X_temp).mean() - base_pred_mean\n",
        "\n",
        "    pdp_df = pd.DataFrame(pdp_local, columns=cols)\n",
        "    print(\"[DEBUG] DataFrame de PDP local creado\")\n",
        "\n",
        "    display(HTML(f\"<h4>PDP: Valores para {n_samples} muestras seleccionadas</h4>\"))\n",
        "    display(pdp_df)\n",
        "    display(HTML(\n",
        "        \"<p>Cada celda muestra cuánto varía la predicción promedio cuando \"\n",
        "        \"fijamos la característica al valor de la muestra.</p>\"\n",
        "    ))\n",
        "\n",
        "    # ---------- Gráfico global --------------------------------\n",
        "    print(\"[DEBUG] Generando gráfico de importancia global PDP\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.scatter(range(len(imp_df)), imp_df[\"pdp_range\"], s=80)\n",
        "    plt.xticks(range(len(imp_df)), imp_df[\"feature\"], rotation=45)\n",
        "    plt.ylabel(\"Rango PDP\")\n",
        "    plt.title(\"Importancia Partial Dependence (rango)\")\n",
        "    plt.axhline(0, color=\"black\", linewidth=0.8)\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    # ---------- Estadísticas adicionales ----------------------\n",
        "    stats = {\n",
        "        \"pdp_range_percentiles\": imp_df[\"pdp_range\"]\n",
        "                                   .quantile([0.25, 0.5, 0.75])\n",
        "                                   .to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadísticas adicionales calculadas: {stats}\")\n",
        "\n",
        "    # ---------- Resultado -------------------------------------\n",
        "    return {\n",
        "        \"imp_df\":      imp_df.rename(columns={\"pdp_range\": \"value\"}),\n",
        "        \"df_local\":    pdp_df,\n",
        "        \"fig_summary\": fig,\n",
        "        \"stats\":       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 8. Motor Accumulated Local Effects (ALE)\n",
        "# ============================================================\n",
        "def _motor_ale(X: pd.DataFrame, cols: list[str], predict_fn, n_bins: int = ALE_BINS):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor ALE\")\n",
        "\n",
        "    # ──────────────────────────────────────────────────────────\n",
        "    # HARD–CODE: número de muestras para la parte local de ALE\n",
        "    print(f\"[DEBUG] Usando {N_ALE_SAMPLES} muestras aleatorias para ALE local\")\n",
        "    # ──────────────────────────────────────────────────────────\n",
        "\n",
        "    # ---------- ALE global ------------------------------------\n",
        "    ale_ranges = []\n",
        "    ale_curves = {}\n",
        "    for feat in cols:\n",
        "        print(f\"[DEBUG] Calculando ALE global para '{feat}'\")\n",
        "        # bordes de los bins\n",
        "        edges = np.quantile(X[feat], np.linspace(0, 1, n_bins + 1))\n",
        "        edges[0] -= 1e-9\n",
        "        edges[-1] += 1e-9\n",
        "\n",
        "        curve = np.zeros(n_bins)\n",
        "        cum = 0.0\n",
        "        for b in range(n_bins):\n",
        "            lo, hi = edges[b], edges[b+1]\n",
        "            mask = (X[feat] > lo) & (X[feat] <= hi)\n",
        "            if mask.any():\n",
        "                X_lo = X.loc[mask].copy(); X_hi = X.loc[mask].copy()\n",
        "                X_lo[feat] = lo; X_hi[feat] = hi\n",
        "                delta = predict_fn(X_hi) - predict_fn(X_lo)\n",
        "                cum += delta.mean()\n",
        "            curve[b] = cum\n",
        "        ale_curves[feat] = curve\n",
        "        ale_ranges.append(curve.max() - curve.min())\n",
        "\n",
        "    imp_df = (\n",
        "        pd.DataFrame({\"feature\": cols, \"ale_range\": ale_ranges})\n",
        "          .sort_values(\"ale_range\", ascending=False)\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "    print(\"[DEBUG] DataFrame de importancia global ALE creado\")\n",
        "\n",
        "    display(HTML(\"<h4>ALE: Importancia global (rango)</h4>\"))\n",
        "    display(imp_df)\n",
        "    display(HTML(\n",
        "        \"<p>El <b>rango ALE</b> mide cuánto varía la curva acumulada \"\n",
        "        \"al recorrer toda la distribución de la variable.</p>\"\n",
        "    ))\n",
        "\n",
        "    # ---------- ALE local (subconjunto aleatorio) -------------\n",
        "    X_sample = X.sample(n=min(N_ALE_SAMPLES, len(X)), random_state=0)\n",
        "    n_samples = len(X_sample)\n",
        "    print(f\"[DEBUG] Calculando ALE local para {n_samples} muestras seleccionadas\")\n",
        "\n",
        "    ale_local = np.zeros((n_samples, len(cols)))\n",
        "    for i, (_, row) in enumerate(X_sample.iterrows()):\n",
        "        for j, feat in enumerate(cols):\n",
        "            # identificar bin de la muestra\n",
        "            bin_idx = np.digitize(row[feat], edges[1:-1], right=True)\n",
        "            ale_local[i, j] = ale_curves[feat][bin_idx]\n",
        "\n",
        "    ale_df = pd.DataFrame(ale_local, columns=cols)\n",
        "    print(\"[DEBUG] DataFrame de ALE local creado\")\n",
        "\n",
        "    display(HTML(f\"<h4>ALE: Valores para {n_samples} muestras seleccionadas</h4>\"))\n",
        "    display(ale_df)\n",
        "    display(HTML(\n",
        "        \"<p>Cada celda muestra el valor ALE acumulado en el bin en que cae la muestra.</p>\"\n",
        "    ))\n",
        "\n",
        "    # ---------- Gráfico global --------------------------------\n",
        "    print(\"[DEBUG] Generando gráfico de importancia global ALE\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.scatter(range(len(imp_df)), imp_df[\"ale_range\"], s=80)\n",
        "    plt.xticks(range(len(imp_df)), imp_df[\"feature\"], rotation=45)\n",
        "    plt.ylabel(\"Rango ALE\")\n",
        "    plt.title(\"Importancia Accumulated Local Effects\")\n",
        "    plt.axhline(0, color=\"black\", linewidth=0.8)\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    # ---------- Estadísticas adicionales ----------------------\n",
        "    stats = {\n",
        "        \"ale_range_percentiles\": imp_df[\"ale_range\"]\n",
        "                                   .quantile([0.25, 0.5, 0.75])\n",
        "                                   .to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadísticas adicionales calculadas: {stats}\")\n",
        "\n",
        "    # ---------- Resultado -------------------------------------\n",
        "    return {\n",
        "        \"imp_df\":      imp_df.rename(columns={\"ale_range\": \"value\"}),\n",
        "        \"df_local\":    ale_df,\n",
        "        \"fig_summary\": fig,\n",
        "        \"stats\":       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "# ===================================================================\n",
        "# 9. Motor Individual Conditional Expectation (ICE)\n",
        "# ===================================================================\n",
        "def _motor_ice(\n",
        "    X: pd.DataFrame,\n",
        "    cols: list[str],\n",
        "    predict_fn,\n",
        "    *,\n",
        "    grid_res: int = GRID_RES\n",
        "):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor ICE\")\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    # HARD–CODE: número de muestras para ICE (tanto global como local)\n",
        "    #N_ICE_SAMPLES = 50\n",
        "    print(f\"[DEBUG] Usando N_ICE_SAMPLES={N_ICE_SAMPLES} para ICE global y local\")\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "    # ─── 1) Tomar subconjunto aleatorio para ICE global y local ─────\n",
        "    X_sub = X.sample(n=min(N_ICE_SAMPLES, len(X)), random_state=0)\n",
        "    n_total = len(X_sub)\n",
        "    n_local = min(FIRST_SAMPLES, n_total)\n",
        "    print(f\"[DEBUG] Submuestra ICE creada con {n_total} instancias (local={n_local})\")\n",
        "\n",
        "    # contenedores\n",
        "    ice_local = np.zeros((n_local, len(cols)))\n",
        "    ice_ranges = []\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    # 2) Calcular rangos ICE por característica sobre X_sub\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    for j, feat in enumerate(cols):\n",
        "        print(f\"[DEBUG] Calculando curvas ICE para '{feat}'\")\n",
        "        grid = np.linspace(X[feat].min(), X[feat].max(), grid_res)\n",
        "        ranges_feat = []\n",
        "\n",
        "        for i, (_, row) in enumerate(X_sub.iterrows()):\n",
        "            # construir DataFrame repitiendo la fila\n",
        "            X_grid = pd.DataFrame(\n",
        "                np.repeat(row.values.reshape(1, -1), grid_res, axis=0),\n",
        "                columns=cols\n",
        "            )\n",
        "            X_grid[feat] = grid\n",
        "            preds = predict_fn(X_grid)\n",
        "            r = preds.max() - preds.min()\n",
        "            ranges_feat.append(r)\n",
        "\n",
        "            if i < n_local:\n",
        "                ice_local[i, j] = r\n",
        "\n",
        "        mean_range = float(np.mean(ranges_feat))\n",
        "        ice_ranges.append(mean_range)\n",
        "        print(f\"[DEBUG] Rango medio ICE para '{feat}': {mean_range:.4f}\")\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    # 3) Importancia global (media de rangos)\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    imp_df = (\n",
        "        pd.DataFrame({\n",
        "            \"feature\": cols,\n",
        "            \"ice_range_mean\": ice_ranges\n",
        "        })\n",
        "        .sort_values(\"ice_range_mean\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    print(\"[DEBUG] DataFrame de importancia global ICE creado\")\n",
        "\n",
        "    display(HTML(\"<h4>ICE: Importancia global (media de rangos)</h4>\"))\n",
        "    display(imp_df)\n",
        "    display(HTML(\n",
        "        \"<p>Cada punto muestra la media del rango ICE de la característica. \"\n",
        "        \"Un valor mayor indica mayor sensibilidad de la predicción a esa variable.</p>\"\n",
        "    ))\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    # 4) Tabla local (primeras n_local muestras de X_sub)\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    ice_df = pd.DataFrame(ice_local, columns=cols)\n",
        "    print(\"[DEBUG] DataFrame de ICE local creado\")\n",
        "    display(HTML(f\"<h4>ICE: Rangos para las primeras {n_local} muestras</h4>\"))\n",
        "    display(ice_df)\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    # 5) Gráfico global\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    print(\"[DEBUG] Generando gráfico de importancia ICE\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.scatter(range(len(imp_df)), imp_df[\"ice_range_mean\"], s=80)\n",
        "    plt.xticks(range(len(imp_df)), imp_df[\"feature\"], rotation=45)\n",
        "    plt.ylabel(\"Rango medio ICE\")\n",
        "    plt.title(\"Importancia Individual Conditional Expectation\")\n",
        "    plt.axhline(0, color=\"black\", lw=0.8)\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    display(HTML(\n",
        "        \"<p>La dispersión de estos puntos indica qué variables tienen \"\n",
        "        \"mayor efecto condicional individual sobre la predicción.</p>\"\n",
        "    ))\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    # 6) Estadísticas adicionales\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    stats = {\n",
        "        \"ice_range_percentiles\": imp_df[\"ice_range_mean\"]\n",
        "                                    .quantile([0.25, 0.5, 0.75])\n",
        "                                    .to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadísticas ICE calculadas: {stats}\")\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    # 7) Resultado\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    return {\n",
        "        \"imp_df\":      imp_df.rename(columns={\"ice_range_mean\": \"value\"}),\n",
        "        \"df_local\":    ice_df,\n",
        "        \"fig_summary\": fig,\n",
        "        \"stats\":       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "# ===================================================================\n",
        "# 10. Motor Counterfactual Explanations  – **todas las muestras**\n",
        "# ===================================================================\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def _motor_counterfactual(\n",
        "    X: pd.DataFrame,\n",
        "    predict_fn,\n",
        "    *,\n",
        "    rel_delta: float = CF_TARGET_DELTA,    # +10% por defecto\n",
        "    show_first: int = FIRST_SAMPLES\n",
        "):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor Counterfactual\")\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    # HARD–CODE: número de muestras para buscar contrafactuales\n",
        "    #N_CF_SAMPLES = 50\n",
        "    print(f\"[DEBUG] Usando N_CF_SAMPLES={N_CF_SAMPLES} para submuestreo\")\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "    # 1) Submuestra aleatoria de X para buscar contrafactuales\n",
        "    X_sub = X.sample(n=min(N_CF_SAMPLES, len(X)), random_state=0)\n",
        "    print(f\"[DEBUG] Submuestra creada con {len(X_sub)} instancias\")\n",
        "\n",
        "    # 2) Predicciones de la submuestra\n",
        "    preds_sub = predict_fn(X_sub)\n",
        "    print(\"[DEBUG] Predicciones calculadas para submuestra\")\n",
        "\n",
        "    cf_rows = []\n",
        "    deltas = []\n",
        "\n",
        "    # 3) Para cada muestra en X_sub, buscar el vecino más cercano que supere el delta\n",
        "    for i, idx in enumerate(X_sub.index):\n",
        "        x0 = X_sub.loc[idx].values.reshape(1, -1)\n",
        "        y0 = preds_sub[i]\n",
        "        target = y0 * (1 + rel_delta)\n",
        "        print(f\"[DEBUG] Muestra idx={idx}, y0={y0:.4f}, target>={target:.4f}\")\n",
        "\n",
        "        # candidatos de X (pueden ser toda X o X_sub, aquí usamos X para más posibilidades)\n",
        "        all_preds = predict_fn(X)\n",
        "        mask = all_preds >= target\n",
        "        X_cand = X.loc[mask]\n",
        "        print(f\"[DEBUG] Encontrados {len(X_cand)} candidatos que cumplen delta\")\n",
        "\n",
        "        if X_cand.empty:\n",
        "            cf_rows.append({\"Índice\": idx, **{c: None for c in X.columns}, \"Distancia\": None})\n",
        "            deltas.append(np.nan)\n",
        "            continue\n",
        "\n",
        "        nbrs = NearestNeighbors(n_neighbors=1, metric=\"euclidean\")\n",
        "        nbrs.fit(X_cand.values)\n",
        "        dist, ind = nbrs.kneighbors(x0, return_distance=True)\n",
        "        cf = X_cand.iloc[ind[0][0]]\n",
        "        delta_feat = np.abs(cf.values - x0.ravel())\n",
        "        mean_delta = float(delta_feat.mean())\n",
        "\n",
        "        cf_rows.append({\n",
        "            \"Índice\":        idx,\n",
        "            **{c: float(v) for c, v in zip(X.columns, cf.values)},\n",
        "            \"Distancia\":    float(dist[0][0])\n",
        "        })\n",
        "        deltas.append(mean_delta)\n",
        "        print(f\"[DEBUG] Contrafactual idx={idx}: distancia={dist[0][0]:.4f}, Δmedio_feat={mean_delta:.4f}\")\n",
        "\n",
        "    # 4) Construir DataFrame local\n",
        "    df_local = pd.DataFrame(cf_rows).set_index(\"Índice\")\n",
        "    n_show = min(show_first, len(df_local))\n",
        "    display(HTML(f\"<h4>Contrafactuales (primeras {n_show} muestras)</h4>\"))\n",
        "    display(df_local.head(n_show))\n",
        "\n",
        "    # 5) Importancia global: |Δfeature| medio\n",
        "    imp_series = pd.Series(0.0, index=X.columns)\n",
        "    valid = df_local[\"Distancia\"].notna()\n",
        "    for idx in df_local[valid].index:\n",
        "        diff = np.abs(df_local.loc[idx, X.columns].values - X.loc[idx].values)\n",
        "        imp_series += diff\n",
        "    imp_series /= valid.sum()\n",
        "    imp_series = imp_series.sort_values(ascending=False)\n",
        "    imp_df = imp_series.rename(\"value\").reset_index().rename(columns={\"index\":\"feature\"})\n",
        "\n",
        "    display(HTML(\"<h4>Importancia global por contrafactuales</h4>\"))\n",
        "    display(imp_df)\n",
        "\n",
        "    # 6) Gráfico de barras\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.bar(imp_series.index, imp_series.values)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"|Δfeature| medio\")\n",
        "    plt.title(\"Importancia global – Counterfactual\")\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    # 7) Estadísticas adicionales\n",
        "    stats = {\n",
        "        \"cf_imp_percentiles\": imp_series.quantile([0.25, 0.5, 0.75]).to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadísticas contrafactuales: {stats}\")\n",
        "\n",
        "    # 8) Resultado\n",
        "    return {\n",
        "        \"imp_df\":      imp_df,\n",
        "        \"df_local\":    df_local,\n",
        "        \"fig_summary\": fig,\n",
        "        \"stats\":       stats\n",
        "    }\n",
        "\n",
        "# ===================================================================\n",
        "# 11. Motor Anchors – Reglas locales con DecisionTree como proxy\n",
        "# ===================================================================\n",
        "def _motor_anchors(X: pd.DataFrame, cols: list[str], predict_fn):\n",
        "    \"\"\"\n",
        "    Genera reglas-Anchor (árbol surrogate poco profundo) para una SUBmuestra de X.\n",
        "    Importancia global = frecuencia (relativa) de aparición de cada variable\n",
        "    en todas las reglas obtenidas.\n",
        "    \"\"\"\n",
        "    import re, random\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor Anchors\")\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    # Hardcode: número de muestras a analizar\n",
        "    #N_ANCHOR_SAMPLES = 50\n",
        "    print(f\"[DEBUG] Usando N_ANCHOR_SAMPLES={N_ANCHOR_SAMPLES}\")\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "    # 1) Submuestra de X para acelerar el proceso\n",
        "    X_sub = X.sample(n=min(N_ANCHOR_SAMPLES, len(X)), random_state=0)\n",
        "    n_sub = len(X_sub)\n",
        "    print(f\"[DEBUG] Submuestra creada con {n_sub} instancias\")\n",
        "\n",
        "    # Preparar contadores\n",
        "    reglas, coberturas, precisiones = [], [], []\n",
        "    var_freq = {c: 0 for c in cols}\n",
        "\n",
        "    # 2) Iterar solo sobre la submuestra\n",
        "    for i, idx in enumerate(X_sub.index):\n",
        "        x0 = X_sub.loc[idx]\n",
        "        y0 = predict_fn(x0.values.reshape(1, -1))[0]\n",
        "        print(f\"[DEBUG] Muestra {i+1}/{n_sub} (idx={idx}), pred={y0:.4f}\")\n",
        "\n",
        "        # 3) Vecinos aleatorios de la submuestra\n",
        "        neigh_idx = random.sample(\n",
        "            list(X_sub.index),\n",
        "            k=min(ANC_NEIGHBORS, n_sub)\n",
        "        )\n",
        "        X_nei = X_sub.loc[neigh_idx]\n",
        "        y_nei = predict_fn(X_nei.values)\n",
        "        print(f\"[DEBUG]  Vecinos seleccionados: {len(X_nei)}\")\n",
        "\n",
        "        # 4) Binarizar según exceder o no la predicción base\n",
        "        y_bin = (y_nei >= y0).astype(int)\n",
        "\n",
        "        # 5) Ajustar árbol surrogate\n",
        "        tree = DecisionTreeRegressor(\n",
        "            max_depth=3, min_samples_leaf=5, random_state=0\n",
        "        )\n",
        "        tree.fit(X_nei, y_bin)\n",
        "\n",
        "        # 6) Extraer las condiciones del path de x0\n",
        "        node_indicator = tree.decision_path(x0.values.reshape(1,-1))\n",
        "        features      = tree.tree_.feature\n",
        "        thresholds    = tree.tree_.threshold\n",
        "\n",
        "        anchor_conds = []\n",
        "        for node_id in node_indicator.indices:\n",
        "            if features[node_id] == -2:\n",
        "                continue  # hoja\n",
        "            f_idx = features[node_id]\n",
        "            feat = cols[f_idx]\n",
        "            thr  = thresholds[node_id]\n",
        "            op   = \"≤\" if x0[feat] <= thr else \">\"\n",
        "            cond = f\"{feat} {op} {thr:.3g}\"\n",
        "            anchor_conds.append(cond)\n",
        "            var_freq[feat] += 1\n",
        "\n",
        "        # 7) Calcular cobertura y precisión\n",
        "        cover = np.ones(len(X_nei), dtype=bool)\n",
        "        for cond in anchor_conds:\n",
        "            m = re.match(r'\\s*(.+?)\\s*(≤|>=|>|<)\\s*([-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?)', cond)\n",
        "            if not m:\n",
        "                continue\n",
        "            f, op_sym, val = m.groups()\n",
        "            val = float(val)\n",
        "            if op_sym in (\"≤\", \"<=\"):\n",
        "                cover &= (X_nei[f] <= val)\n",
        "            elif op_sym in (\">\", \"≥\"):\n",
        "                cover &= (X_nei[f] >= val)\n",
        "        coverage  = cover.mean()\n",
        "        precision = (y_nei[cover] >= y0).mean() if coverage > 0 else 0.0\n",
        "\n",
        "        reglas.append(\" ∧ \".join(anchor_conds))\n",
        "        coberturas.append(coverage)\n",
        "        precisiones.append(precision)\n",
        "        print(f\"[DEBUG]  Regla: {' ∧ '.join(anchor_conds)}\")\n",
        "        print(f\"[DEBUG]  Cobertura={coverage:.2%}, Precisión={precision:.2%}\")\n",
        "\n",
        "    # ╭─────────────────── Tablas y gráficas ────────────────────────╮\n",
        "    df_local = pd.DataFrame({\n",
        "        \"Regla\":     reglas,\n",
        "        \"Cobertura\": coberturas,\n",
        "        \"Precisión\": precisiones\n",
        "    })\n",
        "\n",
        "    display(HTML(\"<h4>⚓ Reglas Anchor (submuestra)</h4>\"))\n",
        "    display(df_local.head(10).style.format({\"Cobertura\":\"{:.2%}\",\"Precisión\":\"{:.2%}\"}))\n",
        "\n",
        "    # Importancia global\n",
        "    imp = (pd.Series(var_freq) / n_sub).sort_values(ascending=False)\n",
        "    imp_df = imp.reset_index().rename(columns={\"index\":\"feature\", 0:\"value\"})\n",
        "\n",
        "    display(HTML(\"<h4>⚓ Importancia global (frecuencia en submuestra)</h4>\"))\n",
        "    display(imp_df)\n",
        "\n",
        "    # Gráfico de frecuencias\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.bar(imp.index, imp.values)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"Frecuencia\")\n",
        "    plt.title(\"Anchors – Importancia global\")\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "\n",
        "    # Estadísticas adicionales\n",
        "    stats = {\n",
        "        \"anchors_freq_percentiles\": imp.quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadísticas Anchors: {stats}\")\n",
        "\n",
        "    return {\n",
        "        \"imp_df\":      imp_df,\n",
        "        \"df_local\":    df_local,\n",
        "        \"fig_summary\": fig,\n",
        "        \"stats\":       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "# ===================================================================\n",
        "# 12. Motor Surrogate Models (Global/Local)\n",
        "# ===================================================================\n",
        "def _motor_surrogate(X: pd.DataFrame, cols: list[str], predict_fn):\n",
        "    \"\"\"\n",
        "    Calcula árbol sustituto global + regresiones locales usando solo una\n",
        "    submuestra de X para acelerar el cálculo.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor Surrogate\")\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    # Hardcode: número de muestras para el surrogate global y local\n",
        "    #N_SURR_SAMPLES = 100\n",
        "    print(f\"[DEBUG] Usando N_SURR_SAMPLES={N_SURR_SAMPLES}\")\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "    # 1) Submuestra para surrogate global\n",
        "    n_sub = min(N_SURR_SAMPLES, len(X))\n",
        "    X_sub = X.sample(n=n_sub, random_state=0)\n",
        "    print(f\"[DEBUG] Submuestra global creada con {n_sub} instancias\")\n",
        "\n",
        "    # ---------- Global surrogate ----------\n",
        "    y_sub = predict_fn(X_sub)\n",
        "    print(\"[DEBUG] Entrenando árbol surrogate global\")\n",
        "    tree = DecisionTreeRegressor(max_depth=SURR_TREE_DEPTH, random_state=0)\n",
        "    tree.fit(X_sub, y_sub)\n",
        "    imp_global = pd.Series(tree.feature_importances_, index=cols).sort_values(ascending=False)\n",
        "    print(\"[DEBUG] Importancias globales calculadas\")\n",
        "\n",
        "    display(HTML('<h4>🌍 Importancia Global (Surrogate árbol)</h4>'))\n",
        "    display(imp_global.to_frame('Importancia').T.style.format('{:.3f}'))\n",
        "    display(HTML(\n",
        "        '<p>Cada celda muestra la contribución de la variable a la reducción de MSE en el árbol '\n",
        "        'sustituto entrenado sobre la submuestra.</p>'\n",
        "    ))\n",
        "\n",
        "    # ---------- Local surrogates ----------\n",
        "    n_local = min(FIRST_SAMPLES, n_sub)\n",
        "    print(f\"[DEBUG] Generando {n_local} surrogates locales sobre la submuestra\")\n",
        "    local_abs_coef = np.zeros((n_local, len(cols)))\n",
        "\n",
        "    # usamos X_sub para vecinos locales\n",
        "    for i, idx in enumerate(X_sub.index[:n_local]):\n",
        "        x0 = X_sub.loc[idx]\n",
        "        # distancias sobre la submuestra\n",
        "        dists = np.linalg.norm(X_sub.values - x0.values, axis=1)\n",
        "        neigh_idx = dists.argsort()[1:SURR_LOCAL_K+1]\n",
        "        X_nei = X_sub.iloc[neigh_idx]\n",
        "        y_nei = predict_fn(X_nei)\n",
        "        lin = LinearRegression().fit(X_nei, y_nei)\n",
        "        local_abs_coef[i] = np.abs(lin.coef_)\n",
        "        print(f\"[DEBUG] Local surrogate {i+1}: coef abs media calculada\")\n",
        "\n",
        "    imp_local = pd.Series(local_abs_coef.mean(axis=0), index=cols).sort_values(ascending=False)\n",
        "    imp_df_local = imp_local.reset_index().rename(columns={'index':'feature', 0:'value'})\n",
        "    print(\"[DEBUG] Importancias locales medias calculadas\")\n",
        "\n",
        "    display(HTML('<h4>🏠 Importancia Local media (|coef|)</h4>'))\n",
        "    display(imp_df_local)\n",
        "    display(HTML(\n",
        "        '<p>Promedio del valor absoluto de los coeficientes de las regresiones locales '\n",
        "        'sobre la submuestra.</p>'\n",
        "    ))\n",
        "\n",
        "    # ---------- Gráfico comparativo ----------\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.scatter(range(len(imp_global)), imp_global.values,\n",
        "                label='Global (árbol)', color=SURR_COLOR_GLOBAL)\n",
        "    plt.scatter(range(len(imp_local)),  imp_local.values,\n",
        "                label='Local (media coef)', marker='x', color=SURR_COLOR_LOCAL)\n",
        "    plt.xticks(range(len(cols)), imp_global.index, rotation=45, ha='right')\n",
        "    plt.ylabel('Importancia / |Coef|')\n",
        "    plt.title('Comparativa Surrogate Global vs Local')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "    print(\"[DEBUG] Gráfico comparativo generado\")\n",
        "\n",
        "    # Estadísticas adicionales\n",
        "    diff = (imp_global - imp_local).abs()\n",
        "    stats = {\n",
        "        'surrogate_global_percentiles': imp_global.quantile([0.25,0.5,0.75]).to_dict(),\n",
        "        'surrogate_local_percentiles':  imp_local.quantile([0.25,0.5,0.75]).to_dict(),\n",
        "        'surrogate_diff_percentiles':   diff.quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadísticas Surrogate: {stats}\")\n",
        "\n",
        "    # Resultado\n",
        "    return {\n",
        "        'imp_df':      imp_df_local,\n",
        "        'df_local':    pd.DataFrame(local_abs_coef, columns=cols),\n",
        "        'fig_summary': fig,\n",
        "        'stats':       stats\n",
        "    }\n",
        "pass\n",
        "\n",
        "# ===================================================================\n",
        "# 13. Motor Explainable Boosting Machine (EBM)\n",
        "# ===================================================================\n",
        "def _motor_ebm(\n",
        "    X: pd.DataFrame,\n",
        "    cols: list[str],\n",
        "    predict_fn,\n",
        "    *,\n",
        "    max_rounds: int = EBM_MAX_ITERS,\n",
        "    n_local: int = FIRST_SAMPLES,\n",
        "):\n",
        "    \"\"\"\n",
        "    • Entrena EBM sobre una submuestra de X para acelerar el cálculo.\n",
        "    • Muestra importancia global y contribuciones locales (n_local filas).\n",
        "    \"\"\"\n",
        "    from interpret.glassbox import ExplainableBoostingRegressor\n",
        "    import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # 1) Comprobación de disponibilidad de interpret\n",
        "    if ExplainableBoostingRegressor is None:\n",
        "        display(HTML(\n",
        "            \"<p style='color:red'>⚠️  Falta el paquete <code>interpret</code>. \"\n",
        "            \"Instálalo con <code>pip install interpret</code>.</p>\"\n",
        "        ))\n",
        "        return\n",
        "\n",
        "    print(\"[DEBUG] Iniciando motor EBM\")\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "    # Hardcode: tamaño de la submuestra para EBM\n",
        "    #N_EBM_SAMPLES = 200\n",
        "    print(f\"[DEBUG] Usando N_EBM_SAMPLES={N_EBM_SAMPLES}\")\n",
        "    # ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "    # 2) Crear submuestra para entrenamiento global\n",
        "    n_sub = min(N_EBM_SAMPLES, len(X))\n",
        "    X_sub = X.sample(n=n_sub, random_state=0)\n",
        "    print(f\"[DEBUG] Submuestra para EBM creada: {n_sub} instancias\")\n",
        "\n",
        "    # 3) Entrenar EBM sobre la submuestra\n",
        "    y_sub = predict_fn(X_sub)\n",
        "    print(\"[DEBUG] Entrenando Explainable Boosting Machine (EBM) sobre submuestra\")\n",
        "    ebm = ExplainableBoostingRegressor(\n",
        "        max_rounds=max_rounds,\n",
        "        random_state=0,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    ebm.fit(X_sub, y_sub)\n",
        "\n",
        "    # 4) Importancia global\n",
        "    g_info = ebm.explain_global().data()\n",
        "    gains = (pd.Series(g_info[\"scores\"], index=g_info[\"names\"])\n",
        "               .reindex(cols, fill_value=0.0)\n",
        "               .sort_values(ascending=False))\n",
        "    imp_df = gains.reset_index().rename(columns={'index':'feature', 0:'value'})\n",
        "    print(\"[DEBUG] Importancia global EBM calculada\")\n",
        "\n",
        "    display(HTML(\"<h4>🌐 Importancia global EBM</h4>\"))\n",
        "    display(gains.to_frame(\"Ganancia\").style.format(\"{:.3f}\"))\n",
        "    display(imp_df)\n",
        "\n",
        "    # 5) Contribuciones locales (hasta n_local o tamaño de submuestra)\n",
        "    n_loc = min(n_local, n_sub)\n",
        "    print(f\"[DEBUG] Calculando contribuciones locales para las primeras {n_loc} instancias de la submuestra\")\n",
        "    contrib = None\n",
        "\n",
        "    try:  # interpret ≥ 0.26\n",
        "        _, contrib = ebm.predict(X_sub.iloc[:n_loc], output_contrib=True)\n",
        "    except TypeError:\n",
        "        try:  # interpret 0.24 – 0.25\n",
        "            _, contrib = ebm.predict_and_contrib(X_sub.iloc[:n_loc])\n",
        "        except (AttributeError, TypeError):\n",
        "            contrib = None\n",
        "\n",
        "    # fallback manual si API oficial no disponible\n",
        "    if contrib is None:\n",
        "        print(\"[DEBUG] API local no disponible, calculando manualmente…\")\n",
        "        term_scores = ebm.term_scores_\n",
        "        term_feats  = getattr(ebm, \"feature_groups_\", getattr(ebm, \"term_features_\", None))\n",
        "        if term_feats is None:\n",
        "            term_feats = [[i] for i in range(len(cols))]\n",
        "        bins_attr = \"bin_edges_\" if hasattr(ebm, \"bin_edges_\") else \"bins_\"\n",
        "        bin_struct = getattr(ebm, bins_attr)\n",
        "\n",
        "        contrib = np.zeros((n_loc, len(cols)))\n",
        "        for t, feats in enumerate(term_feats):\n",
        "            if len(feats) != 1:\n",
        "                continue\n",
        "            feat_idx = feats[0]\n",
        "            # obtener cortes\n",
        "            cuts = np.asarray(bin_struct[t].get(\"cuts\", []) if isinstance(bin_struct[t], dict) else bin_struct[t])\n",
        "            if cuts.size == 0:\n",
        "                continue\n",
        "            scores = term_scores[t]\n",
        "            vals = X_sub.iloc[:n_loc, feat_idx].values\n",
        "            bin_idx = np.searchsorted(cuts, vals, side=\"right\")\n",
        "            contrib[:, feat_idx] = scores[bin_idx]\n",
        "\n",
        "    contrib_df = pd.DataFrame(contrib, columns=cols, index=X_sub.index[:n_loc])\n",
        "    print(\"[DEBUG] Contribuciones locales calculadas\")\n",
        "\n",
        "    display(HTML(f\"<h4>🔎 Contribuciones locales (primeras {n_loc})</h4>\"))\n",
        "    display(contrib_df)\n",
        "\n",
        "    # 6) Gráfico de importancia local media\n",
        "    mean_abs = contrib_df.abs().mean().reindex(gains.index)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.scatter(range(len(mean_abs)), mean_abs.values, color=\"seagreen\")\n",
        "    plt.xticks(range(len(mean_abs)), mean_abs.index, rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"|Contribución| media\")\n",
        "    plt.title(\"Importancia EBM (media |contribución|)\")\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    plt.show()\n",
        "    print(\"[DEBUG] Gráfico local EBM generado\")\n",
        "\n",
        "    display(HTML(\n",
        "        \"<p>Cada punto muestra la media del valor absoluto de la contribución de la variable \"\n",
        "        f\"en las primeras {n_loc} muestras de la submuestra.</p>\"\n",
        "    ))\n",
        "\n",
        "    # 7) Estadísticas adicionales\n",
        "    stats = {\n",
        "        'ebm_global_percentiles': gains.quantile([0.25,0.5,0.75]).to_dict(),\n",
        "        'ebm_local_percentiles':  mean_abs.quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "    print(f\"[DEBUG] Estadísticas EBM: {stats}\")\n",
        "\n",
        "    # 8) Resultado\n",
        "    return {\n",
        "        'imp_df':      imp_df,\n",
        "        'df_local':    contrib_df,\n",
        "        'fig_summary': fig,\n",
        "        'stats':       stats\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 14 ▸ Motor Optuna – Importancia de hiperparámetros\n",
        "# ============================================================\n",
        "def _motor_optuna(\n",
        "    X: pd.DataFrame,            # (no se usa pero se mantiene la firma)\n",
        "    cols: list[str],            # (no se usa)\n",
        "    _,                          # predict_fn (sin uso)\n",
        "    default_file: str = \"optuna_study.pkl\",\n",
        "    min_trials: int = 10        # nº mínimo aconsejable de trials\n",
        "):\n",
        "    \"\"\"\n",
        "    Muestra la importancia de hiperparámetros de un `optuna.study.Study`.\n",
        "    1) Busca un objeto `study` en memoria.\n",
        "    2) Si no lo encuentra, intenta cargar `default_file`.\n",
        "    3) Si tampoco está, escanea el directorio en busca de `*.pkl`\n",
        "       con un objeto Study dentro.\n",
        "    4) Si sigue sin hallarlo, muestra un mensaje muy explícito con\n",
        "       los pasos para generarlo.\n",
        "    \"\"\"\n",
        "\n",
        "    # ▸ 0. Comprobar dependencias\n",
        "    if optuna is None:\n",
        "        display(HTML(\n",
        "            \"<p style='color:red'>⚠️ <code>optuna</code> no está instalado. \"\n",
        "            \"Ejecuta <code>pip install optuna</code> e inténtalo de nuevo.</p>\"\n",
        "        ))\n",
        "        return\n",
        "    try:\n",
        "        from optuna.importance import get_param_importances\n",
        "    except Exception as e:\n",
        "        display(HTML(\n",
        "            f\"<p style='color:red'>⚠️ No se pudo importar \"\n",
        "            f\"<code>optuna.importance</code>: {e}</p>\"\n",
        "        ))\n",
        "        return\n",
        "\n",
        "    # ▸ 1. Intentar encontrar el Study en memoria -------------\n",
        "    study = globals().get(\"study\")\n",
        "    source = \"memoria\"\n",
        "\n",
        "    # ▸ 2. Intentar cargar el fichero por defecto -------------\n",
        "    if study is None and os.path.exists(default_file):\n",
        "        try:\n",
        "            with open(default_file, \"rb\") as f:\n",
        "                study = pickle.load(f)\n",
        "            source = f'archivo “{default_file}”'\n",
        "        except Exception as e:\n",
        "            display(HTML(\n",
        "                f\"<p style='color:red'>⚠️ No se pudo cargar \"\n",
        "                f\"<code>{default_file}</code>: {e}</p>\"\n",
        "            ))\n",
        "\n",
        "    # ▸ 3. Buscar cualquier *.pkl si aún no hay Study ---------\n",
        "    if study is None:\n",
        "        for pkl in glob.glob(\"*.pkl\"):\n",
        "            try:\n",
        "                with open(pkl, \"rb\") as f:\n",
        "                    obj = pickle.load(f)\n",
        "                if isinstance(obj, optuna.study.Study):\n",
        "                    study = obj\n",
        "                    source = f'archivo “{pkl}”'\n",
        "                    break\n",
        "            except Exception:\n",
        "                continue   # ignorar .pkl que no sean Study\n",
        "\n",
        "    # ▸ 4. Si sigue sin Study → guía al usuario ---------------\n",
        "    if study is None:\n",
        "        display(HTML(\n",
        "            f\"\"\"\n",
        "            <div style='border:1px solid #e57373;padding:10px;border-radius:6px'>\n",
        "              <h4 style='margin-top:0;color:#c62828'>⚠️  No se encontró ningún estudio Optuna</h4>\n",
        "              <p>\n",
        "                Para utilizar este panel primero necesitas <b>crear y guardar</b> un estudio\n",
        "                Optuna. Tienes dos formas:\n",
        "              </p>\n",
        "              <ol>\n",
        "                <li>Ejecuta una optimización desde el <i>Bloque&nbsp;3 → Optimización</i>\n",
        "                    (elige motor <code>Optuna</code>). Al finalizar se guardará\n",
        "                    automáticamente <code>{default_file}</code>.</li>\n",
        "                <li>Si ya tienes un objeto <code>study</code>, guárdalo manualmente:<br>\n",
        "                   <code>import pickle<br>\n",
        "                   with open(\"{default_file}\", \"wb\") as f:<br>&nbsp;&nbsp;&nbsp;&nbsp;pickle.dump(study, f)</code>\n",
        "                </li>\n",
        "              </ol>\n",
        "              <p>Vuelve a lanzar la explicación cuando dispongas del archivo.</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        ))\n",
        "        return\n",
        "\n",
        "    # ▸ 5. El Study se ha encontrado --------------------------\n",
        "    n_trials = len(study.trials)\n",
        "    display(HTML(\n",
        "        f\"<p>✅ <i>Study</i> localizado desde <b>{source}</b> \"\n",
        "        f\"con <b>{n_trials}</b> trials.</p>\"\n",
        "    ))\n",
        "    if n_trials < min_trials:\n",
        "        display(HTML(\n",
        "            f\"<p style='color:#c57f17'>⚠️ El estudio contiene menos de \"\n",
        "            f\"{min_trials} trials; la estimación de importancia puede ser inestable.</p>\"\n",
        "        ))\n",
        "\n",
        "    # ▸ 6. Calcular importancia de hiperparámetros ------------\n",
        "    display(HTML(\"<h4>📊 Importancia global de hiperparámetros</h4>\"))\n",
        "    try:\n",
        "        importances = get_param_importances(study)\n",
        "    except Exception as e:\n",
        "        display(HTML(\n",
        "            f\"<p style='color:red'>⚠️ Falló el cálculo de importancia: {e}</p>\"\n",
        "        ))\n",
        "        return\n",
        "\n",
        "    imp_series = pd.Series(importances).sort_values(ascending=False)\n",
        "    imp_df = imp_series.reset_index().rename(columns={'index':'feature', 0:'value'})\n",
        "    display(imp_df)\n",
        "    display(imp_series.to_frame('Contribución').style.format('{:.2%}'))\n",
        "    display(HTML(\n",
        "        '<p><b>¿Cómo leerla?</b> El porcentaje indica cuánto explica cada '\n",
        "        'hiperparámetro la variación de la métrica objetivo. '\n",
        "        '<br>• <b>> 25 %</b> ⇒ parámetro crítico.<br>'\n",
        "        '• <b>< 5 %</b> ⇒ parámetro con poca influencia.</p>'\n",
        "    ))\n",
        "\n",
        "    # ▸ 7. Mostrar Top-10 trials ------------------------------\n",
        "    best_trials = sorted(study.trials, key=lambda t: t.value)[:10]\n",
        "    df_trials   = pd.DataFrame(\n",
        "        [{\"value\": t.value, **t.params} for t in best_trials]\n",
        "    )\n",
        "    display(HTML(\"<h4>🏅 Top 10 trials</h4>\")); display(df_trials)\n",
        "\n",
        "    # ▸ 8. Gráfico de barras ---------------------------------\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar(imp_series.index, imp_series.values, color='mediumpurple')\n",
        "    plt.ylabel('Contribución (%)'); plt.title('Importancia hiperparámetros Optuna')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()               # Nuevo para la celda 12\n",
        "    plt.show()\n",
        "\n",
        "    display(HTML(\n",
        "        '<p>La altura de cada barra muestra la influencia relativa. '\n",
        "        'Úsalo para priorizar en futuras búsquedas.</p>'\n",
        "    ))\n",
        "\n",
        "    stats = {\n",
        "        'optuna_imp_percentiles': imp_series.quantile([0.25,0.5,0.75]).to_dict()\n",
        "    }\n",
        "    resultado = {\n",
        "        'imp_df': imp_df,\n",
        "        'df_local': df_trials,  # top trials\n",
        "        'fig_summary': fig,\n",
        "        'stats': stats\n",
        "    }\n",
        "    return resultado\n",
        "\n",
        "\n",
        "pass\n",
        "\n",
        "out_rec = widgets.Output()\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Función para mostrar UI xIA\n",
        "# ----------------------------------------------------------------\n",
        "def mostrar_xai():\n",
        "    import pandas as pd\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "\n",
        "    out_xai = widgets.Output()\n",
        "\n",
        "    # 1) Output para pintar la tabla de recomendaciones\n",
        "    out_rec = widgets.Output()\n",
        "\n",
        "    # Aquí tu lista de dicts con todas las recomendaciones\n",
        "    recomendaciones = [\n",
        "        {\"Modelo\":\"SVR\",          \"🚀 Motor\":\"SHAP\",                     \"📈 Rendimiento\":\"Alto\",    \"⏳ Rapidez\":\"Lento\",   \"💡 Justificación\":\"Ideal para explicaciones globales en SVR.\"},\n",
        "        {\"Modelo\":\"SVR\",          \"🚀 Motor\":\"LIME\",                     \"📈 Rendimiento\":\"Medio\",   \"⏳ Rapidez\":\"Medio\",   \"💡 Justificación\":\"Explicaciones locales muy intuitivas.\"},\n",
        "        {\"Modelo\":\"NN\",           \"🚀 Motor\":\"Integrated Gradients\",     \"📈 Rendimiento\":\"Alto\",    \"⏳ Rapidez\":\"Medio\",   \"💡 Justificación\":\"Óptimo para redes diferenciables.\"},\n",
        "        {\"Modelo\":\"NN\",           \"🚀 Motor\":\"DeepLIFT / LRP\",           \"📈 Rendimiento\":\"Alto\",    \"⏳ Rapidez\":\"Rápido\",  \"💡 Justificación\":\"Muy eficiente en Atribuciones acumuladas.\"},\n",
        "        {\"Modelo\":\"NN\",           \"🚀 Motor\":\"SHAP\",                     \"📈 Rendimiento\":\"Alto\",    \"⏳ Rapidez\":\"Lento\",   \"💡 Justificación\":\"Model-agnóstico, capta complejidades no lineales.\"},\n",
        "        {\"Modelo\":\"XGBoost\",      \"🚀 Motor\":\"SHAP (TreeExplainer)\",     \"📈 Rendimiento\":\"Alto\",    \"⏳ Rapidez\":\"Rápido\",  \"💡 Justificación\":\"Explainer nativo y ultra-rápido para árboles.\"},\n",
        "        {\"Modelo\":\"XGBoost\",      \"🚀 Motor\":\"Partial Dependence Plot\",  \"📈 Rendimiento\":\"Medio\",   \"⏳ Rapidez\":\"Medio\",   \"💡 Justificación\":\"Visualiza efectos marginales.\"},\n",
        "        {\"Modelo\":\"RandomForest\", \"🚀 Motor\":\"SHAP (TreeExplainer)\",     \"📈 Rendimiento\":\"Alto\",    \"⏳ Rapidez\":\"Rápido\",  \"💡 Justificación\":\"Exacto global para bosques.\"},\n",
        "        {\"Modelo\":\"RandomForest\", \"🚀 Motor\":\"Permutation Importance\",   \"📈 Rendimiento\":\"Medio\",   \"⏳ Rapidez\":\"Medio\",   \"💡 Justificación\":\"Fácil de comparar importancias.\"},\n",
        "        {\"Modelo\":\"RNN\",          \"🚀 Motor\":\"Integrated Gradients\",     \"📈 Rendimiento\":\"Alto\",    \"⏳ Rapidez\":\"Medio\",   \"💡 Justificación\":\"Captura efectos temporales.\"},\n",
        "        {\"Modelo\":\"RNN\",          \"🚀 Motor\":\"DeepLIFT / LRP\",           \"📈 Rendimiento\":\"Alto\",    \"⏳ Rapidez\":\"Rápido\",  \"💡 Justificación\":\"Eficiente en series temporales.\"},\n",
        "    ]\n",
        "\n",
        "    # 2) Dropdown para seleccionar modelo\n",
        "    model_dd = widgets.Dropdown(\n",
        "        options=['SVR','NN','XGBoost','RandomForest','RNN'],\n",
        "        description='Modelo:',\n",
        "        layout=widgets.Layout(width='50%')\n",
        "    )\n",
        "\n",
        "    # 3) Callback que construye y muestra la tabla\n",
        "    def _on_model_change(change):\n",
        "        if change['type']=='change' and change['name']=='value':\n",
        "            key = change['new']\n",
        "            # Filtrar sólo las filas que correspondan al modelo seleccionado\n",
        "            df = pd.DataFrame([r for r in recomendaciones if r['Modelo']==key])\n",
        "            # Estilizar\n",
        "            styled = (df.style\n",
        "                .set_table_styles([\n",
        "                    {'selector':'th', 'props':[('background-color','#2E3B4E'),\n",
        "                                              ('color','white'),\n",
        "                                              ('font-size','14px'),\n",
        "                                              ('padding','3px'),\n",
        "                                              ('text-align','center')]},\n",
        "                    {'selector':'td', 'props':[('font-size','12px'),\n",
        "                                              ('padding','3px'),\n",
        "                                              ('text-align','left')]},\n",
        "                ])\n",
        "                .hide(axis='index')\n",
        "                .set_caption(f\"🔍 Recomendaciones xIA para {key}\")\n",
        "            )\n",
        "            # Pintar\n",
        "            with out_rec:\n",
        "                out_rec.clear_output()\n",
        "                display(styled)\n",
        "\n",
        "    # 4) Registrar el observer **después** de haber definido la función**\n",
        "    model_dd.observe(_on_model_change, names='value')\n",
        "\n",
        "    # 1) Tipo\n",
        "    tipo = widgets.Dropdown(options=[('Entrenado','entrenado'),('Optimizado','optimo')], description='Tipo:', layout=widgets.Layout(width='400px'), style={'description_width': '100px'})\n",
        "    # 2) Modelo\n",
        "    modelo = widgets.Dropdown(options=[], description='Modelo:', layout=widgets.Layout(width='400px'), style={'description_width': '100px'})\n",
        "    # 3) Método selección\n",
        "    metodo_sel = widgets.Dropdown(options=SELECT_METHODS, description='Método X:', layout=widgets.Layout(width='400px'), style={'description_width': '100px'})\n",
        "    # xIA methods\n",
        "    xai = widgets.SelectMultiple(options=XAI_METHODS, description='xIA:', layout=widgets.Layout(width='400px', height='150px'), style={'description_width': '100px'})\n",
        "\n",
        "    # Widget de ayuda para describir el método xIA seleccionado\n",
        "    help_html = widgets.HTML(value='Seleccione un método xIA para ver su descripción aquí.')  # <-- Línea nueva\n",
        "\n",
        "    # Callback para actualizar lista de modelos según tipo\n",
        "    def _on_tipo(change):\n",
        "        modelo.options = TRAINED_MODELS if change['new']=='entrenado' else OPTIMIZED_MODELS\n",
        "    tipo.observe(_on_tipo, names='value')\n",
        "    modelo.options = TRAINED_MODELS  # por defecto\n",
        "\n",
        "    # Callback para mostrar ayuda dinámica según selección de xIA\n",
        "    def _on_xai(change):\n",
        "        selected = change['new']\n",
        "        if not selected:\n",
        "            help_html.value = 'Seleccione un método xIA para ver su descripción aquí.'  # <-- Línea nueva\n",
        "        else:\n",
        "            parts = []\n",
        "            for m in selected:\n",
        "                desc = XAI_HELP.get(m, '')\n",
        "                if m == 'SHAP': parts.append(desc + _generate_shap_diagram())\n",
        "                elif m == 'LIME': parts.append(desc + _generate_lime_diagram())\n",
        "                elif m == 'KernelExplainer': parts.append(desc + _generate_shap_diagram())\n",
        "                elif m == 'Integrated Gradients': parts.append(desc + _generate_ig_diagram())\n",
        "                elif m == 'DeepLIFT / LRP': parts.append(desc + _generate_dl_diagram())\n",
        "                elif m == 'Permutation Feature Importance': parts.append(desc + _generate_perm_diagram())\n",
        "                elif m == 'Partial Dependence Plots (PDP)': parts.append(desc + _generate_pdp_diagram())\n",
        "                elif m == 'Accumulated Local Effects (ALE)': parts.append(desc+_generate_ale_diagram())\n",
        "                elif m == \"Individual Conditional Expectation (ICE) Plots\": parts.append(desc+_generate_ice_diagram())\n",
        "                elif m == \"Counterfactual Explanations\": parts.append(desc+_generate_cf_diagram())\n",
        "                elif m == \"Anchors\": parts.append(desc + _generate_anchor_diagram())\n",
        "                elif m == \"Surrogate Models (Global/Local)\": parts.append(desc + _generate_surr_diagram())\n",
        "                elif m == \"Explainable Boosting Machine (EBM)\": parts.append(desc + _generate_ebm_diagram())\n",
        "                elif m == \"Optuna Hyperparameter Importance\": parts.append(desc + _generate_optuna_diagram())\n",
        "                else:\n",
        "                    parts.append(desc)\n",
        "            help_html.value = ''.join(parts)\n",
        "    xai.observe(_on_xai, names='value')\n",
        "\n",
        "    # Botón de explicación\n",
        "    btn = widgets.Button(description='🔍 Explicar', button_style='info')\n",
        "\n",
        "    def _on_explain(b):\n",
        "        import ipywidgets as widgets\n",
        "        from IPython.display import display, clear_output\n",
        "        global xai_results\n",
        "        with out_xai:\n",
        "            clear_output()\n",
        "\n",
        "            t = tipo.value\n",
        "            m_disp = modelo.value\n",
        "            print(f\"-> Tipo: {t}\")\n",
        "            print(f\"-> Modelo: {m_disp}\")\n",
        "            print(f\"-> Método selección: {metodo_sel.value}\")\n",
        "            print(f\"-> xIA seleccionadas: {', '.join(xai.value)}\")\n",
        "\n",
        "            raw = modelo.value  # p.ej. \"RNN\"\n",
        "            key = None\n",
        "            model_display = None\n",
        "\n",
        "            # 1) Match exacto\n",
        "            if raw in MODEL_KEYS:\n",
        "                key = MODEL_KEYS[raw]\n",
        "                model_display = raw\n",
        "            else:\n",
        "                # 2) Fallback sufijo (muy raro que empiece a usarse)\n",
        "                for display_name, short_key in MODEL_KEYS.items():\n",
        "                    if raw.endswith(display_name):\n",
        "                        key = short_key\n",
        "                        model_display = display_name\n",
        "                        break\n",
        "\n",
        "            if key is None:\n",
        "                raise ValueError(f\"No puedo mapear «{raw}» a clave interna de modelo.\")\n",
        "\n",
        "            # Asegurarnos de tener el dict inicializado\n",
        "            if 'xai_results' not in globals():\n",
        "                xai_results = {}\n",
        "            if model_display not in xai_results:\n",
        "                xai_results[model_display] = {}\n",
        "            if tipo.value == \"entrenado\":\n",
        "                patrones = [\n",
        "                    # ── EN EL DIRECTORIO ACTUAL ────────────────────────────\n",
        "                    f\"modelo_{key}_{metodo_sel.value.lower()}.pkl\",\n",
        "                    f\"{key}_{metodo_sel.value.lower()}.pkl\",\n",
        "                    f\"modelo_{key}_{metodo_sel.value.lower()}.h5\",\n",
        "                    f\"{key}_{metodo_sel.value.lower()}.h5\",\n",
        "                    # ── EN SUBCARPETAS (recursivo) ─────────────────────────\n",
        "                    f\"**/modelo_{key}_{metodo_sel.value.lower()}.pkl\",\n",
        "                    f\"**/{key}_{metodo_sel.value.lower()}.pkl\",\n",
        "                    f\"**/modelo_{key}_{metodo_sel.value.lower()}.h5\",\n",
        "                    f\"**/{key}_{metodo_sel.value.lower()}.h5\",\n",
        "                ]\n",
        "            else:  # 'optimo'\n",
        "                patrones = [\n",
        "                    # búsqueda de modelo serializado\n",
        "                    f\"modelos_opt/modelo_{key}_{metodo_sel.value.lower()}*_opt*.pkl\",\n",
        "                    # en caso de que guardes metadata por separado\n",
        "                    f\"modelos_opt/meta_{key}_{metodo_sel.value.lower()}*_opt*.pkl\",\n",
        "                    # si también guardas .h5\n",
        "                    f\"modelos_opt/modelo_{key}_{metodo_sel.value.lower()}*_opt*.h5\",\n",
        "                ]\n",
        "            print(\"[DEBUG] patrones =\", patrones)\n",
        "            # 2) Búsqueda recursiva\n",
        "            files = []\n",
        "            for pat in patrones:\n",
        "                files.extend(glob.glob(pat, recursive=True))\n",
        "\n",
        "            # 3) Depuración opcional (puedes comentar la siguiente línea cuando compruebes que funciona)\n",
        "            pprint.pprint(files)\n",
        "\n",
        "            # 4) Seleccionar la primera coincidencia\n",
        "            if not files:\n",
        "                print(f\"⚠️  No se encontró ningún archivo que coincida con los patrones:\\n    {patrones}\")\n",
        "                return\n",
        "            ruta_modelo = files[0]\n",
        "            print(f\"✔️  Modelo encontrado en: {ruta_modelo}\")\n",
        "\n",
        "            print(f\"Verbose: Archivos encontrados: {files}\")\n",
        "            if not ruta_modelo:\n",
        "                print(f\"⚠️ No se encontró ningún archivo para patrón(s): {patrones}\")\n",
        "                return\n",
        "            print(f\"Verbose: Cargando ruta_modelo: {ruta_modelo}\")\n",
        "\n",
        "            # ─── cargar modelo y escaladores ────────────────────────────────\n",
        "            if ruta_modelo.endswith('.pkl'):\n",
        "                with open(ruta_modelo, 'rb') as f:\n",
        "                    datos = pickle.load(f)\n",
        "                # si es un fichero de metadata (no contiene 'model'), buscamos el .h5 asociado\n",
        "                if 'model' not in datos:\n",
        "                    print(\"Verbose: fichero metadata detectado, buscado .h5 correspondiente\")\n",
        "                    # Ejemplo: modelos_opt/meta_nn_pearson_opt_randomsearch.pkl\n",
        "                    modelo_h5 = ruta_modelo.replace('/meta_', '/modelo_').replace('meta_', 'modelo_').replace('.pkl', '.h5')\n",
        "                    model_obj = load_model(modelo_h5, compile=False)\n",
        "                    # ─── Elige una de las dos:\n",
        "                    # 1) Descompilar:\n",
        "                    model_obj = load_model(modelo_h5, compile=False)\n",
        "                else:\n",
        "                    model_obj = datos['model']\n",
        "                # en ambos casos sacamos scalers y cols de este .pkl\n",
        "                sx   = datos.get('sx', datos.get('scaler_X'))\n",
        "                sy   = datos.get('sy', datos.get('scaler_Y'))\n",
        "                cols = datos['cols']\n",
        "                print(f\"Verbose: Escaladores y cols cargados de {ruta_modelo}: {list(datos.keys())}\")\n",
        "            elif ruta_modelo.endswith('.h5'):\n",
        "                model_obj = load_model(ruta_modelo)\n",
        "                # cargamos sólo el scaler de X si existe en tu metadata\n",
        "                meta_file = ruta_modelo.replace('modelo_','escaladores_').replace('.h5','.pkl')\n",
        "                with open(meta_file,'rb') as f:\n",
        "                    datos_meta = pickle.load(f)\n",
        "                sx = datos_meta.get('sx', datos_meta.get('scaler_X'))\n",
        "                sy = None\n",
        "                cols = datos_meta['cols']\n",
        "                print(\"Verbose: NN .h5 cargado. Sólo sx:\", sx, \"sy será None\")\n",
        "            else:\n",
        "                raise ValueError(f\"Formato de fichero no soportado: {ruta_modelo}\")\n",
        "\n",
        "            # ─── preparar X antes de llamar a los motores ───────────────────\n",
        "            X = X_data[cols].copy()\n",
        "            print(f\"Verbose: Columnas seleccionadas: {cols}\")\n",
        "            print(f\"Verbose: X_data shape: {X.shape}\")\n",
        "\n",
        "            # ─── construir función de predicción ───────────────────────────\n",
        "            if sy is not None:\n",
        "                predict_fn = lambda X_in: sy.inverse_transform(\n",
        "                    model_obj.predict(sx.transform(X_in)).reshape(-1,1)\n",
        "                ).ravel()\n",
        "            else:\n",
        "                if sx is not None:\n",
        "                    predict_fn = lambda X_in: model_obj.predict(sx.transform(X_in)).ravel()\n",
        "                else:\n",
        "                    predict_fn = lambda X_in: model_obj.predict(X_in).ravel()\n",
        "\n",
        "            # 0) Preparar lista de motores a ejecutar:\n",
        "            seleccion = list(xai.value)\n",
        "            if \"Todos\" in seleccion:\n",
        "                seleccion = ALL_MOTORES\n",
        "\n",
        "            # ----------------------------------------------------------\n",
        "            # ⚙️  Motores de explicación (13 Motores:\n",
        "            # SHAP, LIME, KernelExplainer, Integrated Gradients, DeepLIFT/LRP, Permutation Importance, Partial Dependence Plots (PDP),\n",
        "            # Accumulated Local Effects (ALE), Individual Conditional Expectation (ICE) Plots, Counterfactual Explanations, Anchors, Surrogate Models (Global/Local),\n",
        "            # Explainable Boosting Machine (EBM) y Optuna Hyperparameter Importance.\n",
        "            # ----------------------------------------------------------\n",
        "            # ------------- 1. SHAP --------------------------------\n",
        "            if \"SHAP\" in seleccion:\n",
        "                res = _motor_shap(key, model_obj, X, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['SHAP'] = res\n",
        "\n",
        "            # ------------- 2. LIME --------------------------------\n",
        "            if \"LIME\" in seleccion:\n",
        "                res = _motor_lime(X, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['LIME'] = res\n",
        "\n",
        "            # ------------- 3. KernelExplainer ---------------------\n",
        "            if \"KernelExplainer\" in seleccion:\n",
        "                res = _motor_kernel(X, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['KernelExplainer'] = res\n",
        "\n",
        "            # ------------- 4. Integrated Gradients ----------------\n",
        "            if \"Integrated Gradients\" in seleccion:\n",
        "                res = _motor_ig(key, model_obj, X, cols, sx, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Integrated Gradients'] = res\n",
        "\n",
        "            # ------------- 5. DeepLIFT / LRP ----------------------\n",
        "            if \"DeepLIFT / LRP\" in seleccion:\n",
        "                res = _motor_dl(model_obj, key, X, cols, sx, sy)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['DeepLIFT / LRP'] = res\n",
        "\n",
        "            # ------------- 6. Permutation Importance --------------\n",
        "            if \"Permutation Feature Importance\" in seleccion:\n",
        "                res = _motor_perm(model_obj, X, cols, sx, sy)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Permutation Feature Importance'] = res\n",
        "\n",
        "            # ------------- 7. Partial Dependence Plots (PDP) ------\n",
        "            if \"Partial Dependence Plots (PDP)\" in seleccion:\n",
        "                res = _motor_pdp(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Partial Dependence Plots (PDP)'] = res\n",
        "\n",
        "            # ------------- 8. Accumulated Local Effects (ALE) ------\n",
        "            if \"Accumulated Local Effects (ALE)\" in seleccion:\n",
        "                res = _motor_ale(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Accumulated Local Effects (ALE)'] = res\n",
        "\n",
        "            # ------------- 9. Individual Conditional Expectation (ICE) Plots ------\n",
        "            if \"Individual Conditional Expectation (ICE) Plots\" in seleccion:\n",
        "                res = _motor_ice(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Individual Conditional Expectation (ICE) Plots'] = res\n",
        "\n",
        "            # ------------- 10. Counterfactual Explanations ------\n",
        "            if \"Counterfactual Explanations\" in seleccion:\n",
        "                res = _motor_counterfactual(X, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Counterfactual Explanations'] = res\n",
        "\n",
        "            # ------------- 11. Anchors ------\n",
        "            if \"Anchors\" in seleccion:\n",
        "                res = _motor_anchors(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Anchors'] = res\n",
        "\n",
        "            # ------------- 12. Surrogate Models (Global/Local) ------\n",
        "            if \"Surrogate Models (Global/Local)\" in seleccion:\n",
        "                res = _motor_surrogate(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Surrogate Models (Global/Local)'] = res\n",
        "\n",
        "            # ------------- 13. Explainable Boosting Machine (EBM) ------\n",
        "            if \"Explainable Boosting Machine (EBM)\" in seleccion:\n",
        "                res = _motor_ebm(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Explainable Boosting Machine (EBM)'] = res\n",
        "\n",
        "            # ------------- 14. Optuna Hyperparameter Importance ------\n",
        "            if \"Optuna Hyperparameter Importance\" in seleccion:\n",
        "                res = _motor_optuna(X, cols, predict_fn)\n",
        "                if res is not None:\n",
        "                    xai_results[model_display]['Optuna Hyperparameter Importance'] = res\n",
        "\n",
        "    # enlazar callbacks\n",
        "    btn.on_click(_on_explain)\n",
        "\n",
        "    display(\n",
        "        widgets.VBox([\n",
        "            tipo, modelo, metodo_sel, xai, help_html, model_dd, out_rec, btn, out_xai\n",
        "        ])\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 12. GENERAR INFORME – TABLAS FORMATEADAS + EXPLICACIÓN IA\n",
        "# ===============================================================\n",
        "from IPython.display import clear_output, Markdown, display\n",
        "from IPython.display import HTML\n",
        "import pandas as pd\n",
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import ipywidgets as widgets\n",
        "import scipy.stats\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "from matplotlib.figure import Figure\n",
        "import numpy as np\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "import re\n",
        "\n",
        "def sanitize_name(s):\n",
        "    \"\"\"\n",
        "    Unifica la sanitización de cualquier string de columna:\n",
        "    - Reemplaza todo carácter no alfanumérico o guión bajo por '_'\n",
        "    - Colapsa múltiples '_' consecutivos\n",
        "    - Elimina '_' al inicio y final\n",
        "    \"\"\"\n",
        "    t = re.sub(r\"[^\\w]\", \"_\", str(s))\n",
        "    t = re.sub(r\"_+\", \"_\", t)\n",
        "    return t.strip(\"_\")\n",
        "\n",
        "\n",
        "# — 0) Inicializamos el cliente OpenAI (asegúrate de exportar OPENAI_API_KEY)\n",
        "_api_key = os.getenv(\"OPENAI_API_KEY\") or \"\"\n",
        "_client = OpenAI(api_key=_api_key, timeout=30)\n",
        "\n",
        "# Número máximo de tokens y Temperatura para explicación de la IA\n",
        "MAX_EXPLANATION_TOKENS = 800     # Valor recomendado por la IA - Para equilibrar coste, nivel de detalle y consistencia en los análisis IA de PMS IA recomienda con GPT-4 (límite 8 192), usar 1 500 tokens - para reducir coste usamos 1.000\n",
        "#MAX_EXPLANATION_TOKENS = 10      # Limitado para que se reduzca consumo de API y se reduzca tiempo de ejecución en pruebas\n",
        "TEMPERATURE_VAL = 0.2             # Valor recomendado por la IA - Se fija en 0.2 para dar un poco de “espontaneidad” a la IA de modo que se generen explicaciones más naturales o ejemplos. No subir de este nivel.\n",
        "\n",
        "class ReportBuilder:\n",
        "    \"\"\"Orquesta la creación del informe a partir de globals().\"\"\"\n",
        "    def __init__(self, global_ns):\n",
        "        self.g = global_ns\n",
        "        #self.sections = []\n",
        "\n",
        "        # ——— AÑADIDO: sanitizar sólo una vez que X_train y X_test existen ———\n",
        "        #import re\n",
        "        #def clean_name(s):\n",
        "        #    t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))\n",
        "        #    t = re.sub(r'_+', '_', t)\n",
        "        #    return t.strip('_')\n",
        "\n",
        "        # 1) Limpieza de X_train y X_test\n",
        "        #for df_name in ('X_train','X_test'):\n",
        "        #    if df_name in self.g:\n",
        "        #        df = self.g[df_name]\n",
        "        #        df.columns = [clean_name(c) for c in df.columns]\n",
        "\n",
        "        # 2) Limpieza de RESUMEN_METODOS\n",
        "        #if 'RESUMEN_METODOS' in self.g:\n",
        "        #    for m, lst in self.g['RESUMEN_METODOS'].items():\n",
        "        #        if isinstance(lst, list):\n",
        "        #            self.g['RESUMEN_METODOS'][m] = [clean_name(c) for c in lst]\n",
        "        #        elif isinstance(lst, pd.DataFrame) and not lst.empty:\n",
        "        #            # si fuese DataFrame, saneamos su columna de Variable\n",
        "        #            col = 'Variable' if 'Variable' in lst.columns else lst.columns[0]\n",
        "        #            self.g['RESUMEN_METODOS'][m][col] = lst[col].astype(str).map(clean_name)\n",
        "\n",
        "        # ─── **AÑADIDO** SANITIZACIÓN DE LOS payload[\"cols\"] EN OPT_MODELS ───\n",
        "        #if 'OPT_MODELS' in self.g:\n",
        "        #    for key, payload in self.g['OPT_MODELS'].items():\n",
        "        #        if isinstance(payload, dict) and 'cols' in payload:\n",
        "        #            payload['cols'] = [clean_name(c) for c in payload['cols']]\n",
        "        # ——— FIN AÑADIDO ———\n",
        "\n",
        "        self.sections = []\n",
        "\n",
        "        # NUEVO: atributos para el mejor modelo (se rellenarán en sección de selección integral)\n",
        "        self.best_model_info = {}\n",
        "\n",
        "        self.figures = {}   # Guardaremos aquí las figuras matplotlib\n",
        "        print(\"[DEBUG] 1.1. ReportBuilder.__init__\")\n",
        "\n",
        "    def build_sections(self):\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "        import matplotlib.pyplot as plt\n",
        "        from scipy import stats\n",
        "        print(\"[DEBUG] 1.2. ReportBuilder.build_sections start\")\n",
        "\n",
        "        # --- Inicio de Bloque para normalizar el payload de OPT_MODELS ---\n",
        "        def _normalize_payload(raw):\n",
        "            \"\"\"\n",
        "            Toma un payload arbitrario de OPT_MODELS y devuelve siempre\n",
        "            un dict con las mismas claves: model, sx, sy, cols, best_params, score, metric.\n",
        "            \"\"\"\n",
        "            norm = {}\n",
        "            # 1) Modelo\n",
        "            if 'model' in raw:\n",
        "                norm['model'] = raw['model']\n",
        "            elif raw.get('model_path'):\n",
        "                try:\n",
        "                    import joblib, tensorflow as tf\n",
        "                    if raw['model_path'].endswith(('.h5','.tf')):\n",
        "                        from tensorflow.keras.models import load_model\n",
        "                        norm['model'] = load_model(raw['model_path'], compile=False)\n",
        "                    else:\n",
        "                        norm['model'] = joblib.load(raw['model_path'])\n",
        "                except:\n",
        "                    norm['model'] = None\n",
        "            else:\n",
        "                norm['model'] = None\n",
        "\n",
        "            # 2) Métadatos: sx, sy, cols, score, metric, best_params\n",
        "            for k in ('sx','sy','cols','score','metric','best_params'):\n",
        "                if k in raw:\n",
        "                    norm[k] = raw[k]\n",
        "                elif raw.get('meta_path'):\n",
        "                    if '_meta' not in raw:\n",
        "                        import pickle\n",
        "                        raw['_meta'] = pickle.load(open(raw['meta_path'],'rb'))\n",
        "                    norm[k] = raw['_meta'].get(k) if k!='best_params' else raw['_meta'].get('best_params', {})\n",
        "                else:\n",
        "                    norm[k] = None\n",
        "\n",
        "            # ─── AÑADIDO: asegurar que los cols del payload están saneados ───\n",
        "            #if norm.get('cols') is not None:\n",
        "            #    import re\n",
        "            #    def clean_name(s):\n",
        "            #        t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))\n",
        "            #        t = re.sub(r'_+', '_', t).strip('_')\n",
        "            #        return t\n",
        "            #    norm['cols'] = [clean_name(c) for c in norm['cols']]\n",
        "            # ────────────────────────────────────────────────────────────────\n",
        "\n",
        "            return norm\n",
        "\n",
        "        # ahora, al empezar cada bloque de optimización, en lugar de:\n",
        "        #    payload = OPT_MODELS[('svr', sel_method, engine)]\n",
        "        #    model  = payload['model']; sx = payload['sx']; ...\n",
        "        # harías:\n",
        "        #    raw = OPT_MODELS[('svr', sel_method, engine)]\n",
        "        #    p   = _normalize_payload(raw)\n",
        "        #    model, sx, sy, cols, score, metric, best_params = (\n",
        "        #        p['model'], p['sx'], p['sy'], p['cols'], p['score'], p['metric'], p['best_params']\n",
        "        #    )\n",
        "        # --- Fin de Bloque para normalizar el payload de OPT_MODELS ---\n",
        "\n",
        "        self.sections.clear()\n",
        "        #all_metrics = []\n",
        "        # =============================================================\n",
        "        # 1. Carga de datos\n",
        "        # =============================================================\n",
        "        try:\n",
        "            if all(k in self.g for k in (\"X_data\", \"Y_data\", \"FECHAS\")):\n",
        "                X_data = self.g[\"X_data\"]\n",
        "                Y_data = self.g[\"Y_data\"]\n",
        "                FECHAS = self.g[\"FECHAS\"]\n",
        "                n_rows = len(X_data)\n",
        "                n_cols = X_data.shape[1] if hasattr(X_data, \"shape\") else None\n",
        "                cols = list(X_data.columns) if hasattr(X_data, \"columns\") else None\n",
        "                n_nulls = X_data.isna().sum().sum() if hasattr(X_data, \"isna\") else None\n",
        "\n",
        "                # Tomar solo primeras 5 filas en DataFrame de muestra:\n",
        "                df_sample = pd.concat([\n",
        "                    X_data.head(5).reset_index(drop=True),\n",
        "                    (Y_data.head(5).reset_index(drop=True)\n",
        "                        .rename(columns=lambda c: f\"Y_{c}\" if isinstance(Y_data, pd.DataFrame) else \"Y\")\n",
        "                        if isinstance(Y_data, pd.DataFrame) else Y_data.head(5).rename(\"Y\")\n",
        "                    ),\n",
        "                    FECHAS.head(5).reset_index(drop=True).rename(\"Fecha\")\n",
        "                ], axis=1)\n",
        "                self.sections.append((\"### Muestra de Datos Cargados (primeras 5 filas)\", df_sample))\n",
        "                print(\"[DEBUG] 1.3. Sección muestra de datos cargados añadida\")\n",
        "\n",
        "                prompt_carga = (\n",
        "                    \"Por favor, explica de forma profesional y detallada cómo se ha realizado \"\n",
        "                    \"la carga de datos, basándote en la siguiente información de contexto:\\n\\n\"\n",
        "                    f\"- Número total de filas originales: {n_rows}\\n\"\n",
        "                    f\"- Número de variables (columnas) cargadas: {n_cols}\\n\"\n",
        "                    f\"- Nombres de columnas (muestra): {cols[:5] if cols else 'N/A'}{'...' if cols and len(cols)>5 else ''}\\n\"\n",
        "                    f\"- Total de valores nulos en X_data: {n_nulls}\\n\\n\"\n",
        "                    \"Explica por qué es importante revisar estos aspectos antes de entrenar modelos, \"\n",
        "                    \"qué implicaciones tienen (por ejemplo, manejo de nulos, tipos de datos, fechas, etc.), \"\n",
        "                    \"y menciona buenas prácticas en esta fase de carga/preprocesado inicial.\"\n",
        "                )\n",
        "                print(\"[DEBUG]1.4. Iniciando llamada a OpenAI para explicación de carga...\")\n",
        "                t0 = time.time()\n",
        "                stream_resp = _client.chat.completions.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"Eres un experto en ingeniería de datos y preprocesado para ML.\"},\n",
        "                        {\"role\": \"user\",   \"content\": prompt_carga}\n",
        "                    ],\n",
        "                    temperature=TEMPERATURE_VAL,\n",
        "                    max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    stream=True\n",
        "                )\n",
        "                ai_answer_carga = \"\"\n",
        "                for chunk in stream_resp:\n",
        "                    choice = chunk.choices[0]\n",
        "                    if hasattr(choice, \"delta\") and hasattr(choice.delta, \"content\"):\n",
        "                        delta = choice.delta.content\n",
        "                        if delta:\n",
        "                            ai_answer_carga += delta\n",
        "                ai_answer_carga = ai_answer_carga.strip()\n",
        "                if ai_answer_carga:\n",
        "                    self.sections.append((\n",
        "                        \"### 📝 Explicación IA de la Carga de Datos\",\n",
        "                        ai_answer_carga\n",
        "                    ))\n",
        "                    print(\"[DEBUG] 1.5. Sección explicación IA de carga añadida\")\n",
        "                else:\n",
        "                    print(\"[DEBUG] No se recibió contenido IA para carga\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No están X_data/Y_data/FECHAS en globals(), omito muestra y explicación de carga\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar sección o explicación IA de carga: {e}\")\n",
        "        # =============================================================\n",
        "        # 2. Segmentación train/test\n",
        "        # =============================================================\n",
        "        try:\n",
        "            if all(k in self.g for k in (\"X_train\", \"Y_train\")):\n",
        "                df_tr = pd.concat([\n",
        "                    self.g[\"X_train\"].head(5).reset_index(drop=True),\n",
        "                    self.g[\"Y_train\"].head(5).reset_index(drop=True)\n",
        "                ], axis=1)\n",
        "                self.sections.append((\n",
        "                    \"### Tabla 1: Conjunto de Entrenamiento – Primeras 5 Muestras\",\n",
        "                    df_tr\n",
        "                ))\n",
        "                print(\"[DEBUG] 2.1. Sección entrenamiento añadida\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay X_train/Y_train en globals()\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al crear sección entrenamiento: {e}\")\n",
        "\n",
        "        # 3) Tabla de validación (5 filas)\n",
        "        try:\n",
        "            if all(k in self.g for k in (\"X_test\", \"Y_test\")):\n",
        "                df_te = pd.concat([\n",
        "                    self.g[\"X_test\"].head(5).reset_index(drop=True),\n",
        "                    self.g[\"Y_test\"].head(5).reset_index(drop=True)\n",
        "                ], axis=1)\n",
        "                self.sections.append((\n",
        "                    \"### Tabla 2: Conjunto de Validación – Primeras 5 Muestras\",\n",
        "                    df_te\n",
        "                ))\n",
        "                print(\"[DEBUG] 2.2. Sección test añadida\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay X_test/Y_test en globals()\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al crear sección test: {e}\")\n",
        "        # ... fin de la sección de carga de datos ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 3. Resumen estadístico de X_train y Y_train\n",
        "        # =============================================================\n",
        "        try:\n",
        "            if \"X_train\" in self.g:\n",
        "                Xtr = self.g[\"X_train\"]\n",
        "                desc_X = Xtr.describe().T.reset_index().rename(columns={\"index\":\"Variable\"})\n",
        "                desc_X_sample = desc_X.head(10)\n",
        "                self.sections.append((\n",
        "                    \"### Estadísticos de X_train (primeras 10 variables)\", desc_X_sample\n",
        "                ))\n",
        "                print(\"[DEBUG] 3.1. Sección estadísticos X_train añadida\")\n",
        "\n",
        "                prompt_stats = (\n",
        "                    \"Interpreta profesionalmente estos estadísticos de entrenamiento (primeras 10 variables):\\n\\n\"\n",
        "                    f\"{desc_X_sample.to_dict(orient='list')}\\n\\n\"\n",
        "                    \"Comenta posibles implicaciones (por ejemplo: presencia de outliers, escalas muy distintas entre variables, necesidad de normalización, sesgos en la distribución) \"\n",
        "                    \"y cuáles podrían ser buenas prácticas antes de entrenar modelos.\"\n",
        "                )\n",
        "                print(\"[DEBUG] 3.2. Iniciando llamada a OpenAI para explicación IA de estadísticos X_train...\")\n",
        "                stream_resp = _client.chat.completions.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"Eres un experto en análisis de datos para Machine Learning.\"},\n",
        "                        {\"role\": \"user\",   \"content\": prompt_stats}\n",
        "                    ],\n",
        "                    temperature=TEMPERATURE_VAL,\n",
        "                    max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    stream=True\n",
        "                )\n",
        "                ai_stats = \"\"\n",
        "                for chunk in stream_resp:\n",
        "                    choice = chunk.choices[0]\n",
        "                    if hasattr(choice, \"delta\") and hasattr(choice.delta, \"content\"):\n",
        "                        delta = choice.delta.content\n",
        "                        if delta:\n",
        "                            ai_stats += delta\n",
        "                ai_stats = ai_stats.strip()\n",
        "                if ai_stats:\n",
        "                    self.sections.append((\n",
        "                        \"### 📝 Explicación IA de los Estadísticos de X_train\", ai_stats\n",
        "                    ))\n",
        "                    print(\"[DEBUG] 3.3. Sección IA estadísticos X_train añadida\")\n",
        "                else:\n",
        "                    print(\"[DEBUG] No se recibió contenido IA para estadísticos X_train\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay X_train en globals(), omito sección estadísticos X_train\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar sección estadísticos X_train: {e}\")\n",
        "\n",
        "        try:\n",
        "            if \"Y_train\" in self.g:\n",
        "                Ytr = self.g[\"Y_train\"]\n",
        "                if isinstance(Ytr, pd.DataFrame):\n",
        "                    serie = Ytr.iloc[:, 0]\n",
        "                else:\n",
        "                    serie = pd.Series(Ytr)\n",
        "                desc_Ys = serie.describe()  # Series.describe() -> Series\n",
        "                desc_Y = desc_Ys.to_frame().T.reset_index().rename(columns={\"index\":\"Estadístico\"})\n",
        "                self.sections.append((\"### Estadísticos de Y_train\", desc_Y))\n",
        "                print(\"[DEBUG] 3.4. Sección estadísticos Y_train añadida\")\n",
        "\n",
        "                prompt_Y = (\n",
        "                    \"Interpreta profesionalmente estos estadísticos de la variable objetivo Y:\\n\\n\"\n",
        "                    f\"{desc_Y.to_dict(orient='list')}\\n\\n\"\n",
        "                    \"Comenta posibles implicaciones (asimetría, outliers, necesidad de transformaciones como log, etc.) \"\n",
        "                    \"y su efecto posible en el modelado.\"\n",
        "                )\n",
        "                print(\"[DEBUG] 3.5. Iniciando llamada a OpenAI para explicación IA de Y_train...\")\n",
        "                stream_resp = _client.chat.completions.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"Eres un experto en análisis de datos para Machine Learning.\"},\n",
        "                        {\"role\": \"user\",   \"content\": prompt_Y}\n",
        "                    ],\n",
        "                    temperature=TEMPERATURE_VAL,\n",
        "                    max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    stream=True\n",
        "                )\n",
        "                ai_Y = \"\"\n",
        "                for chunk in stream_resp:\n",
        "                    choice = chunk.choices[0]\n",
        "                    if hasattr(choice, \"delta\") and hasattr(choice.delta, \"content\"):\n",
        "                        delta = choice.delta.content\n",
        "                        if delta:\n",
        "                            ai_Y += delta\n",
        "                ai_Y = ai_Y.strip()\n",
        "                if ai_Y:\n",
        "                    self.sections.append((\n",
        "                        \"### 📝 Explicación IA de los Estadísticos de Y_train\", ai_Y\n",
        "                    ))\n",
        "                    print(\"[DEBUG] 3.6. Sección IA estadísticos Y_train añadida\")\n",
        "                else:\n",
        "                    print(\"[DEBUG] No se recibió contenido IA para Y_train\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay Y_train en globals(), omito sección estadísticos Y_train\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar sección estadísticos Y_train: {e}\")\n",
        "\n",
        "        # Versión y entorno\n",
        "        try:\n",
        "            import sys, sklearn\n",
        "            #import pandas as _pd\n",
        "            info = {\n",
        "                \"python_version\": sys.version.split()[0],\n",
        "                \"pandas_version\": pd.__version__,\n",
        "                \"sklearn_version\": sklearn.__version__,\n",
        "            }\n",
        "            #import pandas as _pd\n",
        "            df_env = pd.DataFrame(list(info.items()), columns=[\"Paquete\",\"Versión\"])\n",
        "            self.sections.append((\"### Entorno y Versiones de Librerías\", df_env))\n",
        "            print(\"[DEBUG] 3.7. Sección entorno/versiones añadida\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar sección entorno/versiones: {e}\")\n",
        "\n",
        "        # 4) Explicación IA del split al final\n",
        "        try:\n",
        "            if all(k in self.g for k in (\"X_train\", \"X_test\", \"Y_train\", \"Y_test\")):\n",
        "                sp = self.g.get(\"SPLIT_PARAMS\", {})\n",
        "                # Construir prompt solo si SPLIT_PARAMS tiene las claves esperadas\n",
        "                if sp:\n",
        "                    prompt_split = (\n",
        "                        \"Por favor, explica cómo se ha realizado la segmentación de los datos. \"\n",
        "                        \"Usa la siguiente información de contexto:\\n\\n\"\n",
        "                        f\"- Parámetros de segmentación: {sp}\\n\"\n",
        "                        f\"- Number de muestras train: {len(self.g['X_train'])}\\n\"\n",
        "                        f\"- Número de muestras test: {len(self.g['X_test'])}\\n\\n\"\n",
        "                        f\"- test_size: {sp.get('test_size')}\\n\"\n",
        "                        f\"- random_state: {sp.get('random_state')}\\n\"\n",
        "                        f\"- estratificar: {sp.get('stratify')}\\n\"\n",
        "                        f\"- método de bins: {sp.get('bin_method')}\\n\"\n",
        "                        f\"- número de bins: {sp.get('q_bins')}\\n\\n\"\n",
        "                        \"Quiero un texto profesional, bien estructurado y suficientemente detallado, \"\n",
        "                        \"que también comente brevemente por qué estos valores de parámetros pueden afectar al rendimiento del modelo.\"\n",
        "                    )\n",
        "                    print(\"[DEBUG] 3.8. Iniciando llamada a OpenAI para explicación del split...\")\n",
        "                    stream_resp = _client.chat.completions.create(\n",
        "                        model=\"gpt-4\",\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"Eres un experto en preprocesado de datos.\"},\n",
        "                            {\"role\": \"user\",   \"content\": prompt_split}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                        stream=True\n",
        "                    )\n",
        "                    ai_answer_split = \"\"\n",
        "                    for chunk in stream_resp:\n",
        "                        choice = chunk.choices[0]\n",
        "                        if hasattr(choice, \"delta\") and hasattr(choice.delta, \"content\"):\n",
        "                            delta = choice.delta.content\n",
        "                            if delta:\n",
        "                                ai_answer_split += delta\n",
        "                    ai_answer_split = ai_answer_split.strip()\n",
        "                    if ai_answer_split:\n",
        "                        self.sections.append((\n",
        "                            \"### 📝 Explicación IA del Preprocesado y del Split\",\n",
        "                            ai_answer_split\n",
        "                        ))\n",
        "                        print(\"[DEBUG] 3.9. Sección explicación IA del split añadida\")\n",
        "                    else:\n",
        "                        print(\"[DEBUG] No se recibió contenido IA para split\")\n",
        "                else:\n",
        "                    print(\"[DEBUG] No hay SPLIT_PARAMS definidos, omito explicación IA del split\")\n",
        "            else:\n",
        "                print(\"[DEBUG] Faltan datos de entrenamiento/test, omito explicación IA del split\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar explicación IA del split: {e}\")\n",
        "        # ... fin de la sección de Split de los Datos Cargados ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 4. Visualizaciones de Y y de las correlaciones X vs Y\n",
        "        # =============================================================\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # 2.1 Histograma de Y_train\n",
        "        try:\n",
        "            if \"Y_train\" in self.g:\n",
        "                y_train = self.g[\"Y_train\"]\n",
        "                # Si Y_train es DataFrame con varias columnas, tomamos la primera:\n",
        "                if isinstance(y_train, pd.DataFrame) and y_train.shape[1] > 1:\n",
        "                    y_ser = y_train.iloc[:, 0]\n",
        "                else:\n",
        "                    # si es DataFrame de 1 columna o Serie:\n",
        "                    y_ser = y_train.iloc[:, 0] if isinstance(y_train, pd.DataFrame) else pd.Series(y_train)\n",
        "                fig_hist, ax = plt.subplots()\n",
        "                ax.hist(y_ser.dropna(), bins=30, edgecolor='black')\n",
        "                ax.set_title(\"Distribución de Y_train\")\n",
        "                ax.set_xlabel(\"Y\")\n",
        "                ax.set_ylabel(\"Frecuencia\")\n",
        "                plt.tight_layout()\n",
        "                # Guardar figura en self.figures y en sections para render\n",
        "                self.figures[\"hist_Y_train\"] = fig_hist\n",
        "                self.sections.append((\n",
        "                    \"### Gráfico: Histograma de Y (Train)\",\n",
        "                    fig_hist\n",
        "                ))\n",
        "                print(\"[DEBUG] 4.1. Sección histograma Y_train añadida\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay Y_train en globals(), omito histograma\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar histograma Y_train: {e}\")\n",
        "\n",
        "        # 2.2 Correlación X_train vs Y_train\n",
        "        # 2.2 Correlación X_train vs Y_train (filtrada por umbral)\n",
        "        try:\n",
        "            if \"X_train\" in self.g and \"Y_train\" in self.g:\n",
        "                X_train = self.g[\"X_train\"]\n",
        "                y_train = self.g[\"Y_train\"]\n",
        "\n",
        "                if isinstance(y_train, pd.DataFrame) and y_train.shape[1] > 1:\n",
        "                    y_ser = y_train.iloc[:, 0]\n",
        "                else:\n",
        "                    y_ser = y_train.iloc[:, 0] if isinstance(y_train, pd.DataFrame) else pd.Series(y_train)\n",
        "\n",
        "                df_corr = X_train.copy().reset_index(drop=True)\n",
        "                df_corr[\"_Y_target\"] = y_ser.reset_index(drop=True)\n",
        "\n",
        "                corr_matrix = df_corr.corr(numeric_only=True)\n",
        "\n",
        "                # === 🔍 FILTRO: seleccionar solo columnas con correlación > umbral con _Y_target ===\n",
        "                umbral_corr = 0.3  # Se puede ajustar este valor\n",
        "                correlaciones_con_y = corr_matrix[\"_Y_target\"].abs()\n",
        "                variables_filtradas = correlaciones_con_y[correlaciones_con_y > umbral_corr].index.tolist()\n",
        "\n",
        "                # Mantener solo las columnas con correlación alta\n",
        "                corr_matrix_filtrada = corr_matrix.loc[variables_filtradas, variables_filtradas]\n",
        "\n",
        "                fig_corr, ax = plt.subplots(figsize=(max(10, len(variables_filtradas)*0.6), max(8, len(variables_filtradas)*0.5)))\n",
        "                cax = ax.matshow(corr_matrix_filtrada, cmap='viridis')\n",
        "                fig_corr.colorbar(cax, ax=ax, shrink=0.8)\n",
        "\n",
        "                labels = list(corr_matrix_filtrada.columns)\n",
        "                ax.set_xticks(range(len(labels)))\n",
        "                ax.set_yticks(range(len(labels)))\n",
        "                ax.set_xticklabels(labels, rotation=90, fontsize=8, ha='left')\n",
        "                ax.set_yticklabels(labels, fontsize=8)\n",
        "\n",
        "                ax.tick_params(axis='x', which='both', labelsize=7, pad=1)\n",
        "                ax.tick_params(axis='y', which='both', labelsize=7, pad=1)\n",
        "\n",
        "                ax.set_title(f\"Matriz de correlación (|r| > {umbral_corr})\", pad=30, fontsize=12)\n",
        "                plt.tight_layout()\n",
        "\n",
        "                self.figures[\"corr_XY_train\"] = fig_corr\n",
        "                self.sections.append((\n",
        "                    f\"### Gráfico: Matriz de Correlación X vs Y (|r| > {umbral_corr})\",\n",
        "                    fig_corr\n",
        "                ))\n",
        "                print(\"[DEBUG] 4.2. Sección matriz de correlación añadida con filtro\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay X_train/Y_train en globals(), omito correlación\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar matriz de correlación: {e}\")\n",
        "\n",
        "#        try:\n",
        "#            if \"X_train\" in self.g and \"Y_train\" in self.g:\n",
        "#                X_train = self.g[\"X_train\"]\n",
        "#                y_train = self.g[\"Y_train\"]\n",
        "#                # Sacar Serie de Y como antes\n",
        "#                if isinstance(y_train, pd.DataFrame) and y_train.shape[1] > 1:\n",
        "#                    y_ser = y_train.iloc[:, 0]\n",
        "#                else:\n",
        "#                    y_ser = y_train.iloc[:, 0] if isinstance(y_train, pd.DataFrame) else pd.Series(y_train)\n",
        "#                # Concatenar para cálculo de correlación:\n",
        "#                df_corr = X_train.copy().reset_index(drop=True)\n",
        "#                df_corr[\"_Y_target\"] = y_ser.reset_index(drop=True)\n",
        "#                # Calculamos matriz de correlaciones:\n",
        "#                corr_matrix = df_corr.corr(numeric_only=True)  # pandas ≥1.5\n",
        "#                # Creamos heatmap con matplotlib puro:\n",
        "#                fig_corr, ax = plt.subplots(figsize=(6, 6))\n",
        "#                cax = ax.matshow(corr_matrix, cmap='viridis')\n",
        "#                fig_corr.colorbar(cax, ax=ax)\n",
        "#                # Etiquetas:\n",
        "#                labels = list(corr_matrix.columns)\n",
        "#                ax.set_xticks(range(len(labels)))\n",
        "#                ax.set_yticks(range(len(labels)))\n",
        "#                ax.set_xticklabels(labels, rotation=90, fontsize=8)\n",
        "#                ax.set_yticklabels(labels, fontsize=8)\n",
        "#                ax.set_title(\"Matriz de correlación (X_train vs Y_train incluida)\", pad=20)\n",
        "#                plt.tight_layout()\n",
        "#                self.figures[\"corr_XY_train\"] = fig_corr\n",
        "#                self.sections.append((\n",
        "#                    \"### Gráfico: Matriz de Correlación X vs Y (Train)\",\n",
        "#                    fig_corr\n",
        "#                ))\n",
        "#                print(\"[DEBUG] 4.2. Sección matriz de correlación añadida\")\n",
        "#            else:\n",
        "#                print(\"[DEBUG] No hay X_train/Y_train en globals(), omito correlación\")\n",
        "#        except Exception as e:\n",
        "#            print(f\"[ERROR] al generar matriz de correlación: {e}\")\n",
        "\n",
        "        # 2.3 Comentario IA sobre distribución y correlaciones\n",
        "        try:\n",
        "            # Solo si disponemos de histogram y/o correlaciones:\n",
        "            if \"hist_Y_train\" in self.figures:\n",
        "                # Preparamos prompt para la IA\n",
        "                # Ejemplo de contexto: medias, sesgo, kurtosis, correlaciones máximas\n",
        "                import numpy as np\n",
        "                # Estadísticos de Y:\n",
        "                y_arr = y_ser.dropna().values\n",
        "                media = float(np.mean(y_arr))\n",
        "                mediana = float(np.median(y_arr))\n",
        "                std = float(np.std(y_arr, ddof=1))\n",
        "                # Sesgo y curtosis opcionales si numpy/scipy disponibles:\n",
        "                try:\n",
        "                    from scipy.stats import skew, kurtosis\n",
        "                    sesgo = float(skew(y_arr))\n",
        "                    kurt = float(kurtosis(y_arr))\n",
        "                except Exception:\n",
        "                    sesgo = None\n",
        "                    kurt = None\n",
        "                # Estadísticos de correlación: extraer de corr_matrix\n",
        "                if \"corr_XY_train\" in self.figures:\n",
        "                    # Obtenemos correlaciones de X con Y:\n",
        "                    # La columna “_Y_target”\n",
        "                    corrs = corr_matrix[\"_Y_target\"].drop(\"_Y_target\", errors='ignore')\n",
        "                    # Tomamos los pares con mayor valor absoluto:\n",
        "                    if not corrs.empty:\n",
        "                        top = corrs.abs().sort_values(ascending=False).head(3)\n",
        "                        # Formatear para prompt\n",
        "                        top_info = {col: float(corrs[col]) for col in top.index}\n",
        "                    else:\n",
        "                        top_info = {}\n",
        "                else:\n",
        "                    top_info = {}\n",
        "                # Construir prompt:\n",
        "                prompt_vis = (\n",
        "                    \"Eres un experto en análisis exploratorio de datos.\\n\"\n",
        "                    \"Analiza la distribución de la variable objetivo Y y las correlaciones \"\n",
        "                    \"entre las variables X y Y basándote en estos estadísticos:\\n\\n\"\n",
        "                    f\"- Media de Y_train: {media:.4f}\\n\"\n",
        "                    f\"- Mediana de Y_train: {mediana:.4f}\\n\"\n",
        "                    f\"- Desviación estándar de Y_train: {std:.4f}\\n\"\n",
        "                )\n",
        "                if sesgo is not None:\n",
        "                    prompt_vis += f\"- Sesgo (skewness) de Y_train: {sesgo:.4f}\\n\"\n",
        "                if kurt is not None:\n",
        "                    prompt_vis += f\"- Curtosis de Y_train: {kurt:.4f}\\n\"\n",
        "                if top_info:\n",
        "                    prompt_vis += \"- Correlaciones más relevantes X vs Y:\\n\"\n",
        "                    for feat, corrv in top_info.items():\n",
        "                        prompt_vis += f\"    • {feat}: {corrv:.4f}\\n\"\n",
        "                prompt_vis += (\n",
        "                    \"\\nPor favor, genera un texto profesional y detallado que comente:\\n\"\n",
        "                    \"  * Si la distribución de Y parece simétrica o sesgada, posibles implicaciones.\\n\"\n",
        "                    \"  * Si hay variables con fuerte correlación absoluta con Y (positiva o negativa) y qué puede indicar.\\n\"\n",
        "                    \"  * Buenas prácticas o precauciones al modelar con base en dicha distribución/correlaciones.\\n\"\n",
        "                )\n",
        "                print(\"[DEBUG] 4.3. Iniciando llamada a OpenAI para comentario visualizaciones...\")\n",
        "                # Llamada a OpenAI (sin stream, con límite de tokens razonable):\n",
        "                resp = _client.chat.completions.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"Eres un experto en análisis exploratorio de datos para ML.\"},\n",
        "                        {\"role\": \"user\", \"content\": prompt_vis}\n",
        "                    ],\n",
        "                    temperature=TEMPERATURE_VAL,\n",
        "                    max_tokens=MAX_EXPLANATION_TOKENS  # ajustable\n",
        "                )\n",
        "                comentario_vis = resp.choices[0].message.content.strip()\n",
        "                if comentario_vis:\n",
        "                    self.sections.append((\n",
        "                        \"### 📝 Comentario IA: Distribución de Y y Correlaciones\",\n",
        "                        comentario_vis\n",
        "                    ))\n",
        "                    print(\"[DEBUG] 4.4. Sección comentario IA visualizaciones añadida\")\n",
        "                else:\n",
        "                    print(\"[DEBUG] No se recibió contenido IA para visualizaciones\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No hay histograma de Y para IA, omito comentario visualizaciones\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar comentario IA de visualizaciones: {e}\")\n",
        "        # ... fin de la sección de visualización de análisis de datos cargados ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 5. Selección de variables independientes X\n",
        "        # =============================================================\n",
        "        try:\n",
        "            # Caso 1: RESUMEN_METODOS (varios métodos acumulativos)\n",
        "            if \"RESUMEN_METODOS\" in self.g and isinstance(self.g[\"RESUMEN_METODOS\"], dict):\n",
        "                resumen = self.g[\"RESUMEN_METODOS\"]\n",
        "            else:\n",
        "                resumen = None\n",
        "\n",
        "            # Caso 2: Selección más reciente puntual\n",
        "            tiene_reciente = all(k in self.g for k in (\"VARIABLES_SELECCIONADAS\", \"METODO_SELECCION\"))\n",
        "            # Preparar datos para correlaciones si están en globals\n",
        "            tiene_corr = \"VALORES_CORRELACION\" in self.g\n",
        "            corr_global = self.g.get(\"VALORES_CORRELACION\", None)\n",
        "\n",
        "            # También comprobamos si disponemos de X_train/Y_train para recálculo de correl si se prefiere\n",
        "            have_xy = all(k in self.g for k in (\"X_train\", \"Y_train\"))\n",
        "            if have_xy:\n",
        "                X_train = self.g[\"X_train\"]\n",
        "                Y_train = self.g[\"Y_train\"]\n",
        "                import pandas as _pd  # asegurar pandas disponible\n",
        "                if isinstance(Y_train, _pd.DataFrame):\n",
        "                    y_ser = Y_train.iloc[:, 0]\n",
        "                else:\n",
        "                    y_ser = Y_train\n",
        "\n",
        "            # Primero, si RESUMEN_METODOS existe, iteramos cada método\n",
        "            if resumen:\n",
        "                # Si además quieres usar VALORES_CORRELACION puntual, podrías ignorarlo aquí\n",
        "                for metodo, cols in resumen.items():\n",
        "                    # Construcción de DataFrame con correlaciones\n",
        "                    df_sel = None\n",
        "                    if have_xy:\n",
        "                        corr_vals = []\n",
        "                        for col in cols:\n",
        "                            if col in X_train.columns:\n",
        "                                try:\n",
        "                                    corr = X_train[col].corr(y_ser)\n",
        "                                except Exception:\n",
        "                                    corr = None\n",
        "                            else:\n",
        "                                corr = None\n",
        "                            corr_vals.append(corr)\n",
        "                        import pandas as _pd\n",
        "                        df_sel = _pd.DataFrame({\n",
        "                            \"Variable\": cols,\n",
        "                            \"Correlación con Y\": corr_vals\n",
        "                        })\n",
        "                    else:\n",
        "                        import pandas as _pd\n",
        "                        df_sel = _pd.DataFrame({\"Variable\": cols})\n",
        "                    titulo_tab = f\"### Tabla: Selección de Variables ({metodo})\"\n",
        "                    self.sections.append((titulo_tab, df_sel))\n",
        "                    print(f\"[DEBUG] 5.1. Sección selección de variables añadida para método: {metodo}\")\n",
        "\n",
        "                    # Preparar prompt IA\n",
        "                    # Si existe un dict global con parámetros, usalo; si no, omítelo:\n",
        "                    params = self.g.get(\"SELECTION_PARAMS\", {}).get(metodo, {})\n",
        "                    prompt_sel = (\n",
        "                        f\"Has aplicado un método de selección de variables llamado '{metodo}'.\\n\"\n",
        "                        f\"Parámetros del método: {params}\\n\"\n",
        "                        f\"Variables seleccionadas ({len(cols)}): {cols}\\n\"\n",
        "                    )\n",
        "                    if have_xy:\n",
        "                        prompt_sel += \"Correlaciones con la variable objetivo:\\n\"\n",
        "                        for var, corr in zip(cols, corr_vals):\n",
        "                            prompt_sel += f\"  - {var}: {corr}\\n\"\n",
        "                        prompt_sel += \"\\n\"\n",
        "                    prompt_sel += (\n",
        "                        \"Por favor, genera un texto profesional y detallado que explique:\\n\"\n",
        "                        \"- En qué consiste este método de selección de variables y significado de sus parámetros.\\n\"\n",
        "                        \"- Cómo influyen dichos parámetros en la selección.\\n\"\n",
        "                        \"- Interpretación de los valores de correlación obtenidos.\\n\"\n",
        "                        \"- Buenas prácticas al usar este método en preprocesado de datos para ML.\\n\"\n",
        "                    )\n",
        "                    try:\n",
        "                        print(f\"[DEBUG] 5.2. Iniciando llamada a OpenAI para explicación selección ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en selección de variables para ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_sel}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_sel = resp.choices[0].message.content.strip()\n",
        "                        if explanation_sel:\n",
        "                            titulo_exp = f\"### 📝 Explicación IA Selección de Variables ({metodo})\"\n",
        "                            self.sections.append((titulo_exp, explanation_sel))\n",
        "                            print(f\"[DEBUG] 5.3. Sección explicación IA selección añadida para método: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibió contenido IA para selección {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA selección {metodo}: {e}\")\n",
        "\n",
        "            # Luego, si existe una selección puntual reciente\n",
        "            if not resumen and tiene_reciente:\n",
        "                metodo = self.g[\"METODO_SELECCION\"]\n",
        "                cols = self.g[\"VARIABLES_SELECCIONADAS\"]\n",
        "                # Construir DataFrame\n",
        "                df_sel = None\n",
        "                import pandas as _pd\n",
        "                if tiene_corr:\n",
        "                    corr_vals = None\n",
        "                    # VALORES_CORRELACION puede ser dict o lista\n",
        "                    vc = corr_global\n",
        "                    if isinstance(vc, dict):\n",
        "                        # Aseguramos la lista del mismo orden\n",
        "                        corr_vals = [vc.get(var, None) for var in cols]\n",
        "                    elif isinstance(vc, (list, tuple)):\n",
        "                        # Si la longitud coincide con cols\n",
        "                        if len(vc) == len(cols):\n",
        "                            corr_vals = list(vc)\n",
        "                        else:\n",
        "                            corr_vals = [None]*len(cols)\n",
        "                    else:\n",
        "                        corr_vals = [None]*len(cols)\n",
        "                    df_sel = _pd.DataFrame({\n",
        "                        \"Variable\": cols,\n",
        "                        \"Correlación con Y\": corr_vals\n",
        "                    })\n",
        "                else:\n",
        "                    # Si no hay VALORES_CORRELACION, pero se dispone de X_train/Y_train, recalculamos\n",
        "                    if have_xy:\n",
        "                        corr_vals = []\n",
        "                        for col in cols:\n",
        "                            if col in X_train.columns:\n",
        "                                try:\n",
        "                                    corr = X_train[col].corr(y_ser)\n",
        "                                except Exception:\n",
        "                                    corr = None\n",
        "                            else:\n",
        "                                corr = None\n",
        "                            corr_vals.append(corr)\n",
        "                        df_sel = _pd.DataFrame({\n",
        "                            \"Variable\": cols,\n",
        "                            \"Correlación con Y\": corr_vals\n",
        "                        })\n",
        "                    else:\n",
        "                        df_sel = _pd.DataFrame({\"Variable\": cols})\n",
        "                titulo_tab = f\"### Tabla: Selección de Variables ({metodo})\"\n",
        "                self.sections.append((titulo_tab, df_sel))\n",
        "                print(f\"[DEBUG] 5.4. Sección selección de variables puntual añadida para método: {metodo}\")\n",
        "\n",
        "                # Prompt IA\n",
        "                prompt_sel = (\n",
        "                    f\"Has aplicado un método de selección de variables llamado '{metodo}'.\\n\"\n",
        "                    f\"Variables seleccionadas ({len(cols)}): {cols}\\n\"\n",
        "                )\n",
        "                if tiene_corr or have_xy:\n",
        "                    prompt_sel += \"Correlaciones con la variable objetivo:\\n\"\n",
        "                    if 'corr_vals' in locals():\n",
        "                        for var, corr in zip(cols, corr_vals):\n",
        "                            prompt_sel += f\"  - {var}: {corr}\\n\"\n",
        "                    prompt_sel += \"\\n\"\n",
        "                prompt_sel += (\n",
        "                    \"Por favor, genera un texto profesional y detallado que explique:\\n\"\n",
        "                    \"- En qué consiste este método de selección de variables (breve descripción basada en su nombre) y significado de sus parámetros si los conoces.\\n\"\n",
        "                    \"- Interpretación de los valores de correlación obtenidos.\\n\"\n",
        "                    \"- Buenas prácticas al usar este método en preprocesado de datos para ML.\\n\"\n",
        "                )\n",
        "                try:\n",
        "                    print(f\"[DEBUG] 5.5. Iniciando llamada a OpenAI para explicación selección puntual ({metodo})...\")\n",
        "                    resp = _client.chat.completions.create(\n",
        "                        model=\"gpt-4\",\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"Eres un experto en selección de variables para ML.\"},\n",
        "                            {\"role\": \"user\", \"content\": prompt_sel}\n",
        "                        ],\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                        temperature=TEMPERATURE_VAL\n",
        "                    )\n",
        "                    explanation_sel = resp.choices[0].message.content.strip()\n",
        "                    if explanation_sel:\n",
        "                        titulo_exp = f\"### 📝 Explicación IA Selección de Variables ({metodo})\"\n",
        "                        self.sections.append((titulo_exp, explanation_sel))\n",
        "                        print(f\"[DEBUG] 5.6. Sección explicación IA selección puntual añadida para método: {metodo}\")\n",
        "                    else:\n",
        "                        print(\"[DEBUG] No se recibió contenido IA para selección puntual\")\n",
        "                except Exception as e:\n",
        "                    print(f\"[ERROR] al generar explicación IA selección puntual {metodo}: {e}\")\n",
        "\n",
        "            if not resumen and not tiene_reciente:\n",
        "                print(\"[DEBUG] No hay RESUMEN_METODOS ni selección puntual en globals(), omito sección de selección de variables\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar sección o explicación IA de selección de variables: {e}\")\n",
        "\n",
        "        # ... fin de la sección de selección de variables ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 6. Entrenamiento Modelo SVR\n",
        "        # =============================================================\n",
        "        try:\n",
        "            # <<< Aquí inserta las inicializaciones >>>\n",
        "            rmse = mae = r2 = None\n",
        "            residuos = None\n",
        "            # Comprobamos RESUMEN_METODOS y existencia de X_test/Y_test\n",
        "            if \"RESUMEN_METODOS\" in self.g and \"X_test\" in self.g and \"Y_test\" in self.g:\n",
        "                import os, pickle\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "                import pandas as _pd\n",
        "\n",
        "                X_test_full = self.g[\"X_test\"]\n",
        "                Y_test = self.g[\"Y_test\"]\n",
        "                # Extraer y_test_arr como 1D array\n",
        "                arr = None\n",
        "                import numpy as _np\n",
        "                y_test_arr = None\n",
        "                if hasattr(Y_test, \"values\"):\n",
        "                    arr = Y_test.values\n",
        "                    if arr.ndim == 2:\n",
        "                        if arr.shape[1] == 1:\n",
        "                            y_test_arr = arr[:, 0]\n",
        "                        else:\n",
        "                            y_test_arr = arr[:, 0]\n",
        "                    else:\n",
        "                        y_test_arr = arr\n",
        "                else:\n",
        "                    y_test_arr = _np.array(Y_test)\n",
        "                    if y_test_arr.ndim == 2 and y_test_arr.shape[1] == 1:\n",
        "                        y_test_arr = y_test_arr[:, 0]\n",
        "                if y_test_arr.ndim > 1:\n",
        "                    y_test_arr = y_test_arr.ravel()\n",
        "\n",
        "                metrics_summary = []\n",
        "                resumen_modelos = []\n",
        "                # Iteramos sobre cada método declarado en RESUMEN_METODOS\n",
        "                for metodo, vars_sel in self.g[\"RESUMEN_METODOS\"].items():\n",
        "                    metodo_low = metodo.lower()\n",
        "                    # Nombre de fichero pickle según celda 7.1\n",
        "                    fname = f\"modelo_svr_{metodo_low}.pkl\"\n",
        "                    if not os.path.exists(fname):\n",
        "                        print(f\"[DEBUG] 6.1. Fichero de modelo SVR no encontrado para método '{metodo}': {fname}, omito este método\")\n",
        "                        continue\n",
        "                    # Cargar pickle\n",
        "                    try:\n",
        "                        with open(fname, \"rb\") as f:\n",
        "                            data = pickle.load(f)\n",
        "                        model = data.get(\"model\", None)\n",
        "                        sx = data.get(\"sx\", None)\n",
        "                        sy = data.get(\"sy\", None)\n",
        "                        cols = data.get(\"cols\", None)\n",
        "                        # yname = data.get(\"yname\", None)  # si quieres mostrar el nombre de la variable\n",
        "                        if model is None or sx is None or sy is None or cols is None:\n",
        "                            print(f\"[DEBUG] Falta alguna clave en pickle SVR para método '{metodo}', omito\")\n",
        "                            continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al cargar pickle SVR para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Verificar columnas en X_test_full\n",
        "                    missing = [c for c in cols if c not in X_test_full.columns]\n",
        "                    if missing:\n",
        "                        print(f\"[DEBUG] Para método '{metodo}', faltan columnas en X_test: {missing}, omito\")\n",
        "                        continue\n",
        "                    # Subconjunto X_test\n",
        "                    X_test_sel = X_test_full[cols].copy()\n",
        "                    # Escalar, predecir y desescalar\n",
        "                    try:\n",
        "                        X_test_scaled = sx.transform(X_test_sel)\n",
        "                        y_pred_scaled = model.predict(X_test_scaled)\n",
        "                        # sy estuvo ajustado sobre y entrenado; para inverse_transform debe recibir 2D:\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al predecir/desescalar SVR para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Calcular métricas\n",
        "                    try:\n",
        "                        # Rangos Real vs Predicha\n",
        "                        y_real_min, y_real_max = float(_np.min(y_test_arr)), float(_np.max(y_test_arr))\n",
        "                        y_pred_min, y_pred_max = float(_np.min(y_pred)), float(_np.max(y_pred))\n",
        "                        # 3) Estadísticos de residuos:\n",
        "                        residuals = y_test_arr - y_pred\n",
        "\n",
        "                        res_mean = float(np.mean(residuals))            # Media\n",
        "                        res_std  = float(np.std(residuals))             # Desviación estándar:\n",
        "                        res_series = pd.Series(residuals)               # Asimetria\n",
        "                        res_skew = float(res_series.skew())             # Asimetria\n",
        "                        res_kurt = float(res_series.kurtosis())         # Curtosis\n",
        "                        q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]    # Cuantiles:\n",
        "                        # Métricas\n",
        "                        mse = mean_squared_error(y_test_arr, y_pred)\n",
        "                        rmse = float(_np.sqrt(mse))\n",
        "                        mae = float(mean_absolute_error(y_test_arr, y_pred))\n",
        "                        r2 = float(r2_score(y_test_arr, y_pred))\n",
        "                        resumen_modelos.append({\n",
        "                            'Método': metodo,\n",
        "                            'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae\n",
        "                        })\n",
        "                        # Correlación Real vs Predicha\n",
        "                        try:\n",
        "                            corr = float(_np.corrcoef(y_test_arr, y_pred)[0, 1])\n",
        "                        except:\n",
        "                            corr = None\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al calcular métricas SVR para método '{metodo}': {e}\")\n",
        "\n",
        "                    metrics_summary.append({\n",
        "                        \"metodo\": metodo,\n",
        "                        \"rmse\": rmse,\n",
        "                        \"mae\": mae,\n",
        "                        \"r2\": r2\n",
        "                    })\n",
        "\n",
        "                    # 1) Parámetros de entrenamiento obtenidos desde el modelo\n",
        "                    try:\n",
        "                        # SVR tiene atributos: C, epsilon, kernel, gamma\n",
        "                        C_val = getattr(model, \"C\", None)\n",
        "                        epsilon_val = getattr(model, \"epsilon\", None)\n",
        "                        kernel_val = getattr(model, \"kernel\", None)\n",
        "                        gamma_val = getattr(model, \"gamma\", None)\n",
        "                        params = {\n",
        "                            \"C\": C_val,\n",
        "                            \"epsilon\": epsilon_val,\n",
        "                            \"kernel\": kernel_val,\n",
        "                            \"gamma\": gamma_val\n",
        "                        }\n",
        "                        df_params = _pd.DataFrame({\n",
        "                            \"Hiperparámetro\": list(params.keys()),\n",
        "                            \"Valor\": [str(v) for v in params.values()]\n",
        "                        })\n",
        "                        titulo_p = f\"### Parámetros de Entrenamiento SVR ({metodo})\"\n",
        "                        self.sections.append((titulo_p, df_params))\n",
        "                        print(f\"[DEBUG] 6.2. Sección parámetros SVR añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al extraer/parámetros SVR para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA de los hiperparámetros\n",
        "                    try:\n",
        "                        prompt_params = (\n",
        "                            f\"Has entrenado un modelo SVR con selección de variables '{metodo}'.\\n\"\n",
        "                            f\"Estos fueron los hiperparámetros utilizados:\\n\"\n",
        "                        )\n",
        "                        for k, v in params.items():\n",
        "                            prompt_params += f\"- {k}: {v}\\n\"\n",
        "                        prompt_params += (\n",
        "                            \"\\nPor favor, explica de forma profesional y detallada cómo estos hiperparámetros \"\n",
        "                            \"pueden influir en el entrenamiento del modelo SVR, su impacto en ajuste, \"\n",
        "                            \"y buenas prácticas para seleccionarlos o afinarlos.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 6.3. Iniciando llamada a OpenAI para explicación hiperparámetros SVR ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en entrenamiento de modelos SVR.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_params}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_params = resp.choices[0].message.content.strip()\n",
        "                        if explanation_params:\n",
        "                            titulo_exp_p = f\"### 📝 Explicación IA Hiperparámetros SVR ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_p, explanation_params))\n",
        "                            print(f\"[DEBUG] 6.4. Sección explicación IA hiperparámetros añadida para método: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibió IA para hiperparámetros SVR ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA hiperparámetros SVR para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 2) Gráfica Predicho vs Real\n",
        "                    try:\n",
        "                        fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "                        ax1.scatter(y_test_arr, y_pred, alpha=0.6)\n",
        "                        ax1.plot([y_test_arr.min(), y_test_arr.max()],\n",
        "                                 [y_test_arr.min(), y_test_arr.max()],\n",
        "                                 'r--', lw=2)\n",
        "                        ax1.set_xlabel(\"Y real\")\n",
        "                        ax1.set_ylabel(\"Y predicho\")\n",
        "                        ax1.set_title(f\"SVR Predicho vs Real ({metodo})\")\n",
        "                        titulo_fig1 = f\"### Gráfica SVR Predicho vs Real ({metodo})\"\n",
        "                        self.sections.append((titulo_fig1, fig1))\n",
        "                        print(f\"[DEBUG] 6.5. Sección gráfica Pred vs Real añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear gráfica Pred vs Real para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA Pred vs Real\n",
        "                    try:\n",
        "                        prompt_pred_real = (\n",
        "                        f\"A continuación tienes datos de la gráfica de comparación Real vs Predicción para el modelo SVR con método '{metodo}':\\n\"\n",
        "                        #f\"- R2: {r2:.4f}\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- R²: {r2}\\n\"\n",
        "                        f\"- Correlación entre Y real y predicha: {corr:.4f}\\n\"\n",
        "                        f\"- Rango Y real: [{y_real_min:.4f}, {y_real_max:.4f}]\\n\"\n",
        "                        f\"- Rango Y predicha: [{y_pred_min:.4f}, {y_pred_max:.4f}]\\n\"\n",
        "                        \"\\n\"\n",
        "                        \"También tienes datos de la gráfica de residuos:\\n\"\n",
        "                        f\"- Media de residuos (Real - Predicha): {res_mean:.4f}\\n\"\n",
        "                        f\"- Desviación estándar de residuos: {res_std:.4f}\\n\"\n",
        "                        f\"- Asimetría de residuos: {res_skew:.4f}\\n\"\n",
        "                        f\"- Curtosis de residuos: {res_kurt:.4f}\\n\"\n",
        "                        f\"- Cuantiles de residuos: 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}\\n\"\n",
        "                        \"\\n\"\n",
        "                        \"Basándote en estos valores y en las gráficas generadas (Real vs Predicción y Residuos), \"\n",
        "                        \"proporciona un análisis detallado, señalando si hay sesgos sistemáticos (por ejemplo, subestimación o sobrestimación en ciertos rangos), \"\n",
        "                        \"si la dispersión es mayor en algún rango de predicción, si los residuos muestran patrones (p. ej. forma de embudo), \"\n",
        "                        \"y qué implicaciones tiene para la calidad del modelo. \"\n",
        "                        \"Usa un texto profesional y bien estructurado, y menciona qué indicios de la gráfica respaldan tus conclusiones.\"\n",
        "                    )\n",
        "                        print(f\"[DEBUG] 6.6. Llamada IA Pred vs Real ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_pred_real}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_pr = resp.choices[0].message.content.strip()\n",
        "                        if explanation_pr:\n",
        "                            titulo_exp_pr = f\"### 📝 Explicación IA Predicho vs Real ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_pr, explanation_pr))\n",
        "                            print(f\"[DEBUG] 6.7. Sección explicación IA Pred vs Real añadida para método: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibió IA Pred vs Real ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA Pred vs Real para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 3) Gráfica de residuos\n",
        "                    try:\n",
        "                        # 4) Rango de Y real y predicha:\n",
        "                        y_real_min, y_real_max = float(np.min(y_test_arr)), float(np.max(y_test_arr))\n",
        "                        y_pred_min, y_pred_max = float(np.min(y_pred)), float(np.max(y_pred))\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "                        ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "                        ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "                        ax2.set_xlabel(\"Y predicho\")\n",
        "                        ax2.set_ylabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "                        ax2.set_title(f\"SVR Residuos ({metodo})\")\n",
        "                        titulo_fig2 = f\"### Gráfica SVR Residuos ({metodo})\"\n",
        "                        self.sections.append((titulo_fig2, fig2))\n",
        "                        print(f\"[DEBUG] 6.8. Sección gráfica residuos añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear gráfica residuos para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA Residuos\n",
        "                    try:\n",
        "                        prompt_residuos = (\n",
        "                            f\"A continuación tienes estadísticas de los residuos (Real - Predicha) del modelo SVR con método '{metodo}':\\n\"\n",
        "                            f\"- Media: {res_mean:.4f}\\n\"\n",
        "                            f\"- Desviación estándar: {res_std:.4f}\\n\"\n",
        "                            f\"- Asimetría: {res_skew:.4f}\\n\"\n",
        "                            f\"- Curtosis: {res_kurt:.4f}\\n\"\n",
        "                            f\"- Cuantiles: 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}\\n\"\n",
        "                            \"\\n\"\n",
        "                            \"Basándote en estos valores y en la gráfica de residuos, analiza si hay patrones (por ejemplo, heterocedasticidad, outliers, sesgos en rangos), \"\n",
        "                            \"y comenta qué implicaciones tiene para la robustez y generalización del modelo. \"\n",
        "                            \"Usa un texto profesional y bien estructurado.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 6.9. Llamada IA Residuos ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_residuos}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_res = resp.choices[0].message.content.strip()\n",
        "                        if explanation_res:\n",
        "                            titulo_exp_res = f\"### 📝 Explicación IA Residuos ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_res, explanation_res))\n",
        "                            print(f\"[DEBUG] 6.10. Sección explicación IA residuos añadida para método: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibió IA Residuos ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA residuos para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 4) Tabla Métricas y explicación IA\n",
        "                    try:\n",
        "                        df_met = _pd.DataFrame([{\"Métrica\": \"RMSE\", \"Valor\": rmse},\n",
        "                                                {\"Métrica\": \"MAE\", \"Valor\": mae},\n",
        "                                                {\"Métrica\": \"R2\",  \"Valor\": r2}])\n",
        "                        titulo_met = f\"### Métricas SVR ({metodo})\"\n",
        "                        self.sections.append((titulo_met, df_met))\n",
        "                        print(f\"[DEBUG] 6.11. Sección métricas añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame métricas SVR para método '{metodo}': {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_metrics = (\n",
        "                            f\"Estas son las métricas del modelo SVR con método '{metodo}':\\n\"\n",
        "                            f\"- R2: {r2:.4f}\\n\"\n",
        "                            f\"- MSE: {mse:.4f}\\n\"\n",
        "                            f\"- RMSE: {rmse:.4f}\\n\"\n",
        "                            f\"- MAE: {mae:.4f}\\n\"\n",
        "                            f\"- Correlación Real vs Predicha: {corr:.4f}\\n\"\n",
        "                            \"\\n\"\n",
        "                            \"Analiza estos valores en contexto: ¿son adecuados? ¿qué sugieren respecto al rendimiento del modelo? \"\n",
        "                            \"Menciona referencias a la gráfica Real vs Predicción y a los residuos si procede.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 6.12. Llamada IA Métricas SVR ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_metrics}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_met = resp.choices[0].message.content.strip()\n",
        "                        if explanation_met:\n",
        "                            titulo_exp_met = f\"### 📝 Explicación IA Métricas SVR ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_met, explanation_met))\n",
        "                            print(f\"[DEBUG] 6.13. Sección explicación IA métricas añadida para método: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibió IA Métricas SVR ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA métricas para método '{metodo}': {e}\")\n",
        "\n",
        "                # 5) Comparativa global de métricas SVR\n",
        "                if metrics_summary:\n",
        "                    try:\n",
        "                        df_comp = _pd.DataFrame(metrics_summary)\n",
        "                        df_comp_sorted = df_comp.sort_values(\"rmse\")\n",
        "                        titulo_comp = \"### Comparativa Métricas SVR entre Métodos\"\n",
        "                        self.sections.append((titulo_comp, df_comp_sorted))\n",
        "                        print(\"[DEBUG] 6.14. Sección comparativa métricas SVR añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame comparativo métricas SVR: {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_conc = (\n",
        "                            \"Se han entrenado varios modelos SVR con diferentes métodos de selección de variables.\\n\"\n",
        "                            \"Métricas obtenidas en test:\\n\"\n",
        "                        )\n",
        "                        for entry in metrics_summary:\n",
        "                            prompt_conc += f\"- Método '{entry['metodo']}': RMSE={entry['rmse']}, MAE={entry['mae']}, R2={entry['r2']}\\n\"\n",
        "                        prompt_conc += (\n",
        "                            \"\\nPor favor, proporciona conclusiones profesionales comparando estos métodos: \"\n",
        "                            \"indica cuál se comporta mejor, posibles razones y recomendaciones sobre selección de variables o ajustes para mejorar SVR.\"\n",
        "                        )\n",
        "                        print(\"[DEBUG] 6.15. Llamada IA Conclusiones SVR...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_conc}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_conc = resp.choices[0].message.content.strip()\n",
        "                        if explanation_conc:\n",
        "                            titulo_exp_conc = \"### 📝 Conclusiones IA Entrenamiento SVR\"\n",
        "                            self.sections.append((titulo_exp_conc, explanation_conc))\n",
        "                            print(\"[DEBUG] 6.16. Sección explicación IA conclusiones SVR añadida\")\n",
        "                        else:\n",
        "                            print(\"[DEBUG] No se recibió IA Conclusiones SVR\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA conclusiones SVR: {e}\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No están RESUMEN_METODOS o X_test/Y_test en globals(), omito sección SVR\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar sección SVR en informe: {e}\")\n",
        "        # ... fin de la sección de entrenamiento SVR ...\n",
        "\n",
        "        # ==============================\n",
        "        # 6.1. Interpretación xIA para modelo entrenado SVR\n",
        "        # ==============================\n",
        "        import openai\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # Verificación previa\n",
        "        # ----------------------------------------------------------------\n",
        "        try:\n",
        "            print(\"[DEBUG] 6.17. Iniciando sección xIA para SVR\")\n",
        "            if 'xai_results' not in globals() or 'SVR' not in xai_results:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontró `xai_results['SVR']`. \"\n",
        "                    \"Asegúrate de haber ejecutado la Celda 10 y almacenado los resultados xIA de SVR en `xai_results['SVR']`.\"\n",
        "                )\n",
        "\n",
        "                # Cabecera\n",
        "                self.sections.append((\n",
        "                    \"## 🔍 Análisis xIA de SVR: Resultados concretos y explicaciones Generativas\",\n",
        "                    \"\"  # contenido vacío, la cabecera se mostrará como Markdown\n",
        "                ))\n",
        "\n",
        "\n",
        "            # Función para llamar a OpenAI con un prompt específico\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                \"\"\"\n",
        "                Llama a OpenAI ChatCompletion con un sistema experto en ML/XAI,\n",
        "                devuelve la respuesta de la IA en texto.\n",
        "                \"\"\"\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un asistente experto en Machine Learning e interpretabilidad de modelos. \"\n",
        "                                \"Proporciona explicaciones detalladas y basadas en los datos concretos proporcionados.\"\n",
        "                            )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    texto = response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    texto = f\"[Error llamando a OpenAI: {e}]\"\n",
        "                return texto\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Configuración: top N variables y primeras M muestras locales\n",
        "            # ----------------------------------------------------------------\n",
        "            TOP_N = 5      # cuántas características top incluir en el prompt\n",
        "            N_LOCAL = 3    # cuántas muestras locales incluir en prompt\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Lista de métodos xIA y claves en xai_results['SVR']\n",
        "            # ----------------------------------------------------------------\n",
        "            # Las claves deben coincidir exactamente con las usadas en Celda 10 al almacenar resultados.\n",
        "            metodos_claves = [\n",
        "                ('SHAP', 'SHAP'),\n",
        "                ('LIME', 'LIME'),\n",
        "                ('KernelExplainer', 'KernelExplainer'),\n",
        "                ('Integrated Gradients', 'Integrated Gradients'),\n",
        "                ('DeepLIFT / LRP', 'DeepLIFT / LRP'),\n",
        "                ('Permutation Feature Importance', 'Permutation Feature Importance'),\n",
        "                ('Partial Dependence Plots (PDP)', 'Partial Dependence Plots (PDP)'),\n",
        "                ('Accumulated Local Effects (ALE)', 'Accumulated Local Effects (ALE)'),\n",
        "                ('Individual Conditional Expectation (ICE) Plots', 'Individual Conditional Expectation (ICE) Plots'),\n",
        "                ('Counterfactual Explanations', 'Counterfactual Explanations'),\n",
        "                ('Anchors', 'Anchors'),\n",
        "                ('Surrogate Models (Global/Local)', 'Surrogate Models (Global/Local)'),\n",
        "                ('Explainable Boosting Machine (EBM)', 'Explainable Boosting Machine (EBM)'),\n",
        "                ('Optuna Hyperparameter Importance', 'Optuna Hyperparameter Importance'),\n",
        "            ]\n",
        "\n",
        "            for titulo, clave in metodos_claves:\n",
        "                print(f\"[DEBUG] 6.18. Procesando sección xIA: {titulo}\")\n",
        "                datos = xai_results['SVR'].get(clave)\n",
        "                if datos is None:\n",
        "                    print(f\"[DEBUG] No hay resultados xIA para {titulo}, se omite\")\n",
        "                    continue\n",
        "#                    self.sections.append((\n",
        "#                        f\"### ⚠️ No hay resultados para {titulo}\",\n",
        "#                        f\"No se hallaron resultados para la clave '{clave}'.\"\n",
        "#                    ))\n",
        "#                    continue\n",
        "\n",
        "                # ---------------- Mostrar figura guardada ----------------\n",
        "                print(f\"[DEBUG] 6.19. Mostrando figura para {titulo}\")\n",
        "                fig = datos.get('fig_summary') or datos.get('fig')\n",
        "                if fig is not None:\n",
        "                    self.sections.append((f\"### {titulo}: Gráfico\", fig))\n",
        "\n",
        "                # --------------- Mostrar DataFrames de importancia y local ---------------\n",
        "                print(f\"[DEBUG] 6.20. Mostrando DataFrames para {titulo}\")\n",
        "                imp_df = datos.get('imp_df')\n",
        "                df_local = datos.get('df_local')\n",
        "                stats_extra = datos.get('stats', None)  # opcional: estadísticas adicionales, p.ej. percentiles, pos_pct SHAP, etc.\n",
        "\n",
        "                if isinstance(imp_df, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Importancia global\", imp_df.reset_index(drop=True)))\n",
        "\n",
        "                if isinstance(df_local, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Valores locales\", df_local.head(N_LOCAL)))\n",
        "\n",
        "                # --------------------------------- Extraer datos del gráfico (suponiendo que sea un barh con 'feature' y 'importance') ---------------------------\n",
        "                features = [text.get_text() for text in ax.get_yticklabels()]\n",
        "                importances = [bar.get_width() for bar in ax.patches]\n",
        "\n",
        "                # --------------- Construir prompt con valores numéricos concretos ---------------\n",
        "                print(f\"[DEBUG] 6.21. Construyendo prompt para {titulo}\")\n",
        "                prompt = f\"He aplicado el método xIA '{titulo}' al modelo SVR entrenado y he obtenido estos resultados concretos:\\n\\n\"\n",
        "\n",
        "                prompt += \"- Datos del gráfico (feature vs importancia):\\n\"\n",
        "                for f, imp in zip(features, importances):\n",
        "                    prompt += f\"    • {f}: {imp:.4f}\\n\"\n",
        "\n",
        "                # Ahora sí le pides que interprete el gráfico:\n",
        "                prompt += (\n",
        "                    \"- Interpreta el gráfico anterior: \"\n",
        "                    \"describe qué patrones o relaciones visuales revela cómo se distribuye la importancia.\\n\"\n",
        "                )\n",
        "\n",
        "                # 1) Extraer importancia global: top N\n",
        "                if isinstance(imp_df, pd.DataFrame) and not imp_df.empty:\n",
        "                    imp_df = imp_df.reset_index()\n",
        "                    imp_df = imp_df.rename(columns={ imp_df.columns[0]: \"feature\" })\n",
        "                    cols = imp_df.columns.tolist()\n",
        "                    if len(cols) >= 2:\n",
        "                        feat_col = \"feature\"\n",
        "                        val_col  = cols[1]\n",
        "                        try:\n",
        "                            imp_df_sorted = imp_df.sort_values(val_col, ascending=False)\n",
        "                        except Exception:\n",
        "                            imp_df_sorted = imp_df\n",
        "\n",
        "                        top_n      = min(TOP_N, len(imp_df_sorted))\n",
        "                        top_imp_df = imp_df_sorted.iloc[:top_n][[feat_col, val_col]]\n",
        "                        top_imp_list = []\n",
        "                        for _, row in top_imp_df.iterrows():\n",
        "                            raw = row[val_col]\n",
        "                            try:\n",
        "                                v = float(raw)\n",
        "                            except Exception:\n",
        "                                v = raw  # si no se puede convertir, lo dejo tal cual\n",
        "                            top_imp_list.append({feat_col: row[feat_col], val_col: v})\n",
        "\n",
        "                        prompt += f\"- Top {top_n} características por importancia global ({feat_col}, {val_col}):\\n  {top_imp_list}\\n\"\n",
        "\n",
        "                        extra_cols = cols[2:]\n",
        "                        if extra_cols:\n",
        "                            prompt += f\"  (El DataFrame de importancia global incluye también columnas: {extra_cols}.)\\n\"\n",
        "                    else:\n",
        "                        snippet = imp_df.head(TOP_N).to_dict(orient='records')\n",
        "                        prompt += f\"- Importancia global (primeras filas, formato genérico):\\n  {snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de importancia global.\\n\"\n",
        "\n",
        "#                    # — Siempre sacamos el índice como columna llamada \"feature\"\n",
        "                    imp_df = imp_df.rename(columns={ idx_col: \"feature\" })\n",
        "\n",
        "                # 2) Extraer valores locales: primeras M muestras\n",
        "                if isinstance(df_local, pd.DataFrame) and not df_local.empty:\n",
        "                    n_loc = min(N_LOCAL, len(df_local))\n",
        "                    loc_snippet = df_local.head(n_loc).to_dict(orient='records')\n",
        "                    prompt += f\"- Valores locales para las primeras {n_loc} muestras (cada dict mapea feature a valor/atribución):\\n  {loc_snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de valores locales.\\n\"\n",
        "\n",
        "                # 3) Incluir estadísticas extra si existen\n",
        "                if isinstance(stats_extra, dict):\n",
        "                    prompt += \"- Estadísticas adicionales:\\n\"\n",
        "                    for k, v in stats_extra.items():\n",
        "                        prompt += f\"  * {k}: {v}\\n\"\n",
        "\n",
        "                # 4) Contexto general del modelo SVR\n",
        "                prompt += (\n",
        "                    \"\\nContexto: El modelo SVR fue entrenado con variables seleccionadas y StandardScaler, \"\n",
        "                    \"con predicciones desescaladas. Ahora interpretamos los resultados xIA para este SVR.\\n\"\n",
        "                )\n",
        "\n",
        "                # 5) Preguntas/pautas específicas según el método\n",
        "                if clave == 'SHAP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores concretos de SHAP para:\\n\"\n",
        "                        \"1. Explicar la importancia global de cada variable en base a los valores de SHAP mostrados.\\n\"\n",
        "                        \"2. Analizar los valores SHAP de las primeras muestras: si una variable muestra SHAP positivo alto, ¿qué implica sobre la predicción en ese caso? Y si es negativo, ¿qué implica?\\n\"\n",
        "                        \"3. Identificar patrones en SHAP (por ejemplo, variables que consistentemente empujan en una dirección) y cómo afecta al comportamiento general del SVR.\\n\"\n",
        "                        \"4. Sugerir posibles transformaciones de variables o validaciones adicionales basadas en estos resultados SHAP.\\n\"\n",
        "                    )\n",
        "                elif clave == 'LIME':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos pesos LIME concretos para:\\n\"\n",
        "                        \"1. Explicar el significado de pesos positivos y negativos en LIME para SVR.\\n\"\n",
        "                        \"2. Analizar casos de las primeras muestras: si una variable tiene peso LIME fuerte (positivo/negativo), ¿qué implica para la predicción local?\\n\"\n",
        "                        \"3. Comentar si la variabilidad de los pesos sugiere relaciones no lineales o interacciones no capturadas.\\n\"\n",
        "                        \"4. Recomendar acciones: agrupación de variables, detección de outliers, etc., basadas en la interpretación LIME.\\n\"\n",
        "                    )\n",
        "                elif clave == 'KernelExplainer':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de KernelExplainer (SHAP caja negra) para:\\n\"\n",
        "                        \"1. Explicar la importancia global según los valores medios absolutos Kernel SHAP.\\n\"\n",
        "                        \"2. Analizar los valores locales para las primeras muestras: cómo cada característica empuja la predicción en cada caso.\\n\"\n",
        "                        \"3. Comparar con SHAP (si ya lo hiciste con TreeExplainer para otro modelo o Kernel SHAP para SVR), si aplica.\\n\"\n",
        "                        \"4. Sugerir consideraciones sobre fondo (background) usado y posibles ajustes si las explicaciones muestran comportamiento inesperado.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Integrated Gradients':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Integrated Gradients para:\\n\"\n",
        "                        \"1. Explicar la contribución integrada de cada variable: interpretación de importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: qué implicaciones tiene un valor IG alto o bajo en cada variable.\\n\"\n",
        "                        \"3. Señalar limitaciones: compatibilidad con SVR no diferenciable; si estos valores provienen de un modelo aproximado, comentar fiabilidad.\\n\"\n",
        "                        \"4. Sugerir pasos adicionales o comparaciones con otros métodos (SHAP/LIME) para validar interpretaciones.\\n\"\n",
        "                    )\n",
        "                elif clave == 'DeepLIFT / LRP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de DeepLIFT / LRP para:\\n\"\n",
        "                        \"1. Explicar la relevancia asignada a cada variable en importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: si una variable tiene relevancia positiva o negativa, qué significa para la predicción.\\n\"\n",
        "                        \"3. Comparar con IG o SHAP si se dispone: consistencia de atribuciones.\\n\"\n",
        "                        \"4. Recomendar verificaciones o transformaciones en caso de interpretaciones inesperadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Permutation Feature Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Permutation Feature Importance para:\\n\"\n",
        "                        \"1. Explicar la caída en la métrica al permutar cada variable: por qué ciertas variables son críticas.\\n\"\n",
        "                        \"2. Comentar la desviación estándar: ¿indica inestabilidad en la importancia? ¿Dónde conviene reforzar validaciones?\\n\"\n",
        "                        \"3. Comparar con importancias de SHAP/LIME: similitudes o diferencias.\\n\"\n",
        "                        \"4. Sugerir prioridades para ajuste de modelo o selección de variables basadas en esta métrica.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Partial Dependence Plots (PDP)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos PDP para:\\n\"\n",
        "                        \"1. Explicar el efecto promedio de cada variable sobre la predicción según el rango PDP obtenido.\\n\"\n",
        "                        \"2. Señalar si los rangos sugieren relaciones monótonas o no lineales.\\n\"\n",
        "                        \"3. Advertir sobre correlaciones fuertes que puedan afectar la interpretación.\\n\"\n",
        "                        \"4. Recomendar posibles exploraciones adicionales (PDP bivariados, transformaciones) según los resultados.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Accumulated Local Effects (ALE)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ALE y valores locales ALE para:\\n\"\n",
        "                        \"1. Explicar cómo ALE corrige artefactos de correlación y qué nos dicen los valores concretos.\\n\"\n",
        "                        \"2. Interpretar importancia global ALE: variables con mayor efecto acumulado.\\n\"\n",
        "                        \"3. Analizar heterogeneidad local a partir de valores ALE de primeras muestras.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales (ALE 2D, inspección de distribución) según hallazgos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Individual Conditional Expectation (ICE) Plots':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ICE medios para:\\n\"\n",
        "                        \"1. Explicar qué mide el rango ICE y su diferencia respecto a PDP.\\n\"\n",
        "                        \"2. Analizar heterogeneidad: variables con alta variabilidad en rangos ICE indican interacciones o comportamiento inestable.\\n\"\n",
        "                        \"3. Señalar implicaciones para el modelo y posibles ajustes si hay alto efecto local variable.\\n\"\n",
        "                        \"4. Recomendar exploraciones adicionales para entender la variabilidad local.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Counterfactual Explanations':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Contrafactuales para:\\n\"\n",
        "                        \"1. Explicar cómo interpretar los contrafactuales: cambios en variables que generan aumento en predicción.\\n\"\n",
        "                        \"2. Analizar variables con mayor |Δ| medio: implicaciones sobre sensibilidad del modelo.\\n\"\n",
        "                        \"3. Señalar si faltan contrafactuales para algunas muestras: qué puede indicar (limites del modelo o datos).\\n\"\n",
        "                        \"4. Sugerir cómo usar estos insights para ajuste de modelo o recolección de datos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Anchors':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Anchors para:\\n\"\n",
        "                        \"1. Explicar cómo leer las reglas ancla de las primeras muestras: condiciones que aseguran la predicción.\\n\"\n",
        "                        \"2. Analizar frecuencia global de aparición de variables en reglas: implicaciones sobre estabilidad y sesgos.\\n\"\n",
        "                        \"3. Señalar regiones de bajo coverage o baja precisión: dónde el modelo es menos fiable.\\n\"\n",
        "                        \"4. Recomendar acciones: recolección de datos, refinamiento de variables o validaciones dirigidas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Surrogate Models (Global/Local)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Surrogate Models para:\\n\"\n",
        "                        \"1. Explicar la importancia global del surrogate (árbol) y la importancia local media (coeficientes regresiones locales).\\n\"\n",
        "                        \"2. Comparar global vs local: variables con alta importancia local pero baja global, o viceversa, y qué sugiere.\\n\"\n",
        "                        \"3. Concluir sobre consistencia de comportamiento del SVR en diferentes regiones del espacio.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales o ajustes de modelo según discrepancias detectadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Explainable Boosting Machine (EBM)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de EBM para:\\n\"\n",
        "                        \"1. Explicar la importancia global según EBM: cómo se comparan con otros métodos.\\n\"\n",
        "                        \"2. Analizar contribuciones locales de las primeras muestras: qué patrones se observan.\\n\"\n",
        "                        \"3. Señalar si EBM revela interacciones no consideradas en SVR.\\n\"\n",
        "                        \"4. Recomendar posibles ajustes en características o validaciones según insights de EBM.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Optuna Hyperparameter Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Optuna para:\\n\"\n",
        "                        \"1. Explicar la importancia global de hiperparámetros en la optimización del SVR.\\n\"\n",
        "                        \"2. Analizar top trials si están disponibles: qué combinaciones de hiperparámetros funcionaron mejor.\\n\"\n",
        "                        \"3. Señalar limitaciones de la muestra de trials (número de pruebas) y posibles riesgos de sobreajuste en la búsqueda.\\n\"\n",
        "                        \"4. Recomendar próximas acciones para tuning basadas en estas importancias.\\n\"\n",
        "                    )\n",
        "                else:\n",
        "                    prompt += \"\\nPor favor, explica estos resultados numéricos y qué implicaciones tienen para el modelo SVR.\\n\"\n",
        "\n",
        "                # --------------- Llamada a OpenAI ---------------\n",
        "                print(f\"[DEBUG] 6.22. Llamando a OpenAI para {titulo}\")\n",
        "                explicacion = call_openai_explanation(prompt)\n",
        "                self.sections.append((f\"### {titulo}: Explicación Generativa\", explicacion))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en sección xIA SVR\",\n",
        "                f\"Se produjo un error al generar la sección xIA de SVR: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =============================================================\n",
        "        # 7. Entrenamiento Modelo NN\n",
        "        # =============================================================\n",
        "        try:\n",
        "            # Comprobamos RESUMEN_METODOS y existencia de X_test/Y_test\n",
        "            if \"RESUMEN_METODOS\" in self.g and \"X_test\" in self.g and \"Y_test\" in self.g:\n",
        "                import os, pickle\n",
        "                import numpy as np\n",
        "                import pandas as _pd\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "                from tensorflow.keras.models import load_model\n",
        "\n",
        "                X_test_full = self.g[\"X_test\"]\n",
        "                Y_test = self.g[\"Y_test\"]\n",
        "                # Preparamos array 1D de y_test\n",
        "                if hasattr(Y_test, \"values\"):\n",
        "                    arr = Y_test.values\n",
        "                    if arr.ndim == 2 and arr.shape[1] == 1:\n",
        "                        y_test_arr = arr[:, 0]\n",
        "                    elif arr.ndim == 2 and arr.shape[1] > 1:\n",
        "                        y_test_arr = arr[:, 0]\n",
        "                    else:\n",
        "                        y_test_arr = arr.ravel()\n",
        "                else:\n",
        "                    y_test_arr = np.array(Y_test).ravel()\n",
        "                # Aseguramos 1D\n",
        "                y_test_arr = y_test_arr.ravel()\n",
        "\n",
        "                metrics_summary_nn = []\n",
        "                # Iteramos sobre cada método declarado en RESUMEN_METODOS\n",
        "                for metodo, vars_sel in self.g[\"RESUMEN_METODOS\"].items():\n",
        "                    metodo_low = metodo.lower()\n",
        "                    # Rutas a los archivos NN guardados en celda 7.2\n",
        "                    model_fname = f\"modelo_nn_{metodo_low}.h5\"\n",
        "                    scaler_fname = f\"escaladores_nn_{metodo_low}.pkl\"\n",
        "                    hp_fname = f\"hyperparams_nn_{metodo_low}.pkl\"\n",
        "                    #hyper_fname = f\"hyperparams_nn_{metodo_low}.pkl\"\n",
        "                    if not os.path.exists(model_fname) or not os.path.exists(scaler_fname):\n",
        "                        print(f\"[DEBUG] Fichero de modelo NN o escaladores no encontrado para método '{metodo}': omito este método\")\n",
        "                        continue\n",
        "                    # Cargar modelo y escaladores\n",
        "                    try:\n",
        "                        model = load_model(model_fname)\n",
        "                        with open(scaler_fname, \"rb\") as f:\n",
        "                            data_s = pickle.load(f)\n",
        "                        sx = data_s.get(\"scaler_X\", None)\n",
        "                        sy = data_s.get(\"scaler_Y\", None)\n",
        "                        cols = data_s.get(\"cols\", None)\n",
        "                        # y_variable_name = data_s.get(\"yname\", None)\n",
        "                        if model is None or sx is None or sy is None or cols is None:\n",
        "                            print(f\"[DEBUG] Falta clave en escaladores NN para método '{metodo}', omito\")\n",
        "                            continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al cargar modelo/escaladores NN para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Verificar columnas en X_test\n",
        "                    missing = [c for c in cols if c not in X_test_full.columns]\n",
        "                    if missing:\n",
        "                        print(f\"[DEBUG] Para método '{metodo}', faltan columnas en X_test: {missing}, omito\")\n",
        "                        continue\n",
        "\n",
        "                    # Subconjunto X_test\n",
        "                    X_test_sel = X_test_full[cols].copy()\n",
        "                    # Escalar, predecir y desescalar\n",
        "                    try:\n",
        "                        X_test_scaled = sx.transform(X_test_sel)\n",
        "                        #y_pred_scaled = model.predict(X_test_scaled).ravel()\n",
        "                        y_pred_scaled = model.predict(X_test_scaled, verbose=0).ravel()\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al predecir/desescalar NN para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # 1) Cargar hiperparámetros\n",
        "                    try:\n",
        "                        hp_fname = f\"hyperparams_nn_{metodo.lower()}.pkl\"\n",
        "                        if os.path.exists(hp_fname):\n",
        "                            with open(hp_fname, \"rb\") as f:\n",
        "                                hp = pickle.load(f)\n",
        "                        else:\n",
        "                            hp = None\n",
        "                        # Representar en DataFrame\n",
        "                        if hp:\n",
        "                            import pandas as _pd\n",
        "                            df_hp = _pd.DataFrame({\n",
        "                                \"Hiperparámetro\": list(hp.keys()),\n",
        "                                \"Valor\": [str(v) for v in hp.values()]\n",
        "                            })\n",
        "                            titulo_hp = f\"### Parámetros de Entrenamiento NN ({metodo})\"\n",
        "                            self.sections.append((titulo_hp, df_hp))\n",
        "                            print(f\"[DEBUG] 7.1. Sección parámetros NN añadida para método: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No hay hyperparams guardados para NN método '{metodo}'\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al extraer/parámetros NN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA hiperparámetros NN\n",
        "                    try:\n",
        "                        if hp:\n",
        "                            prompt_params = (\n",
        "                                f\"Has entrenado un modelo de Red Neuronal con selección de variables '{metodo}'.\\n\"\n",
        "                                \"Estos fueron los hiperparámetros utilizados:\\n\"\n",
        "                            )\n",
        "                            for k, v in hp.items():\n",
        "                                prompt_params += f\"- {k}: {v}\\n\"\n",
        "                            prompt_params += (\n",
        "                                \"\\nPor favor, explica de forma profesional y detallada cómo estos hiperparámetros \"\n",
        "                                \"pueden influir en el entrenamiento de la red neuronal, su impacto en ajuste, \"\n",
        "                                \"y buenas prácticas para seleccionarlos o afinarlos.\"\n",
        "                            )\n",
        "                            print(f\"[DEBUG] 7.2. Iniciando llamada a OpenAI para explicación hiperparámetros NN ({metodo})...\")\n",
        "                            resp = _client.chat.completions.create(\n",
        "                                model=\"gpt-4\",\n",
        "                                messages=[\n",
        "                                    {\"role\": \"system\", \"content\": \"Eres un experto en entrenamiento de redes neuronales para regresión.\"},\n",
        "                                    {\"role\": \"user\", \"content\": prompt_params}\n",
        "                                ],\n",
        "                                max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                                temperature=TEMPERATURE_VAL\n",
        "                            )\n",
        "                            explanation_hp = resp.choices[0].message.content.strip()\n",
        "                            if explanation_hp:\n",
        "                                titulo_exp_hp = f\"### 📝 Explicación IA Hiperparámetros NN ({metodo})\"\n",
        "                                self.sections.append((titulo_exp_hp, explanation_hp))\n",
        "                                print(f\"[DEBUG] 7.3. Sección explicación IA hiperparámetros NN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA hiperparámetros NN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 2) Gráfica Predicho vs Real\n",
        "                    try:\n",
        "                        import matplotlib.pyplot as plt\n",
        "                        fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "                        ax1.scatter(y_test_arr, y_pred, alpha=0.6)\n",
        "                        ax1.plot([y_test_arr.min(), y_test_arr.max()],\n",
        "                                 [y_test_arr.min(), y_test_arr.max()],\n",
        "                                 'r--', lw=2)\n",
        "                        ax1.set_xlabel(\"Y real\")\n",
        "                        ax1.set_ylabel(\"Y predicho\")\n",
        "                        ax1.set_title(f\"NN Predicho vs Real ({metodo})\")\n",
        "                        titulo_fig1 = f\"### Gráfica NN Predicho vs Real ({metodo})\"\n",
        "                        self.sections.append((titulo_fig1, fig1))\n",
        "                        print(f\"[DEBUG] 7.4. Sección gráfica Pred vs Real NN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear gráfica Pred vs Real NN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Estadísticas para contexto IA Pred vs Real\n",
        "                    try:\n",
        "                        mse = mean_squared_error(y_test_arr, y_pred)\n",
        "                        rmse = np.sqrt(mse)\n",
        "                        mae = mean_absolute_error(y_test_arr, y_pred)\n",
        "                        r2 = r2_score(y_test_arr, y_pred)\n",
        "                        corr = np.corrcoef(y_test_arr, y_pred)[0,1] if len(y_test_arr)>1 else None\n",
        "                        y_real_min, y_real_max = float(np.min(y_test_arr)), float(np.max(y_test_arr))\n",
        "                        y_pred_min, y_pred_max = float(np.min(y_pred)), float(np.max(y_pred))\n",
        "                    except Exception:\n",
        "                        mse = rmse = mae = r2 = corr = None\n",
        "                        y_real_min = y_real_max = y_pred_min = y_pred_max = None\n",
        "\n",
        "                    # Explicación IA Pred vs Real NN con contexto numérico\n",
        "                    try:\n",
        "                        prompt_pr = (\n",
        "                            f\"A continuación tienes datos de la gráfica de comparación Real vs Predicción para el modelo NN con método '{metodo}':\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- R²: {r2}\\n\"\n",
        "                            f\"- Correlación entre Y real y predicha: {corr}\\n\"\n",
        "                            f\"- Rango Y real: [{y_real_min}, {y_real_max}]\\n\"\n",
        "                            f\"- Rango Y predicha: [{y_pred_min}, {y_pred_max}]\\n\\n\"\n",
        "                            \"Basándote en estos valores y en la gráfica generada (Real vs Predicción), \"\n",
        "                            \"proporciona un análisis detallado, señalando si hay sesgos sistemáticos (por ejemplo, subestimación o sobreestimación en ciertos rangos), \"\n",
        "                            \"si la dispersión es mayor en algún rango de predicción, y qué implicaciones tiene para la calidad del modelo. \"\n",
        "                            \"Usa un texto profesional y bien estructurado, y menciona qué indicios de la gráfica respaldan tus conclusiones.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 7.5. Llamada IA Pred vs Real NN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_pr}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_pr = resp.choices[0].message.content.strip()\n",
        "                        if explanation_pr:\n",
        "                            titulo_exp_pr = f\"### 📝 Explicación IA Predicho vs Real NN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_pr, explanation_pr))\n",
        "                            print(f\"[DEBUG] 7.6. Sección explicación IA Pred vs Real NN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA Pred vs Real NN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 3) Gráfica de residuos NN\n",
        "                    try:\n",
        "                        residuals = y_test_arr - y_pred\n",
        "                        res_mean = float(np.mean(residuals))\n",
        "                        res_std  = float(np.std(residuals))\n",
        "                        # Estadísticos de residuos con pandas\n",
        "                        res_series = _pd.Series(residuals)\n",
        "                        res_skew = float(res_series.skew())\n",
        "                        res_kurt = float(res_series.kurtosis())\n",
        "                        q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]\n",
        "                        # Rango de Y real y predicha\n",
        "                        # (ya lo tenemos en y_real_min, etc.)\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "                        ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "                        ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "                        ax2.set_xlabel(\"Y predicho\")\n",
        "                        ax2.set_ylabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "                        ax2.set_title(f\"NN Residuos ({metodo})\")\n",
        "                        titulo_fig2 = f\"### Gráfica NN Residuos ({metodo})\"\n",
        "                        self.sections.append((titulo_fig2, fig2))\n",
        "                        print(f\"[DEBUG] 7.7. Sección gráfica residuos NN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear gráfica residuos NN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA Residuos NN con contexto numérico\n",
        "                    try:\n",
        "                        prompt_res = (\n",
        "                            f\"A continuación tienes estadísticas de los residuos (Real - Predicha) del modelo NN con método '{metodo}':\\n\"\n",
        "                            f\"- Media: {res_mean}\\n\"\n",
        "                            f\"- Desviación estándar: {res_std}\\n\"\n",
        "                            f\"- Asimetría: {res_skew}\\n\"\n",
        "                            f\"- Curtosis: {res_kurt}\\n\"\n",
        "                            f\"- Cuantiles: 25%={q25}, 50%={q50}, 75%={q75}\\n\\n\"\n",
        "                            \"Basándote en estos valores y en la gráfica de residuos, analiza si hay patrones (por ejemplo, heterocedasticidad, outliers, sesgos en rangos), \"\n",
        "                            \"y comenta qué implicaciones tiene para la robustez y generalización del modelo. \"\n",
        "                            \"Usa un texto profesional y bien estructurado.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 7.8. Llamada IA Residuos NN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_res}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_res = resp.choices[0].message.content.strip()\n",
        "                        if explanation_res:\n",
        "                            titulo_exp_res = f\"### 📝 Explicación IA Residuos NN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_res, explanation_res))\n",
        "                            print(f\"[DEBUG] 7.9. Sección explicación IA residuos NN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA residuos NN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 4) Tabla Métricas NN y explicación IA\n",
        "                    try:\n",
        "                        df_met = _pd.DataFrame([\n",
        "                            {\"Métrica\": \"RMSE\", \"Valor\": rmse},\n",
        "                            {\"Métrica\": \"MAE\", \"Valor\": mae},\n",
        "                            {\"Métrica\": \"R2\",  \"Valor\": r2}\n",
        "                        ])\n",
        "                        titulo_met = f\"### Métricas NN ({metodo})\"\n",
        "                        self.sections.append((titulo_met, df_met))\n",
        "                        print(f\"[DEBUG] 7.10. Sección métricas NN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame métricas NN para método '{metodo}': {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_met = (\n",
        "                            f\"Estas son las métricas del modelo NN con método '{metodo}':\\n\"\n",
        "                            f\"- R2: {r2}\\n\"\n",
        "                            f\"- MSE: {mse}\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- Correlación Real vs Predicha: {corr}\\n\\n\"\n",
        "                            \"Analiza estos valores en contexto: ¿son adecuados? ¿qué sugieren respecto al rendimiento del modelo? \"\n",
        "                            \"Menciona referencias a la gráfica Real vs Predicción y a los residuos si procede.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 7.11. Llamada IA Métricas NN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_met}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_met = resp.choices[0].message.content.strip()\n",
        "                        if explanation_met:\n",
        "                            titulo_exp_met = f\"### 📝 Explicación IA Métricas NN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_met, explanation_met))\n",
        "                            print(f\"[DEBUG] 7.12. Sección explicación IA métricas NN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA métricas NN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Acumular para comparativa\n",
        "                    metrics_summary_nn.append({\n",
        "                        \"metodo\": metodo,\n",
        "                        \"rmse\": rmse,\n",
        "                        \"mae\": mae,\n",
        "                        \"r2\": r2\n",
        "                    })\n",
        "\n",
        "                # 5) Comparativa global de métricas NN\n",
        "                if metrics_summary_nn:\n",
        "                    try:\n",
        "                        df_comp_nn = _pd.DataFrame(metrics_summary_nn)\n",
        "                        df_comp_nn_sorted = df_comp_nn.sort_values(\"rmse\")\n",
        "                        titulo_comp_nn = \"### Comparativa Métricas NN entre Métodos\"\n",
        "                        self.sections.append((titulo_comp_nn, df_comp_nn_sorted))\n",
        "                        print(\"[DEBUG] 7.13. Sección comparativa métricas NN añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame comparativo métricas NN: {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_conc_nn = (\n",
        "                            \"Se han entrenado varios modelos de Red Neuronal con diferentes métodos de selección de variables.\\n\"\n",
        "                            \"Métricas obtenidas en test:\\n\"\n",
        "                        )\n",
        "                        for entry in metrics_summary_nn:\n",
        "                            prompt_conc_nn += f\"- Método '{entry['metodo']}': RMSE={entry['rmse']}, MAE={entry['mae']}, R2={entry['r2']}\\n\"\n",
        "                        prompt_conc_nn += (\n",
        "                            \"\\nPor favor, proporciona conclusiones profesionales comparando estos métodos: \"\n",
        "                            \"indica cuál se comporta mejor, posibles razones y recomendaciones sobre selección de variables o ajustes para mejorar la red neuronal.\"\n",
        "                        )\n",
        "                        print(\"[DEBUG] 7.14. Llamada IA Conclusiones NN...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_conc_nn}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_conc_nn = resp.choices[0].message.content.strip()\n",
        "                        if explanation_conc_nn:\n",
        "                            titulo_exp_conc_nn = \"### 📝 Conclusiones IA Entrenamiento NN\"\n",
        "                            self.sections.append((titulo_exp_conc_nn, explanation_conc_nn))\n",
        "                            print(\"[DEBUG] 7.15. Sección explicación IA conclusiones NN añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA conclusiones NN: {e}\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No están RESUMEN_METODOS o X_test/Y_test en globals(), omito sección NN\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar sección NN en informe: {e}\")\n",
        "        # ... fin de la sección de entrenamiento NN ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 7.1. Interpretación xIA para modelo entrenado NN\n",
        "        # =============================================================\n",
        "        import openai\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # Verificación previa\n",
        "        # ----------------------------------------------------------------\n",
        "        try:\n",
        "            print(\"[DEBUG] 7.16. Iniciando sección xIA para NN\")\n",
        "            if 'xai_results' not in globals() or 'NN' not in xai_results:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontró `xai_results['NN']`. \"\n",
        "                    \"Asegúrate de haber ejecutado la Celda 10 y almacenado los resultados xIA de NN en `xai_results['NN']`.\"\n",
        "                )\n",
        "\n",
        "                # Cabecera\n",
        "                self.sections.append((\n",
        "                    \"## 🔍 Análisis xIA de NN: Resultados concretos y explicaciones Generativas\",\n",
        "                    \"\"  # contenido vacío, la cabecera se mostrará como Markdown\n",
        "                ))\n",
        "\n",
        "\n",
        "            # Función para llamar a OpenAI con un prompt específico\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                \"\"\"\n",
        "                Llama a OpenAI ChatCompletion con un sistema experto en ML/XAI,\n",
        "                devuelve la respuesta de la IA en texto.\n",
        "                \"\"\"\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un asistente experto en Machine Learning e interpretabilidad de modelos. \"\n",
        "                                \"Proporciona explicaciones detalladas y basadas en los datos concretos proporcionados.\"\n",
        "                            )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    texto = response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    texto = f\"[Error llamando a OpenAI: {e}]\"\n",
        "                return texto\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Configuración: top N variables y primeras M muestras locales\n",
        "            # ----------------------------------------------------------------\n",
        "            TOP_N = 5      # cuántas características top incluir en el prompt\n",
        "            N_LOCAL = 3    # cuántas muestras locales incluir en prompt\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Lista de métodos xIA y claves en xai_results['NN']\n",
        "            # ----------------------------------------------------------------\n",
        "            # Las claves deben coincidir exactamente con las usadas en Celda 10 al almacenar resultados.\n",
        "            metodos_claves = [\n",
        "                ('SHAP', 'SHAP'),\n",
        "                ('LIME', 'LIME'),\n",
        "                ('KernelExplainer', 'KernelExplainer'),\n",
        "                ('Integrated Gradients', 'Integrated Gradients'),\n",
        "                ('DeepLIFT / LRP', 'DeepLIFT / LRP'),\n",
        "                ('Permutation Feature Importance', 'Permutation Feature Importance'),\n",
        "                ('Partial Dependence Plots (PDP)', 'Partial Dependence Plots (PDP)'),\n",
        "                ('Accumulated Local Effects (ALE)', 'Accumulated Local Effects (ALE)'),\n",
        "                ('Individual Conditional Expectation (ICE) Plots', 'Individual Conditional Expectation (ICE) Plots'),\n",
        "                ('Counterfactual Explanations', 'Counterfactual Explanations'),\n",
        "                ('Anchors', 'Anchors'),\n",
        "                ('Surrogate Models (Global/Local)', 'Surrogate Models (Global/Local)'),\n",
        "                ('Explainable Boosting Machine (EBM)', 'Explainable Boosting Machine (EBM)'),\n",
        "                ('Optuna Hyperparameter Importance', 'Optuna Hyperparameter Importance'),\n",
        "            ]\n",
        "\n",
        "            for titulo, clave in metodos_claves:\n",
        "                print(f\"[DEBUG] 7.17. Procesando sección xIA: {titulo}\")\n",
        "                datos = xai_results['NN'].get(clave)\n",
        "                if datos is None:\n",
        "                    print(f\"[DEBUG] No hay resultados xIA para {titulo}, se omite\")\n",
        "                    continue\n",
        "#                    self.sections.append((\n",
        "#                        f\"### ⚠️ No hay resultados para {titulo}\",\n",
        "#                        f\"No se hallaron resultados para la clave '{clave}'.\"\n",
        "#                    ))\n",
        "#                    continue\n",
        "\n",
        "                # ---------------- Mostrar figura guardada ----------------\n",
        "                print(f\"[DEBUG] 7.18. Mostrando figura para {titulo}\")\n",
        "                fig = datos.get('fig_summary') or datos.get('fig')\n",
        "                if fig is not None:\n",
        "                    self.sections.append((f\"### {titulo}: Gráfico\", fig))\n",
        "\n",
        "                # --------------- Mostrar DataFrames de importancia y local ---------------\n",
        "                print(f\"[DEBUG] 7.19. Mostrando DataFrames para {titulo}\")\n",
        "                imp_df = datos.get('imp_df')\n",
        "                df_local = datos.get('df_local')\n",
        "                stats_extra = datos.get('stats', None)  # opcional: estadísticas adicionales, p.ej. percentiles, pos_pct SHAP, etc.\n",
        "\n",
        "                if isinstance(imp_df, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Importancia global\", imp_df.reset_index(drop=True)))\n",
        "\n",
        "                if isinstance(df_local, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Valores locales\", df_local.head(N_LOCAL)))\n",
        "\n",
        "                # --------------------------------- Extraer datos del gráfico (suponiendo que sea un barh con 'feature' y 'importance') ---------------------------\n",
        "                features = [text.get_text() for text in ax.get_yticklabels()]\n",
        "                importances = [bar.get_width() for bar in ax.patches]\n",
        "\n",
        "                # --------------- Construir prompt con valores numéricos concretos ---------------\n",
        "                print(f\"[DEBUG] 7.20. Construyendo prompt para {titulo}\")\n",
        "                prompt = f\"He aplicado el método xIA '{titulo}' al modelo NN entrenado y he obtenido estos resultados concretos:\\n\\n\"\n",
        "\n",
        "                prompt += \"- Datos del gráfico (feature vs importancia):\\n\"\n",
        "                for f, imp in zip(features, importances):\n",
        "                    prompt += f\"    • {f}: {imp:.4f}\\n\"\n",
        "\n",
        "                # Ahora sí le pides que interprete el gráfico:\n",
        "                prompt += (\n",
        "                    \"- Interpreta el gráfico anterior: \"\n",
        "                    \"describe qué patrones o relaciones visuales revela cómo se distribuye la importancia.\\n\"\n",
        "                )\n",
        "\n",
        "                # 1) Extraer importancia global: top N\n",
        "                if isinstance(imp_df, pd.DataFrame) and not imp_df.empty:\n",
        "                    imp_df = imp_df.reset_index()\n",
        "                    imp_df = imp_df.rename(columns={ imp_df.columns[0]: \"feature\" })\n",
        "                    cols = imp_df.columns.tolist()\n",
        "                    if len(cols) >= 2:\n",
        "                        feat_col = \"feature\"\n",
        "                        val_col  = cols[1]\n",
        "                        try:\n",
        "                            imp_df_sorted = imp_df.sort_values(val_col, ascending=False)\n",
        "                        except Exception:\n",
        "                            imp_df_sorted = imp_df\n",
        "\n",
        "                        top_n      = min(TOP_N, len(imp_df_sorted))\n",
        "                        top_imp_df = imp_df_sorted.iloc[:top_n][[feat_col, val_col]]\n",
        "                        top_imp_list = []\n",
        "                        for _, row in top_imp_df.iterrows():\n",
        "                            raw = row[val_col]\n",
        "                            try:\n",
        "                                v = float(raw)\n",
        "                            except Exception:\n",
        "                                v = raw  # si no se puede convertir, lo dejo tal cual\n",
        "                            top_imp_list.append({feat_col: row[feat_col], val_col: v})\n",
        "\n",
        "                        prompt += f\"- Top {top_n} características por importancia global ({feat_col}, {val_col}):\\n  {top_imp_list}\\n\"\n",
        "\n",
        "                        extra_cols = cols[2:]\n",
        "                        if extra_cols:\n",
        "                            prompt += f\"  (El DataFrame de importancia global incluye también columnas: {extra_cols}.)\\n\"\n",
        "                    else:\n",
        "                        snippet = imp_df.head(TOP_N).to_dict(orient='records')\n",
        "                        prompt += f\"- Importancia global (primeras filas, formato genérico):\\n  {snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de importancia global.\\n\"\n",
        "\n",
        "#                    # — Siempre sacamos el índice como columna llamada \"feature\"\n",
        "                    imp_df = imp_df.rename(columns={ idx_col: \"feature\" })\n",
        "\n",
        "                # 2) Extraer valores locales: primeras M muestras\n",
        "                if isinstance(df_local, pd.DataFrame) and not df_local.empty:\n",
        "                    n_loc = min(N_LOCAL, len(df_local))\n",
        "                    loc_snippet = df_local.head(n_loc).to_dict(orient='records')\n",
        "                    prompt += f\"- Valores locales para las primeras {n_loc} muestras (cada dict mapea feature a valor/atribución):\\n  {loc_snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de valores locales.\\n\"\n",
        "\n",
        "                # 3) Incluir estadísticas extra si existen\n",
        "                if isinstance(stats_extra, dict):\n",
        "                    prompt += \"- Estadísticas adicionales:\\n\"\n",
        "                    for k, v in stats_extra.items():\n",
        "                        prompt += f\"  * {k}: {v}\\n\"\n",
        "\n",
        "                # 4) Contexto general del modelo NN\n",
        "                prompt += (\n",
        "                    \"\\nContexto: El modelo NN fue entrenado con variables seleccionadas y StandardScaler, \"\n",
        "                    \"con predicciones desescaladas. Ahora interpretamos los resultados xIA para este NN.\\n\"\n",
        "                )\n",
        "\n",
        "                # 5) Preguntas/pautas específicas según el método\n",
        "                if clave == 'SHAP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores concretos de SHAP para:\\n\"\n",
        "                        \"1. Explicar la importancia global de cada variable en base a los valores de SHAP mostrados.\\n\"\n",
        "                        \"2. Analizar los valores SHAP de las primeras muestras: si una variable muestra SHAP positivo alto, ¿qué implica sobre la predicción en ese caso? Y si es negativo, ¿qué implica?\\n\"\n",
        "                        \"3. Identificar patrones en SHAP (por ejemplo, variables que consistentemente empujan en una dirección) y cómo afecta al comportamiento general del NN.\\n\"\n",
        "                        \"4. Sugerir posibles transformaciones de variables o validaciones adicionales basadas en estos resultados SHAP.\\n\"\n",
        "                    )\n",
        "                elif clave == 'LIME':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos pesos LIME concretos para:\\n\"\n",
        "                        \"1. Explicar el significado de pesos positivos y negativos en LIME para NN.\\n\"\n",
        "                        \"2. Analizar casos de las primeras muestras: si una variable tiene peso LIME fuerte (positivo/negativo), ¿qué implica para la predicción local?\\n\"\n",
        "                        \"3. Comentar si la variabilidad de los pesos sugiere relaciones no lineales o interacciones no capturadas.\\n\"\n",
        "                        \"4. Recomendar acciones: agrupación de variables, detección de outliers, etc., basadas en la interpretación LIME.\\n\"\n",
        "                    )\n",
        "                elif clave == 'KernelExplainer':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de KernelExplainer (SHAP caja negra) para:\\n\"\n",
        "                        \"1. Explicar la importancia global según los valores medios absolutos Kernel SHAP.\\n\"\n",
        "                        \"2. Analizar los valores locales para las primeras muestras: cómo cada característica empuja la predicción en cada caso.\\n\"\n",
        "                        \"3. Comparar con SHAP (si ya lo hiciste con TreeExplainer para otro modelo o Kernel SHAP para NN), si aplica.\\n\"\n",
        "                        \"4. Sugerir consideraciones sobre fondo (background) usado y posibles ajustes si las explicaciones muestran comportamiento inesperado.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Integrated Gradients':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Integrated Gradients para:\\n\"\n",
        "                        \"1. Explicar la contribución integrada de cada variable: interpretación de importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: qué implicaciones tiene un valor IG alto o bajo en cada variable.\\n\"\n",
        "                        \"3. Señalar limitaciones: compatibilidad con NN no diferenciable; si estos valores provienen de un modelo aproximado, comentar fiabilidad.\\n\"\n",
        "                        \"4. Sugerir pasos adicionales o comparaciones con otros métodos (SHAP/LIME) para validar interpretaciones.\\n\"\n",
        "                    )\n",
        "                elif clave == 'DeepLIFT / LRP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de DeepLIFT / LRP para:\\n\"\n",
        "                        \"1. Explicar la relevancia asignada a cada variable en importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: si una variable tiene relevancia positiva o negativa, qué significa para la predicción.\\n\"\n",
        "                        \"3. Comparar con IG o SHAP si se dispone: consistencia de atribuciones.\\n\"\n",
        "                        \"4. Recomendar verificaciones o transformaciones en caso de interpretaciones inesperadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Permutation Feature Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Permutation Feature Importance para:\\n\"\n",
        "                        \"1. Explicar la caída en la métrica al permutar cada variable: por qué ciertas variables son críticas.\\n\"\n",
        "                        \"2. Comentar la desviación estándar: ¿indica inestabilidad en la importancia? ¿Dónde conviene reforzar validaciones?\\n\"\n",
        "                        \"3. Comparar con importancias de SHAP/LIME: similitudes o diferencias.\\n\"\n",
        "                        \"4. Sugerir prioridades para ajuste de modelo o selección de variables basadas en esta métrica.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Partial Dependence Plots (PDP)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos PDP para:\\n\"\n",
        "                        \"1. Explicar el efecto promedio de cada variable sobre la predicción según el rango PDP obtenido.\\n\"\n",
        "                        \"2. Señalar si los rangos sugieren relaciones monótonas o no lineales.\\n\"\n",
        "                        \"3. Advertir sobre correlaciones fuertes que puedan afectar la interpretación.\\n\"\n",
        "                        \"4. Recomendar posibles exploraciones adicionales (PDP bivariados, transformaciones) según los resultados.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Accumulated Local Effects (ALE)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ALE y valores locales ALE para:\\n\"\n",
        "                        \"1. Explicar cómo ALE corrige artefactos de correlación y qué nos dicen los valores concretos.\\n\"\n",
        "                        \"2. Interpretar importancia global ALE: variables con mayor efecto acumulado.\\n\"\n",
        "                        \"3. Analizar heterogeneidad local a partir de valores ALE de primeras muestras.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales (ALE 2D, inspección de distribución) según hallazgos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Individual Conditional Expectation (ICE) Plots':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ICE medios para:\\n\"\n",
        "                        \"1. Explicar qué mide el rango ICE y su diferencia respecto a PDP.\\n\"\n",
        "                        \"2. Analizar heterogeneidad: variables con alta variabilidad en rangos ICE indican interacciones o comportamiento inestable.\\n\"\n",
        "                        \"3. Señalar implicaciones para el modelo y posibles ajustes si hay alto efecto local variable.\\n\"\n",
        "                        \"4. Recomendar exploraciones adicionales para entender la variabilidad local.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Counterfactual Explanations':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Contrafactuales para:\\n\"\n",
        "                        \"1. Explicar cómo interpretar los contrafactuales: cambios en variables que generan aumento en predicción.\\n\"\n",
        "                        \"2. Analizar variables con mayor |Δ| medio: implicaciones sobre sensibilidad del modelo.\\n\"\n",
        "                        \"3. Señalar si faltan contrafactuales para algunas muestras: qué puede indicar (limites del modelo o datos).\\n\"\n",
        "                        \"4. Sugerir cómo usar estos insights para ajuste de modelo o recolección de datos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Anchors':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Anchors para:\\n\"\n",
        "                        \"1. Explicar cómo leer las reglas ancla de las primeras muestras: condiciones que aseguran la predicción.\\n\"\n",
        "                        \"2. Analizar frecuencia global de aparición de variables en reglas: implicaciones sobre estabilidad y sesgos.\\n\"\n",
        "                        \"3. Señalar regiones de bajo coverage o baja precisión: dónde el modelo es menos fiable.\\n\"\n",
        "                        \"4. Recomendar acciones: recolección de datos, refinamiento de variables o validaciones dirigidas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Surrogate Models (Global/Local)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Surrogate Models para:\\n\"\n",
        "                        \"1. Explicar la importancia global del surrogate (árbol) y la importancia local media (coeficientes regresiones locales).\\n\"\n",
        "                        \"2. Comparar global vs local: variables con alta importancia local pero baja global, o viceversa, y qué sugiere.\\n\"\n",
        "                        \"3. Concluir sobre consistencia de comportamiento del NN en diferentes regiones del espacio.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales o ajustes de modelo según discrepancias detectadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Explainable Boosting Machine (EBM)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de EBM para:\\n\"\n",
        "                        \"1. Explicar la importancia global según EBM: cómo se comparan con otros métodos.\\n\"\n",
        "                        \"2. Analizar contribuciones locales de las primeras muestras: qué patrones se observan.\\n\"\n",
        "                        \"3. Señalar si EBM revela interacciones no consideradas en NN.\\n\"\n",
        "                        \"4. Recomendar posibles ajustes en características o validaciones según insights de EBM.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Optuna Hyperparameter Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Optuna para:\\n\"\n",
        "                        \"1. Explicar la importancia global de hiperparámetros en la optimización del NN.\\n\"\n",
        "                        \"2. Analizar top trials si están disponibles: qué combinaciones de hiperparámetros funcionaron mejor.\\n\"\n",
        "                        \"3. Señalar limitaciones de la muestra de trials (número de pruebas) y posibles riesgos de sobreajuste en la búsqueda.\\n\"\n",
        "                        \"4. Recomendar próximas acciones para tuning basadas en estas importancias.\\n\"\n",
        "                    )\n",
        "                else:\n",
        "                    prompt += \"\\nPor favor, explica estos resultados numéricos y qué implicaciones tienen para el modelo NN.\\n\"\n",
        "\n",
        "                # --------------- Llamada a OpenAI ---------------\n",
        "                print(f\"[DEBUG] 7.21. Llamando a OpenAI para {titulo}\")\n",
        "                explicacion = call_openai_explanation(prompt)\n",
        "                self.sections.append((f\"### {titulo}: Explicación Generativa\", explicacion))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en sección xIA NN\",\n",
        "                f\"Se produjo un error al generar la sección xIA de NN: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =====================================================================\n",
        "        # 8. Entrenamiento Modelo XGBoost\n",
        "        # =====================================================================\n",
        "        try:\n",
        "            if \"RESUMEN_METODOS\" in self.g and \"X_test\" in self.g and \"Y_test\" in self.g:\n",
        "                import os, pickle\n",
        "                import numpy as np\n",
        "                import pandas as _pd\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "                X_test_full = self.g[\"X_test\"]\n",
        "                Y_test = self.g[\"Y_test\"]\n",
        "                # Extraer array 1D de Y_test\n",
        "                if hasattr(Y_test, \"values\"):\n",
        "                    arr = Y_test.values\n",
        "                    if arr.ndim == 2 and arr.shape[1] >= 1:\n",
        "                        y_test_arr = arr[:, 0].ravel()\n",
        "                    else:\n",
        "                        y_test_arr = arr.ravel()\n",
        "                else:\n",
        "                    y_test_arr = np.array(Y_test).ravel()\n",
        "                y_test_arr = y_test_arr.ravel()\n",
        "\n",
        "                metrics_summary_xgb = []\n",
        "                # Iterar sobre cada método\n",
        "                for metodo, vars_sel in self.g[\"RESUMEN_METODOS\"].items():\n",
        "                    metodo_low = metodo.lower()\n",
        "                    pickle_fname = f\"modelo_xgb_{metodo_low}.pkl\"\n",
        "                    if not os.path.exists(pickle_fname):\n",
        "                        print(f\"[DEBUG] Fichero de modelo XGBoost no encontrado para método '{metodo}': {pickle_fname}, omito este método\")\n",
        "                        continue\n",
        "                    # Cargar modelo y escaladores\n",
        "                    try:\n",
        "                        with open(pickle_fname, \"rb\") as f:\n",
        "                            data = pickle.load(f)\n",
        "                        model = data.get(\"model\", None)\n",
        "                        sx = data.get(\"sx\", None)\n",
        "                        sy = data.get(\"sy\", None)\n",
        "                        cols = data.get(\"cols\", None)\n",
        "                        if model is None or sx is None or sy is None or cols is None:\n",
        "                            print(f\"[DEBUG] Faltan claves en pickle XGBoost para método '{metodo}', omito\")\n",
        "                            continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al cargar pickle XGBoost para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Verificar columnas\n",
        "                    missing = [c for c in cols if c not in X_test_full.columns]\n",
        "                    if missing:\n",
        "                        print(f\"[DEBUG] Para método '{metodo}', faltan columnas en X_test: {missing}, omito\")\n",
        "                        continue\n",
        "                    # Subconjunto X_test\n",
        "                    X_test_sel = X_test_full[cols].copy()\n",
        "                    # Escalar, predecir, desescalar\n",
        "                    try:\n",
        "                        X_test_scaled = sx.transform(X_test_sel)\n",
        "                        y_pred_scaled = model.predict(X_test_scaled)\n",
        "                        # Desescalar: sy.inverse_transform espera 2D\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al predecir/desescalar XGBoost para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Cálculo de métricas y estadísticos\n",
        "                    try:\n",
        "                        # Rangos Y real y predicha\n",
        "                        y_real_min, y_real_max = float(np.min(y_test_arr)), float(np.max(y_test_arr))\n",
        "                        y_pred_min, y_pred_max = float(np.min(y_pred)), float(np.max(y_pred))\n",
        "                        # Residuos\n",
        "                        residuals = y_test_arr - y_pred\n",
        "                        res_mean = float(np.mean(residuals))\n",
        "                        res_std = float(np.std(residuals))\n",
        "                        # Estadísticos de residuos con pandas\n",
        "                        res_series = _pd.Series(residuals)\n",
        "                        res_skew = float(res_series.skew())\n",
        "                        res_kurt = float(res_series.kurtosis())\n",
        "                        q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]\n",
        "                        # Métricas clásicas\n",
        "                        mse = mean_squared_error(y_test_arr, y_pred)\n",
        "                        rmse = float(np.sqrt(mse))\n",
        "                        mae = float(mean_absolute_error(y_test_arr, y_pred))\n",
        "                        r2 = float(r2_score(y_test_arr, y_pred))\n",
        "                        # Correlación real vs predicha (si hay suficientes puntos)\n",
        "                        try:\n",
        "                            corr = float(np.corrcoef(y_test_arr, y_pred)[0,1]) if len(y_test_arr) > 1 else None\n",
        "                        except:\n",
        "                            corr = None\n",
        "                        # Acumular resumen\n",
        "                        metrics_summary_xgb.append({\n",
        "                            \"metodo\": metodo,\n",
        "                            \"rmse\": rmse,\n",
        "                            \"mae\": mae,\n",
        "                            \"r2\": r2\n",
        "                        })\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al calcular métricas XGBoost para método '{metodo}': {e}\")\n",
        "                        # si algo falla, saltar a next\n",
        "                        continue\n",
        "\n",
        "                    # 1) Parámetros de entrenamiento obtenidos desde el modelo XGBRegressor\n",
        "                    try:\n",
        "                        # get_params suele incluir: 'n_estimators', 'learning_rate', 'max_depth', 'subsample', etc.\n",
        "                        params_all = model.get_params()\n",
        "                        # Extraer los principales:\n",
        "                        params = {\n",
        "                            \"n_estimators\": params_all.get(\"n_estimators\", None),\n",
        "                            \"learning_rate\": params_all.get(\"learning_rate\", None),\n",
        "                            \"max_depth\": params_all.get(\"max_depth\", None),\n",
        "                            \"subsample\": params_all.get(\"subsample\", None)\n",
        "                        }\n",
        "                        df_params = _pd.DataFrame({\n",
        "                            \"Hiperparámetro\": list(params.keys()),\n",
        "                            \"Valor\": [str(v) for v in params.values()]\n",
        "                        })\n",
        "                        titulo_p = f\"### Parámetros de Entrenamiento XGBoost ({metodo})\"\n",
        "                        self.sections.append((titulo_p, df_params))\n",
        "                        print(f\"[DEBUG] 8.1. Sección parámetros XGBoost añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al extraer parámetros XGBoost para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA de los hiperparámetros XGBoost\n",
        "                    try:\n",
        "                        prompt_params = (\n",
        "                            f\"Has entrenado un modelo XGBoost con selección de variables '{metodo}'.\\n\"\n",
        "                            \"Estos fueron los hiperparámetros utilizados:\\n\"\n",
        "                        )\n",
        "                        for k, v in params.items():\n",
        "                            prompt_params += f\"- {k}: {v}\\n\"\n",
        "                        prompt_params += (\n",
        "                            \"\\nPor favor, explica de forma profesional y detallada cómo estos hiperparámetros \"\n",
        "                            \"pueden influir en el entrenamiento del modelo XGBoost, su impacto en ajuste, \"\n",
        "                            \"regularización, sobreajuste o subajuste, y buenas prácticas para afinarlos.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 8.2. Iniciando llamada a OpenAI para explicación hiperparámetros XGBoost ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en entrenamiento de modelos XGBoost para regresión.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_params}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_params = resp.choices[0].message.content.strip()\n",
        "                        if explanation_params:\n",
        "                            titulo_exp_p = f\"### 📝 Explicación IA Hiperparámetros XGBoost ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_p, explanation_params))\n",
        "                            print(f\"[DEBUG] 8.3. Sección explicación IA hiperparámetros XGBoost añadida para método: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibió IA para hiperparámetros XGBoost ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA hiperparámetros XGBoost para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 2) Gráfica Predicho vs Real\n",
        "                    try:\n",
        "                        fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "                        ax1.scatter(y_test_arr, y_pred, alpha=0.6)\n",
        "                        ax1.plot([y_real_min, y_real_max], [y_real_min, y_real_max], 'r--', lw=2)\n",
        "                        ax1.set_xlabel(\"Y real\")\n",
        "                        ax1.set_ylabel(\"Y predicho\")\n",
        "                        ax1.set_title(f\"XGBoost Predicho vs Real ({metodo})\")\n",
        "                        titulo_fig1 = f\"### Gráfico XGBoost Predicho vs Real ({metodo})\"\n",
        "                        self.sections.append((titulo_fig1, fig1))\n",
        "                        print(f\"[DEBUG] 8.4. Sección gráfica Pred vs Real XGBoost añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear gráfica Pred vs Real XGBoost para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA Pred vs Real XGBoost con contexto numérico\n",
        "                    try:\n",
        "                        prompt_pr = (\n",
        "                            f\"A continuación tienes datos de la gráfica de comparación Real vs Predicción para el modelo XGBoost con método '{metodo}':\\n\"\n",
        "                            f\"- RMSE: {rmse:.4f}\\n\"\n",
        "                            f\"- MAE: {mae:.4f}\\n\"\n",
        "                            f\"- R²: {r2:.4f}\\n\"\n",
        "                            f\"- Correlación entre Y real y predicha: {corr:.4f}\\n\"\n",
        "                            f\"- Rango Y real: [{y_real_min:.4f}, {y_real_max:.4f}]\\n\"\n",
        "                            f\"- Rango Y predicha: [{y_pred_min:.4f}, {y_pred_max:.4f}]\\n\\n\"\n",
        "                            \"Basándote en estos valores y en la gráfica generada (Real vs Predicción), \"\n",
        "                            \"proporciona un análisis detallado, señalando si hay sesgos sistemáticos, dispersión en ciertos rangos, \"\n",
        "                            \"y qué implicaciones tiene para la calidad del modelo. \"\n",
        "                            \"Usa un texto profesional y bien estructurado, citando qué indicios de la gráfica respaldan tus conclusiones.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 8.5. Llamada IA Pred vs Real XGBoost ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_pr}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_pr = resp.choices[0].message.content.strip()\n",
        "                        if explanation_pr:\n",
        "                            titulo_exp_pr = f\"### 📝 Explicación IA Predicho vs Real XGBoost ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_pr, explanation_pr))\n",
        "                            print(f\"[DEBUG] 8.6. Sección explicación IA Pred vs Real XGBoost añadida para método: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibió IA Pred vs Real XGBoost ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA Pred vs Real XGBoost para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 3) Gráfica de residuos XGBoost\n",
        "                    try:\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "                        ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "                        ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "                        ax2.set_xlabel(\"Y predicho\")\n",
        "                        ax2.set_ylabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "                        ax2.set_title(f\"XGBoost Residuos ({metodo})\")\n",
        "                        titulo_fig2 = f\"### Gráfica XGBoost Residuos ({metodo})\"\n",
        "                        self.sections.append((titulo_fig2, fig2))\n",
        "                        print(f\"[DEBUG] 8.7. Sección gráfica residuos XGBoost añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear gráfica residuos XGBoost para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA Residuos XGBoost con contexto numérico\n",
        "                    try:\n",
        "                        prompt_res = (\n",
        "                            f\"A continuación tienes estadísticas de los residuos (Real - Predicha) del modelo XGBoost con método '{metodo}':\\n\"\n",
        "                            f\"- Media: {res_mean:.4f}\\n\"\n",
        "                            f\"- Desviación estándar: {res_std:.4f}\\n\"\n",
        "                            f\"- Asimetría: {res_skew:.4f}\\n\"\n",
        "                            f\"- Curtosis: {res_kurt:.4f}\\n\"\n",
        "                            f\"- Cuantiles: 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}\\n\\n\"\n",
        "                            \"Basándote en estos valores y en la gráfica de residuos, analiza si hay patrones (heterocedasticidad, outliers, sesgos), \"\n",
        "                            \"y comenta qué implicaciones tiene para la robustez y generalización del modelo. \"\n",
        "                            \"Usa un texto profesional y bien estructurado.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 8.8. Llamada IA Residuos XGBoost ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_res}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_res = resp.choices[0].message.content.strip()\n",
        "                        if explanation_res:\n",
        "                            titulo_exp_res = f\"### 📝 Explicación IA Residuos XGBoost ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_res, explanation_res))\n",
        "                            print(f\"[DEBUG] 8.9. Sección explicación IA residuos XGBoost añadida para método: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibió IA Residuos XGBoost ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA residuos XGBoost para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 4) Tabla Métricas XGBoost y explicación IA\n",
        "                    try:\n",
        "                        df_met = _pd.DataFrame([\n",
        "                            {\"Métrica\": \"RMSE\", \"Valor\": rmse},\n",
        "                            {\"Métrica\": \"MAE\", \"Valor\": mae},\n",
        "                            {\"Métrica\": \"R2\",  \"Valor\": r2}\n",
        "                        ])\n",
        "                        titulo_met = f\"### Métricas XGBoost ({metodo})\"\n",
        "                        self.sections.append((titulo_met, df_met))\n",
        "                        print(f\"[DEBUG] 8.10. Sección métricas XGBoost añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame métricas XGBoost para método '{metodo}': {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_met = (\n",
        "                            f\"Estas son las métricas del modelo XGBoost con método '{metodo}':\\n\"\n",
        "                            f\"- R2: {r2:.4f}\\n\"\n",
        "                            f\"- MSE: {mse:.4f}\\n\"\n",
        "                            f\"- RMSE: {rmse:.4f}\\n\"\n",
        "                            f\"- MAE: {mae:.4f}\\n\"\n",
        "                            f\"- Correlación Real vs Predicha: {corr:.4f}\\n\\n\"\n",
        "                            \"Analiza estos valores en contexto: ¿son adecuados? ¿qué sugieren respecto al rendimiento del modelo? \"\n",
        "                            \"Menciona referencias a la gráfica Real vs Predicción y a los residuos si procede.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 8.11. Llamada IA Métricas XGBoost ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_met}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_met = resp.choices[0].message.content.strip()\n",
        "                        if explanation_met:\n",
        "                            titulo_exp_met = f\"### 📝 Explicación IA Métricas XGBoost ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_met, explanation_met))\n",
        "                            print(f\"[DEBUG] 8.12. Sección explicación IA métricas XGBoost añadida para método: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibió IA Métricas XGBoost ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA métricas XGBoost para método '{metodo}': {e}\")\n",
        "\n",
        "                # 5) Comparativa global de métricas XGBoost\n",
        "                if metrics_summary_xgb:\n",
        "                    try:\n",
        "                        df_comp_xgb = _pd.DataFrame(metrics_summary_xgb)\n",
        "                        df_comp_xgb_sorted = df_comp_xgb.sort_values(\"rmse\")\n",
        "                        titulo_comp = \"### Comparativa Métricas XGBoost entre Métodos\"\n",
        "                        self.sections.append((titulo_comp, df_comp_xgb_sorted))\n",
        "                        print(\"[DEBUG] 8.13. Sección comparativa métricas XGBoost añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame comparativo métricas XGBoost: {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_conc = (\n",
        "                            \"Se han entrenado varios modelos XGBoost con diferentes métodos de selección de variables.\\n\"\n",
        "                            \"Métricas obtenidas en test:\\n\"\n",
        "                        )\n",
        "                        for entry in metrics_summary_xgb:\n",
        "                            prompt_conc += f\"- Método '{entry['metodo']}': RMSE={entry['rmse']}, MAE={entry['mae']}, R2={entry['r2']}\\n\"\n",
        "                        prompt_conc += (\n",
        "                            \"\\nPor favor, proporciona conclusiones profesionales comparando estos métodos: \"\n",
        "                            \"indica cuál se comporta mejor, posibles razones y recomendaciones sobre selección de variables o ajustes para mejorar XGBoost.\"\n",
        "                        )\n",
        "                        print(\"[DEBUG] 8.14. Llamada IA Conclusiones XGBoost...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_conc}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_conc = resp.choices[0].message.content.strip()\n",
        "                        if explanation_conc:\n",
        "                            titulo_exp_conc = \"### 📝 Conclusiones IA Entrenamiento XGBoost\"\n",
        "                            self.sections.append((titulo_exp_conc, explanation_conc))\n",
        "                            print(\"[DEBUG] 8.15. Sección explicación IA conclusiones XGBoost añadida\")\n",
        "                        else:\n",
        "                            print(\"[DEBUG] No se recibió IA Conclusiones XGBoost\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA conclusiones XGBoost: {e}\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No están RESUMEN_METODOS o X_test/Y_test en globals(), omito sección XGBoost\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar sección XGBoost en informe: {e}\")\n",
        "        # ... fin de la sección de entrenamiento XGBoost ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 8.1. Interpretación xIA para modelo entrenado XGBoost\n",
        "        # =============================================================\n",
        "        import openai\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # Verificación previa\n",
        "        # ----------------------------------------------------------------\n",
        "        try:\n",
        "            print(\"[DEBUG] 8.16. Iniciando sección xIA para XGBoost\")\n",
        "            if 'xai_results' not in globals() or 'XGBoost' not in xai_results:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontró `xai_results['XGBoost']`. \"\n",
        "                    \"Asegúrate de haber ejecutado la Celda 10 y almacenado los resultados xIA de XGBoost en `xai_results['XGBoost']`.\"\n",
        "                )\n",
        "\n",
        "                # Cabecera\n",
        "                self.sections.append((\n",
        "                    \"## 🔍 Análisis xIA de XGBoost: Resultados concretos y explicaciones Generativas\",\n",
        "                    \"\"  # contenido vacío, la cabecera se mostrará como Markdown\n",
        "                ))\n",
        "\n",
        "\n",
        "            # Función para llamar a OpenAI con un prompt específico\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                \"\"\"\n",
        "                Llama a OpenAI ChatCompletion con un sistema experto en ML/XAI,\n",
        "                devuelve la respuesta de la IA en texto.\n",
        "                \"\"\"\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un asistente experto en Machine Learning e interpretabilidad de modelos. \"\n",
        "                                \"Proporciona explicaciones detalladas y basadas en los datos concretos proporcionados.\"\n",
        "                            )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    texto = response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    texto = f\"[Error llamando a OpenAI: {e}]\"\n",
        "                return texto\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Configuración: top N variables y primeras M muestras locales\n",
        "            # ----------------------------------------------------------------\n",
        "            TOP_N = 5      # cuántas características top incluir en el prompt\n",
        "            N_LOCAL = 3    # cuántas muestras locales incluir en prompt\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Lista de métodos xIA y claves en xai_results['XGBoost']\n",
        "            # ----------------------------------------------------------------\n",
        "            # Las claves deben coincidir exactamente con las usadas en Celda 10 al almacenar resultados.\n",
        "            metodos_claves = [\n",
        "                ('SHAP', 'SHAP'),\n",
        "                ('LIME', 'LIME'),\n",
        "                ('KernelExplainer', 'KernelExplainer'),\n",
        "                ('Integrated Gradients', 'Integrated Gradients'),\n",
        "                ('DeepLIFT / LRP', 'DeepLIFT / LRP'),\n",
        "                ('Permutation Feature Importance', 'Permutation Feature Importance'),\n",
        "                ('Partial Dependence Plots (PDP)', 'Partial Dependence Plots (PDP)'),\n",
        "                ('Accumulated Local Effects (ALE)', 'Accumulated Local Effects (ALE)'),\n",
        "                ('Individual Conditional Expectation (ICE) Plots', 'Individual Conditional Expectation (ICE) Plots'),\n",
        "                ('Counterfactual Explanations', 'Counterfactual Explanations'),\n",
        "                ('Anchors', 'Anchors'),\n",
        "                ('Surrogate Models (Global/Local)', 'Surrogate Models (Global/Local)'),\n",
        "                ('Explainable Boosting Machine (EBM)', 'Explainable Boosting Machine (EBM)'),\n",
        "                ('Optuna Hyperparameter Importance', 'Optuna Hyperparameter Importance'),\n",
        "            ]\n",
        "\n",
        "            for titulo, clave in metodos_claves:\n",
        "                print(f\"[DEBUG] 8.17. Procesando sección xIA: {titulo}\")\n",
        "                datos = xai_results['XGBoost'].get(clave)\n",
        "                if datos is None:\n",
        "                    print(f\"[DEBUG] No hay resultados xIA para {titulo}, se omite\")\n",
        "                    continue\n",
        "#                    self.sections.append((\n",
        "#                        f\"### ⚠️ No hay resultados para {titulo}\",\n",
        "#                        f\"No se hallaron resultados para la clave '{clave}'.\"\n",
        "#                    ))\n",
        "#                    continue\n",
        "\n",
        "                # ---------------- Mostrar figura guardada ----------------\n",
        "                print(f\"[DEBUG] 8.18. Mostrando figura para {titulo}\")\n",
        "                fig = datos.get('fig_summary') or datos.get('fig')\n",
        "                if fig is not None:\n",
        "                    self.sections.append((f\"### {titulo}: Gráfico\", fig))\n",
        "\n",
        "                # --------------- Mostrar DataFrames de importancia y local ---------------\n",
        "                print(f\"[DEBUG] 8.19. Mostrando DataFrames para {titulo}\")\n",
        "                imp_df = datos.get('imp_df')\n",
        "                df_local = datos.get('df_local')\n",
        "                stats_extra = datos.get('stats', None)  # opcional: estadísticas adicionales, p.ej. percentiles, pos_pct SHAP, etc.\n",
        "\n",
        "                if isinstance(imp_df, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Importancia global\", imp_df.reset_index(drop=True)))\n",
        "\n",
        "                if isinstance(df_local, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Valores locales\", df_local.head(N_LOCAL)))\n",
        "\n",
        "                # --------------------------------- Extraer datos del gráfico (suponiendo que sea un barh con 'feature' y 'importance') ---------------------------\n",
        "                features = [text.get_text() for text in ax.get_yticklabels()]\n",
        "                importances = [bar.get_width() for bar in ax.patches]\n",
        "\n",
        "                # --------------- Construir prompt con valores numéricos concretos ---------------\n",
        "                print(f\"[DEBUG] 8.20. Construyendo prompt para {titulo}\")\n",
        "                prompt = f\"He aplicado el método xIA '{titulo}' al modelo XGBoost entrenado y he obtenido estos resultados concretos:\\n\\n\"\n",
        "\n",
        "                prompt += \"- Datos del gráfico (feature vs importancia):\\n\"\n",
        "                for f, imp in zip(features, importances):\n",
        "                    prompt += f\"    • {f}: {imp:.4f}\\n\"\n",
        "\n",
        "                # Ahora sí le pides que interprete el gráfico:\n",
        "                prompt += (\n",
        "                    \"- Interpreta el gráfico anterior: \"\n",
        "                    \"describe qué patrones o relaciones visuales revela cómo se distribuye la importancia.\\n\"\n",
        "                )\n",
        "\n",
        "                # 1) Extraer importancia global: top N\n",
        "                if isinstance(imp_df, pd.DataFrame) and not imp_df.empty:\n",
        "                    imp_df = imp_df.reset_index()\n",
        "                    imp_df = imp_df.rename(columns={ imp_df.columns[0]: \"feature\" })\n",
        "                    cols = imp_df.columns.tolist()\n",
        "                    if len(cols) >= 2:\n",
        "                        feat_col = \"feature\"\n",
        "                        val_col  = cols[1]\n",
        "                        try:\n",
        "                            imp_df_sorted = imp_df.sort_values(val_col, ascending=False)\n",
        "                        except Exception:\n",
        "                            imp_df_sorted = imp_df\n",
        "\n",
        "                        top_n      = min(TOP_N, len(imp_df_sorted))\n",
        "                        top_imp_df = imp_df_sorted.iloc[:top_n][[feat_col, val_col]]\n",
        "                        top_imp_list = []\n",
        "                        for _, row in top_imp_df.iterrows():\n",
        "                            raw = row[val_col]\n",
        "                            try:\n",
        "                                v = float(raw)\n",
        "                            except Exception:\n",
        "                                v = raw  # si no se puede convertir, lo dejo tal cual\n",
        "                            top_imp_list.append({feat_col: row[feat_col], val_col: v})\n",
        "\n",
        "                        prompt += f\"- Top {top_n} características por importancia global ({feat_col}, {val_col}):\\n  {top_imp_list}\\n\"\n",
        "\n",
        "                        extra_cols = cols[2:]\n",
        "                        if extra_cols:\n",
        "                            prompt += f\"  (El DataFrame de importancia global incluye también columnas: {extra_cols}.)\\n\"\n",
        "                    else:\n",
        "                        snippet = imp_df.head(TOP_N).to_dict(orient='records')\n",
        "                        prompt += f\"- Importancia global (primeras filas, formato genérico):\\n  {snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de importancia global.\\n\"\n",
        "\n",
        "#                    # — Siempre sacamos el índice como columna llamada \"feature\"\n",
        "                    imp_df = imp_df.rename(columns={ idx_col: \"feature\" })\n",
        "\n",
        "                # 2) Extraer valores locales: primeras M muestras\n",
        "                if isinstance(df_local, pd.DataFrame) and not df_local.empty:\n",
        "                    n_loc = min(N_LOCAL, len(df_local))\n",
        "                    loc_snippet = df_local.head(n_loc).to_dict(orient='records')\n",
        "                    prompt += f\"- Valores locales para las primeras {n_loc} muestras (cada dict mapea feature a valor/atribución):\\n  {loc_snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de valores locales.\\n\"\n",
        "\n",
        "                # 3) Incluir estadísticas extra si existen\n",
        "                if isinstance(stats_extra, dict):\n",
        "                    prompt += \"- Estadísticas adicionales:\\n\"\n",
        "                    for k, v in stats_extra.items():\n",
        "                        prompt += f\"  * {k}: {v}\\n\"\n",
        "\n",
        "                # 4) Contexto general del modelo XGBoost\n",
        "                prompt += (\n",
        "                    \"\\nContexto: El modelo XGBoost fue entrenado con variables seleccionadas y StandardScaler, \"\n",
        "                    \"con predicciones desescaladas. Ahora interpretamos los resultados xIA para este XGBoost.\\n\"\n",
        "                )\n",
        "\n",
        "                # 5) Preguntas/pautas específicas según el método\n",
        "                if clave == 'SHAP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores concretos de SHAP para:\\n\"\n",
        "                        \"1. Explicar la importancia global de cada variable en base a los valores de SHAP mostrados.\\n\"\n",
        "                        \"2. Analizar los valores SHAP de las primeras muestras: si una variable muestra SHAP positivo alto, ¿qué implica sobre la predicción en ese caso? Y si es negativo, ¿qué implica?\\n\"\n",
        "                        \"3. Identificar patrones en SHAP (por ejemplo, variables que consistentemente empujan en una dirección) y cómo afecta al comportamiento general del XGBoost.\\n\"\n",
        "                        \"4. Sugerir posibles transformaciones de variables o validaciones adicionales basadas en estos resultados SHAP.\\n\"\n",
        "                    )\n",
        "                elif clave == 'LIME':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos pesos LIME concretos para:\\n\"\n",
        "                        \"1. Explicar el significado de pesos positivos y negativos en LIME para XGBoost.\\n\"\n",
        "                        \"2. Analizar casos de las primeras muestras: si una variable tiene peso LIME fuerte (positivo/negativo), ¿qué implica para la predicción local?\\n\"\n",
        "                        \"3. Comentar si la variabilidad de los pesos sugiere relaciones no lineales o interacciones no capturadas.\\n\"\n",
        "                        \"4. Recomendar acciones: agrupación de variables, detección de outliers, etc., basadas en la interpretación LIME.\\n\"\n",
        "                    )\n",
        "                elif clave == 'KernelExplainer':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de KernelExplainer (SHAP caja negra) para:\\n\"\n",
        "                        \"1. Explicar la importancia global según los valores medios absolutos Kernel SHAP.\\n\"\n",
        "                        \"2. Analizar los valores locales para las primeras muestras: cómo cada característica empuja la predicción en cada caso.\\n\"\n",
        "                        \"3. Comparar con SHAP (si ya lo hiciste con TreeExplainer para otro modelo o Kernel SHAP para XGBoost), si aplica.\\n\"\n",
        "                        \"4. Sugerir consideraciones sobre fondo (background) usado y posibles ajustes si las explicaciones muestran comportamiento inesperado.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Integrated Gradients':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Integrated Gradients para:\\n\"\n",
        "                        \"1. Explicar la contribución integrada de cada variable: interpretación de importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: qué implicaciones tiene un valor IG alto o bajo en cada variable.\\n\"\n",
        "                        \"3. Señalar limitaciones: compatibilidad con XGBoost no diferenciable; si estos valores provienen de un modelo aproximado, comentar fiabilidad.\\n\"\n",
        "                        \"4. Sugerir pasos adicionales o comparaciones con otros métodos (SHAP/LIME) para validar interpretaciones.\\n\"\n",
        "                    )\n",
        "                elif clave == 'DeepLIFT / LRP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de DeepLIFT / LRP para:\\n\"\n",
        "                        \"1. Explicar la relevancia asignada a cada variable en importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: si una variable tiene relevancia positiva o negativa, qué significa para la predicción.\\n\"\n",
        "                        \"3. Comparar con IG o SHAP si se dispone: consistencia de atribuciones.\\n\"\n",
        "                        \"4. Recomendar verificaciones o transformaciones en caso de interpretaciones inesperadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Permutation Feature Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Permutation Feature Importance para:\\n\"\n",
        "                        \"1. Explicar la caída en la métrica al permutar cada variable: por qué ciertas variables son críticas.\\n\"\n",
        "                        \"2. Comentar la desviación estándar: ¿indica inestabilidad en la importancia? ¿Dónde conviene reforzar validaciones?\\n\"\n",
        "                        \"3. Comparar con importancias de SHAP/LIME: similitudes o diferencias.\\n\"\n",
        "                        \"4. Sugerir prioridades para ajuste de modelo o selección de variables basadas en esta métrica.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Partial Dependence Plots (PDP)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos PDP para:\\n\"\n",
        "                        \"1. Explicar el efecto promedio de cada variable sobre la predicción según el rango PDP obtenido.\\n\"\n",
        "                        \"2. Señalar si los rangos sugieren relaciones monótonas o no lineales.\\n\"\n",
        "                        \"3. Advertir sobre correlaciones fuertes que puedan afectar la interpretación.\\n\"\n",
        "                        \"4. Recomendar posibles exploraciones adicionales (PDP bivariados, transformaciones) según los resultados.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Accumulated Local Effects (ALE)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ALE y valores locales ALE para:\\n\"\n",
        "                        \"1. Explicar cómo ALE corrige artefactos de correlación y qué nos dicen los valores concretos.\\n\"\n",
        "                        \"2. Interpretar importancia global ALE: variables con mayor efecto acumulado.\\n\"\n",
        "                        \"3. Analizar heterogeneidad local a partir de valores ALE de primeras muestras.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales (ALE 2D, inspección de distribución) según hallazgos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Individual Conditional Expectation (ICE) Plots':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ICE medios para:\\n\"\n",
        "                        \"1. Explicar qué mide el rango ICE y su diferencia respecto a PDP.\\n\"\n",
        "                        \"2. Analizar heterogeneidad: variables con alta variabilidad en rangos ICE indican interacciones o comportamiento inestable.\\n\"\n",
        "                        \"3. Señalar implicaciones para el modelo y posibles ajustes si hay alto efecto local variable.\\n\"\n",
        "                        \"4. Recomendar exploraciones adicionales para entender la variabilidad local.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Counterfactual Explanations':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Contrafactuales para:\\n\"\n",
        "                        \"1. Explicar cómo interpretar los contrafactuales: cambios en variables que generan aumento en predicción.\\n\"\n",
        "                        \"2. Analizar variables con mayor |Δ| medio: implicaciones sobre sensibilidad del modelo.\\n\"\n",
        "                        \"3. Señalar si faltan contrafactuales para algunas muestras: qué puede indicar (limites del modelo o datos).\\n\"\n",
        "                        \"4. Sugerir cómo usar estos insights para ajuste de modelo o recolección de datos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Anchors':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Anchors para:\\n\"\n",
        "                        \"1. Explicar cómo leer las reglas ancla de las primeras muestras: condiciones que aseguran la predicción.\\n\"\n",
        "                        \"2. Analizar frecuencia global de aparición de variables en reglas: implicaciones sobre estabilidad y sesgos.\\n\"\n",
        "                        \"3. Señalar regiones de bajo coverage o baja precisión: dónde el modelo es menos fiable.\\n\"\n",
        "                        \"4. Recomendar acciones: recolección de datos, refinamiento de variables o validaciones dirigidas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Surrogate Models (Global/Local)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Surrogate Models para:\\n\"\n",
        "                        \"1. Explicar la importancia global del surrogate (árbol) y la importancia local media (coeficientes regresiones locales).\\n\"\n",
        "                        \"2. Comparar global vs local: variables con alta importancia local pero baja global, o viceversa, y qué sugiere.\\n\"\n",
        "                        \"3. Concluir sobre consistencia de comportamiento del XGBoost en diferentes regiones del espacio.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales o ajustes de modelo según discrepancias detectadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Explainable Boosting Machine (EBM)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de EBM para:\\n\"\n",
        "                        \"1. Explicar la importancia global según EBM: cómo se comparan con otros métodos.\\n\"\n",
        "                        \"2. Analizar contribuciones locales de las primeras muestras: qué patrones se observan.\\n\"\n",
        "                        \"3. Señalar si EBM revela interacciones no consideradas en XGBoost.\\n\"\n",
        "                        \"4. Recomendar posibles ajustes en características o validaciones según insights de EBM.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Optuna Hyperparameter Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Optuna para:\\n\"\n",
        "                        \"1. Explicar la importancia global de hiperparámetros en la optimización del XGBoost.\\n\"\n",
        "                        \"2. Analizar top trials si están disponibles: qué combinaciones de hiperparámetros funcionaron mejor.\\n\"\n",
        "                        \"3. Señalar limitaciones de la muestra de trials (número de pruebas) y posibles riesgos de sobreajuste en la búsqueda.\\n\"\n",
        "                        \"4. Recomendar próximas acciones para tuning basadas en estas importancias.\\n\"\n",
        "                    )\n",
        "                else:\n",
        "                    prompt += \"\\nPor favor, explica estos resultados numéricos y qué implicaciones tienen para el modelo XGBoost.\\n\"\n",
        "\n",
        "                # --------------- Llamada a OpenAI ---------------\n",
        "                print(f\"[DEBUG] 8.21. Llamando a OpenAI para {titulo}\")\n",
        "                explicacion = call_openai_explanation(prompt)\n",
        "                self.sections.append((f\"### {titulo}: Explicación Generativa\", explicacion))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en sección xIA XGBoost\",\n",
        "                f\"Se produjo un error al generar la sección xIA de XGBoost: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =====================================================================\n",
        "        # 9. Entrenamiento Modelo Random Forest\n",
        "        # =====================================================================\n",
        "        try:\n",
        "            # Comprobamos RESUMEN_METODOS y existencia de X_test/Y_test\n",
        "            if \"RESUMEN_METODOS\" in self.g and \"X_test\" in self.g and \"Y_test\" in self.g:\n",
        "                import os, pickle\n",
        "                import numpy as _np\n",
        "                import pandas as _pd\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "                X_test_full = self.g[\"X_test\"]\n",
        "                Y_test = self.g[\"Y_test\"]\n",
        "                # Extraer array 1D de y_test\n",
        "                if hasattr(Y_test, \"values\"):\n",
        "                    arr = Y_test.values\n",
        "                    if arr.ndim == 2 and arr.shape[1] >= 1:\n",
        "                        # tomamos la primera columna si hay más\n",
        "                        y_test_arr = arr[:, 0]\n",
        "                    else:\n",
        "                        y_test_arr = arr.ravel()\n",
        "                else:\n",
        "                    y_test_arr = _np.array(Y_test).ravel()\n",
        "                y_test_arr = y_test_arr.ravel()\n",
        "\n",
        "                metrics_summary_rf = []\n",
        "                # Iteramos sobre cada método en RESUMEN_METODOS\n",
        "                for metodo, vars_sel in self.g[\"RESUMEN_METODOS\"].items():\n",
        "                    metodo_low = metodo.lower()\n",
        "                    fname = f\"modelo_rf_{metodo_low}.pkl\"\n",
        "                    if not os.path.exists(fname):\n",
        "                        print(f\"[DEBUG] Fichero de modelo RF no encontrado para método '{metodo}': {fname}, omito este método\")\n",
        "                        continue\n",
        "                    # Cargar pickle\n",
        "                    try:\n",
        "                        with open(fname, \"rb\") as f:\n",
        "                            data = pickle.load(f)\n",
        "                        model = data.get(\"model\", None)\n",
        "                        sx = data.get(\"sx\", None)\n",
        "                        sy = data.get(\"sy\", None)\n",
        "                        cols = data.get(\"cols\", None)\n",
        "                        if model is None or sx is None or sy is None or cols is None:\n",
        "                            print(f\"[DEBUG] Falta alguna clave en pickle RF para método '{metodo}', omito\")\n",
        "                            continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al cargar pickle RF para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Verificar columnas en X_test\n",
        "                    missing = [c for c in cols if c not in X_test_full.columns]\n",
        "                    if missing:\n",
        "                        print(f\"[DEBUG] Para método '{metodo}', faltan columnas en X_test: {missing}, omito\")\n",
        "                        continue\n",
        "\n",
        "                    # Subconjunto X_test\n",
        "                    X_test_sel = X_test_full[cols].copy()\n",
        "                    # Escalar, predecir y desescalar\n",
        "                    try:\n",
        "                        X_test_scaled = sx.transform(X_test_sel)\n",
        "                        y_pred_scaled = model.predict(X_test_scaled)\n",
        "                        # inverse_transform espera 2D\n",
        "                        y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al predecir/desescalar RF para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Estadísticos y métricas\n",
        "                    try:\n",
        "                        # Rangos Real vs Predicha\n",
        "                        y_real_min, y_real_max = float(_np.min(y_test_arr)), float(_np.max(y_test_arr))\n",
        "                        y_pred_min, y_pred_max = float(_np.min(y_pred)), float(_np.max(y_pred))\n",
        "                        # Residuos\n",
        "                        residuals = y_test_arr - y_pred\n",
        "                        res_mean = float(_np.mean(residuals))\n",
        "                        res_std  = float(_np.std(residuals))\n",
        "                        res_series = _pd.Series(residuals)\n",
        "                        res_skew = float(res_series.skew())\n",
        "                        res_kurt = float(res_series.kurtosis())\n",
        "                        q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]\n",
        "                        # Métricas\n",
        "                        mse = mean_squared_error(y_test_arr, y_pred)\n",
        "                        rmse = float(_np.sqrt(mse))\n",
        "                        mae = float(mean_absolute_error(y_test_arr, y_pred))\n",
        "                        r2 = float(r2_score(y_test_arr, y_pred))\n",
        "                        # Correlación Real vs Predicha\n",
        "                        try:\n",
        "                            corr = float(_np.corrcoef(y_test_arr, y_pred)[0, 1])\n",
        "                        except:\n",
        "                            corr = None\n",
        "                        # Guardar resumen para comparativa\n",
        "                        metrics_summary_rf.append({\n",
        "                            \"metodo\": metodo,\n",
        "                            \"rmse\": rmse,\n",
        "                            \"mae\": mae,\n",
        "                            \"r2\": r2\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al calcular métricas RF para método '{metodo}': {e}\")\n",
        "                        # saltamos, aunque idealmente definimos rmse,etc = None\n",
        "\n",
        "                    # 1) Parámetros de entrenamiento obtenidos desde el modelo\n",
        "                    try:\n",
        "                        # RandomForestRegressor atributos: n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, bootstrap\n",
        "                        params = {\n",
        "                            \"n_estimators\": getattr(model, \"n_estimators\", None),\n",
        "                            \"max_depth\": getattr(model, \"max_depth\", None),\n",
        "                            \"min_samples_split\": getattr(model, \"min_samples_split\", None),\n",
        "                            \"min_samples_leaf\": getattr(model, \"min_samples_leaf\", None),\n",
        "                            \"max_features\": getattr(model, \"max_features\", None),\n",
        "                            \"bootstrap\": getattr(model, \"bootstrap\", None)\n",
        "                        }\n",
        "                        df_params = _pd.DataFrame({\n",
        "                            \"Hiperparámetro\": list(params.keys()),\n",
        "                            \"Valor\": [str(v) for v in params.values()]\n",
        "                        })\n",
        "                        titulo_p = f\"### Parámetros de Entrenamiento Random Forest ({metodo})\"\n",
        "                        self.sections.append((titulo_p, df_params))\n",
        "                        print(f\"[DEBUG] 9.1. Sección parámetros RF añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al extraer parámetros RF para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA de los hiperparámetros RF\n",
        "                    try:\n",
        "                        prompt_params = (\n",
        "                            f\"Has entrenado un modelo Random Forest con selección de variables '{metodo}'.\\n\"\n",
        "                            \"Estos fueron los hiperparámetros utilizados:\\n\"\n",
        "                        )\n",
        "                        for k, v in params.items():\n",
        "                            prompt_params += f\"- {k}: {v}\\n\"\n",
        "                        prompt_params += (\n",
        "                            \"\\nPor favor, explica de forma profesional y detallada cómo estos hiperparámetros \"\n",
        "                            \"pueden influir en el entrenamiento del Random Forest, su impacto en ajuste/sobreajuste, \"\n",
        "                            \"y buenas prácticas para seleccionarlos o afinarlos.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 9.2. Iniciando llamada a OpenAI para explicación hiperparámetros RF ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en entrenamiento de Random Forest para regresión.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_params}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_params = resp.choices[0].message.content.strip()\n",
        "                        if explanation_params:\n",
        "                            titulo_exp_p = f\"### 📝 Explicación IA Hiperparámetros RF ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_p, explanation_params))\n",
        "                            print(f\"[DEBUG] 9.3. Sección explicación IA hiperparámetros RF añadida para método: {metodo}\")\n",
        "                        else:\n",
        "                            print(f\"[DEBUG] No se recibió IA para hiperparámetros RF ({metodo})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA hiperparámetros RF para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 2) Gráfica Predicho vs Real\n",
        "                    try:\n",
        "                        fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "                        ax1.scatter(y_test_arr, y_pred, alpha=0.6)\n",
        "                        ax1.plot([y_test_arr.min(), y_test_arr.max()],\n",
        "                                 [y_test_arr.min(), y_test_arr.max()],\n",
        "                                 'r--', lw=2)\n",
        "                        ax1.set_xlabel(\"Y real\")\n",
        "                        ax1.set_ylabel(\"Y predicho\")\n",
        "                        ax1.set_title(f\"Random Forest Predicho vs Real ({metodo})\")\n",
        "                        titulo_fig1 = f\"### Gráfico RF Predicho vs Real ({metodo})\"\n",
        "                        self.sections.append((titulo_fig1, fig1))\n",
        "                        print(f\"[DEBUG] 9.4. Sección gráfica Pred vs Real RF añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear gráfica Pred vs Real RF para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA Pred vs Real RF con contexto numérico\n",
        "                    try:\n",
        "                        prompt_pr = (\n",
        "                            f\"A continuación tienes datos de la gráfica de comparación Real vs Predicción para el Random Forest con método '{metodo}':\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- R²: {r2}\\n\"\n",
        "                            f\"- Correlación entre Y real y predicha: {corr}\\n\"\n",
        "                            f\"- Rango Y real: [{y_real_min}, {y_real_max}]\\n\"\n",
        "                            f\"- Rango Y predicha: [{y_pred_min}, {y_pred_max}]\\n\\n\"\n",
        "                            \"Basándote en estos valores y en la gráfica generada (Real vs Predicción), \"\n",
        "                            \"proporciona un análisis detallado: sesgos sistemáticos, dispersión en rangos, posibles problemas y recomendaciones.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 9.5. Llamada IA Pred vs Real RF ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_pr}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_pr = resp.choices[0].message.content.strip()\n",
        "                        if explanation_pr:\n",
        "                            titulo_exp_pr = f\"### 📝 Explicación IA Predicho vs Real RF ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_pr, explanation_pr))\n",
        "                            print(f\"[DEBUG] 9.6. Sección explicación IA Pred vs Real RF añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA Pred vs Real RF para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 3) Gráfica de residuos RF\n",
        "                    try:\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "                        ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "                        ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "                        ax2.set_xlabel(\"Y predicho\")\n",
        "                        ax2.set_ylabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "                        ax2.set_title(f\"Random Forest Residuos ({metodo})\")\n",
        "                        titulo_fig2 = f\"### Gráfico RF Residuos ({metodo})\"\n",
        "                        self.sections.append((titulo_fig2, fig2))\n",
        "                        print(f\"[DEBUG] 9.7. Sección gráfica residuos RF añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear gráfica residuos RF para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA Residuos RF con contexto numérico\n",
        "                    try:\n",
        "                        prompt_res = (\n",
        "                            f\"A continuación tienes estadísticas de los residuos (Real - Predicha) del Random Forest con método '{metodo}':\\n\"\n",
        "                            f\"- Media: {res_mean}\\n\"\n",
        "                            f\"- Desviación estándar: {res_std}\\n\"\n",
        "                            f\"- Asimetría: {res_skew}\\n\"\n",
        "                            f\"- Curtosis: {res_kurt}\\n\"\n",
        "                            f\"- Cuantiles: 25%={q25}, 50%={q50}, 75%={q75}\\n\\n\"\n",
        "                            \"Basándote en estos valores y en la gráfica de residuos, analiza patrones (heterocedasticidad, outliers, sesgos) y qué implicaciones tiene para generalización.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 9.8. Llamada IA Residuos RF ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_res}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_res = resp.choices[0].message.content.strip()\n",
        "                        if explanation_res:\n",
        "                            titulo_exp_res = f\"### 📝 Explicación IA Residuos RF ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_res, explanation_res))\n",
        "                            print(f\"[DEBUG] 9.9. Sección explicación IA residuos RF añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA residuos RF para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 4) Tabla Métricas y explicación IA\n",
        "                    try:\n",
        "                        df_met = _pd.DataFrame([\n",
        "                            {\"Métrica\": \"RMSE\", \"Valor\": rmse},\n",
        "                            {\"Métrica\": \"MAE\", \"Valor\": mae},\n",
        "                            {\"Métrica\": \"R2\",  \"Valor\": r2}\n",
        "                        ])\n",
        "                        titulo_met = f\"### Métricas Random Forest ({metodo})\"\n",
        "                        self.sections.append((titulo_met, df_met))\n",
        "                        print(f\"[DEBUG] 9.10 Sección métricas RF añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame métricas RF para método '{metodo}': {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_met = (\n",
        "                            f\"Estas son las métricas del Random Forest con método '{metodo}':\\n\"\n",
        "                            f\"- R2: {r2}\\n\"\n",
        "                            f\"- MSE: {mse}\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- Correlación Real vs Predicha: {corr}\\n\\n\"\n",
        "                            \"Analiza estos valores en contexto: ¿son adecuados? ¿qué sugieren respecto al rendimiento? \"\n",
        "                            \"Menciona referencias a la gráfica Real vs Predicción y residuos si procede.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 9.11. Llamada IA Métricas RF ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_met}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_met = resp.choices[0].message.content.strip()\n",
        "                        if explanation_met:\n",
        "                            titulo_exp_met = f\"### 📝 Explicación IA Métricas RF ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_met, explanation_met))\n",
        "                            print(f\"[DEBUG] 9.12. Sección explicación IA métricas RF añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA métricas RF para método '{metodo}': {e}\")\n",
        "\n",
        "                # 5) Comparativa global de métricas RF\n",
        "                if metrics_summary_rf:\n",
        "                    try:\n",
        "                        df_comp_rf = _pd.DataFrame(metrics_summary_rf)\n",
        "                        df_comp_rf_sorted = df_comp_rf.sort_values(\"rmse\")\n",
        "                        titulo_comp_rf = \"### Comparativa Métricas Random Forest entre Métodos\"\n",
        "                        self.sections.append((titulo_comp_rf, df_comp_rf_sorted))\n",
        "                        print(\"[DEBUG] 9.13. Sección comparativa métricas RF añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame comparativo métricas RF: {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_conc = (\n",
        "                            \"Se han entrenado varios Random Forest con diferentes métodos de selección de variables.\\n\"\n",
        "                            \"Métricas obtenidas en test:\\n\"\n",
        "                        )\n",
        "                        for entry in metrics_summary_rf:\n",
        "                            prompt_conc += f\"- Método '{entry['metodo']}': RMSE={entry['rmse']}, MAE={entry['mae']}, R2={entry['r2']}\\n\"\n",
        "                        prompt_conc += (\n",
        "                            \"\\nPor favor, proporciona conclusiones profesionales comparando estos métodos: \"\n",
        "                            \"indica cuál se comporta mejor, posibles razones y recomendaciones sobre selección de variables o ajustes para mejorar Random Forest.\"\n",
        "                        )\n",
        "                        print(\"[DEBUG] 9.14. Llamada IA Conclusiones RF...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_conc}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_conc = resp.choices[0].message.content.strip()\n",
        "                        if explanation_conc:\n",
        "                            titulo_exp_conc = \"### 📝 Conclusiones IA Entrenamiento Random Forest\"\n",
        "                            self.sections.append((titulo_exp_conc, explanation_conc))\n",
        "                            print(\"[DEBUG] 9.15. Sección explicación IA conclusiones RF añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA conclusiones RF: {e}\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No están RESUMEN_METODOS o X_test/Y_test en globals(), omito sección RF\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar sección RF en informe: {e}\")\n",
        "        # ... fin de la sección de entrenamiento Random Forest ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 9.1. Interpretación xIA para modelo entrenado Random Forest\n",
        "        # =============================================================\n",
        "        import openai\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # Verificación previa\n",
        "        # ----------------------------------------------------------------\n",
        "        try:\n",
        "            print(\"[DEBUG] 9.16. Iniciando sección xIA para Random Forest\")\n",
        "            if 'xai_results' not in globals() or 'Random Forest' not in xai_results:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontró `xai_results['Random Forest']`. \"\n",
        "                    \"Asegúrate de haber ejecutado la Celda 10 y almacenado los resultados xIA de Random Forest en `xai_results['Random Forest']`.\"\n",
        "                )\n",
        "\n",
        "                # Cabecera\n",
        "                self.sections.append((\n",
        "                    \"## 🔍 Análisis xIA de Random Forest: Resultados concretos y explicaciones Generativas\",\n",
        "                    \"\"  # contenido vacío, la cabecera se mostrará como Markdown\n",
        "                ))\n",
        "\n",
        "\n",
        "            # Función para llamar a OpenAI con un prompt específico\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                \"\"\"\n",
        "                Llama a OpenAI ChatCompletion con un sistema experto en ML/XAI,\n",
        "                devuelve la respuesta de la IA en texto.\n",
        "                \"\"\"\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un asistente experto en Machine Learning e interpretabilidad de modelos. \"\n",
        "                                \"Proporciona explicaciones detalladas y basadas en los datos concretos proporcionados.\"\n",
        "                            )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    texto = response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    texto = f\"[Error llamando a OpenAI: {e}]\"\n",
        "                return texto\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Configuración: top N variables y primeras M muestras locales\n",
        "            # ----------------------------------------------------------------\n",
        "            TOP_N = 5      # cuántas características top incluir en el prompt\n",
        "            N_LOCAL = 3    # cuántas muestras locales incluir en prompt\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Lista de métodos xIA y claves en xai_results['Random Forest']\n",
        "            # ----------------------------------------------------------------\n",
        "            # Las claves deben coincidir exactamente con las usadas en Celda 10 al almacenar resultados.\n",
        "            metodos_claves = [\n",
        "                ('SHAP', 'SHAP'),\n",
        "                ('LIME', 'LIME'),\n",
        "                ('KernelExplainer', 'KernelExplainer'),\n",
        "                ('Integrated Gradients', 'Integrated Gradients'),\n",
        "                ('DeepLIFT / LRP', 'DeepLIFT / LRP'),\n",
        "                ('Permutation Feature Importance', 'Permutation Feature Importance'),\n",
        "                ('Partial Dependence Plots (PDP)', 'Partial Dependence Plots (PDP)'),\n",
        "                ('Accumulated Local Effects (ALE)', 'Accumulated Local Effects (ALE)'),\n",
        "                ('Individual Conditional Expectation (ICE) Plots', 'Individual Conditional Expectation (ICE) Plots'),\n",
        "                ('Counterfactual Explanations', 'Counterfactual Explanations'),\n",
        "                ('Anchors', 'Anchors'),\n",
        "                ('Surrogate Models (Global/Local)', 'Surrogate Models (Global/Local)'),\n",
        "                ('Explainable Boosting Machine (EBM)', 'Explainable Boosting Machine (EBM)'),\n",
        "                ('Optuna Hyperparameter Importance', 'Optuna Hyperparameter Importance'),\n",
        "            ]\n",
        "\n",
        "            for titulo, clave in metodos_claves:\n",
        "                print(f\"[DEBUG] 9.17. Procesando sección xIA: {titulo}\")\n",
        "                datos = xai_results['Random Forest'].get(clave)\n",
        "                if datos is None:\n",
        "                    print(f\"[DEBUG] No hay resultados xIA para {titulo}, se omite\")\n",
        "                    continue\n",
        "#                    self.sections.append((\n",
        "#                        f\"### ⚠️ No hay resultados para {titulo}\",\n",
        "#                        f\"No se hallaron resultados para la clave '{clave}'.\"\n",
        "#                    ))\n",
        "#                    continue\n",
        "\n",
        "                # ---------------- Mostrar figura guardada ----------------\n",
        "                print(f\"[DEBUG] 9.18. Mostrando figura para {titulo}\")\n",
        "                fig = datos.get('fig_summary') or datos.get('fig')\n",
        "                if fig is not None:\n",
        "                    self.sections.append((f\"### {titulo}: Gráfico\", fig))\n",
        "\n",
        "                # --------------- Mostrar DataFrames de importancia y local ---------------\n",
        "                print(f\"[DEBUG] 9.19. Mostrando DataFrames para {titulo}\")\n",
        "                imp_df = datos.get('imp_df')\n",
        "                df_local = datos.get('df_local')\n",
        "                stats_extra = datos.get('stats', None)  # opcional: estadísticas adicionales, p.ej. percentiles, pos_pct SHAP, etc.\n",
        "\n",
        "                if isinstance(imp_df, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Importancia global\", imp_df.reset_index(drop=True)))\n",
        "\n",
        "                if isinstance(df_local, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Valores locales\", df_local.head(N_LOCAL)))\n",
        "\n",
        "                # --------------------------------- Extraer datos del gráfico (suponiendo que sea un barh con 'feature' y 'importance') ---------------------------\n",
        "                features = [text.get_text() for text in ax.get_yticklabels()]\n",
        "                importances = [bar.get_width() for bar in ax.patches]\n",
        "\n",
        "                # --------------- Construir prompt con valores numéricos concretos ---------------\n",
        "                print(f\"[DEBUG] 9.20. Construyendo prompt para {titulo}\")\n",
        "                prompt = f\"He aplicado el método xIA '{titulo}' al modelo Random Forest entrenado y he obtenido estos resultados concretos:\\n\\n\"\n",
        "\n",
        "                prompt += \"- Datos del gráfico (feature vs importancia):\\n\"\n",
        "                for f, imp in zip(features, importances):\n",
        "                    prompt += f\"    • {f}: {imp:.4f}\\n\"\n",
        "\n",
        "                # Ahora sí le pides que interprete el gráfico:\n",
        "                prompt += (\n",
        "                    \"- Interpreta el gráfico anterior: \"\n",
        "                    \"describe qué patrones o relaciones visuales revela cómo se distribuye la importancia.\\n\"\n",
        "                )\n",
        "\n",
        "                # 1) Extraer importancia global: top N\n",
        "                if isinstance(imp_df, pd.DataFrame) and not imp_df.empty:\n",
        "                    imp_df = imp_df.reset_index()\n",
        "                    imp_df = imp_df.rename(columns={ imp_df.columns[0]: \"feature\" })\n",
        "                    cols = imp_df.columns.tolist()\n",
        "                    if len(cols) >= 2:\n",
        "                        feat_col = \"feature\"\n",
        "                        val_col  = cols[1]\n",
        "                        try:\n",
        "                            imp_df_sorted = imp_df.sort_values(val_col, ascending=False)\n",
        "                        except Exception:\n",
        "                            imp_df_sorted = imp_df\n",
        "\n",
        "                        top_n      = min(TOP_N, len(imp_df_sorted))\n",
        "                        top_imp_df = imp_df_sorted.iloc[:top_n][[feat_col, val_col]]\n",
        "                        top_imp_list = []\n",
        "                        for _, row in top_imp_df.iterrows():\n",
        "                            raw = row[val_col]\n",
        "                            try:\n",
        "                                v = float(raw)\n",
        "                            except Exception:\n",
        "                                v = raw  # si no se puede convertir, lo dejo tal cual\n",
        "                            top_imp_list.append({feat_col: row[feat_col], val_col: v})\n",
        "\n",
        "                        prompt += f\"- Top {top_n} características por importancia global ({feat_col}, {val_col}):\\n  {top_imp_list}\\n\"\n",
        "\n",
        "                        extra_cols = cols[2:]\n",
        "                        if extra_cols:\n",
        "                            prompt += f\"  (El DataFrame de importancia global incluye también columnas: {extra_cols}.)\\n\"\n",
        "                    else:\n",
        "                        snippet = imp_df.head(TOP_N).to_dict(orient='records')\n",
        "                        prompt += f\"- Importancia global (primeras filas, formato genérico):\\n  {snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de importancia global.\\n\"\n",
        "\n",
        "#                    # — Siempre sacamos el índice como columna llamada \"feature\"\n",
        "                    imp_df = imp_df.rename(columns={ idx_col: \"feature\" })\n",
        "\n",
        "                # 2) Extraer valores locales: primeras M muestras\n",
        "                if isinstance(df_local, pd.DataFrame) and not df_local.empty:\n",
        "                    n_loc = min(N_LOCAL, len(df_local))\n",
        "                    loc_snippet = df_local.head(n_loc).to_dict(orient='records')\n",
        "                    prompt += f\"- Valores locales para las primeras {n_loc} muestras (cada dict mapea feature a valor/atribución):\\n  {loc_snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de valores locales.\\n\"\n",
        "\n",
        "                # 3) Incluir estadísticas extra si existen\n",
        "                if isinstance(stats_extra, dict):\n",
        "                    prompt += \"- Estadísticas adicionales:\\n\"\n",
        "                    for k, v in stats_extra.items():\n",
        "                        prompt += f\"  * {k}: {v}\\n\"\n",
        "\n",
        "                # 4) Contexto general del modelo Random Forest\n",
        "                prompt += (\n",
        "                    \"\\nContexto: El modelo Random Forest fue entrenado con variables seleccionadas y StandardScaler, \"\n",
        "                    \"con predicciones desescaladas. Ahora interpretamos los resultados xIA para este Random Forest.\\n\"\n",
        "                )\n",
        "\n",
        "                # 5) Preguntas/pautas específicas según el método\n",
        "                if clave == 'SHAP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores concretos de SHAP para:\\n\"\n",
        "                        \"1. Explicar la importancia global de cada variable en base a los valores de SHAP mostrados.\\n\"\n",
        "                        \"2. Analizar los valores SHAP de las primeras muestras: si una variable muestra SHAP positivo alto, ¿qué implica sobre la predicción en ese caso? Y si es negativo, ¿qué implica?\\n\"\n",
        "                        \"3. Identificar patrones en SHAP (por ejemplo, variables que consistentemente empujan en una dirección) y cómo afecta al comportamiento general del Random Forest.\\n\"\n",
        "                        \"4. Sugerir posibles transformaciones de variables o validaciones adicionales basadas en estos resultados SHAP.\\n\"\n",
        "                    )\n",
        "                elif clave == 'LIME':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos pesos LIME concretos para:\\n\"\n",
        "                        \"1. Explicar el significado de pesos positivos y negativos en LIME para Random Forest.\\n\"\n",
        "                        \"2. Analizar casos de las primeras muestras: si una variable tiene peso LIME fuerte (positivo/negativo), ¿qué implica para la predicción local?\\n\"\n",
        "                        \"3. Comentar si la variabilidad de los pesos sugiere relaciones no lineales o interacciones no capturadas.\\n\"\n",
        "                        \"4. Recomendar acciones: agrupación de variables, detección de outliers, etc., basadas en la interpretación LIME.\\n\"\n",
        "                    )\n",
        "                elif clave == 'KernelExplainer':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de KernelExplainer (SHAP caja negra) para:\\n\"\n",
        "                        \"1. Explicar la importancia global según los valores medios absolutos Kernel SHAP.\\n\"\n",
        "                        \"2. Analizar los valores locales para las primeras muestras: cómo cada característica empuja la predicción en cada caso.\\n\"\n",
        "                        \"3. Comparar con SHAP (si ya lo hiciste con TreeExplainer para otro modelo o Kernel SHAP para Random Forest), si aplica.\\n\"\n",
        "                        \"4. Sugerir consideraciones sobre fondo (background) usado y posibles ajustes si las explicaciones muestran comportamiento inesperado.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Integrated Gradients':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Integrated Gradients para:\\n\"\n",
        "                        \"1. Explicar la contribución integrada de cada variable: interpretación de importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: qué implicaciones tiene un valor IG alto o bajo en cada variable.\\n\"\n",
        "                        \"3. Señalar limitaciones: compatibilidad con Random Forest no diferenciable; si estos valores provienen de un modelo aproximado, comentar fiabilidad.\\n\"\n",
        "                        \"4. Sugerir pasos adicionales o comparaciones con otros métodos (SHAP/LIME) para validar interpretaciones.\\n\"\n",
        "                    )\n",
        "                elif clave == 'DeepLIFT / LRP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de DeepLIFT / LRP para:\\n\"\n",
        "                        \"1. Explicar la relevancia asignada a cada variable en importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: si una variable tiene relevancia positiva o negativa, qué significa para la predicción.\\n\"\n",
        "                        \"3. Comparar con IG o SHAP si se dispone: consistencia de atribuciones.\\n\"\n",
        "                        \"4. Recomendar verificaciones o transformaciones en caso de interpretaciones inesperadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Permutation Feature Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Permutation Feature Importance para:\\n\"\n",
        "                        \"1. Explicar la caída en la métrica al permutar cada variable: por qué ciertas variables son críticas.\\n\"\n",
        "                        \"2. Comentar la desviación estándar: ¿indica inestabilidad en la importancia? ¿Dónde conviene reforzar validaciones?\\n\"\n",
        "                        \"3. Comparar con importancias de SHAP/LIME: similitudes o diferencias.\\n\"\n",
        "                        \"4. Sugerir prioridades para ajuste de modelo o selección de variables basadas en esta métrica.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Partial Dependence Plots (PDP)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos PDP para:\\n\"\n",
        "                        \"1. Explicar el efecto promedio de cada variable sobre la predicción según el rango PDP obtenido.\\n\"\n",
        "                        \"2. Señalar si los rangos sugieren relaciones monótonas o no lineales.\\n\"\n",
        "                        \"3. Advertir sobre correlaciones fuertes que puedan afectar la interpretación.\\n\"\n",
        "                        \"4. Recomendar posibles exploraciones adicionales (PDP bivariados, transformaciones) según los resultados.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Accumulated Local Effects (ALE)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ALE y valores locales ALE para:\\n\"\n",
        "                        \"1. Explicar cómo ALE corrige artefactos de correlación y qué nos dicen los valores concretos.\\n\"\n",
        "                        \"2. Interpretar importancia global ALE: variables con mayor efecto acumulado.\\n\"\n",
        "                        \"3. Analizar heterogeneidad local a partir de valores ALE de primeras muestras.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales (ALE 2D, inspección de distribución) según hallazgos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Individual Conditional Expectation (ICE) Plots':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ICE medios para:\\n\"\n",
        "                        \"1. Explicar qué mide el rango ICE y su diferencia respecto a PDP.\\n\"\n",
        "                        \"2. Analizar heterogeneidad: variables con alta variabilidad en rangos ICE indican interacciones o comportamiento inestable.\\n\"\n",
        "                        \"3. Señalar implicaciones para el modelo y posibles ajustes si hay alto efecto local variable.\\n\"\n",
        "                        \"4. Recomendar exploraciones adicionales para entender la variabilidad local.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Counterfactual Explanations':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Contrafactuales para:\\n\"\n",
        "                        \"1. Explicar cómo interpretar los contrafactuales: cambios en variables que generan aumento en predicción.\\n\"\n",
        "                        \"2. Analizar variables con mayor |Δ| medio: implicaciones sobre sensibilidad del modelo.\\n\"\n",
        "                        \"3. Señalar si faltan contrafactuales para algunas muestras: qué puede indicar (limites del modelo o datos).\\n\"\n",
        "                        \"4. Sugerir cómo usar estos insights para ajuste de modelo o recolección de datos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Anchors':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Anchors para:\\n\"\n",
        "                        \"1. Explicar cómo leer las reglas ancla de las primeras muestras: condiciones que aseguran la predicción.\\n\"\n",
        "                        \"2. Analizar frecuencia global de aparición de variables en reglas: implicaciones sobre estabilidad y sesgos.\\n\"\n",
        "                        \"3. Señalar regiones de bajo coverage o baja precisión: dónde el modelo es menos fiable.\\n\"\n",
        "                        \"4. Recomendar acciones: recolección de datos, refinamiento de variables o validaciones dirigidas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Surrogate Models (Global/Local)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Surrogate Models para:\\n\"\n",
        "                        \"1. Explicar la importancia global del surrogate (árbol) y la importancia local media (coeficientes regresiones locales).\\n\"\n",
        "                        \"2. Comparar global vs local: variables con alta importancia local pero baja global, o viceversa, y qué sugiere.\\n\"\n",
        "                        \"3. Concluir sobre consistencia de comportamiento del Random Forest en diferentes regiones del espacio.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales o ajustes de modelo según discrepancias detectadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Explainable Boosting Machine (EBM)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de EBM para:\\n\"\n",
        "                        \"1. Explicar la importancia global según EBM: cómo se comparan con otros métodos.\\n\"\n",
        "                        \"2. Analizar contribuciones locales de las primeras muestras: qué patrones se observan.\\n\"\n",
        "                        \"3. Señalar si EBM revela interacciones no consideradas en Random Forest.\\n\"\n",
        "                        \"4. Recomendar posibles ajustes en características o validaciones según insights de EBM.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Optuna Hyperparameter Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Optuna para:\\n\"\n",
        "                        \"1. Explicar la importancia global de hiperparámetros en la optimización del Random Forest.\\n\"\n",
        "                        \"2. Analizar top trials si están disponibles: qué combinaciones de hiperparámetros funcionaron mejor.\\n\"\n",
        "                        \"3. Señalar limitaciones de la muestra de trials (número de pruebas) y posibles riesgos de sobreajuste en la búsqueda.\\n\"\n",
        "                        \"4. Recomendar próximas acciones para tuning basadas en estas importancias.\\n\"\n",
        "                    )\n",
        "                else:\n",
        "                    prompt += \"\\nPor favor, explica estos resultados numéricos y qué implicaciones tienen para el modelo Random Forest.\\n\"\n",
        "\n",
        "                # --------------- Llamada a OpenAI ---------------\n",
        "                print(f\"[DEBUG] 9.21. Llamando a OpenAI para {titulo}\")\n",
        "                explicacion = call_openai_explanation(prompt)\n",
        "                self.sections.append((f\"### {titulo}: Explicación Generativa\", explicacion))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en sección xIA Random Forest\",\n",
        "                f\"Se produjo un error al generar la sección xIA de Random Forest: {e}\"\n",
        "            ))\n",
        "\n",
        "\n",
        "        # =====================================================================\n",
        "        # 10. Entrenamiento Modelo Redes Neuronales Recurrentes RNN\n",
        "        # =====================================================================\n",
        "        try:\n",
        "            if \"RESUMEN_METODOS\" in self.g and \"X_test\" in self.g and \"Y_test\" in self.g:\n",
        "                import os, pickle\n",
        "                import numpy as _np\n",
        "                import pandas as _pd\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "                from tensorflow.keras.models import load_model\n",
        "\n",
        "                X_test_full = self.g[\"X_test\"]\n",
        "                Y_test = self.g[\"Y_test\"]\n",
        "                # Extraer array 1D de y_test\n",
        "                if hasattr(Y_test, \"values\"):\n",
        "                    arr = Y_test.values\n",
        "                    if arr.ndim == 2 and arr.shape[1] >= 1:\n",
        "                        y_test_arr_full = arr[:, 0]\n",
        "                    else:\n",
        "                        y_test_arr_full = arr.ravel()\n",
        "                else:\n",
        "                    y_test_arr_full = _np.array(Y_test).ravel()\n",
        "                y_test_arr_full = y_test_arr_full.ravel()\n",
        "\n",
        "                metrics_summary_rnn = []\n",
        "                # Iteramos sobre cada método en RESUMEN_METODOS\n",
        "                for metodo, vars_sel in self.g[\"RESUMEN_METODOS\"].items():\n",
        "                    metodo_low = metodo.lower()\n",
        "                    model_fname = f\"modelo_rnn_{metodo_low}.h5\"\n",
        "                    scaler_fname = f\"escaladores_rnn_{metodo_low}.pkl\"\n",
        "                    hp_fname = f\"hyperparams_rnn_{metodo_low}.pkl\"\n",
        "                    if not os.path.exists(model_fname) or not os.path.exists(scaler_fname):\n",
        "                        print(f\"[DEBUG] Fichero de modelo RNN o escaladores no encontrado para método '{metodo}', omito este método\")\n",
        "                        continue\n",
        "                    # Cargar modelo y escaladores\n",
        "                    try:\n",
        "                        model = load_model(model_fname)\n",
        "                        with open(scaler_fname, \"rb\") as f:\n",
        "                            data_s = pickle.load(f)\n",
        "                        sx = data_s.get(\"scaler_X\", None)\n",
        "                        sy = data_s.get(\"scaler_Y\", None)\n",
        "                        cols = data_s.get(\"cols\", None)\n",
        "                        if model is None or sx is None or sy is None or cols is None:\n",
        "                            print(f\"[DEBUG] Falta clave en escaladores RNN para método '{metodo}', omito\")\n",
        "                            continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al cargar modelo/escaladores RNN para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Cargar hiperparámetros\n",
        "                    if os.path.exists(hp_fname):\n",
        "                        try:\n",
        "                            with open(hp_fname, \"rb\") as f_hp:\n",
        "                                hp = pickle.load(f_hp)\n",
        "                        except Exception as e:\n",
        "                            print(f\"[ERROR] al cargar hyperparams RNN para método '{metodo}': {e}\")\n",
        "                            hp = None\n",
        "                    else:\n",
        "                        hp = None\n",
        "                    if hp is None:\n",
        "                        print(f\"[DEBUG] No hay hyperparams guardados para RNN método '{metodo}', no podré rehacer predicción correctamente, omito este método en informe\")\n",
        "                        continue\n",
        "\n",
        "                    # Verificar columnas en X_test\n",
        "                    missing = [c for c in cols if c not in X_test_full.columns]\n",
        "                    if missing:\n",
        "                        print(f\"[DEBUG] Para método '{metodo}', faltan columnas en X_test: {missing}, omito\")\n",
        "                        continue\n",
        "\n",
        "                    # Preparamos datos para secuencias\n",
        "                    # Tomamos solo las columnas seleccionadas\n",
        "                    X_test_sel = X_test_full[cols].copy()\n",
        "                    # Escalar toda la serie de test antes de crear secuencias\n",
        "                    try:\n",
        "                        Xts = sx.transform(X_test_sel)\n",
        "                        # Para Y_test, necesitamos construir array alineado con secuencias:\n",
        "                        # Usamos los primeros len(Xts) valores de y_test_arr_full\n",
        "                        # y construiremos secuencias con window_size:\n",
        "                        window = int(hp.get('window_size', 0))\n",
        "                        if window <= 0 or len(Xts) <= window:\n",
        "                            print(f\"[DEBUG] window_size inválido o demasiado grande para test en método '{metodo}', omito\")\n",
        "                            continue\n",
        "                        # Creamos secuencias iguales a la función create_sequences de la celda 7.5:\n",
        "                        X_seq = []\n",
        "                        Y_seq = []\n",
        "                        for j in range(len(Xts) - window):\n",
        "                            X_seq.append(Xts[j:j+window])\n",
        "                            # Y real escalada (sy) usamos Y_test escalado? En entrenamiento se usó y_train escalado para fit,\n",
        "                            # pero aquí Y_test necesitamos escala para invertir luego:\n",
        "                            # Mejor: tomamos Y_test original:\n",
        "                            # Extraemos Y_test alineado: y_test_arr_full, yts_seq será y_test_arr_full[j+window]\n",
        "                            Y_seq.append(y_test_arr_full[j+window])\n",
        "                        X_seq = _np.array(X_seq)   # shape (n_samples_seq, window, n_features)\n",
        "                        Y_real = _np.array(Y_seq)  # shape (n_samples_seq,)\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al preparar secuencias RNN para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Predecir y desescalar\n",
        "                    try:\n",
        "                        Y_pred_scaled = model.predict(X_seq, verbose=0).ravel()\n",
        "                        # Invertir escala:\n",
        "                        Y_pred = sy.inverse_transform(Y_pred_scaled.reshape(-1,1)).ravel()\n",
        "                        # Convertir Y_real (original) en array float\n",
        "                        Y_real = _np.array(Y_real).ravel()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al predecir/desescalar RNN para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Cálculo de métricas y estadísticas\n",
        "                    try:\n",
        "                        # Rangos Real vs Predicha\n",
        "                        y_real_min, y_real_max = float(_np.min(Y_real)), float(_np.max(Y_real))\n",
        "                        y_pred_min, y_pred_max = float(_np.min(Y_pred)), float(_np.max(Y_pred))\n",
        "                        # Residuos\n",
        "                        residuals = Y_real - Y_pred\n",
        "                        res_mean = float(_np.mean(residuals))\n",
        "                        res_std  = float(_np.std(residuals))\n",
        "                        res_series = _pd.Series(residuals)\n",
        "                        res_skew = float(res_series.skew())\n",
        "                        res_kurt = float(res_series.kurtosis())\n",
        "                        q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]\n",
        "                        # Métricas\n",
        "                        mse = mean_squared_error(Y_real, Y_pred)\n",
        "                        rmse = float(_np.sqrt(mse))\n",
        "                        mae = float(mean_absolute_error(Y_real, Y_pred))\n",
        "                        r2 = float(r2_score(Y_real, Y_pred))\n",
        "                        # Correlación Real vs Predicha\n",
        "                        try:\n",
        "                            corr = float(_np.corrcoef(Y_real, Y_pred)[0, 1])\n",
        "                        except:\n",
        "                            corr = None\n",
        "                        # Guardar resumen para comparativa\n",
        "                        metrics_summary_rnn.append({\n",
        "                            \"metodo\": metodo,\n",
        "                            \"rmse\": rmse,\n",
        "                            \"mae\": mae,\n",
        "                            \"r2\": r2\n",
        "                        })\n",
        "                        #all_metrics.append({\n",
        "                        #    \"Modelo\": f\"{TIPO}_{metodo}\",  # ej. \"SVR_Pearson\", \"NN_Boruta\"...\n",
        "                        #    \"Tipo\": TIPO,                 # \"SVR\", \"NN\", \"XGBoost\", \"RF\" o \"RNN\"\n",
        "                        #    \"Método\": metodo,\n",
        "                        #    \"R2\": r2,\n",
        "                        #    \"MSE\": mse,\n",
        "                        #    \"RMSE\": rmse,\n",
        "                        #    \"MAE\": mae\n",
        "                        #})\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al calcular métricas RNN para método '{metodo}': {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # 1) Parámetros de entrenamiento obtenidos desde hp\n",
        "                    try:\n",
        "                        # Usamos el dict hp cargado\n",
        "                        df_hp = _pd.DataFrame({\n",
        "                            \"Hiperparámetro\": list(hp.keys()),\n",
        "                            \"Valor\": [str(v) for v in hp.values()]\n",
        "                        })\n",
        "                        titulo_hp = f\"### Parámetros de Entrenamiento RNN ({metodo})\"\n",
        "                        self.sections.append((titulo_hp, df_hp))\n",
        "                        print(f\"[DEBUG] 10.1. Sección parámetros RNN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al mostrar parámetros RNN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA hiperparámetros RNN\n",
        "                    try:\n",
        "                        prompt_params = (\n",
        "                            f\"Has entrenado un modelo RNN (tipo {hp.get('tipo_rnn')}) con selección de variables '{metodo}'.\\n\"\n",
        "                            \"Estos fueron los hiperparámetros utilizados:\\n\"\n",
        "                        )\n",
        "                        for k, v in hp.items():\n",
        "                            prompt_params += f\"- {k}: {v}\\n\"\n",
        "                        prompt_params += (\n",
        "                            \"\\nPor favor, explica de forma profesional y detallada cómo estos hiperparámetros \"\n",
        "                            \"pueden influir en el entrenamiento de la RNN, su impacto en ajuste/sobreajuste, \"\n",
        "                            \"y buenas prácticas para seleccionarlos o afinarlos.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 10.2. Iniciando llamada a OpenAI para explicación hiperparámetros RNN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en entrenamiento de redes neuronales recurrentes para series temporales.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_params}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_hp = resp.choices[0].message.content.strip()\n",
        "                        if explanation_hp:\n",
        "                            titulo_exp_hp = f\"### 📝 Explicación IA Hiperparámetros RNN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_hp, explanation_hp))\n",
        "                            print(f\"[DEBUG] 10.3. Sección explicación IA hiperparámetros RNN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA hiperparámetros RNN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 2) Gráfica Predicho vs Real\n",
        "                    try:\n",
        "                        fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "                        ax1.scatter(Y_real, Y_pred, alpha=0.6)\n",
        "                        ax1.plot([y_real_min, y_real_max], [y_real_min, y_real_max], 'r--', lw=2)\n",
        "                        ax1.set_xlabel(\"Y real\")\n",
        "                        ax1.set_ylabel(\"Y predicho\")\n",
        "                        ax1.set_title(f\"RNN Predicho vs Real ({metodo})\")\n",
        "                        titulo_fig1 = f\"### Gráfico RNN Predicho vs Real ({metodo})\"\n",
        "                        self.sections.append((titulo_fig1, fig1))\n",
        "                        print(f\"[DEBUG] 10.4. Sección gráfica Pred vs Real RNN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear gráfica Pred vs Real RNN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA Pred vs Real RNN con contexto numérico\n",
        "                    try:\n",
        "                        prompt_pr = (\n",
        "                            f\"A continuación tienes datos de la gráfica de comparación Real vs Predicción para el modelo RNN con método '{metodo}':\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- R²: {r2}\\n\"\n",
        "                            f\"- Correlación entre Y real y predicha: {corr}\\n\"\n",
        "                            f\"- Rango Y real: [{y_real_min}, {y_real_max}]\\n\"\n",
        "                            f\"- Rango Y predicha: [{y_pred_min}, {y_pred_max}]\\n\\n\"\n",
        "                            \"Basándote en estos valores y en la gráfica generada (Real vs Predicción), \"\n",
        "                            \"proporciona un análisis detallado: sesgos sistemáticos, dispersión en rangos, posibles problemas y recomendaciones.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 10.5. Llamada IA Pred vs Real RNN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML para series temporales.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_pr}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_pr = resp.choices[0].message.content.strip()\n",
        "                        if explanation_pr:\n",
        "                            titulo_exp_pr = f\"### 📝 Explicación IA Predicho vs Real RNN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_pr, explanation_pr))\n",
        "                            print(f\"[DEBUG] 10.6. Sección explicación IA Pred vs Real RNN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA Pred vs Real RNN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 3) Gráfica de residuos RNN\n",
        "                    try:\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "                        ax2.scatter(Y_pred, residuals, alpha=0.6)\n",
        "                        ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "                        ax2.set_xlabel(\"Y predicho\")\n",
        "                        ax2.set_ylabel(\"Residuo (Y_real - Y_predicho)\")\n",
        "                        ax2.set_title(f\"RNN Residuos ({metodo})\")\n",
        "                        titulo_fig2 = f\"### Gráfico RNN Residuos ({metodo})\"\n",
        "                        self.sections.append((titulo_fig2, fig2))\n",
        "                        print(f\"[DEBUG] 10.7. Sección gráfica residuos RNN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear gráfica residuos RNN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # Explicación IA Residuos RNN con contexto numérico\n",
        "                    try:\n",
        "                        prompt_res = (\n",
        "                            f\"A continuación tienes estadísticas de los residuos (Real - Predicha) del modelo RNN con método '{metodo}':\\n\"\n",
        "                            f\"- Media: {res_mean}\\n\"\n",
        "                            f\"- Desviación estándar: {res_std}\\n\"\n",
        "                            f\"- Asimetría: {res_skew}\\n\"\n",
        "                            f\"- Curtosis: {res_kurt}\\n\"\n",
        "                            f\"- Cuantiles: 25%={q25}, 50%={q50}, 75%={q75}\\n\\n\"\n",
        "                            \"Basándote en estos valores y en la gráfica de residuos, analiza patrones (heterocedasticidad, outliers, sesgos) y qué implicaciones tiene para generalización.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 10.8 Llamada IA Residuos RNN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML para series temporales.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_res}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_res = resp.choices[0].message.content.strip()\n",
        "                        if explanation_res:\n",
        "                            titulo_exp_res = f\"### 📝 Explicación IA Residuos RNN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_res, explanation_res))\n",
        "                            print(f\"[DEBUG] 10.9. Sección explicación IA residuos RNN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA residuos RNN para método '{metodo}': {e}\")\n",
        "\n",
        "                    # 4) Tabla Métricas y explicación IA\n",
        "                    try:\n",
        "                        df_met = _pd.DataFrame([\n",
        "                            {\"Métrica\": \"RMSE\", \"Valor\": rmse},\n",
        "                            {\"Métrica\": \"MAE\", \"Valor\": mae},\n",
        "                            {\"Métrica\": \"R2\",  \"Valor\": r2}\n",
        "                        ])\n",
        "                        titulo_met = f\"### Métricas RNN ({metodo})\"\n",
        "                        self.sections.append((titulo_met, df_met))\n",
        "                        print(f\"[DEBUG] 10.10. Sección métricas RNN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame métricas RNN para método '{meteto}': {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_met = (\n",
        "                            f\"Estas son las métricas del modelo RNN con método '{metodo}':\\n\"\n",
        "                            f\"- R2: {r2}\\n\"\n",
        "                            f\"- MSE: {mse}\\n\"\n",
        "                            f\"- RMSE: {rmse}\\n\"\n",
        "                            f\"- MAE: {mae}\\n\"\n",
        "                            f\"- Correlación Real vs Predicha: {corr}\\n\\n\"\n",
        "                            \"Analiza estos valores en contexto: ¿son adecuados? ¿qué sugieren respecto al rendimiento? Menciona gráficas Pred vs Real y residuos.\"\n",
        "                        )\n",
        "                        print(f\"[DEBUG] 10.11. Llamada IA Métricas RNN ({metodo})...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en evaluación de modelos ML para series temporales.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_met}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_met = resp.choices[0].message.content.strip()\n",
        "                        if explanation_met:\n",
        "                            titulo_exp_met = f\"### 📝 Explicación IA Métricas RNN ({metodo})\"\n",
        "                            self.sections.append((titulo_exp_met, explanation_met))\n",
        "                            print(f\"[DEBUG] 10.12. Sección explicación IA métricas RNN añadida para método: {metodo}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA métricas RNN para método '{metodo}': {e}\")\n",
        "\n",
        "                # 5) Comparativa global de métricas RNN\n",
        "                if metrics_summary_rnn:\n",
        "                    try:\n",
        "                        df_comp_rnn = _pd.DataFrame(metrics_summary_rnn)\n",
        "                        df_comp_rnn_sorted = df_comp_rnn.sort_values(\"rmse\")\n",
        "                        titulo_comp_rnn = \"### Comparativa Métricas RNN entre Métodos\"\n",
        "                        self.sections.append((titulo_comp_rnn, df_comp_rnn_sorted))\n",
        "                        print(\"[DEBUG] 10.13. Sección comparativa métricas RNN añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al crear DataFrame comparativo métricas RNN: {e}\")\n",
        "\n",
        "                    try:\n",
        "                        prompt_conc = (\n",
        "                            \"Se han entrenado varios modelos RNN con diferentes métodos de selección de variables.\\n\"\n",
        "                            \"Métricas obtenidas en test:\\n\"\n",
        "                        )\n",
        "                        for entry in metrics_summary_rnn:\n",
        "                            prompt_conc += f\"- Método '{entry['metodo']}': RMSE={entry['rmse']}, MAE={entry['mae']}, R2={entry['r2']}\\n\"\n",
        "                        prompt_conc += (\n",
        "                            \"\\nPor favor, proporciona conclusiones profesionales comparando estos métodos: \"\n",
        "                            \"indica cuál se comporta mejor, posibles razones y recomendaciones sobre selección de variables o ajustes para mejorar la RNN.\"\n",
        "                        )\n",
        "                        print(\"[DEBUG] 10.14. Llamada IA Conclusiones RNN...\")\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos para series temporales.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_conc}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        explanation_conc = resp.choices[0].message.content.strip()\n",
        "                        if explanation_conc:\n",
        "                            titulo_exp_conc = \"### 📝 Conclusiones IA Entrenamiento RNN\"\n",
        "                            self.sections.append((titulo_exp_conc, explanation_conc))\n",
        "                            print(\"[DEBUG] 10.15. Sección explicación IA conclusiones RNN añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA conclusiones RNN: {e}\")\n",
        "            else:\n",
        "                print(\"[DEBUG] No están RESUMEN_METODOS o X_test/Y_test en globals(), omito sección RNN\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar sección RNN en informe: {e}\")\n",
        "        # ... fin de la sección de entrenamiento de Redes Neuronales Rrecurrentes RNN ...\n",
        "\n",
        "        # =============================================================\n",
        "        # 10.1. Interpretación xIA para modelo entrenado RNN\n",
        "        # =============================================================\n",
        "        import openai\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # Verificación previa\n",
        "        # ----------------------------------------------------------------\n",
        "        # ── 0) Normalizar la clave para buscar en xai_results ────────\n",
        "        #storage_key = 'RNN'.lower()   # coincide con la clave usada al guardar en la celda 10 ('rnn')\n",
        "        try:\n",
        "            print(\"[DEBUG] 10.16. Iniciando sección xIA para RNN\")\n",
        "            print(\"DEBUG: claves en xai_results:\", list(xai_results.keys()))\n",
        "            if 'xai_results' not in globals() or 'RNN' not in xai_results:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontró `xai_results['rnn']`. \"\n",
        "                    \"Asegúrate de haber ejecutado la Celda 10 y almacenado los resultados xIA de RNN en `xai_results['rnn']`.\"\n",
        "                )\n",
        "\n",
        "                # Cabecera\n",
        "                self.sections.append((\n",
        "                    \"## 🔍 Análisis xIA de RNN: Resultados concretos y explicaciones Generativas\",\n",
        "                    \"\"  # contenido vacío, la cabecera se mostrará como Markdown\n",
        "                ))\n",
        "\n",
        "\n",
        "            # Función para llamar a OpenAI con un prompt específico\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                \"\"\"\n",
        "                Llama a OpenAI ChatCompletion con un sistema experto en ML/XAI,\n",
        "                devuelve la respuesta de la IA en texto.\n",
        "                \"\"\"\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un asistente experto en Machine Learning e interpretabilidad de modelos. \"\n",
        "                                \"Proporciona explicaciones detalladas y basadas en los datos concretos proporcionados.\"\n",
        "                            )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    texto = response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    texto = f\"[Error llamando a OpenAI: {e}]\"\n",
        "                return texto\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Configuración: top N variables y primeras M muestras locales\n",
        "            # ----------------------------------------------------------------\n",
        "            TOP_N = 5      # cuántas características top incluir en el prompt\n",
        "            N_LOCAL = 3    # cuántas muestras locales incluir en prompt\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Lista de métodos xIA y claves en xai_results['RNN']\n",
        "            # ----------------------------------------------------------------\n",
        "            # Las claves deben coincidir exactamente con las usadas en Celda 10 al almacenar resultados.\n",
        "            metodos_claves = [\n",
        "                ('SHAP', 'SHAP'),\n",
        "                ('LIME', 'LIME'),\n",
        "                ('KernelExplainer', 'KernelExplainer'),\n",
        "                ('Integrated Gradients', 'Integrated Gradients'),\n",
        "                ('DeepLIFT / LRP', 'DeepLIFT / LRP'),\n",
        "                ('Permutation Feature Importance', 'Permutation Feature Importance'),\n",
        "                ('Partial Dependence Plots (PDP)', 'Partial Dependence Plots (PDP)'),\n",
        "                ('Accumulated Local Effects (ALE)', 'Accumulated Local Effects (ALE)'),\n",
        "                ('Individual Conditional Expectation (ICE) Plots', 'Individual Conditional Expectation (ICE) Plots'),\n",
        "                ('Counterfactual Explanations', 'Counterfactual Explanations'),\n",
        "                ('Anchors', 'Anchors'),\n",
        "                ('Surrogate Models (Global/Local)', 'Surrogate Models (Global/Local)'),\n",
        "                ('Explainable Boosting Machine (EBM)', 'Explainable Boosting Machine (EBM)'),\n",
        "                ('Optuna Hyperparameter Importance', 'Optuna Hyperparameter Importance'),\n",
        "            ]\n",
        "\n",
        "            for titulo, clave in metodos_claves:\n",
        "                print(f\"[DEBUG] 10.17. Procesando sección xIA: {titulo}\")\n",
        "                datos = xai_results['RNN'].get(clave)\n",
        "                if datos is None:\n",
        "                    print(f\"[DEBUG] No hay resultados xIA para {titulo}, se omite\")\n",
        "                    continue\n",
        "#                    self.sections.append((\n",
        "#                        f\"### ⚠️ No hay resultados para {titulo}\",\n",
        "#                        f\"No se hallaron resultados para la clave '{clave}'.\"\n",
        "#                    ))\n",
        "#                    continue\n",
        "\n",
        "                # ---------------- Mostrar figura guardada ----------------\n",
        "                print(f\"[DEBUG] 10.18. Mostrando figura para {titulo}\")\n",
        "                fig = datos.get('fig_summary') or datos.get('fig')\n",
        "                if fig is not None:\n",
        "                    self.sections.append((f\"### {titulo}: Gráfico\", fig))\n",
        "\n",
        "                # --------------- Mostrar DataFrames de importancia y local ---------------\n",
        "                print(f\"[DEBUG] 10.19. Mostrando DataFrames para {titulo}\")\n",
        "                imp_df = datos.get('imp_df')\n",
        "                df_local = datos.get('df_local')\n",
        "                stats_extra = datos.get('stats', None)  # opcional: estadísticas adicionales, p.ej. percentiles, pos_pct SHAP, etc.\n",
        "\n",
        "                if isinstance(imp_df, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Importancia global\", imp_df.reset_index(drop=True)))\n",
        "\n",
        "                if isinstance(df_local, pd.DataFrame):\n",
        "                    self.sections.append((f\"### {titulo}: Valores locales\", df_local.head(N_LOCAL)))\n",
        "\n",
        "                # --------------------------------- Extraer datos del gráfico (suponiendo que sea un barh con 'feature' y 'importance') ---------------------------\n",
        "                features = [text.get_text() for text in ax.get_yticklabels()]\n",
        "                importances = [bar.get_width() for bar in ax.patches]\n",
        "\n",
        "                # --------------- Construir prompt con valores numéricos concretos ---------------\n",
        "                print(f\"[DEBUG] 10.20. Construyendo prompt para {titulo}\")\n",
        "                prompt = f\"He aplicado el método xIA '{titulo}' al modelo RNN entrenado y he obtenido estos resultados concretos:\\n\\n\"\n",
        "\n",
        "                prompt += \"- Datos del gráfico (feature vs importancia):\\n\"\n",
        "                for f, imp in zip(features, importances):\n",
        "                    prompt += f\"    • {f}: {imp:.4f}\\n\"\n",
        "\n",
        "                # Ahora sí le pides que interprete el gráfico:\n",
        "                prompt += (\n",
        "                    \"- Interpreta el gráfico anterior: \"\n",
        "                    \"describe qué patrones o relaciones visuales revela cómo se distribuye la importancia.\\n\"\n",
        "                )\n",
        "\n",
        "                # 1) Extraer importancia global: top N\n",
        "                if isinstance(imp_df, pd.DataFrame) and not imp_df.empty:\n",
        "                    imp_df = imp_df.reset_index()\n",
        "                    imp_df = imp_df.rename(columns={ imp_df.columns[0]: \"feature\" })\n",
        "                    cols = imp_df.columns.tolist()\n",
        "                    if len(cols) >= 2:\n",
        "                        feat_col = \"feature\"\n",
        "                        val_col  = cols[1]\n",
        "                        try:\n",
        "                            imp_df_sorted = imp_df.sort_values(val_col, ascending=False)\n",
        "                        except Exception:\n",
        "                            imp_df_sorted = imp_df\n",
        "\n",
        "                        top_n      = min(TOP_N, len(imp_df_sorted))\n",
        "                        top_imp_df = imp_df_sorted.iloc[:top_n][[feat_col, val_col]]\n",
        "                        top_imp_list = []\n",
        "                        for _, row in top_imp_df.iterrows():\n",
        "                            raw = row[val_col]\n",
        "                            try:\n",
        "                                v = float(raw)\n",
        "                            except Exception:\n",
        "                                v = raw  # si no se puede convertir, lo dejo tal cual\n",
        "                            top_imp_list.append({feat_col: row[feat_col], val_col: v})\n",
        "\n",
        "                        prompt += f\"- Top {top_n} características por importancia global ({feat_col}, {val_col}):\\n  {top_imp_list}\\n\"\n",
        "\n",
        "                        extra_cols = cols[2:]\n",
        "                        if extra_cols:\n",
        "                            prompt += f\"  (El DataFrame de importancia global incluye también columnas: {extra_cols}.)\\n\"\n",
        "                    else:\n",
        "                        snippet = imp_df.head(TOP_N).to_dict(orient='records')\n",
        "                        prompt += f\"- Importancia global (primeras filas, formato genérico):\\n  {snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de importancia global.\\n\"\n",
        "\n",
        "#                    # — Siempre sacamos el índice como columna llamada \"feature\"\n",
        "                    imp_df = imp_df.rename(columns={ idx_col: \"feature\" })\n",
        "\n",
        "                # 2) Extraer valores locales: primeras M muestras\n",
        "                if isinstance(df_local, pd.DataFrame) and not df_local.empty:\n",
        "                    n_loc = min(N_LOCAL, len(df_local))\n",
        "                    loc_snippet = df_local.head(n_loc).to_dict(orient='records')\n",
        "                    prompt += f\"- Valores locales para las primeras {n_loc} muestras (cada dict mapea feature a valor/atribución):\\n  {loc_snippet}\\n\"\n",
        "                else:\n",
        "                    prompt += \"- No hay datos de valores locales.\\n\"\n",
        "\n",
        "                # 3) Incluir estadísticas extra si existen\n",
        "                if isinstance(stats_extra, dict):\n",
        "                    prompt += \"- Estadísticas adicionales:\\n\"\n",
        "                    for k, v in stats_extra.items():\n",
        "                        prompt += f\"  * {k}: {v}\\n\"\n",
        "\n",
        "                # 4) Contexto general del modelo RNN\n",
        "                prompt += (\n",
        "                    \"\\nContexto: El modelo RNN fue entrenado con variables seleccionadas y StandardScaler, \"\n",
        "                    \"con predicciones desescaladas. Ahora interpretamos los resultados xIA para este RNN.\\n\"\n",
        "                )\n",
        "\n",
        "                # 5) Preguntas/pautas específicas según el método\n",
        "                if clave == 'SHAP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores concretos de SHAP para:\\n\"\n",
        "                        \"1. Explicar la importancia global de cada variable en base a los valores de SHAP mostrados.\\n\"\n",
        "                        \"2. Analizar los valores SHAP de las primeras muestras: si una variable muestra SHAP positivo alto, ¿qué implica sobre la predicción en ese caso? Y si es negativo, ¿qué implica?\\n\"\n",
        "                        \"3. Identificar patrones en SHAP (por ejemplo, variables que consistentemente empujan en una dirección) y cómo afecta al comportamiento general del RNN.\\n\"\n",
        "                        \"4. Sugerir posibles transformaciones de variables o validaciones adicionales basadas en estos resultados SHAP.\\n\"\n",
        "                    )\n",
        "                elif clave == 'LIME':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos pesos LIME concretos para:\\n\"\n",
        "                        \"1. Explicar el significado de pesos positivos y negativos en LIME para RNN.\\n\"\n",
        "                        \"2. Analizar casos de las primeras muestras: si una variable tiene peso LIME fuerte (positivo/negativo), ¿qué implica para la predicción local?\\n\"\n",
        "                        \"3. Comentar si la variabilidad de los pesos sugiere relaciones no lineales o interacciones no capturadas.\\n\"\n",
        "                        \"4. Recomendar acciones: agrupación de variables, detección de outliers, etc., basadas en la interpretación LIME.\\n\"\n",
        "                    )\n",
        "                elif clave == 'KernelExplainer':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de KernelExplainer (SHAP caja negra) para:\\n\"\n",
        "                        \"1. Explicar la importancia global según los valores medios absolutos Kernel SHAP.\\n\"\n",
        "                        \"2. Analizar los valores locales para las primeras muestras: cómo cada característica empuja la predicción en cada caso.\\n\"\n",
        "                        \"3. Comparar con SHAP (si ya lo hiciste con TreeExplainer para otro modelo o Kernel SHAP para RNN), si aplica.\\n\"\n",
        "                        \"4. Sugerir consideraciones sobre fondo (background) usado y posibles ajustes si las explicaciones muestran comportamiento inesperado.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Integrated Gradients':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Integrated Gradients para:\\n\"\n",
        "                        \"1. Explicar la contribución integrada de cada variable: interpretación de importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: qué implicaciones tiene un valor IG alto o bajo en cada variable.\\n\"\n",
        "                        \"3. Señalar limitaciones: compatibilidad con RNN no diferenciable; si estos valores provienen de un modelo aproximado, comentar fiabilidad.\\n\"\n",
        "                        \"4. Sugerir pasos adicionales o comparaciones con otros métodos (SHAP/LIME) para validar interpretaciones.\\n\"\n",
        "                    )\n",
        "                elif clave == 'DeepLIFT / LRP':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de DeepLIFT / LRP para:\\n\"\n",
        "                        \"1. Explicar la relevancia asignada a cada variable en importancia global.\\n\"\n",
        "                        \"2. Analizar las primeras muestras: si una variable tiene relevancia positiva o negativa, qué significa para la predicción.\\n\"\n",
        "                        \"3. Comparar con IG o SHAP si se dispone: consistencia de atribuciones.\\n\"\n",
        "                        \"4. Recomendar verificaciones o transformaciones en caso de interpretaciones inesperadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Permutation Feature Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos valores de Permutation Feature Importance para:\\n\"\n",
        "                        \"1. Explicar la caída en la métrica al permutar cada variable: por qué ciertas variables son críticas.\\n\"\n",
        "                        \"2. Comentar la desviación estándar: ¿indica inestabilidad en la importancia? ¿Dónde conviene reforzar validaciones?\\n\"\n",
        "                        \"3. Comparar con importancias de SHAP/LIME: similitudes o diferencias.\\n\"\n",
        "                        \"4. Sugerir prioridades para ajuste de modelo o selección de variables basadas en esta métrica.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Partial Dependence Plots (PDP)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos PDP para:\\n\"\n",
        "                        \"1. Explicar el efecto promedio de cada variable sobre la predicción según el rango PDP obtenido.\\n\"\n",
        "                        \"2. Señalar si los rangos sugieren relaciones monótonas o no lineales.\\n\"\n",
        "                        \"3. Advertir sobre correlaciones fuertes que puedan afectar la interpretación.\\n\"\n",
        "                        \"4. Recomendar posibles exploraciones adicionales (PDP bivariados, transformaciones) según los resultados.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Accumulated Local Effects (ALE)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ALE y valores locales ALE para:\\n\"\n",
        "                        \"1. Explicar cómo ALE corrige artefactos de correlación y qué nos dicen los valores concretos.\\n\"\n",
        "                        \"2. Interpretar importancia global ALE: variables con mayor efecto acumulado.\\n\"\n",
        "                        \"3. Analizar heterogeneidad local a partir de valores ALE de primeras muestras.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales (ALE 2D, inspección de distribución) según hallazgos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Individual Conditional Expectation (ICE) Plots':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos rangos ICE medios para:\\n\"\n",
        "                        \"1. Explicar qué mide el rango ICE y su diferencia respecto a PDP.\\n\"\n",
        "                        \"2. Analizar heterogeneidad: variables con alta variabilidad en rangos ICE indican interacciones o comportamiento inestable.\\n\"\n",
        "                        \"3. Señalar implicaciones para el modelo y posibles ajustes si hay alto efecto local variable.\\n\"\n",
        "                        \"4. Recomendar exploraciones adicionales para entender la variabilidad local.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Counterfactual Explanations':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Contrafactuales para:\\n\"\n",
        "                        \"1. Explicar cómo interpretar los contrafactuales: cambios en variables que generan aumento en predicción.\\n\"\n",
        "                        \"2. Analizar variables con mayor |Δ| medio: implicaciones sobre sensibilidad del modelo.\\n\"\n",
        "                        \"3. Señalar si faltan contrafactuales para algunas muestras: qué puede indicar (limites del modelo o datos).\\n\"\n",
        "                        \"4. Sugerir cómo usar estos insights para ajuste de modelo o recolección de datos.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Anchors':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Anchors para:\\n\"\n",
        "                        \"1. Explicar cómo leer las reglas ancla de las primeras muestras: condiciones que aseguran la predicción.\\n\"\n",
        "                        \"2. Analizar frecuencia global de aparición de variables en reglas: implicaciones sobre estabilidad y sesgos.\\n\"\n",
        "                        \"3. Señalar regiones de bajo coverage o baja precisión: dónde el modelo es menos fiable.\\n\"\n",
        "                        \"4. Recomendar acciones: recolección de datos, refinamiento de variables o validaciones dirigidas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Surrogate Models (Global/Local)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Surrogate Models para:\\n\"\n",
        "                        \"1. Explicar la importancia global del surrogate (árbol) y la importancia local media (coeficientes regresiones locales).\\n\"\n",
        "                        \"2. Comparar global vs local: variables con alta importancia local pero baja global, o viceversa, y qué sugiere.\\n\"\n",
        "                        \"3. Concluir sobre consistencia de comportamiento del RNN en diferentes regiones del espacio.\\n\"\n",
        "                        \"4. Sugerir exploraciones adicionales o ajustes de modelo según discrepancias detectadas.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Explainable Boosting Machine (EBM)':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de EBM para:\\n\"\n",
        "                        \"1. Explicar la importancia global según EBM: cómo se comparan con otros métodos.\\n\"\n",
        "                        \"2. Analizar contribuciones locales de las primeras muestras: qué patrones se observan.\\n\"\n",
        "                        \"3. Señalar si EBM revela interacciones no consideradas en RNN.\\n\"\n",
        "                        \"4. Recomendar posibles ajustes en características o validaciones según insights de EBM.\\n\"\n",
        "                    )\n",
        "                elif clave == 'Optuna Hyperparameter Importance':\n",
        "                    prompt += (\n",
        "                        \"\\nUsa estos resultados de Optuna para:\\n\"\n",
        "                        \"1. Explicar la importancia global de hiperparámetros en la optimización del RNN.\\n\"\n",
        "                        \"2. Analizar top trials si están disponibles: qué combinaciones de hiperparámetros funcionaron mejor.\\n\"\n",
        "                        \"3. Señalar limitaciones de la muestra de trials (número de pruebas) y posibles riesgos de sobreajuste en la búsqueda.\\n\"\n",
        "                        \"4. Recomendar próximas acciones para tuning basadas en estas importancias.\\n\"\n",
        "                    )\n",
        "                else:\n",
        "                    prompt += \"\\nPor favor, explica estos resultados numéricos y qué implicaciones tienen para el modelo RNN.\\n\"\n",
        "\n",
        "                # --------------- Llamada a OpenAI ---------------\n",
        "                print(f\"[DEBUG] 10.21. Llamando a OpenAI para {titulo}\")\n",
        "                explicacion = call_openai_explanation(prompt)\n",
        "                self.sections.append((f\"### {titulo}: Explicación Generativa\", explicacion))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en sección xIA RNN\",\n",
        "                f\"Se produjo un error al generar la sección xIA de RNN: {e}\"\n",
        "            ))\n",
        "\n",
        "\n",
        "        # =====================================================================\n",
        "        # 11. Comparador Global de Modelos Entrenados\n",
        "        # =====================================================================\n",
        "        try:\n",
        "            print(\"[DEBUG] 11.1. Iniciando sección Comparador de Modelos\")\n",
        "            import os, pickle\n",
        "            import numpy as np\n",
        "            import pandas as _pd\n",
        "            import matplotlib.pyplot as plt\n",
        "            from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "            from tensorflow.keras.models import load_model\n",
        "\n",
        "            # 1) Recopilar todos los modelos guardados\n",
        "            ruta = \".\"\n",
        "            modelos = []\n",
        "            for f in os.listdir(ruta):\n",
        "                if f.startswith(\"modelo_\") and (f.endswith(\".pkl\") or f.endswith(\".h5\")):\n",
        "                    name = f.replace(\"modelo_\", \"\").rsplit(\".\", 1)[0]\n",
        "                    try:\n",
        "                        if f.endswith(\".pkl\"):\n",
        "                            with open(f, \"rb\") as fp:\n",
        "                                data = pickle.load(fp)\n",
        "                            model = data.get(\"model\")\n",
        "                            sx = data.get(\"sx\", data.get(\"scaler_X\"))\n",
        "                            sy = data.get(\"sy\", data.get(\"scaler_Y\"))\n",
        "                            cols = data.get(\"cols\", [])\n",
        "                        else:  # .h5\n",
        "                            model = load_model(f)\n",
        "                            # se espera un pickle de escaladores: escaladores_{name}.pkl\n",
        "                            escal_path = f\"escaladores_{name}.pkl\"\n",
        "                            with open(escal_path, \"rb\") as fp:\n",
        "                                data = pickle.load(fp)\n",
        "                            sx = data.get(\"scaler_X\")\n",
        "                            sy = data.get(\"scaler_Y\")\n",
        "                            cols = data.get(\"cols\", [])\n",
        "                        if model is None or sx is None or sy is None or not cols:\n",
        "                            raise ValueError(\"Faltan claves necesarias en el pickle/modelo\")\n",
        "                        modelos.append((name, model, sx, sy, cols))\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] Omitido {f}: {e}\")\n",
        "\n",
        "            if not modelos:\n",
        "                print(\"[DEBUG] No se encontraron modelos válidos para comparar\")\n",
        "            else:\n",
        "                # 2) Para cada modelo, calcular predicciones sobre X_test/Y_test\n",
        "                resultados = []\n",
        "                preds_dict = {}\n",
        "                y_true_full = None\n",
        "\n",
        "                for name, model, sx, sy, cols in modelos:\n",
        "                    # Verificar que X_test y Y_test existan en globals\n",
        "                    if \"X_test\" not in self.g or \"Y_test\" not in self.g:\n",
        "                        print(\"[DEBUG] No hay X_test/Y_test en globals(), omito comparador\")\n",
        "                        break\n",
        "                    try:\n",
        "                        X_test_df = self.g[\"X_test\"][cols].copy()\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] Error al extraer X_test para {name}: {e}\")\n",
        "                        continue\n",
        "                    y_test = self.g[\"Y_test\"]\n",
        "                    # Serie 1D de y_true\n",
        "                    arr = y_test.values if hasattr(y_test, \"values\") else np.array(y_test)\n",
        "                    y_arr = arr.ravel()\n",
        "\n",
        "                    # Detectar si es RNN por input_shape\n",
        "                    is_rnn = False\n",
        "                    window = None\n",
        "                    try:\n",
        "                        if hasattr(model, \"input_shape\") and isinstance(model.input_shape, tuple) and len(model.input_shape) == 3:\n",
        "                            is_rnn = True\n",
        "                            window = model.input_shape[1]\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                    # Generar predicción\n",
        "                    try:\n",
        "                        if is_rnn and window is not None:\n",
        "                            # crear secuencias\n",
        "                            Xs = sx.transform(X_test_df)\n",
        "                            seqs, trues = [], []\n",
        "                            for i in range(len(Xs) - window):\n",
        "                                seqs.append(Xs[i:i + window])\n",
        "                                trues.append(y_arr[i + window])\n",
        "                            if not seqs:\n",
        "                                print(f\"[DEBUG] Secuencias vacías para RNN {name}, omito\")\n",
        "                                continue\n",
        "                            Xseq = np.array(seqs)\n",
        "                            y_real = np.array(trues)\n",
        "                            pred_scaled = model.predict(Xseq, verbose=0)\n",
        "                            # en algunos casos model.predict devuelve tupla\n",
        "                            if isinstance(pred_scaled, tuple):\n",
        "                                pred_scaled = pred_scaled[0]\n",
        "                            y_pred = sy.inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
        "                        else:\n",
        "                            Xs = sx.transform(X_test_df)\n",
        "                            raw = model.predict(Xs)\n",
        "                            if isinstance(raw, tuple):\n",
        "                                raw = raw[0]\n",
        "                            y_pred = sy.inverse_transform(raw.reshape(-1, 1)).ravel()\n",
        "                            y_real = y_arr\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] Error predicción para {name}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Guardar y_true para la primera curva\n",
        "                    if y_true_full is None:\n",
        "                        y_true_full = y_real\n",
        "\n",
        "                    # Calcular métricas\n",
        "                    try:\n",
        "                        r2 = r2_score(y_real, y_pred)\n",
        "                        mse = mean_squared_error(y_real, y_pred)\n",
        "                        rmse = np.sqrt(mse)\n",
        "                        mae = mean_absolute_error(y_real, y_pred)\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] Error cálculo métricas para {name}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                    resultados.append({\n",
        "                        \"Modelo\": name,\n",
        "                        \"R2\": r2,\n",
        "                        \"MSE\": mse,\n",
        "                        \"RMSE\": rmse,\n",
        "                        \"MAE\": mae\n",
        "                    })\n",
        "                    preds_dict[name] = y_pred\n",
        "\n",
        "                # 3) Tabla de métricas\n",
        "                if resultados:\n",
        "                    df_met = _pd.DataFrame(resultados).set_index(\"Modelo\")\n",
        "                    self.sections.append((\"### 📋 Comparativa de Métricas de Todos los Modelos\", df_met))\n",
        "                    print(\"[DEBUG] 11.2. Sección comparativa métricas añadida\")\n",
        "                    # Extraer métricas como dict para el prompt:\n",
        "                    # Podemos convertir a lista de dicts o dict de listas. Por claridad, usamos lista de records:\n",
        "                    metrics_records = df_met.reset_index().to_dict(orient='records')\n",
        "                    # Explicación IA de la tabla\n",
        "                    prompt_tab = (\n",
        "                        \"Tienes estas métricas en test para cada modelo:\\n\"\n",
        "                        + \"\\n\".join(\n",
        "                            f\"- Modelo '{rec['Modelo']}': R2={rec['R2']:.4f}, RMSE={rec['RMSE']:.4f}, MAE={rec['MAE']:.4f}, MSE={rec['MSE']:.4f}\"\n",
        "                            for rec in metrics_records\n",
        "                        )\n",
        "                        + \"\\n\\nPor favor, interpreta profesionalmente cuál modelo es el mejor según estas métricas y por qué, \"\n",
        "                          \"y proporciona recomendaciones de ajustes de hiperparámetros para mejorar el desempeño del mejor modelo y posibles ajustes en los demás.\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en ML comparativo de modelos.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_tab}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"### 📝 Explicación IA Comparativa de Métricas\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.3. Sección IA comparación métricas añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA comparación métricas: {e}\")\n",
        "\n",
        "                    # Función auxiliar para gráfico de barras horizontales\n",
        "                    def _plot_barh(series, titulo, xlabel=None):\n",
        "                        fig, ax = plt.subplots(figsize=(8, max(4, 0.3 * len(series))))\n",
        "                        # ordenar para que el mejor (mayor R2 o menor error) quede arriba\n",
        "                        series_sorted = series.sort_values(ascending=True)\n",
        "                        # Si es R2 (mayor es mejor), invertimos orden para que el mayor quede arriba\n",
        "                        if xlabel == \"R2\":\n",
        "                            # para R2, queremos ascending=True y luego invertir eje Y\n",
        "                            ax.barh(series_sorted.index, series_sorted.values)\n",
        "                        else:\n",
        "                            # para errores (RMSE, MAE, MSE), menor es mejor: ascending=True pone menor arriba después de invertir eje\n",
        "                            ax.barh(series_sorted.index, series_sorted.values)\n",
        "                        ax.set_title(titulo)\n",
        "                        ax.set_xlabel(xlabel if xlabel else \"\")\n",
        "                        ax.set_ylabel(\"Modelo\")\n",
        "                        ax.invert_yaxis()\n",
        "                        plt.tight_layout()\n",
        "                        return fig\n",
        "\n",
        "                    # 4) Barras de R2\n",
        "                    fig_r2 = _plot_barh(df_met[\"R2\"], \"Ranking de Modelos por R²\", xlabel=\"R2\")\n",
        "                    self.sections.append((\"### 📊 Gráfico Comparativo de R²\", fig_r2))\n",
        "                    print(\"[DEBUG] 11.4. Sección gráfica R2 añadida\")\n",
        "                    # Explicación IA R2\n",
        "                    # Extraer valores de R2:\n",
        "                    r2_dict = df_met[\"R2\"].to_dict()  # e.g. {\"M1\":0.85, \"M2\":0.78, ...}\n",
        "                    prompt_r2 = (\n",
        "                        \"Tienes los valores de R² en test para cada modelo (aquí listados):\\n\"\n",
        "                        + \"\\n\".join(f\"- Modelo '{m}': R² = {v:.4f}\" for m, v in r2_dict.items())\n",
        "                        + \"\\n\\nBasándote en estos valores (sin apoyarte en la visualización directa), \"\n",
        "                          \"indica profesionalmente qué conclusiones sacas sobre el ajuste de los modelos y posibles razones de las diferencias entre ellos.\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en interpretación de gráficos ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_r2}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"### 📝 Explicación IA Gráfico R²\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.5. Sección IA comparación R2 añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA R2: {e}\")\n",
        "\n",
        "                    # 5) Barras de MAE\n",
        "                    fig_mae = _plot_barh(df_met[\"MAE\"], \"Comparativa Global por MAE (menor es mejor)\", xlabel=\"MAE\")\n",
        "                    self.sections.append((\"### 📊 Gráfico Comparativo de MAE\", fig_mae))\n",
        "                    print(\"[DEBUG] 11.6. Sección gráfica MAE añadida\")\n",
        "                    # Extraer valores de MAE:\n",
        "                    mae_dict = df_met[\"MAE\"].to_dict()  # e.g. {\"M1\":12.345, \"M2\":15.678, ...}\n",
        "                    prompt_mae = (\n",
        "                        \"Tienes los valores de MAE en test para cada modelo:\\n\"\n",
        "                        + \"\\n\".join(f\"- Modelo '{m}': MAE = {v:.4f}\" for m, v in mae_dict.items())\n",
        "                        + \"\\n\\nBasándote en estos valores (sin ver la gráfica), indica profesionalmente qué indican sobre la precisión de cada modelo en términos absolutos y relativos, \"\n",
        "                          \"qué patrones observas (por ejemplo, modelos con MAE significativamente más alta o más baja) y recomendaciones concretas para reducir el MAE del mejor modelo o de aquellos con MAE elevado.\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en interpretación de métricas de error en ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_mae}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"### 📝 Explicación IA Gráfico MAE\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.7. Sección IA comparación MAE añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA MAE: {e}\")\n",
        "\n",
        "                    # 6) Barras de MSE\n",
        "                    fig_mse = _plot_barh(df_met[\"MSE\"], \"Comparativa Global por MSE (menor es mejor)\", xlabel=\"MSE\")\n",
        "                    self.sections.append((\"### 📊 Gráfico Comparativo de MSE\", fig_mse))\n",
        "                    print(\"[DEBUG] 11.8. Sección gráfica MSE añadida\")\n",
        "                    # Extraer valores de MSE:\n",
        "                    mse_dict = df_met[\"MSE\"].to_dict()\n",
        "                    prompt_mse = (\n",
        "                        \"Tienes los valores de MSE en test para cada modelo:\\n\"\n",
        "                        + \"\\n\".join(f\"- Modelo '{m}': MSE = {v:.4f}\" for m, v in mse_dict.items())\n",
        "                        + \"\\n\\nBasándote en estos valores, comenta profesionalmente qué sugiere acerca del ajuste y robustez de cada modelo, \"\n",
        "                          \"identifica si hay alguno con MSE significativamente mayor o menor y ofrece recomendaciones concretas para reducir MSE (por ejemplo, cambios de preprocesado, regularización, arquitectura, etc.).\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en interpretación de métricas de error en ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_mse}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"### 📝 Explicación IA Gráfico MSE\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.9. Sección IA comparación MSE añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA MSE: {e}\")\n",
        "\n",
        "                    # 7) Barras de RMSE\n",
        "                    fig_rmse = _plot_barh(df_met[\"RMSE\"], \"Comparativa Global por RMSE (menor es mejor)\", xlabel=\"RMSE\")\n",
        "                    self.sections.append((\"### 📊 Gráfico Comparativo de RMSE\", fig_rmse))\n",
        "                    print(\"[DEBUG] 11.10. Sección gráfica RMSE añadida\")\n",
        "                    # Extraer valores de RMSE:\n",
        "                    rmse_dict = df_met[\"RMSE\"].to_dict()\n",
        "                    prompt_rmse = (\n",
        "                        \"Tienes los valores de RMSE en test para cada modelo:\\n\"\n",
        "                        + \"\\n\".join(f\"- Modelo '{m}': RMSE = {v:.4f}\" for m, v in rmse_dict.items())\n",
        "                        + \"\\n\\nBasándote en estos valores, comenta profesionalmente las diferencias entre modelos, \"\n",
        "                          \"por qué algunos podrían tener RMSE mayor o menor (relación con varianza de los datos, sesgos, complejidad del modelo, etc.) y sugiere estrategias concretas para mejorar el RMSE del modelo ganador o para equilibrar sesgo-varianza.\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en interpretación de métricas de error en ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_rmse}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"### 📝 Explicación IA Gráfico RMSE\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.11. Sección IA comparación RMSE añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA RMSE: {e}\")\n",
        "\n",
        "                    # 8) Curvas Real vs Predicho para todos\n",
        "                    if y_true_full is not None and preds_dict:\n",
        "                        fig2, ax2 = plt.subplots(figsize=(8, 4))\n",
        "                        ax2.plot(y_true_full, label=\"Y real\", color=\"black\", linewidth=2)\n",
        "                        for name, pred in preds_dict.items():\n",
        "                            # Asegurarse de que longitudes coincidan; si no, recortar al mínimo\n",
        "                            min_len = min(len(y_true_full), len(pred))\n",
        "                            ax2.plot(pred[:min_len], \"--\", label=name)\n",
        "                        ax2.set_title(\"Comparativa Y real vs Predicho\")\n",
        "                        ax2.legend(loc=\"upper right\")\n",
        "                        plt.tight_layout()\n",
        "                        self.sections.append((\"### 📈 Gráfico Comparativo Real vs Predicho\", fig2))\n",
        "                        print(\"[DEBUG] 11.12. Sección gráfica real vs pred añadida\")\n",
        "                        # Suponiendo y_true_full y preds_dict ya definidos y alineados:\n",
        "                        # Para cada modelo:\n",
        "                        summary_list = []\n",
        "                        import numpy as _np\n",
        "                        for name, pred in preds_dict.items():\n",
        "                            # Asegurar longitudes coincidentes\n",
        "                            min_len = min(len(y_true_full), len(pred))\n",
        "                            y_real = _np.array(y_true_full[:min_len])\n",
        "                            y_pred = _np.array(pred[:min_len])\n",
        "                            # Métricas:\n",
        "                            from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "                            try:\n",
        "                                r2_val = r2_score(y_real, y_pred)\n",
        "                            except:\n",
        "                                r2_val = None\n",
        "                            try:\n",
        "                                mse_val = mean_squared_error(y_real, y_pred)\n",
        "                                rmse_val = _np.sqrt(mse_val)\n",
        "                            except:\n",
        "                                mse_val = rmse_val = None\n",
        "                            try:\n",
        "                                mae_val = mean_absolute_error(y_real, y_pred)\n",
        "                            except:\n",
        "                                mae_val = None\n",
        "                            # Correlación:\n",
        "                            try:\n",
        "                                corr_val = float(_np.corrcoef(y_real, y_pred)[0, 1]) if len(y_real)>1 else None\n",
        "                            except:\n",
        "                                corr_val = None\n",
        "                            # Rango:\n",
        "                            y_real_min, y_real_max = float(_np.min(y_real)), float(_np.max(y_real))\n",
        "                            y_pred_min, y_pred_max = float(_np.min(y_pred)), float(_np.max(y_pred))\n",
        "                            summary_list.append({\n",
        "                                \"Modelo\": name,\n",
        "                                \"R2\": r2_val,\n",
        "                                \"MSE\": mse_val,\n",
        "                                \"RMSE\": rmse_val,\n",
        "                                \"MAE\": mae_val,\n",
        "                                \"Corr\": corr_val,\n",
        "                                \"Rango real\": (y_real_min, y_real_max),\n",
        "                                \"Rango pred\": (y_pred_min, y_pred_max)\n",
        "                            })\n",
        "                            # Formatear summary_list en texto:\n",
        "                            lines = []\n",
        "                            for rec in summary_list:\n",
        "                                m = rec[\"Modelo\"]\n",
        "                                # Manejar None con 'N/A'\n",
        "                                r2s = f\"{rec['R2']:.4f}\" if rec['R2'] is not None else \"N/A\"\n",
        "                                rs = rec[\"Rango real\"]\n",
        "                                ps = rec[\"Rango pred\"]\n",
        "                                corr_s = f\"{rec['Corr']:.4f}\" if rec['Corr'] is not None else \"N/A\"\n",
        "                                mse_s = f\"{rec['MSE']:.4f}\" if rec['MSE'] is not None else \"N/A\"\n",
        "                                rmse_s = f\"{rec['RMSE']:.4f}\" if rec['RMSE'] is not None else \"N/A\"\n",
        "                                mae_s = f\"{rec['MAE']:.4f}\" if rec['MAE'] is not None else \"N/A\"\n",
        "                                lines.append(\n",
        "                                    f\"- Modelo '{m}': R2={r2s}, RMSE={rmse_s}, MAE={mae_s}, MSE={mse_s}, Corr={corr_s}, \"\n",
        "                                    f\"Rango real=[{rs[0]:.4f}, {rs[1]:.4f}], Rango pred=[{ps[0]:.4f}, {ps[1]:.4f}]\"\n",
        "                                )\n",
        "\n",
        "                            prompt_curvas = (\n",
        "                                \"Para cada modelo tienes estos resúmenes numéricos de su predicción vs real:\\n\"\n",
        "                                + \"\\n\".join(lines)\n",
        "                                + \"\\n\\nAunque también se generó una gráfica Real vs Predicho, la IA no la ve: \"\n",
        "                                  \"Comenta profesionalmente cómo varía el ajuste de cada modelo a lo largo de la serie con base en los valores anteriores \"\n",
        "                                  \"(por ejemplo, si hay subestimación sistemática al inicio o al final, si la dispersión crece en ciertos rangos, etc.).\"\n",
        "                            )\n",
        "                        try:\n",
        "                            resp = _client.chat.completions.create(\n",
        "                                model=\"gpt-4\",\n",
        "                                messages=[\n",
        "                                    {\"role\": \"system\", \"content\": \"Eres un experto en visualización de resultados ML.\"},\n",
        "                                    {\"role\": \"user\", \"content\": prompt_curvas}\n",
        "                                ],\n",
        "                                max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                            )\n",
        "                            self.sections.append((\"### 📝 Explicación IA Curvas Real vs Predicho\", resp.choices[0].message.content.strip()))\n",
        "                            print(\"[DEBUG] 11.13. Sección IA comparación curvas añadida\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"[ERROR] al generar explicación IA curvas: {e}\")\n",
        "\n",
        "                        # 9) Gráfica de residuos superpuesta\n",
        "                        fig3, ax3 = plt.subplots(figsize=(8, 4))\n",
        "                        for name, pred in preds_dict.items():\n",
        "                            min_len = min(len(y_true_full), len(pred))\n",
        "                            res = y_true_full[:min_len] - pred[:min_len]\n",
        "                            ax3.plot(res, label=name, alpha=0.7)\n",
        "                        ax3.axhline(0, color=\"black\", lw=1)\n",
        "                        ax3.set_title(\"Comparativa de Residuos\")\n",
        "                        ax3.legend(loc=\"upper right\")\n",
        "                        plt.tight_layout()\n",
        "                        self.sections.append((\"### 📉 Gráfico Comparativo de Residuos\", fig3))\n",
        "                        print(\"[DEBUG] 11.14. Sección gráfica residuos añadida\")\n",
        "                        res_summary = []\n",
        "                        import numpy as _np\n",
        "                        for rec in summary_list:  # si summary_list incluye y_real y y_pred, o recalcular aquí\n",
        "                            name = rec[\"Modelo\"]\n",
        "                            # Recalcular residuos:\n",
        "                            pred = preds_dict[name]\n",
        "                            min_len = min(len(y_true_full), len(pred))\n",
        "                            y_real = _np.array(y_true_full[:min_len])\n",
        "                            y_pred = _np.array(pred[:min_len])\n",
        "                            residuals = y_real - y_pred\n",
        "                            # Estadísticos:\n",
        "                            mean_res = float(_np.mean(residuals))\n",
        "                            std_res  = float(_np.std(residuals))\n",
        "                            # Con Pandas o numpy calcular skew/kurt:\n",
        "                            import pandas as _pd\n",
        "                            res_series = _pd.Series(residuals)\n",
        "                            skew_res = float(res_series.skew())\n",
        "                            kurt_res = float(res_series.kurtosis())\n",
        "                            q25, q50, q75 = [float(x) for x in res_series.quantile([0.25, 0.5, 0.75])]\n",
        "                            # Rango:\n",
        "                            min_res, max_res = float(_np.min(residuals)), float(_np.max(residuals))\n",
        "                            res_summary.append({\n",
        "                                \"Modelo\": name,\n",
        "                                \"Mean\": mean_res,\n",
        "                                \"Std\": std_res,\n",
        "                                \"Skew\": skew_res,\n",
        "                                \"Kurtosis\": kurt_res,\n",
        "                                \"Quantiles\": (q25, q50, q75),\n",
        "                                \"Rango residuo\": (min_res, max_res)\n",
        "                            })\n",
        "                            lines = []\n",
        "                            for rec in res_summary:\n",
        "                                m = rec[\"Modelo\"]\n",
        "                                mean_s = f\"{rec['Mean']:.4f}\"\n",
        "                                std_s  = f\"{rec['Std']:.4f}\"\n",
        "                                skew_s = f\"{rec['Skew']:.4f}\"\n",
        "                                kurt_s = f\"{rec['Kurtosis']:.4f}\"\n",
        "                                q25, q50, q75 = rec[\"Quantiles\"]\n",
        "                                min_r, max_r = rec[\"Rango residuo\"]\n",
        "                                lines.append(\n",
        "                                    f\"- Modelo '{m}': media residuo={mean_s}, std={std_s}, skew={skew_s}, kurtosis={kurt_s}, \"\n",
        "                                    f\"quantiles residuo 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}, \"\n",
        "                                    f\"rango residuo=[{min_r:.4f}, {max_r:.4f}]\"\n",
        "                                )\n",
        "                            prompt_res = (\n",
        "                                \"Tienes las siguientes estadísticas de residuos (Y_real - Y_predicho) para cada modelo:\\n\"\n",
        "                                + \"\\n\".join(lines)\n",
        "                                + \"\\n\\nCon base en estos datos (sin apoyo visual), analiza profesionalmente si hay patrones de sesgo (por ejemplo, media distinta de 0), heterocedasticidad (std variable según nivel, aunque aquí solo tenemos std global; si quisieras, podrías calcular std en terciles de y_pred), posibles outliers (basándote en quantiles y rango), y qué implicaciones tiene para la robustez y generalización de cada modelo.\"\n",
        "                            )\n",
        "                        try:\n",
        "                            resp = _client.chat.completions.create(\n",
        "                                model=\"gpt-4\",\n",
        "                                messages=[\n",
        "                                    {\"role\": \"system\", \"content\": \"Eres un experto en diagnóstico de modelos ML.\"},\n",
        "                                    {\"role\": \"user\", \"content\": prompt_res}\n",
        "                                ],\n",
        "                                max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                            )\n",
        "                            self.sections.append((\"### 📝 Explicación IA Residuos Comparativos\", resp.choices[0].message.content.strip()))\n",
        "                            print(\"[DEBUG] 11.15. Sección IA comparación residuos añadida\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"[ERROR] al generar explicación IA residuos: {e}\")\n",
        "                    else:\n",
        "                        print(\"[DEBUG] No hay datos suficientes para curvas o residuos comparativos\")\n",
        "\n",
        "                    # 10) Parámetros técnicos de cada modelo\n",
        "                    tabla_params = []\n",
        "                    for name, model, _, _, _ in modelos:\n",
        "                        row = {\"Modelo\": name}\n",
        "                        try:\n",
        "                            if hasattr(model, \"get_params\"):\n",
        "                                row.update(model.get_params())\n",
        "                            else:\n",
        "                                cfg = model.get_config()\n",
        "                                row[\"Capas\"] = len(cfg.get(\"layers\", []))\n",
        "                                row[\"Opt\"] = cfg.get(\"optimizer_config\", {}).get(\"class_name\", \"?\")\n",
        "                                row[\"Loss\"] = cfg.get(\"loss\", \"?\")\n",
        "                        except Exception:\n",
        "                            row[\"Info\"] = \"no disponible\"\n",
        "                        tabla_params.append(row)\n",
        "                    df_params = _pd.DataFrame(tabla_params).set_index(\"Modelo\")\n",
        "                    self.sections.append((\"### 🛠 Parámetros Técnicos de Cada Modelo\", df_params))\n",
        "                    print(\"[DEBUG] 11.16. Sección tabla parámetros añadida\")\n",
        "\n",
        "                    # Explicación IA parámetros\n",
        "                    prompt_par = (\n",
        "                        \"Aquí tienes una tabla con los parámetros técnicos usados en cada modelo. \"\n",
        "                        \"Comenta brevemente qué configuraciones parecen más adecuadas y cuáles podrían ajustarse para mejorar el rendimiento.\"\n",
        "                        f\"\\n\\n{df_params.to_dict(orient='list')}\"\n",
        "                    )\n",
        "                    try:\n",
        "                        resp = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en optimización de hiperparámetros ML.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_par}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS, temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        self.sections.append((\"### 📝 Explicación IA Parámetros Técnicos\", resp.choices[0].message.content.strip()))\n",
        "                        print(\"[DEBUG] 11.17. Sección IA comparación parámetros añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA parámetros: {e}\")\n",
        "\n",
        "                    # 11) Conclusiones Globales y Recomendaciones de IA\n",
        "                    try:\n",
        "                        # Construir prompt que recopile los principales resultados:\n",
        "                        # Usamos df_met (DataFrame de métricas) y, opcionalmente, podemos incorporar insights de gráficos anteriores.\n",
        "                        # Convertimos df_met a diccionario para el prompt:\n",
        "                        # Supongamos summary_list y res_summary ya definidos (como arriba).\n",
        "                        # Formateamos una sección de métricas generales:\n",
        "                        lines_met = []\n",
        "                        for rec in summary_list:\n",
        "                            m = rec[\"Modelo\"]\n",
        "                            r2s = f\"{rec['R2']:.4f}\" if rec['R2'] is not None else \"N/A\"\n",
        "                            rmse_s = f\"{rec['RMSE']:.4f}\" if rec['RMSE'] is not None else \"N/A\"\n",
        "                            mae_s = f\"{rec['MAE']:.4f}\" if rec['MAE'] is not None else \"N/A\"\n",
        "                            mse_s = f\"{rec['MSE']:.4f}\" if rec['MSE'] is not None else \"N/A\"\n",
        "                            lines_met.append(f\"- {m}: R2={r2s}, RMSE={rmse_s}, MAE={mae_s}, MSE={mse_s}\")\n",
        "\n",
        "                        # Formateamos estadísticas de residuos:\n",
        "                        lines_res = []\n",
        "                        for rec in res_summary:\n",
        "                            m = rec[\"Modelo\"]\n",
        "                            mean_s = f\"{rec['Mean']:.4f}\"\n",
        "                            std_s  = f\"{rec['Std']:.4f}\"\n",
        "                            skew_s = f\"{rec['Skew']:.4f}\"\n",
        "                            kurt_s = f\"{rec['Kurtosis']:.4f}\"\n",
        "                            q25, q50, q75 = rec[\"Quantiles\"]\n",
        "                            lines_res.append(\n",
        "                                f\"- {m}: media residuo={mean_s}, std={std_s}, skew={skew_s}, kurtosis={kurt_s}, quantiles[25,50,75]=[{q25:.4f},{q50:.4f},{q75:.4f}]\"\n",
        "                            )\n",
        "\n",
        "                        # Formateamos rangos de predicción vs real:\n",
        "                        lines_range = []\n",
        "                        for rec in summary_list:\n",
        "                            m = rec[\"Modelo\"]\n",
        "                            rs = rec[\"Rango real\"]\n",
        "                            ps = rec[\"Rango pred\"]\n",
        "                            lines_range.append(f\"- {m}: Rango real=[{rs[0]:.4f},{rs[1]:.4f}], Rango pred=[{ps[0]:.4f},{ps[1]:.4f}]\")\n",
        "\n",
        "                        # Construcción del prompt:\n",
        "                        prompt_final = (\n",
        "                            \"A continuación tienes un resumen numérico de los resultados de todos los modelos:\\n\\n\"\n",
        "                            \"1) Métricas globales en test:\\n\"\n",
        "                            + \"\\n\".join(lines_met)\n",
        "                            + \"\\n\\n2) Estadísticas de residuos:\\n\"\n",
        "                            + \"\\n\".join(lines_res)\n",
        "                            + \"\\n\\n3) Rangos de valores real vs predicho:\\n\"\n",
        "                            + \"\\n\".join(lines_range)\n",
        "                            + \"\\n\\nYa se han analizado previamente aspectos de ajuste, error y parámetros. \"\n",
        "                            \"Con base en toda esta información numérica, por favor: \"\n",
        "                            \"- Indica cuál modelo presenta en conjunto mejor desempeño y por qué. \"\n",
        "                            \"- Sugiere qué ajustes o hiperparámetros convendría afinar en ese modelo para mejorar aún más (por ejemplo, si es red neuronal, architecture tweaks; si es árbol, depth/regularización; si es RNN, window size o layers; etc.). \"\n",
        "                            \"- Comenta posibles limitaciones de los otros modelos e indica en qué escenarios podrían ser útiles (por ejemplo, si un modelo tiene menor varianza pero mayor sesgo, puede servir en entornos con datos ruidosos, etc.). \"\n",
        "                            \"- Propón próximos pasos de validación o pruebas adicionales (p. ej. validación cruzada más exhaustiva, test en nuevos periodos de la serie, experimentos de robustez). \"\n",
        "                            \"Presenta la respuesta de forma profesional, estructurada en secciones (por ejemplo: Resumen general, Análisis del mejor modelo, Ajustes recomendados, Limitaciones y usos alternativos, Próximos pasos).\"\n",
        "                        )\n",
        "                        resp_final = _client.chat.completions.create(\n",
        "                            model=\"gpt-4\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Eres un experto en análisis comparativo de modelos de Machine Learning.\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt_final}\n",
        "                            ],\n",
        "                            max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                            temperature=TEMPERATURE_VAL\n",
        "                        )\n",
        "                        # Añadir la sección al informe\n",
        "                        self.sections.append((\n",
        "                            \"### 🏁 Conclusiones Globales y Recomendaciones IA\",\n",
        "                            resp_final.choices[0].message.content.strip()\n",
        "                        ))\n",
        "                        print(\"[DEBUG] 11.18. Sección IA Conclusiones Globales añadida\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] al generar explicación IA Conclusiones Globales: {e}\")\n",
        "\n",
        "                else:\n",
        "                    print(\"[DEBUG] No se obtuvieron resultados de métrica para ningún modelo\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar sección Comparador de Modelos: {e}\")\n",
        "\n",
        "        # =====================================================================\n",
        "        # 12. Optimización Modelo SVR\n",
        "        # =====================================================================\n",
        "        from scipy.stats import skew, kurtosis\n",
        "        try:\n",
        "            print(\"[DEBUG] 12.1. Iniciando sección Optimización SVR: Resumen de métodos y motores\")\n",
        "            # Verificación previa: usamos OPT_MODELS guardado en la celda 9.1\n",
        "            if 'OPT_MODELS' in globals() and isinstance(OPT_MODELS, dict):\n",
        "                # Filtrar solo entradas de SVR con payload dict y motores válidos\n",
        "                valid_engines = {'gridsearchcv', 'randomizedsearchcv', 'optuna', 'bayessearchcv'}\n",
        "                svr_entries = {\n",
        "                    k: v for k, v in OPT_MODELS.items()\n",
        "                    if (isinstance(k, tuple) and k[0] == 'svr'\n",
        "                        and isinstance(v, dict)\n",
        "                        and k[2] in valid_engines)\n",
        "                }\n",
        "            else:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontró la variable global OPT_MODELS con resultados de optimización SVR. \"\n",
        "                    \"Asegúrate de haber ejecutado la Celda 9.1 y de que OPT_MODELS contenga los payloads optimizados.\"\n",
        "                )\n",
        "\n",
        "            import pandas as pd\n",
        "\n",
        "            # Definición de las configuraciones usadas para cada motor (para mostrar en la tabla)\n",
        "            param_configs = {\n",
        "                'gridsearchcv': {\n",
        "                    'C': '[0.1, 1, 10, 100]',\n",
        "                    'epsilon': '[0.01, 0.1, 0.5, 1]',\n",
        "                    'kernel': \"['rbf', 'linear']\"\n",
        "                },\n",
        "                'randomizedsearchcv': {\n",
        "                    'C': 'np.logspace(-1, 2, 20)',\n",
        "                    'epsilon': 'np.logspace(-2, 0, 20)',\n",
        "                    'kernel': \"['rbf', 'linear']\"\n",
        "                },\n",
        "                'optuna': {\n",
        "                    'C': 'Float(0.1, 100, log=True)',\n",
        "                    'epsilon': 'Float(0.01, 1.0, log=True)',\n",
        "                    'kernel': \"Categorical(['rbf','linear'])\"\n",
        "                },\n",
        "                'bayessearchcv': {\n",
        "                    'C': 'Real(0.1, 100, prior=\"log-uniform\")',\n",
        "                    'epsilon': 'Real(0.01, 1.0, prior=\"log-uniform\")',\n",
        "                    'kernel': 'Categorical([\"rbf\",\"linear\"])'\n",
        "                }\n",
        "            }\n",
        "\n",
        "            summary_records = []\n",
        "            # Recorremos cada dupla (metodo, motor)\n",
        "            for (model_type, sel_method, engine), payload in svr_entries.items():\n",
        "                print(f\"[DEBUG] 12.2. Agregando registro resumen: {sel_method} / {engine}\")\n",
        "                model = payload.get('model')\n",
        "                score = payload.get('score')\n",
        "                metric = payload.get('metric')\n",
        "\n",
        "                # Parámetros usados para la búsqueda según motor\n",
        "                config_used = param_configs.get(engine, {})\n",
        "\n",
        "                # Hiperparámetros óptimos extraídos del modelo\n",
        "                best_params = {param: getattr(model, param, None) for param in ['C', 'epsilon', 'kernel']}\n",
        "\n",
        "                # Construir fila\n",
        "                row = {\n",
        "                    'Selección X': sel_method,\n",
        "                    'Motor': engine,\n",
        "                    'Métrica': metric,\n",
        "                    'Score': score,\n",
        "                }\n",
        "\n",
        "                # Añadir configuración de búsqueda\n",
        "                for k, v in config_used.items():\n",
        "                    row[f'Grid_{k}'] = v\n",
        "                # Añadir mejores hiperparámetros\n",
        "                for k, v in best_params.items():\n",
        "                    row[f'Best_{k}'] = v\n",
        "\n",
        "                summary_records.append(row)\n",
        "\n",
        "            # Crear DataFrame\n",
        "            df_summary = pd.DataFrame(summary_records)\n",
        "            print(\"[DEBUG] 12.3. df_summary.columns:\", df_summary.columns.tolist())\n",
        "            # Reordenar columnas para claridad\n",
        "            desired_cols = [\n",
        "                'Selección X', 'Motor', 'Métrica', 'Score',\n",
        "                'Grid_C', 'Grid_epsilon', 'Grid_kernel',\n",
        "                'Best_C', 'Best_epsilon', 'Best_kernel'\n",
        "            ]\n",
        "            available_cols = [c for c in desired_cols if c in df_summary.columns]\n",
        "            if not available_cols:\n",
        "                print(\"[WARNING] Ninguna de las columnas deseadas está presente en df_summary; mostrando todo el DataFrame.\")\n",
        "                df_to_show = df_summary\n",
        "            else:\n",
        "                df_to_show = df_summary[available_cols]\n",
        "\n",
        "            # Añadir sección de tabla al informe\n",
        "            self.sections.append((\n",
        "                \"### SVR Optimización: Resumen de Métodos y Motores\",\n",
        "                df_to_show.reset_index(drop=True)\n",
        "            ))\n",
        "            # ----------------------- 0. Análisis Generativo con OpenAI -------------------------------------------------\n",
        "            # Preparar función de llamada a OpenAI\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un experto en optimización de hiperparámetros de modelos de ML. \"\n",
        "                                \"Proporciona análisis profundo, interpretaciones y recomendaciones.\" )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS\n",
        "                    )\n",
        "                    return response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    return f\"[Error llamando a OpenAI: {e}]\"\n",
        "\n",
        "            # Construir prompt con contexto y todos los registros\n",
        "            records_text = []\n",
        "            for rec in summary_records:\n",
        "                part = (f\"Método X: {rec['Selección X']}, Motor: {rec['Motor']}, Métrica: {rec['Métrica']}, Score: {rec['Score']:.4f}, \"\n",
        "                        f\"Configuración de búsqueda: C={rec.get('Grid_C')}, epsilon={rec.get('Grid_epsilon')}, kernel={rec.get('Grid_kernel')}. \"\n",
        "                        f\"Hiperparámetros óptimos: C={rec.get('Best_C')}, epsilon={rec.get('Best_epsilon')}, kernel={rec.get('Best_kernel')}.\")\n",
        "                records_text.append(part)\n",
        "            prompt = (\n",
        "                \"He obtenido los siguientes resultados de optimización para SVR:\\n\" +\n",
        "                \"\\n\".join(records_text) +\n",
        "                \"\\n\\n\" +\n",
        "                \"1. Explica el funcionamiento y la estrategia de búsqueda de cada motor (GridSearchCV, RandomizedSearchCV, Optuna, BayesSearchCV).\\n\" +\n",
        "                \"2. Analiza comparativamente los scores obtenidos por cada optimización y justifica cuál es la mejor configuración.\\n\" +\n",
        "                \"3. Ofrece recomendaciones de posibles mejoras cambiando parámetros de ajuste u otros factores para incrementar el desempeño.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 12.4. Llamando a OpenAI para análisis generativo SVR optimización\")\n",
        "            generative_analysis = call_openai_explanation(prompt)\n",
        "            self.sections.append((\n",
        "                \"### SVR Optimización: Análisis Generativo\",\n",
        "                generative_analysis\n",
        "            ))\n",
        "            # ---- Fin bloque 0 ----\n",
        "\n",
        "            # --- 1. Curvas de Ajuste Real vs. Predicho y de Residuos ---\n",
        "            print(\"[DEBUG] 12.5. Iniciando sección Calidad Ajuste SVR optimizado\")\n",
        "            # Seleccionar mejor registro por Score\n",
        "            best_rec = max(summary_records, key=lambda x: x['Score'])\n",
        "            sel_method, engine = best_rec['Selección X'], best_rec['Motor']\n",
        "\n",
        "            # ——— 1.1 Normalizar payload ———\n",
        "            payload_raw = OPT_MODELS[('svr', sel_method, engine)]\n",
        "            p           = _normalize_payload(payload_raw)\n",
        "\n",
        "            model, sx, sy, raw_cols = p['model'], p['sx'], p['sy'], p['cols']\n",
        "            score, metric           = p['score'], p['metric']\n",
        "            best_params             = p['best_params']\n",
        "            # ——— Fin normalización ———\n",
        "\n",
        "            # ——— 1.1.1 Sanitización y filtro unificado de columnas ———\n",
        "            sanitized_cols = [sanitize_name(c) for c in raw_cols]\n",
        "            missing = set(sanitized_cols) - set(X_test.columns)\n",
        "            if missing:\n",
        "                print(f\"[WARNING] SVR omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "            cols_valid = [c for c in sanitized_cols if c in X_test.columns]\n",
        "\n",
        "            # Reemplaza cualquier X_test_sel previo por esta versión saneada\n",
        "            X_test_sel = X_test[cols_valid].copy()\n",
        "            # ——— Fin de sanitización ———\n",
        "\n",
        "            # ——— 1.1.1 Saneamiento y filtro de columnas ———\n",
        "            #import re\n",
        "            #def clean_name(s):\n",
        "            #    # idéntico a tu celda 9.x\n",
        "            #    t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))\n",
        "            #    t = re.sub(r'_+', '_', t)\n",
        "            #    return t.strip('_')\n",
        "\n",
        "            # Limpio cada nombre y luego lo filtro\n",
        "            #cols = [ clean_name(c) for c in raw_cols ]\n",
        "            #cols_valid = [c for c in cols if c in X_test.columns]\n",
        "            #missing = set(cols) - set(cols_valid)\n",
        "            #if missing:\n",
        "            #    print(f\"[WARNING] SVR omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "\n",
        "            # Ahora ya puedo indexar con total seguridad\n",
        "            #X_test_sel = X_test[cols_valid].copy()\n",
        "            y_true     = Y_test.values.ravel()\n",
        "            X_test_scaled = sx.transform(X_test_sel)\n",
        "            y_pred     = sy.inverse_transform(\n",
        "                            model.predict(X_test_scaled).reshape(-1,1)\n",
        "                        ).ravel()\n",
        "            # ——— Fin saneamiento SVR ———\n",
        "\n",
        "            # ——— 1.1 Normalizar payload\n",
        "            #payload_raw        = OPT_MODELS[('svr', sel_method, engine)]\n",
        "            #p                  = _normalize_payload(payload_raw)\n",
        "\n",
        "            #model, sx, sy, cols = p['model'], p['sx'], p['sy'], p['cols']\n",
        "            #score, metric      = p['score'], p['metric']\n",
        "            #best_params        = p['best_params']\n",
        "            # ——— Fin normalización\n",
        "\n",
        "            # ——— Unificar saneamiento de columnas según entrenamiento ———\n",
        "            #import re\n",
        "            #def clean_name(s):\n",
        "            #    t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))\n",
        "            #    t = re.sub(r'_+', '_', t)\n",
        "            #    return t.strip('_')\n",
        "\n",
        "            #cols = [ clean_name(c) for c in cols ]\n",
        "            # ——— Fin unificación ———\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            #X_test_sel = X_test[cols].copy()\n",
        "            #y_true = Y_test.values.ravel()\n",
        "            #X_test_scaled = sx.transform(X_test_sel)\n",
        "            #y_pred = sy.inverse_transform(model.predict(X_test_scaled).reshape(-1,1)).ravel()\n",
        "\n",
        "            # Cálculo de residuos y estadísticas\n",
        "            residuals = y_true - y_pred\n",
        "            mean_res = float(np.mean(residuals))\n",
        "            std_res = float(np.std(residuals))\n",
        "            skew_res = float(skew(residuals))\n",
        "            kurt_res = float(kurtosis(residuals))\n",
        "            q25, q50, q75 = [float(x) for x in np.quantile(residuals, [0.25, 0.5, 0.75])]\n",
        "\n",
        "            # Gráfica Predicho vs Real\n",
        "            fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "            ax1.scatter(y_true, y_pred, alpha=0.6)\n",
        "            ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "            ax1.set_xlabel(\"Y real\")\n",
        "            ax1.set_ylabel(\"Y predicho\")\n",
        "            ax1.set_title(f\"SVR Optimizado (Mejor: {sel_method}-{engine}) Predicho vs Real\")\n",
        "            self.sections.append((\n",
        "                f\"### SVR Optimizado: Predicho vs Real ({sel_method}-{engine})\", fig1\n",
        "            ))\n",
        "\n",
        "            # Gráfica Residuos\n",
        "            fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "            ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "            ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "            ax2.set_xlabel(\"Y predicho\")\n",
        "            ax2.set_ylabel(\"Residuo\")\n",
        "            ax2.set_title(f\"SVR Optimizado (Mejor) Residuos ({sel_method}-{engine})\")\n",
        "            self.sections.append((\n",
        "                f\"### SVR Optimizado: Residuos ({sel_method}-{engine})\", fig2\n",
        "            ))\n",
        "\n",
        "            # Tabla de estadísticas de residuos\n",
        "            df_stats = pd.DataFrame({\n",
        "                'Métrica': ['Media', 'Desviación', 'Skew', 'Kurtosis', '25%', '50%', '75%'],\n",
        "                'Valor': [mean_res, std_res, skew_res, kurt_res, q25, q50, q75]\n",
        "            })\n",
        "            self.sections.append((\n",
        "                f\"### SVR Optimizado: Estadísticas de Residuos ({sel_method}-{engine})\", df_stats\n",
        "            ))\n",
        "\n",
        "            # Análisis generativo de calidad de ajuste\n",
        "            prompt2 = (\n",
        "                f\"Para el mejor modelo SVR optimizado con selección {sel_method} y motor {engine}, \"\n",
        "                f\"tienes los siguientes datos:\\n\"\n",
        "                f\"- Rango Y real: [{float(y_true.min()):.4f}, {float(y_true.max()):.4f}]\\n\"\n",
        "                f\"- Rango Y pred: [{float(y_pred.min()):.4f}, {float(y_pred.max()):.4f}]\\n\"\n",
        "                f\"- Estadísticas residuos: media={mean_res:.4f}, std={std_res:.4f}, skew={skew_res:.4f}, kurtosis={kurt_res:.4f}, \"\n",
        "                f\"quantiles 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}.\\n\\n\"\n",
        "                \"1. Analiza la calidad del ajuste basándote en la gráfica Predicho vs Real y la de residuos.\\n\"\n",
        "                \"2. Comenta sobre sesgos sistemáticos, heterocedasticidad y normalidad de errores.\\n\"\n",
        "                \"3. Ofrece recomendaciones para mejorar el fit si hay problemas detectados.\"\n",
        "            )\n",
        "            analysis2 = call_openai_explanation(prompt2)\n",
        "            self.sections.append((\n",
        "                \"### SVR Optimizado: Análisis Calidad Ajuste\", analysis2\n",
        "            ))\n",
        "            # ---- Fin bloque 1 ----\n",
        "\n",
        "            # --- 2. Importancia Relativa de Hiperparámetros ---\n",
        "            import seaborn as sns\n",
        "            # 2.1 Heatmap Score vs Best_C y Best_epsilon\n",
        "            heat = df_summary.pivot(index='Best_C', columns='Best_epsilon', values='Score')\n",
        "            fig_heat, ax_heat = plt.subplots(figsize=(6,5))\n",
        "            sns.heatmap(heat, annot=True, fmt='.4f', ax=ax_heat)\n",
        "            ax_heat.set_title('SVR Optimizado: Heatmap Score vs C y ε')\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Heatmap Score vs C y ε', fig_heat\n",
        "            ))\n",
        "\n",
        "            # 2.2 Sensibilidad ±10%\n",
        "            sens = []\n",
        "            for rec in summary_records:\n",
        "                for param in ['C','epsilon']:\n",
        "                    base = rec[f'Best_{param}']\n",
        "                    if base is None: continue\n",
        "                    for factor,label in [(1.1,'+10%'),(0.9,'-10%')]:\n",
        "                        sens.append({\n",
        "                            'Parámetro':param,\n",
        "                            'Cambio':label,\n",
        "                            '% Score':rec['Score']*(factor-1)*100,\n",
        "                            'Selección X':rec['Selección X'],\n",
        "                            'Motor':rec['Motor']\n",
        "                        })\n",
        "            df_sens = pd.DataFrame(sens)\n",
        "            fig_sens, ax_sens = plt.subplots(figsize=(6,4))\n",
        "            sns.barplot(data=df_sens, x='% Score', y='Parámetro', hue='Cambio', ax=ax_sens)\n",
        "            ax_sens.set_title('SVR Opt: Sensibilidad Score ±10%')\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Sensibilidad del Score', fig_sens\n",
        "            ))\n",
        "\n",
        "            # 2.3 Análisis IA de Importancia\n",
        "            def call_openai_explanation(prompt, model='gpt-4'):\n",
        "                resp = _client.chat.completions.create(\n",
        "                    model=model,\n",
        "                    messages=[\n",
        "                        {'role':'system','content':'Eres un experto en análisis de hiperparámetros de ML.'},\n",
        "                        {'role':'user','content':prompt}\n",
        "                    ],\n",
        "                    temperature=TEMPERATURE_VAL,\n",
        "                    max_tokens=MAX_EXPLANATION_TOKENS\n",
        "                )\n",
        "                return resp.choices[0].message.content.strip()\n",
        "\n",
        "            lines = [f\"{r['Parámetro']} {r['Cambio']} => {r['% Score']:.2f}%\" for r in sens]\n",
        "            prompt_param = (\n",
        "                \"Sensibilidad de Score al ±10% en C y ε:\\n\" + \"\\n\".join(lines) +\n",
        "                \"\\n\\nExplica qué hiperparámetro impulsa más mejora y por qué, y sugiere focos de tuning.\"    )\n",
        "            print(\"[DEBUG] 12.6. Llamando IA importancia hiperparámetros\")\n",
        "            analysis_hp = call_openai_explanation(prompt_param)\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: IA Importancia Hiperparámetros', analysis_hp\n",
        "            ))\n",
        "            # ---- Fin bloque 2 ----\n",
        "\n",
        "            # --- 3. Distribución de Métricas en Validación Cruzada ---\n",
        "            from sklearn.model_selection import cross_validate\n",
        "            import seaborn as sns\n",
        "            print(\"[DEBUG] 12.7. Calculando distribución de métricas CV para el mejor modelo optimizado\")\n",
        "\n",
        "            # 3. Identificar mejor modelo optimizado\n",
        "            best_rec       = max(summary_records, key=lambda x: x['Score'])\n",
        "            sel_method_cv, engine_cv = best_rec['Selección X'], best_rec['Motor']\n",
        "\n",
        "            # --- Normalizar extracción de payload ---\n",
        "            payload_raw_cv = OPT_MODELS[('svr', sel_method_cv, engine_cv)]\n",
        "            p_cv           = _normalize_payload(payload_raw_cv)\n",
        "            model_cv, sx_cv, sy_cv, cols_cv = (\n",
        "                p_cv['model'], p_cv['sx'], p_cv['sy'], p_cv['cols']\n",
        "            )\n",
        "            # ----------------------------------------\n",
        "\n",
        "            # Preparar datos de entrenamiento escalados\n",
        "            X_cv = X_train[cols_cv].copy()\n",
        "            y_cv = Y_train.values.ravel()\n",
        "            X_cv_scaled = sx_cv.transform(X_cv)\n",
        "\n",
        "            # Cross-validate con múltiples métricas\n",
        "            cv_results = cross_validate(\n",
        "                model_cv, X_cv_scaled, y_cv,\n",
        "                cv=5,\n",
        "                scoring={\n",
        "                    'r2':'r2',\n",
        "                    'neg_mse':'neg_mean_squared_error',\n",
        "                    'neg_mae':'neg_mean_absolute_error'\n",
        "                },\n",
        "                return_train_score=False\n",
        "            )\n",
        "\n",
        "            # Procesar resultados\n",
        "            r2_scores   = cv_results['test_r2']\n",
        "            mse_scores  = [-v for v in cv_results['test_neg_mse']]\n",
        "            mae_scores  = [-v for v in cv_results['test_neg_mae']]\n",
        "            rmse_scores = np.sqrt(mse_scores)\n",
        "\n",
        "            df_cv = pd.DataFrame({\n",
        "                'R2': r2_scores,\n",
        "                'MSE': mse_scores,\n",
        "                'MAE': mae_scores,\n",
        "                'RMSE': rmse_scores\n",
        "            })\n",
        "\n",
        "            # 3.1 Boxplot de métricas por fold\n",
        "            fig_cv, ax_cv = plt.subplots(figsize=(6, 4))\n",
        "            sns.boxplot(data=df_cv, ax=ax_cv)\n",
        "            ax_cv.set_title('SVR Optimizado: Distribución de Métricas CV')\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Distribución de Métricas CV', fig_cv\n",
        "            ))\n",
        "\n",
        "            # 3.2 Tabla con media ± desviación\n",
        "            stats_cv = df_cv.agg(['mean', 'std']).T.reset_index().rename(columns={\n",
        "                'index':'Métrica', 'mean':'Media', 'std':'Desviación'\n",
        "            })\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Estadísticas CV por Fold', stats_cv\n",
        "            ))\n",
        "\n",
        "            # 3.3 Análisis Generativo IA de estabilidad\n",
        "            prompt_cv = (\n",
        "                f\"Valores CV por fold (R2: {r2_scores.tolist()}, MAE: {mae_scores}, RMSE: {rmse_scores}).\\n\"\n",
        "                \"¿Qué nos dice la dispersión sobre la estabilidad del modelo?\"\n",
        "            )\n",
        "            print(\"[DEBUG] 12.8. Llamando IA para estabilidad en CV\")\n",
        "            analysis_cv = call_openai_explanation(prompt_cv)\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Análisis Estabilidad CV', analysis_cv\n",
        "            ))\n",
        "            # ---- Fin bloque 3 ----\n",
        "\n",
        "            # --- 4. Sección Calidad Ajuste Mejor Modelo Optimizado SVR ---\n",
        "            # --- 4.1. Curvas de Aprendizaje y Validación para SVR Optimizado ---\n",
        "            from sklearn.model_selection import learning_curve, validation_curve\n",
        "            print(\"[DEBUG] 12.9. Calculando curvas de aprendizaje y validación para SVR optimizado\")\n",
        "\n",
        "            # 4.1 Seleccionar el mejor payload para curvas de aprendizaje\n",
        "            best_rec       = max(summary_records, key=lambda x: x['Score'])\n",
        "            sel_method, engine = best_rec['Selección X'], best_rec['Motor']\n",
        "\n",
        "            # --- Normalizamos la extracción del payload ---\n",
        "            payload_raw    = OPT_MODELS[('svr', sel_method, engine)]\n",
        "            p              = _normalize_payload(payload_raw)\n",
        "            model, sx, sy, cols = p['model'], p['sx'], p['sy'], p['cols']\n",
        "            # -----------------------------------------------\n",
        "\n",
        "            # Preparar datos de entrenamiento\n",
        "            X_train_sel = X_train[cols].copy()\n",
        "            y_train = Y_train.values.ravel()\n",
        "            X_train_scaled = sx.transform(X_train_sel)\n",
        "\n",
        "            # Curva de aprendizaje (R²)\n",
        "            train_sizes, train_scores, val_scores = learning_curve(\n",
        "                model, X_train_scaled, y_train,\n",
        "                cv=3, scoring='r2', train_sizes=np.linspace(0.1, 1.0, 5), n_jobs=-1\n",
        "            )\n",
        "            train_mean = np.mean(train_scores, axis=1)\n",
        "            val_mean = np.mean(val_scores, axis=1)\n",
        "            fig_lc, ax_lc = plt.subplots(figsize=(6, 4))\n",
        "            ax_lc.plot(train_sizes, train_mean, 'o-', label='Train R²')\n",
        "            ax_lc.plot(train_sizes, val_mean, 'o-', label='CV R²')\n",
        "            ax_lc.set_title('Curva de Aprendizaje SVR Optimizado')\n",
        "            ax_lc.set_xlabel('Tamaño del set de entrenamiento')\n",
        "            ax_lc.set_ylabel('R²')\n",
        "            ax_lc.legend()\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Curva de Aprendizaje', fig_lc\n",
        "            ))\n",
        "\n",
        "            # --- 4.2. Curva de validación para hiperparámetro C\n",
        "            param_range_C = np.logspace(-1, 2, 5)\n",
        "            train_scores_C, val_scores_C = validation_curve(\n",
        "                model, X_train_scaled, y_train,\n",
        "                param_name='C', param_range=param_range_C,\n",
        "                cv=3, scoring='r2', n_jobs=-1\n",
        "            )\n",
        "            fig_vc_C, ax_vc_C = plt.subplots(figsize=(6, 4))\n",
        "            ax_vc_C.plot(param_range_C, np.mean(train_scores_C, axis=1), 'o-', label='Train R²')\n",
        "            ax_vc_C.plot(param_range_C, np.mean(val_scores_C, axis=1), 'o-', label='CV R²')\n",
        "            ax_vc_C.set_xscale('log')\n",
        "            ax_vc_C.set_title('Curva de Validación: C')\n",
        "            ax_vc_C.set_xlabel('C')\n",
        "            ax_vc_C.set_ylabel('R²')\n",
        "            ax_vc_C.legend()\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Curva de Validación C', fig_vc_C\n",
        "            ))\n",
        "\n",
        "            # --- 4.3. Curva de validación para hiperparámetro epsilon\n",
        "            param_range_e = np.logspace(-2, 0, 5)\n",
        "            train_scores_e, val_scores_e = validation_curve(\n",
        "                model, X_train_scaled, y_train,\n",
        "                param_name='epsilon', param_range=param_range_e,\n",
        "                cv=3, scoring='r2', n_jobs=-1\n",
        "            )\n",
        "            fig_vc_e, ax_vc_e = plt.subplots(figsize=(6, 4))\n",
        "            ax_vc_e.plot(param_range_e, np.mean(train_scores_e, axis=1), 'o-', label='Train R²')\n",
        "            ax_vc_e.plot(param_range_e, np.mean(val_scores_e, axis=1), 'o-', label='CV R²')\n",
        "            ax_vc_e.set_xscale('log')\n",
        "            ax_vc_e.set_title('Curva de Validación: Epsilon')\n",
        "            ax_vc_e.set_xlabel('Epsilon')\n",
        "            ax_vc_e.set_ylabel('R²')\n",
        "            ax_vc_e.legend()\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Curva de Validación Epsilon', fig_vc_e\n",
        "            ))\n",
        "\n",
        "            # --- 4.4. Interpretación IA de las Curvas de Aprendizaje y Validación ---\n",
        "            # Extraemos estadísticas para el prompt\n",
        "            lc_train = train_mean.tolist()\n",
        "            lc_val   = val_mean.tolist()\n",
        "            vc_C_train = np.mean(train_scores_C, axis=1).tolist()\n",
        "            vc_C_val   = np.mean(val_scores_C, axis=1).tolist()\n",
        "            vc_e_train = np.mean(train_scores_e, axis=1).tolist()\n",
        "            vc_e_val   = np.mean(val_scores_e, axis=1).tolist()\n",
        "            sizes = train_sizes.tolist()\n",
        "            C_vals = param_range_C.tolist()\n",
        "            e_vals = param_range_e.tolist()\n",
        "\n",
        "            prompt_curvas = (\n",
        "                \"He generado para el SVR optimizado las siguientes curvas:\\n\"\n",
        "                f\"- Curva de Aprendizaje (R²): tamaños={sizes}, train={lc_train}, CV={lc_val}\\n\"\n",
        "                f\"- Curva de Validación para C (R²): C={C_vals}, train={vc_C_train}, CV={vc_C_val}\\n\"\n",
        "                f\"- Curva de Validación para ε (R²): ε={e_vals}, train={vc_e_train}, CV={vc_e_val}\\n\\n\"\n",
        "                \"1. Interpreta cada curva: ¿qué nos dice sobre sesgo (under-/overfitting) y varianza?\\n\"\n",
        "                \"2. Señala si hay señales de under-/overfitting o alta varianza según cada gráfica.\\n\"\n",
        "                \"3. Concluye recomendaciones generales para mejorar el aprendizaje (por ejemplo, ajustar C, ε, más datos, regularización, etc.).\"\n",
        "            )\n",
        "\n",
        "            print(\"[DEBUG] 12.10. Llamando a OpenAI para interpretación IA de curvas\")\n",
        "            analisis_curvas = call_openai_explanation(prompt_curvas)\n",
        "            self.sections.append((\n",
        "                \"### SVR Optimizado: Interpretación IA de Curvas\",\n",
        "                analisis_curvas\n",
        "            ))\n",
        "            # ---- Fin bloque 4 ----\n",
        "\n",
        "            # --- 5. Curvas de Calibración y Predicción de Intervalos ---\n",
        "            from sklearn.calibration import calibration_curve\n",
        "            print(\"[DEBUG] 12.11. Calculando curva de calibración y predicción de intervalos para el mejor modelo optimizado SVR\")\n",
        "\n",
        "            # Identificar mejor modelo optimizado\n",
        "            best_rec_ci       = max(summary_records, key=lambda x: x['Score'])\n",
        "            sel_method_ci, engine_ci = best_rec_ci['Selección X'], best_rec_ci['Motor']\n",
        "\n",
        "            # --- Normalizamos la extracción del payload ---\n",
        "            payload_raw_ci   = OPT_MODELS[('svr', sel_method_ci, engine_ci)]\n",
        "            p_ci             = _normalize_payload(payload_raw_ci)\n",
        "            model_ci, sx_ci, sy_ci, cols_ci = p_ci['model'], p_ci['sx'], p_ci['sy'], p_ci['cols']\n",
        "            # -----------------------------------------------\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            X_test_ci = X_test[cols_ci].copy()\n",
        "            y_true_ci = Y_test.values.ravel()\n",
        "            X_test_scaled_ci = sx_ci.transform(X_test_ci)\n",
        "\n",
        "            # ── Predicción raw ──\n",
        "            y_pred_raw_ci = model_ci.predict(X_test_scaled_ci)\n",
        "\n",
        "            # ── Desescalado (si tienes sy_ci) ──\n",
        "            if sy_ci is not None:\n",
        "                # sy_ci es tu StandardScaler para y\n",
        "                y_pred_ci = sy_ci.inverse_transform(\n",
        "                    y_pred_raw_ci.reshape(-1,1)\n",
        "                ).ravel()\n",
        "            else:\n",
        "                y_pred_ci = y_pred_raw_ci.ravel()\n",
        "\n",
        "            # 5.1 Curva de calibración (binned reliability plot para regresión)\n",
        "            import pandas as _pd\n",
        "            bins = 10\n",
        "            print(\"[DEBUG] 12.12. Calculando curva de calibración manual para regresión SVR optimizado\")\n",
        "            df_cal = _pd.DataFrame({'y_pred': y_pred_ci, 'y_true': y_true_ci})\n",
        "            # Crear bins según cuantiles en predicción\n",
        "            try:\n",
        "                df_cal['bin'] = _pd.qcut(df_cal['y_pred'], q=bins, duplicates='drop')\n",
        "            except Exception:\n",
        "                df_cal['bin'] = _pd.cut(df_cal['y_pred'], bins=bins)\n",
        "            # Agrupar para obtener promedio predicho vs observado\n",
        "            #grp = df_cal.groupby('bin').agg({'y_pred': 'mean', 'y_true': 'mean'})\n",
        "            grp = df_cal.groupby('bin', observed=True).agg({'y_pred': 'mean', 'y_true': 'mean'})\n",
        "            prob_pred = grp['y_pred'].values\n",
        "            prob_true = grp['y_true'].values\n",
        "            fig_cal, ax_cal = plt.subplots(figsize=(6, 4))\n",
        "            ax_cal.plot(prob_pred, prob_true, marker='o', linewidth=2)\n",
        "            ax_cal.plot([prob_pred.min(), prob_pred.max()], [prob_pred.min(), prob_pred.max()], 'k--')\n",
        "            ax_cal.set_xlabel('Predicción promedio en bin')\n",
        "            ax_cal.set_ylabel('Valor real promedio')\n",
        "            ax_cal.set_title('SVR Optimizado: Curva de Calibración')\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Curva de Calibración', fig_cal\n",
        "            ))\n",
        "\n",
        "            # 5.2 Intervalos de predicción ±1 STD de residuos\n",
        "            residuals_ci = y_true_ci - y_pred_ci\n",
        "            std_res_ci = np.std(residuals_ci)\n",
        "            upper = y_pred_ci + std_res_ci\n",
        "            lower = y_pred_ci - std_res_ci\n",
        "            fig_int, ax_int = plt.subplots(figsize=(6, 4))\n",
        "            ax_int.plot(y_true_ci, label='Y real')\n",
        "            ax_int.plot(y_pred_ci, label='Predicción')\n",
        "            ax_int.fill_between(range(len(y_pred_ci)), lower, upper, alpha=0.3, label='±1 STD residuo')\n",
        "            ax_int.set_xlabel('Índice de muestra')\n",
        "            ax_int.set_ylabel('Valor')\n",
        "            ax_int.set_title('SVR Optimizado: Intervalos de Predicción')\n",
        "            ax_int.legend()\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Intervalos de Predicción', fig_int\n",
        "            ))\n",
        "\n",
        "            # 5.3 Análisis Generativo IA de incertidumbre y calibración\n",
        "            prompt_ci = (\n",
        "                f\"Curva de calibración (prob_pred: {prob_pred.tolist()}, prob_true: {prob_true.tolist()}) y \"\n",
        "                f\"intervalos de predicción ±1 STD (std_res={std_res_ci:.4f}).\\n\"\n",
        "                \"1. ¿Son fiables los intervalos de incertidumbre?\"\n",
        "                \"2. ¿Observas infravaloración de errores altos?\"\n",
        "            )\n",
        "            print(\"[DEBUG] 12.13. Llamando IA para análisis de incertidumbre y calibración SVR\")\n",
        "            analysis_ci = call_openai_explanation(prompt_ci)\n",
        "            self.sections.append((\n",
        "                '### SVR Optimizado: Análisis de Incertidumbre y Calibración', analysis_ci\n",
        "            ))\n",
        "            # ---- Fin bloque 5 ----\n",
        "\n",
        "            # --- Sección 6: Resumen Ejecutivo y Road-Map de Siguientes Pasos ---\n",
        "            # 6.1. Bloque Markdown con puntos clave para stakeholders\n",
        "            summary_md = (\n",
        "                \"**Puntos Clave:**\\n\"\n",
        "                f\"- **Mejor combinación:** {sel_method_ci}-{engine_ci} con score={best_rec_ci['Score']:.4f}.\\n\"\n",
        "                f\"- **Grado de robustez:** Variabilidad CV={val_mean.std():.4f}, residuos (std={std_res_ci:.4f}).\\n\"\n",
        "                f\"- **Puntos débiles:** identificar picos de error y alta varianza en ciertos rangos.\\n\"\n",
        "                f\"- **Recomendaciones inmediatas:** ampliar CV, explorar HalvingGridSearchCV, recolectar más datos.\"\n",
        "            )\n",
        "            self.sections.append((\n",
        "                '### Resumen Ejecutivo y Road-Map', summary_md\n",
        "            ))\n",
        "\n",
        "            # 6.2. Análisis Generativo IA para desarrollar cada punto clave y generar resumen ejecutivo\n",
        "            prompt_exec = (\n",
        "                \"Eres un investigador científico para el Instituto de Procesos Sostenibles de la Universidad de Valladolid. \"\n",
        "                \"A continuación se presentan los puntos clave de la optimización SVR:\\n\"\n",
        "                f\"{summary_md}\\n\"\n",
        "                \"Desarrolla un análisis detallado en párrafos separados para cada punto (Mejor combinación, Grado de robustez, Puntos débiles, Recomendaciones inmediatas), \"\n",
        "                \"y finaliza con un resumen ejecutivo de 3 párrafos resumiendo hallazgos y próximos pasos para los stakeholders.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 12.14. Llamando IA para Resumen Ejecutivo y Road-Map SVR optimización\")\n",
        "            analysis_exec = call_openai_explanation(prompt_exec)\n",
        "            self.sections.append((\n",
        "                '### Resumen Ejecutivo IA', analysis_exec\n",
        "            ))\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en sección Optimización SVR\",\n",
        "                f\"Se produjo un error al generar el resumen de métodos y motores: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =============================================================================\n",
        "        # 13. Optimización Modelo Optimización NN\n",
        "        # =============================================================================\n",
        "        try:\n",
        "            print(\"[DEBUG] 13.1. Iniciando sección Optimización NN: Resumen de métodos y motores\")\n",
        "            # Verificación previa: usamos OPT_MODELS guardado en la celda 9.2\n",
        "            if 'OPT_MODELS' in globals() and isinstance(OPT_MODELS, dict):\n",
        "                valid_engines = {'randomsearch', 'bayesianoptimization', 'hyperband', 'optuna'}\n",
        "                nn_entries = {\n",
        "                    k: v for k, v in OPT_MODELS.items()\n",
        "                    if (isinstance(k, tuple) and k[0] == 'nn' and isinstance(v, dict) and k[2] in valid_engines)\n",
        "                }\n",
        "            else:\n",
        "                raise RuntimeError(\n",
        "                    \"No se encontró la variable global OPT_MODELS con resultados de optimización NN. \"\n",
        "                    \"Asegúrate de haber ejecutado la Celda 9.2 y de que OPT_MODELS contenga los payloads optimizados.\"\n",
        "                )\n",
        "\n",
        "            import pickle\n",
        "            import pandas as pd\n",
        "            import numpy as np\n",
        "            import matplotlib.pyplot as plt\n",
        "            import seaborn as sns\n",
        "            from scipy.stats import skew, kurtosis\n",
        "            from tensorflow.keras.models import load_model\n",
        "            import tensorflow as tf\n",
        "            from tensorflow import keras\n",
        "            from tensorflow.keras import layers as keras_layers\n",
        "            from scikeras.wrappers import KerasRegressor\n",
        "            from sklearn.model_selection import learning_curve\n",
        "\n",
        "            #import re\n",
        "\n",
        "            #def _clean_col(name):\n",
        "            #    # sustituye [, ], <, >, % por _\n",
        "            #    return re.sub(r'[\\[\\]<>%]', '_', str(name))\n",
        "\n",
        "            # ——— 1.1.1 Sanitización y filtro unificado de columnas ———\n",
        "            #sanitized_cols = [sanitize_name(c) for c in raw_cols]\n",
        "            #missing = set(sanitized_cols) - set(X_test.columns)\n",
        "            #if missing:\n",
        "            #    print(f\"[WARNING] SVR omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "            #cols_valid = [c for c in sanitized_cols if c in X_test.columns]\n",
        "\n",
        "            #X_test_sel = X_test[cols_valid].copy()\n",
        "\n",
        "            # Parámetros de búsqueda usados en cada motor (para documentar)\n",
        "            param_configs = {\n",
        "                'randomsearch': { 'layers': 'hp.Int(\"layers\", 1, max_layers)', 'units': 'hp.Int(\"units_i\", min_units, max_units)', 'dropout': 'hp.Float(\"dropout_i\", 0.0, max_dropout)' },\n",
        "                'bayesianoptimization': { 'layers': 'hp.Int(\"layers\", 1, max_layers)', 'units': 'hp.Int(\"units_i\", min_units, max_units)', 'dropout': 'hp.Float(\"dropout_i\", 0.0, max_dropout)' },\n",
        "                'hyperband': { 'layers': 'hp.Int(\"layers\", 1, max_layers)', 'units': 'hp.Int(\"units_i\", min_units, max_units)', 'dropout': 'hp.Float(\"dropout_i\", 0.0, max_dropout)' },\n",
        "                'optuna': { 'layers': 'suggest_int(\"n_layers\", 1, max_layers)', 'units': 'suggest_int(\"n_units_l{i}\", min_units, max_units)', 'dropout': 'suggest_float(\"dropout_l{i}\", 0.0, max_dropout)' }\n",
        "            }\n",
        "\n",
        "            summary_records = []\n",
        "            # Iterar cada combinación método/motor\n",
        "            for (_, sel_method, engine), payload in nn_entries.items():\n",
        "                print(f\"[DEBUG] 13.2. Agregando registro resumen NN: {sel_method} / {engine}\")\n",
        "\n",
        "                # 1) métricas\n",
        "                score = payload.get('score')\n",
        "                metric = payload.get('metric')\n",
        "\n",
        "                # Parámetros de búsqueda documentados\n",
        "                config_used = param_configs.get(engine, {})\n",
        "\n",
        "                # 3) hiperparámetros óptimos\n",
        "                if engine in (\"randomsearch\",\"bayesianoptimization\",\"hyperband\"):\n",
        "                    best_params = {\n",
        "                        \"layers\":  payload.get(\"layers\"),\n",
        "                        \"neurons\": payload.get(\"neurons\"),\n",
        "                        \"dropout\": payload.get(\"dropout\"),\n",
        "                        \"epochs\":  payload.get(\"epochs\")\n",
        "                    }\n",
        "                else:  # optuna\n",
        "                    best_params = {\n",
        "                        \"layers\":  payload.get(\"n_layers\"),\n",
        "                        \"neurons\": payload.get(\"n_units_l0\"),\n",
        "                        \"dropout\": payload.get(\"dropout_l0\"),\n",
        "                        \"epochs\":  payload.get(\"epochs\")\n",
        "                    }\n",
        "\n",
        "                # Construir fila de resumen\n",
        "                row = {\n",
        "                    'Selección X': sel_method,\n",
        "                    'Motor': engine,\n",
        "                    'Métrica': metric,\n",
        "                    'Score': score\n",
        "                }\n",
        "                # Añadir configuración de búsqueda al row\n",
        "                for k, v in config_used.items():\n",
        "                    row[f'Config_{k}'] = v\n",
        "                # Añadir mejores hiperparámetros al row\n",
        "                for k, v in best_params.items():\n",
        "                    row[f'Best_{k}'] = v\n",
        "\n",
        "                summary_records.append(row)\n",
        "\n",
        "            # Crear DataFrame de resumen y reordenar columnas\n",
        "            df_summary_nn = pd.DataFrame(summary_records)\n",
        "            desired_cols = [\n",
        "                'Selección X', 'Motor', 'Métrica', 'Score',\n",
        "                'Config_layers', 'Config_units', 'Config_dropout',\n",
        "                'Best_layers', 'Best_neurons', 'Best_dropout', 'Best_epochs'\n",
        "            ]\n",
        "            available_cols = [c for c in desired_cols if c in df_summary_nn.columns]\n",
        "            df_to_show_nn = df_summary_nn[available_cols] if available_cols else df_summary_nn\n",
        "\n",
        "            # Añadir sección de tabla al informe\n",
        "            self.sections.append((\n",
        "                \"### NN Optimización: Resumen de Métodos y Motores\",\n",
        "                df_to_show_nn.reset_index(drop=True)\n",
        "            ))\n",
        "\n",
        "            # ————— Preselección global del mejor modelo NN —————\n",
        "            if df_summary_nn['Métrica'].iloc[0] == 'R2':\n",
        "                idx_best_nn = df_summary_nn['Score'].idxmax()\n",
        "            else:\n",
        "                idx_best_nn = df_summary_nn['Score'].idxmin()\n",
        "\n",
        "            best_entry_nn = df_summary_nn.loc[idx_best_nn]\n",
        "            sel_method_nn = best_entry_nn['Selección X']\n",
        "            engine_nn     = best_entry_nn['Motor']\n",
        "\n",
        "            payload_raw = OPT_MODELS[('nn', sel_method, engine)]\n",
        "            p = _normalize_payload(payload_raw)\n",
        "            model        = p['model']\n",
        "            sx, sy, cols = p['sx'], p['sy'], p['cols']\n",
        "            score, metric, best_params = p['score'], p['metric'], p['best_params']\n",
        "\n",
        "            # Precargo payload_best para usar en todos los bloques\n",
        "            payload_best = p    # ya tienes el payload “normalizado” en p, no necesitas nn_entries aquí\n",
        "\n",
        "            # --- 0. Análisis Generativo IA de Tabla de Resumen ---\n",
        "            def call_openai_explanation(prompt: str, model=\"gpt-4\"):\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un experto en optimización de redes neuronales. \"\n",
        "                                \"Analiza los resultados y ofrece conclusiones detalladas.\" )},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS\n",
        "                    )\n",
        "                    return response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    print(f\"[DEBUG] ¡Exception atrapada en Optimización NN!: {type(e).__name__}: {e}\")\n",
        "                    return f\"[Error llamando a OpenAI: {e}]\"\n",
        "\n",
        "            # Construir prompt a partir de summary_records\n",
        "            records_text = []\n",
        "            for rec in summary_records:\n",
        "                records_text.append(\n",
        "                    f\"Método X: {rec['Selección X']}, Motor: {rec['Motor']}, \"\n",
        "                    f\"Métrica: {rec['Métrica']}, Score: {rec['Score']:.4f}, \"\n",
        "                    f\"Config: layers={rec.get('Config_layers')}, units={rec.get('Config_units')}, dropout={rec.get('Config_dropout')}, \"\n",
        "                    f\"Best: layers={rec.get('Best_layers')}, neurons={rec.get('Best_neurons')}, \"\n",
        "                    f\"dropout={rec.get('Best_dropout')}, epochs={rec.get('Best_epochs')}\"\n",
        "                )\n",
        "            prompt = (\n",
        "                \"He obtenido los siguientes resultados de optimización para la Red Neuronal:\\n\"\n",
        "                + \"\\n\".join(records_text)\n",
        "                + \"\\n\\n\"\n",
        "                + \"1. Describe e interpreta cada combinación método/motor.\\n\"\n",
        "                + \"2. Señala cuál configuración ofrece mejor desempeño y por qué.\\n\"\n",
        "                + \"3. Proporciona recomendaciones para futuros ajustes de HPO.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.3. Llamando IA para análisis generativo NN optimización\")\n",
        "            generative_analysis_nn = call_openai_explanation(prompt)\n",
        "            self.sections.append((\n",
        "                \"### NN Optimización: Análisis Generativo\",\n",
        "                generative_analysis_nn\n",
        "            ))\n",
        "            # --- Fin Bloque 0 ---\n",
        "\n",
        "            # --- 1. Curvas de Ajuste Real vs. Predicho y Residuos ---\n",
        "            print(\"[DEBUG] 13.4. Entrando en Bloque 1: Curvas Predicho vs Real\")\n",
        "            from scipy.stats import skew, kurtosis\n",
        "            # Seleccionar mejor configuración según Score\n",
        "            if df_summary_nn.empty:\n",
        "                raise RuntimeError(\"No hay registros de optimización NN para generar curvas.\")\n",
        "            # Definir criterio de ordenamiento: maximizar R2, minimizar errores\n",
        "            if df_summary_nn['Métrica'].iloc[0] == 'R2':\n",
        "                idx_best = df_summary_nn['Score'].idxmax()\n",
        "            else:\n",
        "                idx_best = df_summary_nn['Score'].idxmin()\n",
        "            best_row = df_summary_nn.loc[idx_best]\n",
        "            sel_method = best_row['Selección X']\n",
        "            engine = best_row['Motor']\n",
        "\n",
        "            # ——— 1.1 Normalizar payload\n",
        "            payload_raw   = OPT_MODELS[('nn', sel_method, engine)]\n",
        "            p             = _normalize_payload(payload_raw)\n",
        "\n",
        "            model         = p['model']\n",
        "            sx, sy, cols  = p['sx'], p['sy'], p['cols']\n",
        "            score, metric = p['score'], p['metric']\n",
        "            best_params   = p['best_params']\n",
        "\n",
        "            import tensorflow as tf  # necesario para custom_objects al cargar el modelo\n",
        "            from tensorflow.keras.models import load_model\n",
        "\n",
        "            # ——— Sanitización unificada de columnas para NN ———\n",
        "            # 1) Partimos de `cols` (payload['cols'])\n",
        "            raw_cols_nn = cols  # o p['cols'] si lo extraes ahí mismo\n",
        "\n",
        "            # 2) Aplicamos `sanitize_name` a cada nombre\n",
        "            sanitized_cols = [sanitize_name(c) for c in raw_cols_nn]\n",
        "\n",
        "            # 3) Detectamos columnas faltantes en X_test\n",
        "            missing = set(sanitized_cols) - set(X_test.columns)\n",
        "            if missing:\n",
        "                print(f\"[WARNING] NN omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "\n",
        "            # 4) Nos quedamos solo con las columnas válidas\n",
        "            cols_valid = [c for c in sanitized_cols if c in X_test.columns]\n",
        "\n",
        "            # 5) Selección segura de test y predicción\n",
        "            X_test_sel    = X_test[cols_valid].copy()\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            # — SANITIZACIÓN DE cols antes de indexar X_test\n",
        "            #cols_clean = [_clean_col(c) for c in cols]\n",
        "            #X_test_sel = X_test[cols_clean].copy()\n",
        "            #X_test_sel = X_test[cols].copy()\n",
        "            y_true = Y_test.values.ravel()\n",
        "            if sx is not None:\n",
        "                X_test_scaled = sx.transform(X_test_sel)\n",
        "            else:\n",
        "                # No se encontró scaler, usar datos sin transformar\n",
        "                X_test_scaled = X_test_sel.values\n",
        "                raw_pred = model.predict(X_test_scaled).ravel()\n",
        "            if sy is not None:\n",
        "                # Aplicar inverse transform si existe scaler de salida\n",
        "                y_pred = sy.inverse_transform(raw_pred.reshape(-1,1)).ravel()\n",
        "            else:\n",
        "                y_pred = raw_pred\n",
        "\n",
        "            # Gráfica Predicho vs Real\n",
        "            fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "            ax1.scatter(y_true, y_pred, alpha=0.6)\n",
        "            ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "            ax1.set_xlabel(\"Y real\"); ax1.set_ylabel(\"Y predicho\")\n",
        "            ax1.set_title(f\"NN Optimizado ({sel_method}-{engine}) Predicho vs Real\")\n",
        "            self.sections.append((\n",
        "                f\"### NN Optimización: Predicho vs Real ({sel_method}-{engine})\", fig1\n",
        "            ))\n",
        "\n",
        "            # Gráfica Residuos\n",
        "            residuals = y_true - y_pred\n",
        "            fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "            ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "            ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "            ax2.set_xlabel(\"Y predicho\"); ax2.set_ylabel(\"Residuo\")\n",
        "            ax2.set_title(f\"NN Optimizado ({sel_method}-{engine}) Residuos\")\n",
        "            self.sections.append((\n",
        "                f\"### NN Optimización: Residuos ({sel_method}-{engine})\", fig2\n",
        "            ))\n",
        "\n",
        "            # Tabla de estadísticas de residuos\n",
        "            stats_df = pd.DataFrame({\n",
        "                'Métrica': ['Media', 'Desviación', 'Skew', 'Kurtosis', '25%', '50%', '75%'],\n",
        "                'Valor': [residuals.mean(), residuals.std(), skew(residuals), kurtosis(residuals), *np.quantile(residuals, [0.25,0.5,0.75])]\n",
        "            })\n",
        "            self.sections.append((\n",
        "                f\"### NN Optimización: Estadísticas de Residuos ({sel_method}-{engine})\", stats_df\n",
        "            ))\n",
        "\n",
        "            # --- Análisis Generativo IA de Curvas de Ajuste (NN) ---\n",
        "            # Construimos un único f-string multilínea para asegurar que todo el texto llegue al modelo\n",
        "            prompt_curvas = f\"\"\"\n",
        "            Para el mejor modelo NN optimizado con selección {sel_method} y motor {engine} tenemos los siguientes datos:\n",
        "            - Rango Y real: [{float(y_true.min()):.4f}, {float(y_true.max()):.4f}]\n",
        "            - Rango Y predicho: [{float(y_pred.min()):.4f}, {float(y_pred.max()):.4f}]\n",
        "            - Estadísticas de residuos:\n",
        "              • media = {residuals.mean():.4f}\n",
        "              • std   = {residuals.std():.4f}\n",
        "              • skew  = {skew(residuals):.4f}\n",
        "              • kurtosis = {kurtosis(residuals):.4f}\n",
        "              • quantiles: 25%={float(np.quantile(residuals, 0.25)):.4f},\n",
        "                          50%={float(np.quantile(residuals, 0.50)):.4f},\n",
        "                          75%={float(np.quantile(residuals, 0.75)):.4f}\n",
        "\n",
        "            1. Analiza detalladamente la gráfica Predicho vs Real: di si hay desviaciones sistemáticas, cuán cerca están los puntos de la diagonal, y si observas heterocedasticidad o patrones claros.\n",
        "            2. Analiza la gráfica de residuos: describe la dispersión, si hay colas pesadas o asimetrías.\n",
        "            3. Comenta sobre la normalidad de los errores y posibles fuentes de sesgo.\n",
        "            4. Propón recomendaciones prácticas para mejorar el ajuste (p. ej. refinar hiperparámetros, transformar variables, etc.).\n",
        "            \"\"\"\n",
        "            analysis_curvas = call_openai_explanation(prompt_curvas)\n",
        "            self.sections.append((\n",
        "                \"### NN Optimización: Análisis Curvas Ajuste\", analysis_curvas\n",
        "            ))\n",
        "            # --- Fin bloque 1 ---\n",
        "\n",
        "            # --- 2. Importancia Relativa de Hiperparámetros ---\n",
        "            # 2.1 Heatmap Score vs Best_layers y Best_neurons\n",
        "            heat_nn = df_summary_nn.pivot(index='Best_layers', columns='Best_neurons', values='Score')\n",
        "            fig_heat_nn, ax_heat_nn = plt.subplots(figsize=(6,5))\n",
        "            sns.heatmap(heat_nn, annot=True, fmt='.4f', ax=ax_heat_nn)\n",
        "            ax_heat_nn.set_title('NN Optimizado: Heatmap Score vs Capas y Neuronas')\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Heatmap Score vs Capas y Neuronas', fig_heat_nn\n",
        "            ))\n",
        "\n",
        "            # 2.2 Sensibilidad ±10%\n",
        "            sens_nn = []\n",
        "            for rec in summary_records:\n",
        "                for param in ['layers','neurons']:\n",
        "                    base = rec.get(f'Best_{param}')\n",
        "                    if base is None: continue\n",
        "                    for factor,label in [(1.1,'+10%'),(0.9,'-10%')]:\n",
        "                        sens_nn.append({\n",
        "                            'Parámetro': param,\n",
        "                            'Cambio': label,\n",
        "                            '% Score': rec['Score'] * (factor - 1) * 100,\n",
        "                            'Selección X': rec['Selección X'],\n",
        "                            'Motor': rec['Motor']\n",
        "                        })\n",
        "            df_sens_nn = pd.DataFrame(sens_nn)\n",
        "            fig_sens_nn, ax_sens_nn = plt.subplots(figsize=(6,4))\n",
        "            sns.barplot(data=df_sens_nn, x='% Score', y='Parámetro', hue='Cambio', ax=ax_sens_nn)\n",
        "            ax_sens_nn.set_title('NN Opt: Sensibilidad Score ±10%')\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Sensibilidad del Score', fig_sens_nn\n",
        "            ))\n",
        "\n",
        "            # 2.3 Análisis IA de Importancia Relativa\n",
        "            lines_nn = [f\"{r['Parámetro']} {r['Cambio']} => {r['% Score']:.2f}%\" for r in sens_nn]\n",
        "            prompt_hp_nn = (\n",
        "                \"Sensibilidad de Score al ±10% en Capas y Neuronas:\\n\" +\n",
        "                \"\\n\".join(lines_nn) +\n",
        "                \"\\n\\nExplica qué hiperparámetro impulsa más mejora y por qué, y sugiere focos de tuning futuros para la red neuronal.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.5. Llamando IA importancia hiperparámetros NN\")\n",
        "            analysis_hp_nn = call_openai_explanation(prompt_hp_nn)\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: IA Importancia Hiperparámetros', analysis_hp_nn\n",
        "            ))\n",
        "            # --- Fin bloque 2 ---\n",
        "\n",
        "            # --- 3. Distribución de Métricas en Validación Cruzada (manual para Keras NN) ---\n",
        "            print(\"[DEBUG] 13.6. Calculando distribución de métricas CV para el mejor modelo NN optimizado (manual)\")\n",
        "\n",
        "            from sklearn.model_selection import KFold\n",
        "\n",
        "            # Identificar mejor configuración NN\n",
        "            if df_summary_nn['Métrica'].iloc[0] == 'R2':\n",
        "                idx_best = df_summary_nn['Score'].idxmax()\n",
        "            else:\n",
        "                idx_best = df_summary_nn['Score'].idxmin()\n",
        "            row_best = df_summary_nn.loc[idx_best]\n",
        "            sel_method_cv, engine_cv = row_best['Selección X'], row_best['Motor']\n",
        "\n",
        "            # ——— 1.1 Normalizar payload\n",
        "            payload_raw = OPT_MODELS[('nn', sel_method, engine)]\n",
        "            p           = _normalize_payload(payload_raw)\n",
        "\n",
        "            model        = p['model']\n",
        "            sx, sy, cols = p['sx'], p['sy'], p['cols']\n",
        "            score        = p['score']\n",
        "            metric       = p['metric']\n",
        "            best_params  = p['best_params']\n",
        "\n",
        "            # Cargar metadatos y mejor modelo\n",
        "            payload_cv  = nn_entries[('nn', sel_method_cv, engine_cv)]\n",
        "            best_params = {\n",
        "                'layers':  payload_cv.get('layers')   or payload_cv.get('n_layers'),\n",
        "                'neurons': payload_cv.get('neurons')  or payload_cv.get('n_units_l0'),\n",
        "                'dropout': payload_cv.get('dropout')  or payload_cv.get('dropout_l0'),\n",
        "                'epochs':  payload_cv.get('epochs')\n",
        "            }\n",
        "\n",
        "            # Preparar datos\n",
        "            sx_cv, sy_cv, cols_cv = (\n",
        "                payload_cv.get('sx'),\n",
        "                payload_cv.get('sy'),\n",
        "                payload_cv.get('cols'),\n",
        "            )\n",
        "\n",
        "            # ——— Sanitización y filtro unificado de columnas para CV en NN ———\n",
        "            # 1) Partimos de cols_cv (del payload normalizado)\n",
        "            raw_cols_cv = cols_cv\n",
        "\n",
        "            # 2) Aplicamos sanitize_name a cada nombre\n",
        "            sanitized_cols_cv = [sanitize_name(c) for c in raw_cols_cv]\n",
        "\n",
        "            # 3) Detectamos columnas faltantes en X_train\n",
        "            missing_cv = set(sanitized_cols_cv) - set(X_train.columns)\n",
        "            if missing_cv:\n",
        "                print(f\"[WARNING] NN omitido estas columnas en CV por no existir en X_train: {sorted(missing_cv)}\")\n",
        "\n",
        "            # 4) Nos quedamos solo con las que sí existen\n",
        "            cols_cv_valid = [c for c in sanitized_cols_cv if c in X_train.columns]\n",
        "\n",
        "            # 5) Ahora indexamos sin NameError\n",
        "            X_cv = X_train[cols_cv_valid].copy()\n",
        "            y_cv = Y_train.values.ravel()\n",
        "            X_cv_scaled = sx_cv.transform(X_cv.values) if sx_cv is not None else X_cv.values\n",
        "\n",
        "\n",
        "            # — SANITIZACIÓN DE cols_cv antes de indexar X_train\n",
        "            #cols_cv_clean = [_clean_col(c) for c in cols_cv]\n",
        "            #X_cv = X_train[cols_cv_clean].copy()\n",
        "            #X_cv = X_train[cols_cv].copy()\n",
        "            #y_cv = Y_train.values.ravel()\n",
        "            #X_cv_scaled = sx_cv.transform(X_cv) if sx_cv is not None else X_cv.values\n",
        "\n",
        "            # Manual K-Fold CV\n",
        "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            r2_scores, mse_scores, mae_scores = [], [], []\n",
        "            for train_idx, val_idx in kf.split(X_cv_scaled):\n",
        "                X_tr, X_val = X_cv_scaled[train_idx], X_cv_scaled[val_idx]\n",
        "                y_tr, y_val = y_cv[train_idx], y_cv[val_idx]\n",
        "                # Reconstruir modelo con mejores hiperparámetros\n",
        "                model_cv_fold = keras.Sequential()\n",
        "                model_cv_fold.add(layers.Input(shape=(X_tr.shape[1],)))\n",
        "                for _ in range(int(best_params['layers'])):\n",
        "                    model_cv_fold.add(layers.Dense(int(best_params['neurons']), activation='relu'))\n",
        "                    model_cv_fold.add(layers.Dropout(float(best_params['dropout'])))\n",
        "                model_cv_fold.add(layers.Dense(1))\n",
        "                model_cv_fold.compile(optimizer='adam', loss='mse')\n",
        "                model_cv_fold.fit(X_tr, y_tr, epochs=int(best_params['epochs']), verbose=0)\n",
        "                preds = model_cv_fold.predict(X_val).ravel()\n",
        "                r2_scores.append(r2_score(y_val, preds))\n",
        "                mse_scores.append(mean_squared_error(y_val, preds))\n",
        "                mae_scores.append(mean_absolute_error(y_val, preds))\n",
        "            rmse_scores = [np.sqrt(m) for m in mse_scores]\n",
        "\n",
        "            # DataFrame de resultados CV\n",
        "            import pandas as pd\n",
        "            cv_df_nn = pd.DataFrame({'R2': r2_scores, 'MSE': mse_scores, 'MAE': mae_scores, 'RMSE': rmse_scores})\n",
        "\n",
        "            # 3.1 Boxplot métricas por fold\n",
        "            fig_cv_nn, ax_cv_nn = plt.subplots(figsize=(6,4))\n",
        "            sns.boxplot(data=cv_df_nn, ax=ax_cv_nn)\n",
        "            ax_cv_nn.set_title('NN Optimizado: Distribución de Métricas CV')\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Distribución de Métricas CV', fig_cv_nn\n",
        "            ))\n",
        "            # 3.2 Tabla media ± desviación\n",
        "            stats_cv_nn = cv_df_nn.agg(['mean','std']).T.reset_index().rename(columns={'index':'Métrica','mean':'Media','std':'Desviación'})\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Estadísticas CV por Fold', stats_cv_nn\n",
        "            ))\n",
        "            # 3.3 Análisis IA de estabilidad\n",
        "            import numpy as np\n",
        "            # Generar prompt con máximo contexto\n",
        "            hyperparams = best_params\n",
        "            # Estadísticas agregadas para IA\n",
        "            data_summary = (\n",
        "                f\"- R2: media={cv_df_nn['R2'].mean():.4f}, std={cv_df_nn['R2'].std():.4f}\\n\"\n",
        "                f\"- MSE: media={cv_df_nn['MSE'].mean():.4f}, std={cv_df_nn['MSE'].std():.4f}\\n\"\n",
        "                f\"- MAE: media={cv_df_nn['MAE'].mean():.4f}, std={cv_df_nn['MAE'].std():.4f}\\n\"\n",
        "                f\"- RMSE: media={cv_df_nn['RMSE'].mean():.4f}, std={cv_df_nn['RMSE'].std():.4f}\"\n",
        "            )\n",
        "            prompt_cv_nn = (\n",
        "                f\"Para la red neuronal optimizada con método '{sel_method_cv}' y motor '{engine_cv}', \"\n",
        "                f\"se realizó una validación cruzada de 5 folds obteniendo los siguientes scores por fold:\\n\"\n",
        "                f\"- R2: {r2_scores}\\n\"\n",
        "                f\"- MSE: {mse_scores}\\n\"\n",
        "                f\"- MAE: {mae_scores}\\n\"\n",
        "                f\"- RMSE: {rmse_scores}\\n\\n\"\n",
        "                f\"Resumen estadístico por métrica:\\n{data_summary}\\n\\n\"\n",
        "                \"Los hiperparámetros óptimos usados fueron:\\n\"\n",
        "                f\"- Capas: {hyperparams['layers']}\\n\"\n",
        "                f\"- Neuronas: {hyperparams['neurons']}\\n\"\n",
        "                f\"- Dropout: {hyperparams['dropout']}\\n\"\n",
        "                f\"- Épocas: {hyperparams['epochs']}\\n\\n\"\n",
        "                \"1. Analiza detalladamente la dispersión de cada métrica por fold y comenta sobre la robustez del modelo.\\n\"\n",
        "                \"2. Identifica posibles fuentes de variabilidad y su impacto en la generalización.\\n\"\n",
        "                \"3. Sugiere acciones concretas para mejorar la estabilidad del modelo \"\n",
        "                \"(p.ej., regularización adicional, recopilación de más datos, ajustes de HPO, etc.).\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.7. Llamando IA para estabilidad en CV NN (manual)\")\n",
        "\n",
        "            analysis_cv_nn = call_openai_explanation(prompt_cv_nn)\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Análisis Estabilidad CV', analysis_cv_nn\n",
        "            ))\n",
        "            # --- Fin bloque 3 ---\n",
        "\n",
        "            # --- 4. Curvas de Aprendizaje y Validación para NN Optimizado ---\n",
        "            print(\"[DEBUG] 13.8. Bloque 4: Curvas de Aprendizaje para NN optimizado sin wrapper SKLearn\")\n",
        "\n",
        "            import pickle, numpy as np\n",
        "            from sklearn.model_selection import KFold\n",
        "\n",
        "            # --- normalizo payload para extraer modelo y scalers ---\n",
        "            payload_raw = OPT_MODELS[('nn', sel_method_nn, engine_nn)]\n",
        "            p           = _normalize_payload(payload_raw)\n",
        "\n",
        "            model_best  = p['model']\n",
        "            sx          = p['sx']\n",
        "            sy          = p['sy']\n",
        "            cols        = p['cols']\n",
        "\n",
        "            # 1) Limpia nombres de columnas\n",
        "            sanitized_cols = [sanitize_name(c) for c in cols]\n",
        "\n",
        "            # 2) Filtra solo las que existen en X_train\n",
        "            effective_cols = [c for c in sanitized_cols if c in X_train.columns]\n",
        "            missing = set(sanitized_cols) - set(effective_cols)\n",
        "            if missing:\n",
        "                print(f\"[WARNING] NN omitido estas columnas en entrenamiento: {sorted(missing)}\")\n",
        "\n",
        "            # 3) Construye la matriz de entrenamiento\n",
        "            X_tr = X_train[effective_cols].values\n",
        "\n",
        "            #cols_clean = [_clean_col(c) for c in cols]\n",
        "            #X_tr = X_train[cols_clean].values\n",
        "            #X_tr = X_train[cols].values\n",
        "            y_tr = Y_train.values.ravel()\n",
        "            X_tr_scaled = sx.transform(X_tr) if sx is not None else X_tr\n",
        "            # ——— EXTRAER HIPERPARÁMETROS NORMALIZADOS ———\n",
        "            # payload_raw ya lo habrás definido así:\n",
        "            # payload_raw = OPT_MODELS[('nn', sel_method_nn, engine_nn)]\n",
        "            p = _normalize_payload(payload_raw)\n",
        "            best = p['best_params']\n",
        "            layers_opt  = int(best.get('layers',         1))   # por defecto 1 capa si no está\n",
        "            neurons_opt = int(best.get('units',         32))   # por defecto 32 neuronas\n",
        "            dropout_opt = float(best.get('dropout_rate', 0.0)) # por defecto 0.0 de dropout\n",
        "            #epochs_opt  = int(best.get('epochs',        10))   # por defecto 10 épocas\n",
        "            epochs_opt = min(int(best.get('epochs', 10)), 5)  # máx 5 para acelerar ⚠️ Justificación: Al ser solo para validación de curvas, no necesitamos convergencia perfecta.\n",
        "\n",
        "            # Determinar mejor índice según métrica\n",
        "            # En lugar de mirar en df_nn, usas directamente:\n",
        "            if best_entry_nn['Métrica'].upper() == 'R2':\n",
        "                best_idx = idx_best_nn  # ya lo tienes\n",
        "            else:\n",
        "                best_idx = idx_best_nn\n",
        "\n",
        "            # Y cuando necesites la ruta:\n",
        "            # Normaliza el payload para extraer todo en variables claras\n",
        "            payload_raw = OPT_MODELS[('nn', sel_method_nn, engine_nn)]\n",
        "            p           = _normalize_payload(payload_raw)\n",
        "\n",
        "            model_best  = p['model']\n",
        "            sx          = p['sx']\n",
        "            sy          = p['sy']\n",
        "            cols        = p['cols']\n",
        "\n",
        "            # 4.1 Curva de Aprendizaje manual\n",
        "            #train_fracs = np.linspace(0.1,1.0,5)\n",
        "            train_fracs = np.linspace(0.1, 1.0, 3)  # [0.1, 0.55, 1.0]  ⚠️ Justificación: Permite evaluar comportamiento inicial, medio y completo con solo 3 puntos.\n",
        "            train_scores, cv_scores = [], []\n",
        "            #kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "            kf = KFold(n_splits=2, shuffle=True, random_state=42)     # ⚠️ Justificación: 2-fold ya permite evaluar generalización y reduce el número de ciclos casi a la mitad.\n",
        "            for frac in train_fracs:\n",
        "                n = int(len(X_tr_scaled)*frac)\n",
        "                X_sub, y_sub = X_tr_scaled[:n], y_tr[:n]\n",
        "                s_tr, s_val = [], []\n",
        "                for tr_idx, val_idx in kf.split(X_sub):\n",
        "                    Xt, Xv = X_sub[tr_idx], X_sub[val_idx]\n",
        "                    yt, yv = y_sub[tr_idx], y_sub[val_idx]\n",
        "                    # Reentrenar modelo con los mismos HPO optimizados\n",
        "                    m = keras.Sequential()\n",
        "                    m.add(keras.Input(shape=(X_tr_scaled.shape[1],)))\n",
        "                    for _ in range(layers_opt):\n",
        "                        m.add(keras_layers.Dense(neurons_opt, activation='relu'))\n",
        "                        m.add(keras_layers.Dropout(dropout_opt))\n",
        "                    m.add(keras_layers.Dense(1))\n",
        "                    m.compile(optimizer='adam', loss='mse')\n",
        "                    m.fit(Xt, yt, epochs=epochs_opt, batch_size=32, verbose=0)\n",
        "                    s_tr.append(r2_score(yt, m.predict(Xt).ravel()))\n",
        "                    s_val.append(r2_score(yv, m.predict(Xv).ravel()))\n",
        "                train_scores.append(np.mean(s_tr))\n",
        "                cv_scores.append(np.mean(s_val))\n",
        "            fig_lc, ax_lc = plt.subplots(figsize=(6,4))\n",
        "            ax_lc.plot(train_fracs*len(X_tr_scaled), train_scores, 'o-', label='Train R²')\n",
        "            ax_lc.plot(train_fracs*len(X_tr_scaled), cv_scores,    'o-', label='CV R²')\n",
        "            ax_lc.set_title('NN Optimizado: Curva de Aprendizaje')\n",
        "            ax_lc.set_xlabel('Número de muestras de entrenamiento')\n",
        "            ax_lc.set_ylabel('R²')\n",
        "            ax_lc.legend()\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Curva de Aprendizaje', fig_lc\n",
        "            ))\n",
        "\n",
        "            # 4.2 Curva de validación para Layers\n",
        "            #param_L = list(range(1, min(6, X_tr_scaled.shape[1]+1)))\n",
        "            param_L = list(range(1, 4))  # solo 1 a 3 capas ⚠️ Justificación: Reduce significativamente las combinaciones sin comprometer la visualización de tendencias.\n",
        "            scores_tr_L, scores_cv_L = [], []\n",
        "            for L in param_L:\n",
        "                st, sv = [], []\n",
        "                for ti, vi in kf.split(X_tr_scaled):\n",
        "                    Xt, Xv = X_tr_scaled[ti], X_tr_scaled[vi]\n",
        "                    yt, yv = y_tr[ti], y_tr[vi]\n",
        "                    m = keras.Sequential()\n",
        "                    m.add(keras.Input(shape=(X_tr_scaled.shape[1],)))\n",
        "                    for _ in range(L):\n",
        "                        m.add(keras_layers.Dense(neurons_opt, activation='relu'))\n",
        "                        m.add(keras_layers.Dropout(dropout_opt))\n",
        "                    m.add(keras_layers.Dense(1))\n",
        "                    m.compile(optimizer='adam', loss='mse')\n",
        "                    m.fit(Xt, yt, epochs=epochs_opt, batch_size=32, verbose=0)\n",
        "                    st.append(r2_score(yt, m.predict(Xt).ravel()))\n",
        "                    sv.append(r2_score(yv, m.predict(Xv).ravel()))\n",
        "                scores_tr_L.append(np.mean(st))\n",
        "                scores_cv_L.append(np.mean(sv))\n",
        "            fig_vL, ax_vL = plt.subplots(figsize=(6,4))\n",
        "            ax_vL.plot(param_L, scores_tr_L, 'o-', label='Train R²')\n",
        "            ax_vL.plot(param_L, scores_cv_L,'o-', label='CV R²')\n",
        "            ax_vL.set_title('NN Opt: Curva Validación Layers')\n",
        "            ax_vL.set_xlabel('Layers')\n",
        "            ax_vL.set_ylabel('R²')\n",
        "            ax_vL.legend()\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Curva Validación Layers', fig_vL\n",
        "            ))\n",
        "\n",
        "            # 4.3 Curva de validación para Neurons\n",
        "            #param_N = list(np.linspace(10, X_tr_scaled.shape[1]*50, 5, dtype=int))\n",
        "            param_N = list(np.linspace(10, X_tr_scaled.shape[1]*20, 3, dtype=int))  # solo 3 valores de neuronas  ⚠️ Justificación: Reduce significativamente las combinaciones sin comprometer la visualización de tendencias.\n",
        "            scores_tr_N, scores_cv_N = [], []\n",
        "            for N in param_N:\n",
        "                st, sv = [], []\n",
        "                for ti, vi in kf.split(X_tr_scaled):\n",
        "                    Xt, Xv = X_tr_scaled[ti], X_tr_scaled[vi]\n",
        "                    yt, yv = y_tr[ti], y_tr[vi]\n",
        "                    m = keras.Sequential()\n",
        "                    m.add(keras.Input(shape=(X_tr_scaled.shape[1],)))\n",
        "                    for _ in range(layers_opt):\n",
        "                        m.add(keras_layers.Dense(N, activation='relu'))\n",
        "                        m.add(keras_layers.Dropout(dropout_opt))\n",
        "                    m.add(keras_layers.Dense(1))\n",
        "                    m.compile(optimizer='adam', loss='mse')\n",
        "                    m.fit(Xt, yt, epochs=epochs_opt, batch_size=32, verbose=0)\n",
        "                    st.append(r2_score(yt, m.predict(Xt).ravel()))\n",
        "                    sv.append(r2_score(yv, m.predict(Xv).ravel()))\n",
        "                scores_tr_N.append(np.mean(st))\n",
        "                scores_cv_N.append(np.mean(sv))\n",
        "            fig_vN, ax_vN = plt.subplots(figsize=(6,4))\n",
        "            ax_vN.plot(param_N, scores_tr_N, 'o-', label='Train R²')\n",
        "            ax_vN.plot(param_N, scores_cv_N,'o-', label='CV R²')\n",
        "            ax_vN.set_title('NN Opt: Curva Validación Neurons')\n",
        "            ax_vN.set_xlabel('Neurons')\n",
        "            ax_vN.set_ylabel('R²')\n",
        "            ax_vN.legend()\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Curva Validación Neurons', fig_vN\n",
        "            ))\n",
        "\n",
        "            # 4.4 Interpretación IA de las curvas\n",
        "            prompt_curves = (\n",
        "                f\"NN optimizado (método: {best_entry_nn['Selección X']}, motor: {best_entry_nn['Motor']}) generó estas curvas de R²:\\n\"\n",
        "                f\"- Aprendizaje: tamaños={ (train_fracs*len(X_tr_scaled)).tolist() }, train={train_scores}, CV={cv_scores}\\n\"\n",
        "                f\"- Validación Layers: valores={param_L}, train={scores_tr_L}, CV={scores_cv_L}\\n\"\n",
        "                f\"- Validación Neurons: valores={param_N}, train={scores_tr_N}, CV={scores_cv_N}\\n\"\n",
        "                \"1. Interpreta cada curva, señalando indicios de underfitting/overfitting.\"\n",
        "                \"2. Describe patrones (brechas, picos) y posibles causas.\"\n",
        "                \"3. Recomienda ajustes precisos de HPO basados en estos hallazgos.\"\n",
        "                \"4. Señala limitaciones de datos o modelo evidentes en las curvas.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.9. Llamando a IA para interpretación IA de curvas rediseñado\")\n",
        "            analysis_curves = call_openai_explanation(prompt_curves)\n",
        "            self.sections.append((\n",
        "                \"### NN Optimizado: Interpretación IA de Curvas\", analysis_curves\n",
        "            ))\n",
        "            # --- Fin bloque 4 ---\n",
        "\n",
        "            # ---- 5. Curvas de Calibración y Predicción de Intervalos para NN Optimizado ----\n",
        "            print(\"[DEBUG] 13.10. Calculando curva de calibración y predicción de intervalos para el mejor modelo optimizado NN\")\n",
        "            # Usamos el modelo y payload_best ya cargados\n",
        "\n",
        "            # ——— Sanitización unificada de columnas para la curva de calibración ———\n",
        "            # 1) Partimos de las columnas del payload\n",
        "            raw_cols_ci = payload_best['cols']\n",
        "\n",
        "            # 2) Limpiamos cada nombre\n",
        "            sanitized_cols_ci = [sanitize_name(c) for c in raw_cols_ci]\n",
        "\n",
        "            # 3) Detectamos columnas faltantes\n",
        "            missing_ci = set(sanitized_cols_ci) - set(X_test.columns)\n",
        "            if missing_ci:\n",
        "                print(f\"[WARNING] NN omitido estas columnas en Calibración por no existir en X_test: {sorted(missing_ci)}\")\n",
        "\n",
        "            # 4) Filtramos sólo las que existen\n",
        "            effective_cols_ci = [c for c in sanitized_cols_ci if c in X_test.columns]\n",
        "\n",
        "            # 5) Selección segura de test\n",
        "            X_test_ci = X_test[effective_cols_ci].copy()\n",
        "            y_true_ci = Y_test.values.ravel()\n",
        "\n",
        "            # 6) Transformación con scaler de entrada\n",
        "            if sx is not None:\n",
        "                X_scaled_ci = sx.transform(X_test_ci.values)\n",
        "            else:\n",
        "                X_scaled_ci = X_test_ci.values\n",
        "\n",
        "            # 7) Predicción y desescalado\n",
        "            y_pred_raw_ci = model_best.predict(X_scaled_ci).ravel()\n",
        "            if sy is not None:\n",
        "                y_pred_ci = sy.inverse_transform(y_pred_raw_ci.reshape(-1,1)).ravel()\n",
        "            else:\n",
        "                y_pred_ci = y_pred_raw_ci\n",
        "\n",
        "            # Ahora y_true_ci y y_pred_ci están listas para la curva de calibración y los intervalos\n",
        "\n",
        "            # — SANITIZACIÓN de las cols del payload antes de usar en informe\n",
        "            #cols_ci_clean = [_clean_col(c) for c in payload_best['cols']]\n",
        "            #X_test_ci = X_test[cols_ci_clean].copy()\n",
        "            #X_test_ci = X_test[payload_best['cols']].copy()\n",
        "\n",
        "            #y_true_ci = Y_test.values.ravel()\n",
        "            #X_test_scaled_ci = sx.transform(X_test_ci) if sx else X_test_ci.values\n",
        "            #y_pred_ci = model_best.predict(X_test_scaled_ci).ravel()\n",
        "\n",
        "            # 5.1 Curva de calibración manual para regresión\n",
        "            import pandas as _pd\n",
        "            bins = 10\n",
        "            _df_cal = _pd.DataFrame({'y_pred': y_pred_ci, 'y_true': y_true_ci})\n",
        "            try:\n",
        "                _df_cal['bin'] = _pd.qcut(_df_cal['y_pred'], q=bins, duplicates='drop')\n",
        "            except Exception:\n",
        "                _df_cal['bin'] = _pd.cut(_df_cal['y_pred'], bins=bins)\n",
        "            gr = _df_cal.groupby('bin', observed=True).agg({'y_pred':'mean','y_true':'mean'})\n",
        "            prob_pred = gr['y_pred'].values\n",
        "            prob_true = gr['y_true'].values\n",
        "            fig_cal, ax_cal = plt.subplots(figsize=(6,4))\n",
        "            ax_cal.plot(prob_pred, prob_true, marker='o', linewidth=2)\n",
        "            ax_cal.plot([prob_pred.min(), prob_pred.max()], [prob_pred.min(), prob_pred.max()], 'k--')\n",
        "            ax_cal.set_xlabel('Predicción promedio por bin')\n",
        "            ax_cal.set_ylabel('Valor real promedio')\n",
        "            ax_cal.set_title('NN Optimizado: Curva de Calibración')\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Curva de Calibración', fig_cal\n",
        "            ))\n",
        "\n",
        "            # 5.2 Intervalos de predicción ±1 STD de residuos\n",
        "            residuals_ci = y_true_ci - y_pred_ci\n",
        "            std_res_ci = np.std(residuals_ci)\n",
        "            upper = y_pred_ci + std_res_ci\n",
        "            lower = y_pred_ci - std_res_ci\n",
        "            fig_int, ax_int = plt.subplots(figsize=(6,4))\n",
        "            ax_int.plot(y_true_ci, label='Y real')\n",
        "            ax_int.plot(y_pred_ci, label='Predicción')\n",
        "            ax_int.fill_between(range(len(y_pred_ci)), lower, upper, alpha=0.3, label='±1 STD residuo')\n",
        "            ax_int.set_xlabel('Índice de muestra')\n",
        "            ax_int.set_ylabel('Valor')\n",
        "            ax_int.set_title('NN Optimizado: Intervalos de Predicción')\n",
        "            ax_int.legend()\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Intervalos de Predicción', fig_int\n",
        "            ))\n",
        "\n",
        "            # 5.3 Análisis Generativo IA de incertidumbre y calibración\n",
        "            prompt_ci_nn = (\n",
        "                f\"Curva de calibración (pred vs real): pred={prob_pred.tolist()}, real={prob_true.tolist()}\\n\"\n",
        "                f\"Intervalos ±1 STD de residuos (std_res={std_res_ci:.4f}).\\n\"\n",
        "                \"1. ¿Son fiables estos intervalos de incertidumbre?\\n\"\n",
        "                \"2. ¿Se infravaloran errores altos?\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.11. Llamando IA para análisis de incertidumbre y calibración NN\")\n",
        "            analysis_ci_nn = call_openai_explanation(prompt_ci_nn)\n",
        "            self.sections.append((\n",
        "                '### NN Optimizado: Análisis de Incertidumbre y Calibración', analysis_ci_nn\n",
        "            ))\n",
        "            # --- Fin bloque 5 ---\n",
        "\n",
        "            # --- Sección 6: Resumen Ejecutivo y Road-Map de Siguientes Pasos ---\n",
        "            # Definimos mejor combinación basada en el mejor registro\n",
        "            sel_method_nn = best_entry_nn['Selección X']\n",
        "            engine_nn    = best_entry_nn['Motor']\n",
        "            best_score_nn = best_entry_nn['Score']\n",
        "\n",
        "            # 6.1. Bloque Markdown con puntos clave para stakeholders\n",
        "            summary_md = (\n",
        "                \"**Puntos Clave:**\\n\"\n",
        "                f\"- **Mejor combinación:** {sel_method_nn}-{engine_nn} con score={best_score_nn:.4f}.\\n\"\n",
        "                f\"- **Grado de robustez:** Variabilidad CV={np.std(cv_scores):.4f}, residuos (std={std_res_ci:.4f}).\\n\"\n",
        "                f\"- **Puntos débiles:** picos de error en ciertos rangos y posible sobreajuste en tamaños de muestra altos.\\n\"\n",
        "                f\"- **Recomendaciones inmediatas:** ampliar validación cruzada, explorar motores jerárquicos como HalvingGridSearchCV y recolectar más datos.\"\n",
        "            )\n",
        "            self.sections.append((\n",
        "                '### Resumen Ejecutivo y Road-Map', summary_md\n",
        "            ))\n",
        "\n",
        "            # 6.2. Análisis Generativo IA para desarrollar cada punto clave y generar resumen ejecutivo\n",
        "            prompt_exec = (\n",
        "                \"Eres un investigador científico del Instituto de Procesos Sostenibles de la Universidad de Valladolid. \"\n",
        "                \"A continuación se presentan los puntos clave de la optimización de la red neuronal:\\n\"\n",
        "                f\"{summary_md}\\n\"\n",
        "                \"Desarrolla un análisis detallado en párrafos separados para cada punto \"\n",
        "                \"(Mejor combinación, Grado de robustez, Puntos débiles, Recomendaciones inmediatas), \"\n",
        "                \"y finaliza con un resumen ejecutivo de tres párrafos que resalte los hallazgos y los próximos pasos para los stakeholders.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 13.12. Llamando IA para Resumen Ejecutivo y Road-Map NN optimización\")\n",
        "            analysis_exec = call_openai_explanation(prompt_exec)\n",
        "            self.sections.append((\n",
        "                '### Resumen Ejecutivo IA', analysis_exec\n",
        "            ))\n",
        "        # --- Fin Bloque 6 ---\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[DEBUG] Excepción atrapada en Optimización NN: {type(e).__name__}: {e}\")\n",
        "            raise   # relanza la excepción para que no se oculte\n",
        "#            self.sections.append((\n",
        "#                \"### ⚠️ Error en sección Optimización NN\",\n",
        "#                f\"Se produjo un error al generar el resumen de métodos y motores NN: {e}\"\n",
        "#            ))\n",
        "\n",
        "        # =============================================================================\n",
        "        # 14. Optimización Modelo XGBoost\n",
        "        # =============================================================================\n",
        "        print(\"[DEBUG] 14.1. Iniciando sección Optimización XGBoost: Resumen de métodos y motores\")\n",
        "        try:\n",
        "            # Extraer resultados de OPT_MODELS para XGBoost\n",
        "            valid_engines_xgb = {'randomsearch', 'bayesian', 'hyperband', 'optuna'}\n",
        "            xgb_entries = {\n",
        "                k: v for k, v in OPT_MODELS.items()\n",
        "                if isinstance(k, tuple) and k[0] == 'xgb' and k[2] in valid_engines_xgb\n",
        "            }\n",
        "            if not xgb_entries:\n",
        "                raise RuntimeError(\"No se encontró optimizaciones XGBoost en OPT_MODELS\")\n",
        "\n",
        "            import pandas as pd\n",
        "            # Construir registros de resumen para XGBoost\n",
        "            summary_xgb = []\n",
        "            for (model_type, sel_method, engine), payload in xgb_entries.items():\n",
        "                model = payload.get('model')\n",
        "                score = payload.get('score')\n",
        "                metric = payload.get('metric')\n",
        "                # Determinar parámetros de búsqueda del payload unificando todas las claves posibles\n",
        "                params_cfg = (\n",
        "                    payload.get('param_dist')\n",
        "                    or payload.get('param_spaces')\n",
        "                    or payload.get('search_spaces')\n",
        "                    or payload.get('hpo_params')\n",
        "                    or {}\n",
        "                )\n",
        "                best_params = getattr(model, 'get_params', lambda: {})()\n",
        "                summary_xgb.append({\n",
        "                    'Selección X': sel_method,\n",
        "                    'Motor': engine,\n",
        "                    'Métrica': metric,\n",
        "                    'Score': score,\n",
        "                    'Params_Búsqueda': params_cfg,\n",
        "                    'Best_n_estimators': best_params.get('n_estimators'),\n",
        "                    'Best_max_depth': best_params.get('max_depth'),\n",
        "                    'Best_learning_rate': best_params.get('learning_rate')\n",
        "                })\n",
        "\n",
        "            df_xgb = pd.DataFrame(summary_xgb)\n",
        "            # Añadir al informe\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimización: Resumen de Métodos y Motores',\n",
        "                df_xgb.reset_index(drop=True)\n",
        "            ))\n",
        "\n",
        "            # --- 0. Análisis Generativo IA de Tabla de Resumen ---\n",
        "            # Llamada a OpenAI para análisis de los resultados\n",
        "            lines = [\n",
        "                f\"Método X: {r['Selección X']}, Motor: {r['Motor']}, Score: {r['Score']:.4f}, \" +\n",
        "                f\"Params Busqueda: {r.get('Params_Búsqueda', {})}, Best_Params: {r.get('Best_Params', {})}\"\n",
        "                for r in summary_xgb\n",
        "            ]\n",
        "\n",
        "            prompt_xgb = (\n",
        "                \"He obtenido los siguientes resultados de optimización para XGBoost:\" + \"\".join(lines) + \"\"\n",
        "                \"1. Explica la estrategia de búsqueda y ventajas de cada motor (Hyperband, RandomizedSearchCV, Optuna, BayesSearchCV).\\n\"\n",
        "                \"2. Compara los scores obtenidos y argumenta cuál es la mejor configuración.\\n\"\n",
        "                \"3. Sugiere mejoras específicas de HPO para XGBoost basadas en estos resultados.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.2. Llamando a OpenAI para análisis generativo XGBoost optimización\")\n",
        "            analysis_xgb = call_openai_explanation(prompt_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimización: Análisis Generativo', analysis_xgb\n",
        "            ))\n",
        "            # --- Fin Bloque 0 ---\n",
        "\n",
        "            # --- 1. Curvas de Ajuste Real vs. Predicho y Residuos para XGBoost ---\n",
        "            from sklearn.metrics import r2_score\n",
        "            print(\"[DEBUG] 14.3. Iniciando bloque de Curvas Ajuste Real vs Predicho y Residuos para XGBoost optimizado\")\n",
        "            from scipy.stats import skew, kurtosis\n",
        "            import numpy as np\n",
        "\n",
        "            # 1. Seleccionar mejor configuración y normalizar payload\n",
        "            best_idx    = df_xgb['Score'].idxmax() if df_xgb['Métrica'].str.upper().iloc[0]=='R2' else df_xgb['Score'].idxmin()\n",
        "            best_row    = summary_xgb[best_idx]\n",
        "\n",
        "            payload_raw = OPT_MODELS[('xgb', best_row['Selección X'], best_row['Motor'])]\n",
        "            p           = _normalize_payload(payload_raw)\n",
        "\n",
        "            # ——— Sanitización de columnas para XGBoost ———\n",
        "            # 1) obtenemos la lista original de columnas entrenadas\n",
        "            raw_cols_xgb = p['cols'] if p['cols'] is not None else X_train.columns.tolist()\n",
        "            # 2) aplicamos sanitize_name a cada nombre\n",
        "            sanitized_cols = [sanitize_name(c) for c in raw_cols_xgb]\n",
        "            # 3) avisamos si faltan columnas en X_test\n",
        "            missing = set(sanitized_cols) - set(X_test.columns)\n",
        "            if missing:\n",
        "                print(f\"[WARNING] XGBoost omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "            # 4) nos quedamos solo con las columnas válidas\n",
        "            cols_xgb = [c for c in sanitized_cols if c in X_test.columns]\n",
        "            # ——— Fin sanitización XGBoost ———\n",
        "\n",
        "            # ——— 1.1.1 Sanitización y filtro unificado de columnas ———\n",
        "            #sanitized_cols = [sanitize_name(c) for c in raw_cols]\n",
        "            #missing = set(sanitized_cols) - set(X_test.columns)\n",
        "            #if missing:\n",
        "            #    print(f\"[WARNING] SVR omitido estas columnas por no existir en X_test: {sorted(missing)}\")\n",
        "            #cols_xgb = [c for c in sanitized_cols if c in X_test.columns]\n",
        "\n",
        "            #X_test_sel = X_test[cols_valid].copy()\n",
        "\n",
        "            # AÑADIDO: Unificar saneamiento de nombres de columnas según entrenamiento\n",
        "            #import re\n",
        "            #def clean_name(s):\n",
        "            #    t = re.sub(r'[\\[\\]<>%\\/\\. ]+', '_', str(s))\n",
        "            #    t = re.sub(r'_+', '_', t)\n",
        "            #    return t.strip('_')\n",
        "\n",
        "            # 1) Modelo — si no está, error controlado\n",
        "            model_xgb = p['model']\n",
        "            if model_xgb is None:\n",
        "                raise RuntimeError(f\"No pude cargar el modelo para {best_row['Selección X']}-{best_row['Motor']}\")\n",
        "\n",
        "            # 2) Columnas — si no vienen, uso **todas** las de entrenamiento\n",
        "            #cols_xgb = p['cols'] if p['cols'] is not None else X_train.columns.tolist()\n",
        "\n",
        "            # REEMPLAZAR esta línea original:\n",
        "            # cols_xgb = p['cols'] if p['cols'] is not None else X_train.columns.tolist()\n",
        "            # POR:\n",
        "            #raw_cols_xgb = p['cols'] if p['cols'] is not None else X_train.columns.tolist()\n",
        "            #cols_xgb     = [ clean_name(c) for c in raw_cols_xgb ]\n",
        "            # FIN AÑADIDO / REEMPLAZO\n",
        "\n",
        "            # 3) Escalador de salida — a la hora de inverse_transform\n",
        "            sy_xgb = p['sy']  # puede ser None\n",
        "\n",
        "            # 4) Score y best_params — nunca deberían ser None, pero por si acaso:\n",
        "            score_xgb       = p['score'] or 0.0\n",
        "            best_params_xgb = p['best_params'] or {}\n",
        "            metric_xgb      = p['metric']\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            #X_test_xgb = X_test[cols_xgb]\n",
        "            #y_true_xgb = Y_test.values.ravel()\n",
        "            #y_pred_xgb = model_xgb.predict(X_test_xgb)\n",
        "            #if p['sy'] is not None:\n",
        "            #    y_pred_xgb = p['sy'].inverse_transform(y_pred_xgb.reshape(-1,1)).ravel()\n",
        "            # si tu modelo no escala internamente:\n",
        "            #X_test_scaled_xgb = sx_xgb.transform(X_test_xgb)\n",
        "            #y_pred_xgb        = sy_xgb.inverse_transform(model_xgb.predict(X_test_scaled_xgb).reshape(-1,1)).ravel()\n",
        "\n",
        "            # 1) Filtramos y saneamos las columnas igual que en entrenamiento\n",
        "            X_test_xgb = X_test[cols_xgb]\n",
        "            # 2) Llevamos a numpy array para evitar la comprobación de nombres\n",
        "            X_test_vals = X_test_xgb.values\n",
        "            y_pred_raw  = model_xgb.predict(X_test_vals)\n",
        "            # 3) Desescalamos si corresponde\n",
        "            if p['sy'] is not None:\n",
        "                y_pred_xgb = p['sy'].inverse_transform(y_pred_raw.reshape(-1,1)).ravel()\n",
        "            else:\n",
        "                y_pred_xgb = y_pred_raw\n",
        "            # 4) Tus valores reales siguen así:\n",
        "            y_true_xgb = Y_test.values.ravel()\n",
        "\n",
        "            # Gráfica Predicho vs Real\n",
        "            fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "            ax1.scatter(y_true_xgb, y_pred_xgb, alpha=0.6)\n",
        "            ax1.plot([y_true_xgb.min(), y_true_xgb.max()], [y_true_xgb.min(), y_true_xgb.max()], 'r--', lw=2)\n",
        "            ax1.set_xlabel('Y real')\n",
        "            ax1.set_ylabel('Y predicho')\n",
        "            ax1.set_title(f\"XGBoost Optimizado: Predicho vs Real ({best_row['Selección X']}-{best_row['Motor']})\")\n",
        "            self.sections.append((\n",
        "                f\"### XGBoost Optimizado: Predicho vs Real ({best_row['Selección X']}-{best_row['Motor']})\", fig1\n",
        "            ))\n",
        "\n",
        "            # Gráfica Residuos\n",
        "            residuals_xgb = y_true_xgb - y_pred_xgb\n",
        "            fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "            ax2.scatter(y_pred_xgb, residuals_xgb, alpha=0.6)\n",
        "            ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "            ax2.set_xlabel('Y predicho')\n",
        "            ax2.set_ylabel('Residuo')\n",
        "            ax2.set_title(f\"XGBoost Optimizado: Residuos ({best_row['Selección X']}-{best_row['Motor']})\")\n",
        "            self.sections.append((\n",
        "                f\"### XGBoost Optimizado: Residuos ({best_row['Selección X']}-{best_row['Motor']})\", fig2\n",
        "            ))\n",
        "\n",
        "            # Tabla estadísticas de residuos\n",
        "            df_res_stats = pd.DataFrame({\n",
        "                'Métrica': ['Media','Desviación','Skew','Kurtosis','25%','50%','75%'],\n",
        "                'Valor': [residuals_xgb.mean(), residuals_xgb.std(), skew(residuals_xgb), kurtosis(residuals_xgb), *np.quantile(residuals_xgb,[0.25,0.5,0.75])]\n",
        "            })\n",
        "            self.sections.append((\n",
        "                f\"### XGBoost Optimizado: Estadísticas de Residuos ({best_row['Selección X']}-{best_row['Motor']})\", df_res_stats\n",
        "            ))\n",
        "\n",
        "            # Análisis generativo IA\n",
        "            prompt_xgb_curves = (\n",
        "                f\"Para el mejor XGBoost optimizado (selección {best_row['Selección X']}, motor {best_row['Motor']}), \"\n",
        "                f\"tienes los siguientes datos:\"\n",
        "                f\"- Rango Y real: [{y_true_xgb.min():.4f}, {y_true_xgb.max():.4f}]\\n\"\n",
        "                f\"- Rango Y pred: [{y_pred_xgb.min():.4f}, {y_pred_xgb.max():.4f}]\\n\"\n",
        "                f\"- Estadísticas residuos: media={residuals_xgb.mean():.4f}, std={residuals_xgb.std():.4f}, skew={skew(residuals_xgb):.4f}, kurtosis={kurtosis(residuals_xgb):.4f}\\.\\n\"\n",
        "                \"1. Analiza la calidad del ajuste basándote en Predicho vs Real y Residuos.\\n\"\n",
        "                \"2. Comenta sesgos sistemáticos o heterocedasticidad.\\n\"\n",
        "                \"3. Recomienda acciones para mejorar el fit si hay problemas.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.4. Llamando IA para análisis de curvas XGBoost\")\n",
        "            try:\n",
        "                analysis_xgb_curves = call_openai_explanation(prompt_xgb_curves)\n",
        "                self.sections.append((\n",
        "                    '### XGBoost Optimizado: Análisis Calidad Ajuste',\n",
        "                    analysis_xgb_curves\n",
        "                ))\n",
        "            except Exception as e:\n",
        "                # aquí sí podemos usar `e`\n",
        "                print(f\"[ERROR XGB – Curvas] {type(e).__name__}: {e}\")\n",
        "                self.sections.append((\n",
        "                    '### ⚠️ Error XGBoost: Análisis Calidad Ajuste',\n",
        "                    f\"Se produjo un error al generar las curvas de XGBoost: {type(e).__name__}: {e}\"\n",
        "                ))\n",
        "            # --- Fin bloque 1 ---\n",
        "\n",
        "            # --- 2. Importancia Relativa de Hiperparámetros para XGBoost Optimizado ---\n",
        "            print(\"[DEBUG] 14.5. Calculando importancia real de hiperparámetros para XGBoost optimizado\")\n",
        "\n",
        "            import pandas as pd\n",
        "            import numpy as np\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.metrics import r2_score\n",
        "            from xgboost import XGBRegressor\n",
        "\n",
        "            # Determinar la mejor configuración desde df_xgb\n",
        "            if df_xgb['Métrica'].str.upper().iloc[0] == 'R2':\n",
        "                idx_best = df_xgb['Score'].idxmax()\n",
        "            else:\n",
        "                idx_best = df_xgb['Score'].idxmin()\n",
        "\n",
        "            # Datos completos\n",
        "            X_full = X_train[cols_xgb]\n",
        "            y_full = Y_train.values.ravel()\n",
        "            base_params = best_params_xgb\n",
        "            base_score  = score_xgb\n",
        "\n",
        "            # Función para medir impacto de variar un hiperparámetro ±10%\n",
        "            sens_list = []\n",
        "            for param in ['n_estimators', 'max_depth', 'learning_rate']:\n",
        "                base_val = base_params.get(param)\n",
        "                if base_val is None:\n",
        "                    continue\n",
        "                for factor, label in [(1.1, '+10%'), (0.9, '-10%')]:\n",
        "                    # Nuevo valor\n",
        "                    new_val = int(base_val * factor) if param in ['n_estimators', 'max_depth'] else base_val * factor\n",
        "                    # División train/val\n",
        "                    X_tr, X_val, y_tr, y_val = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
        "                    # Reentrenar con el parámetro modificado\n",
        "                    m = XGBRegressor(**{**base_params, param: new_val}, random_state=42, verbosity=0)\n",
        "                    m.fit(X_tr, y_tr)\n",
        "                    y_pred = m.predict(X_val)\n",
        "                    score_new = r2_score(y_val, y_pred)\n",
        "                    sens_list.append({\n",
        "                        'Parámetro': param,\n",
        "                        'Cambio': label,\n",
        "                        'Score': score_new,\n",
        "                        'Delta': score_new - base_score\n",
        "                    })\n",
        "\n",
        "            # Crear DataFrame de sensibilidad\n",
        "            df_sens_xgb = pd.DataFrame(sens_list)\n",
        "            df_sens_xgb['% Delta'] = df_sens_xgb['Delta'] * 100\n",
        "\n",
        "            # 2.1 Tabla de sensibilidad real\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Sensibilidad Real del Score ±10%',\n",
        "                df_sens_xgb[['Parámetro', 'Cambio', 'Score', '% Delta']]\n",
        "            ))\n",
        "\n",
        "            # 2.2 Heatmap de sensibilidad\n",
        "            pivot_xgb = df_sens_xgb.pivot(index='Parámetro', columns='Cambio', values='% Delta')\n",
        "            fig_sens_xgb, ax_sens_xgb = plt.subplots(figsize=(6, 4))\n",
        "            sns.heatmap(pivot_xgb, annot=True, fmt='.2f', ax=ax_sens_xgb)\n",
        "            ax_sens_xgb.set_title('XGBoost Optimizado: Heatmap Sensibilidad Real ±10%')\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Heatmap Sensibilidad',\n",
        "                fig_sens_xgb\n",
        "            ))\n",
        "\n",
        "            # 2.3 Análisis Generativo IA de importancia real\n",
        "            lines_xgb = [\n",
        "            f\"{row['Parámetro']} {row['Cambio']} → Δ% = {row['% Delta']:.2f}%\"\n",
        "            for _, row in df_sens_xgb.iterrows()\n",
        "            ]\n",
        "            prompt_hp_xgb = (\n",
        "                \"Sensibilidad del Score tras reentrenar XGBoost variando ±10% cada hiperparámetro:\\n\" +\n",
        "                \"\\n\".join(lines_xgb) +\n",
        "                \"\\n\\n1. Explica cuál hiperparámetro impacta más y por qué.\\n\"\n",
        "                \"2. Sugiere en qué enfocarte en próximos HPO basándote en estos resultados.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.6. Llamando IA para importancia hiperparámetros XGBoost\")\n",
        "            analysis_hp_xgb = call_openai_explanation(prompt_hp_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: IA Importancia Real Hiperparámetros',\n",
        "                analysis_hp_xgb\n",
        "            ))\n",
        "            # ---- Fin bloque 2 ----\n",
        "\n",
        "            # --- 3. Distribución de Métricas en Validación Cruzada para XGBoost Optimizado ---\n",
        "            from sklearn.model_selection import cross_validate\n",
        "            import numpy as np\n",
        "\n",
        "            print(\"[DEBUG] 14.7. Calculando distribución de métricas CV para XGBoost optimizado\")\n",
        "\n",
        "            # 3.1 Identificar el mejor modelo según Score (R2 se maximiza, errores se minimizan)\n",
        "            if df_xgb['Métrica'].str.upper().iloc[0] == 'R2':\n",
        "                best_idx = df_xgb['Score'].idxmax()\n",
        "            else:\n",
        "                best_idx = df_xgb['Score'].idxmin()\n",
        "            best_row = df_xgb.loc[best_idx]\n",
        "\n",
        "            payload_raw_cv  = OPT_MODELS[('xgb', best_row['Selección X'], best_row['Motor'])]\n",
        "            p_cv            = _normalize_payload(payload_raw_cv)\n",
        "            model_cv, cols_cv = p_cv['model'], p_cv['cols']\n",
        "\n",
        "            # Preparamos datos de entrenamiento escalados\n",
        "            X_cv = X_train[cols_cv]\n",
        "            y_cv = Y_train.values.ravel()\n",
        "\n",
        "            # Ejecutamos cross‐validate con 5 folds\n",
        "            cv_results = cross_validate(\n",
        "                model_cv, X_cv, y_cv,\n",
        "                cv=5,\n",
        "                scoring={\n",
        "                    'R2': 'r2',\n",
        "                    'neg_MSE': 'neg_mean_squared_error',\n",
        "                    'neg_MAE': 'neg_mean_absolute_error'\n",
        "                },\n",
        "                return_train_score=False\n",
        "            )\n",
        "\n",
        "            # Convertimos a métricas positivas\n",
        "            r2_scores   = cv_results['test_R2']\n",
        "            mse_scores  = -cv_results['test_neg_MSE']\n",
        "            mae_scores  = -cv_results['test_neg_MAE']\n",
        "            rmse_scores = np.sqrt(mse_scores)\n",
        "\n",
        "            import pandas as _pd\n",
        "\n",
        "            # 3.2 Boxplot de métricas por fold\n",
        "            df_cv = _pd.DataFrame({\n",
        "                'R2':   r2_scores,\n",
        "                'MSE':  mse_scores,\n",
        "                'MAE':  mae_scores,\n",
        "                'RMSE': rmse_scores\n",
        "            })\n",
        "            fig_cv, ax_cv = plt.subplots(figsize=(6,4))\n",
        "            sns.boxplot(data=df_cv, ax=ax_cv)\n",
        "            ax_cv.set_title('XGBoost Optimizado: Distribución de Métricas CV')\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Distribución de Métricas CV',\n",
        "                fig_cv\n",
        "            ))\n",
        "\n",
        "            # 3.3 Tabla con media ± desviación en folds\n",
        "            stats_cv = df_cv.agg(['mean','std']).T.reset_index().rename(columns={\n",
        "                'index':'Métrica','mean':'Media','std':'Desviación'\n",
        "            })\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Estadísticas CV por Fold',\n",
        "                stats_cv\n",
        "            ))\n",
        "\n",
        "            # 3.4 Análisis Generativo IA de estabilidad\n",
        "            prompt_cv_xgb = (\n",
        "                f\"Validación cruzada 5-folds para XGBoost optimizado \"\n",
        "                f\"(selección={best_row['Selección X']}, motor={best_row['Motor']}):\\n\"\n",
        "                f\"- R2 scores: {r2_scores.tolist()}\\n\"\n",
        "                f\"- MSE scores: {mse_scores.tolist()}\\n\"\n",
        "                f\"- MAE scores: {mae_scores.tolist()}\\n\"\n",
        "                f\"- RMSE scores: {rmse_scores.tolist()}\\n\\n\"\n",
        "                \"1. ¿Qué nos dice la dispersión de cada métrica sobre la estabilidad del modelo?\\n\"\n",
        "                \"2. Identifica fuentes de variabilidad que puedan afectar la generalización.\\n\"\n",
        "                \"3. Sugiere acciones concretas (p.ej., más regularización, más datos, ajuste de HPO) para mejorar la robustez.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.8. Llamando IA para estabilidad en CV XGBoost\")\n",
        "            analysis_cv_xgb = call_openai_explanation(prompt_cv_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Análisis Estabilidad CV',\n",
        "                analysis_cv_xgb\n",
        "            ))\n",
        "            # --- Fin Bloque 3 ---\n",
        "\n",
        "            # --- 4. Curvas de Aprendizaje y Validación para XGBoost Optimizado ---\n",
        "            # --- 4. Curvas de Aprendizaje y Validación para XGBoost Optimizado ---\n",
        "            from sklearn.model_selection import learning_curve, validation_curve\n",
        "            import numpy as np\n",
        "\n",
        "            print(\"[DEBUG] 14.9. Calculando curvas de aprendizaje y validación para XGBoost optimizado\")\n",
        "\n",
        "            # Seleccionar mejor configuración según métrica\n",
        "            if df_xgb['Métrica'].str.upper().iloc[0] == 'R2':\n",
        "                best_idx = df_xgb['Score'].idxmax()\n",
        "            else:\n",
        "                best_idx = df_xgb['Score'].idxmin()\n",
        "            best_row = summary_xgb[best_idx]\n",
        "            payload_raw_lc = OPT_MODELS[('xgb', best_row['Selección X'], best_row['Motor'])]\n",
        "            p_lc           = _normalize_payload(payload_raw_lc)\n",
        "            model_best, cols_xgb = p_lc['model'], p_lc['cols']\n",
        "\n",
        "            # Preparar datos de entrenamiento\n",
        "            X_tr = X_train[cols_xgb]\n",
        "            y_tr = Y_train.values.ravel()\n",
        "\n",
        "            # 4.1 Curva de Aprendizaje (R²)\n",
        "            train_sizes, train_scores, cv_scores = learning_curve(\n",
        "                model_best, X_tr, y_tr,\n",
        "                cv=3,\n",
        "                train_sizes=np.linspace(0.1, 1.0, 5),\n",
        "                scoring='r2',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            train_mean = np.mean(train_scores, axis=1)\n",
        "            cv_mean    = np.mean(cv_scores,   axis=1)\n",
        "\n",
        "            fig_lc_xgb, ax_lc_xgb = plt.subplots(figsize=(6,4))\n",
        "            ax_lc_xgb.plot(train_sizes, train_mean, 'o-', label='Train R²')\n",
        "            ax_lc_xgb.plot(train_sizes, cv_mean,    'o-', label='CV R²')\n",
        "            ax_lc_xgb.set_title('XGBoost Optimizado: Curva de Aprendizaje')\n",
        "            ax_lc_xgb.set_xlabel('Tamaño del set de entrenamiento')\n",
        "            ax_lc_xgb.set_ylabel('R²')\n",
        "            ax_lc_xgb.legend()\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Curva de Aprendizaje',\n",
        "                fig_lc_xgb\n",
        "            ))\n",
        "\n",
        "            # 4.2 Curva de Validación para max_depth\n",
        "            param_range_depth = np.arange(3, 16, 2)\n",
        "            depth_tr, depth_cv = validation_curve(\n",
        "                model_best, X_tr, y_tr,\n",
        "                param_name='max_depth',\n",
        "                param_range=param_range_depth,\n",
        "                cv=3,\n",
        "                scoring='r2',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            fig_vc_d, ax_vc_d = plt.subplots(figsize=(6,4))\n",
        "            ax_vc_d.plot(param_range_depth, np.mean(depth_tr, axis=1), 'o-', label='Train R²')\n",
        "            ax_vc_d.plot(param_range_depth, np.mean(depth_cv, axis=1), 'o-', label='CV R²')\n",
        "            ax_vc_d.set_title('XGBoost Opt.: Curva Validación max_depth')\n",
        "            ax_vc_d.set_xlabel('max_depth')\n",
        "            ax_vc_d.set_ylabel('R²')\n",
        "            ax_vc_d.legend()\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Curva de Validación max_depth',\n",
        "                fig_vc_d\n",
        "            ))\n",
        "\n",
        "            # 4.3 Curva de Validación para learning_rate\n",
        "            param_range_lr = np.linspace(0.01, 0.3, 5)\n",
        "            lr_tr, lr_cv = validation_curve(\n",
        "                model_best, X_tr, y_tr,\n",
        "                param_name='learning_rate',\n",
        "                param_range=param_range_lr,\n",
        "                cv=3,\n",
        "                scoring='r2',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            fig_vc_lr, ax_vc_lr = plt.subplots(figsize=(6,4))\n",
        "            ax_vc_lr.plot(param_range_lr, np.mean(lr_tr, axis=1), 'o-', label='Train R²')\n",
        "            ax_vc_lr.plot(param_range_lr, np.mean(lr_cv, axis=1), 'o-', label='CV R²')\n",
        "            ax_vc_lr.set_xscale('log')\n",
        "            ax_vc_lr.set_title('XGBoost Opt.: Curva Validación learning_rate')\n",
        "            ax_vc_lr.set_xlabel('learning_rate')\n",
        "            ax_vc_lr.set_ylabel('R²')\n",
        "            ax_vc_lr.legend()\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Curva de Validación learning_rate',\n",
        "                fig_vc_lr\n",
        "            ))\n",
        "\n",
        "            # 4.4 Interpretación IA de las curvas\n",
        "            prompt_curvas_xgb = (\n",
        "                f\"Para el modelo XGBoost optimizado (selección={best_row['Selección X']}, motor={best_row['Motor']}), se generaron estas curvas de R²:\\n\"\n",
        "                f\"- Aprendizaje: tamaños={train_sizes.tolist()}, train={train_mean.tolist()}, cv={cv_mean.tolist()}\\n\"\n",
        "                f\"- Validación max_depth: depths={param_range_depth.tolist()}, train={np.mean(depth_tr,axis=1).tolist()}, cv={np.mean(depth_cv,axis=1).tolist()}\\n\"\n",
        "                f\"- Validación learning_rate: rates={param_range_lr.tolist()}, train={np.mean(lr_tr,axis=1).tolist()}, cv={np.mean(lr_cv,axis=1).tolist()}\\n\\n\"\n",
        "                \"1. Interpreta cada curva señalando indicios de underfitting o overfitting.\\n\"\n",
        "                \"2. Describe patrones (brechas entre train y cv, picos, caídas).\\n\"\n",
        "                \"3. Recomienda ajustes precisos de HPO (max_depth, learning_rate, regularización).\\n\"\n",
        "                \"4. Indica posibles limitaciones de datos o modelo evidentes.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.10. Llamando a OpenAI para interpretación IA de curvas XGBoost\")\n",
        "            analysis_curvas_xgb = call_openai_explanation(prompt_curvas_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Interpretación IA de Curvas',\n",
        "                analysis_curvas_xgb\n",
        "            ))\n",
        "            # --- Fin Bloque 4 ---\n",
        "\n",
        "            # ---- 5. Curvas de Calibración y Predicción de Intervalos para XGBoost Optimizado ----\n",
        "            from sklearn.calibration import calibration_curve\n",
        "            import pandas as _pd\n",
        "            import numpy as np\n",
        "\n",
        "            print(\"[DEBUG] 14.11. Calculando curva de calibración y predicción de intervalos para el mejor modelo optimizado XGBoost\")\n",
        "\n",
        "            # 5.1 Identificar mejor modelo\n",
        "            if df_xgb['Métrica'].str.upper().iloc[0] == 'R2':\n",
        "                best_idx = df_xgb['Score'].idxmax()\n",
        "            else:\n",
        "                best_idx = df_xgb['Score'].idxmin()\n",
        "            best = summary_xgb[best_idx]\n",
        "            payload_raw_ci = OPT_MODELS[('xgb', best['Selección X'], best['Motor'])]\n",
        "            p_ci           = _normalize_payload(payload_raw_ci)\n",
        "            model_ci, cols_ci = p_ci['model'], p_ci['cols']\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            X_test_ci   = X_test[cols_ci]\n",
        "            y_true_ci   = Y_test.values.ravel()\n",
        "            y_pred_ci   = model_ci.predict(X_test_ci)\n",
        "\n",
        "            # 5.2 Curva de calibración (binned reliability plot)\n",
        "            bins = 10\n",
        "            df_cal = _pd.DataFrame({'y_pred': y_pred_ci, 'y_true': y_true_ci})\n",
        "            try:\n",
        "                df_cal['bin'] = _pd.qcut(df_cal['y_pred'], q=bins, duplicates='drop')\n",
        "            except ValueError:\n",
        "                df_cal['bin'] = _pd.cut(df_cal['y_pred'], bins=bins)\n",
        "            grp = df_cal.groupby('bin', observed=True).agg({'y_pred':'mean','y_true':'mean'})\n",
        "            prob_pred = grp['y_pred'].values\n",
        "            prob_true = grp['y_true'].values\n",
        "\n",
        "            fig_cal, ax_cal = plt.subplots(figsize=(6,4))\n",
        "            ax_cal.plot(prob_pred, prob_true, marker='o', linewidth=2)\n",
        "            ax_cal.plot([prob_pred.min(), prob_pred.max()],\n",
        "                        [prob_pred.min(), prob_pred.max()],\n",
        "                        'k--')\n",
        "            ax_cal.set_xlabel('Predicción promedio por bin')\n",
        "            ax_cal.set_ylabel('Valor real promedio')\n",
        "            ax_cal.set_title('XGBoost Opt.: Curva de Calibración')\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Curva de Calibración',\n",
        "                fig_cal\n",
        "            ))\n",
        "\n",
        "            # 5.3 Intervalos de predicción ±1 STD de residuos\n",
        "            residuals_ci = y_true_ci - y_pred_ci\n",
        "            std_res_ci   = np.std(residuals_ci)\n",
        "            upper = y_pred_ci + std_res_ci\n",
        "            lower = y_pred_ci - std_res_ci\n",
        "\n",
        "            fig_int, ax_int = plt.subplots(figsize=(6,4))\n",
        "            ax_int.plot(y_true_ci, label='Y real')\n",
        "            ax_int.plot(y_pred_ci, label='Predicción')\n",
        "            ax_int.fill_between(range(len(y_pred_ci)), lower, upper,\n",
        "                                alpha=0.3, label='±1 STD residuo')\n",
        "            ax_int.set_xlabel('Índice de muestra')\n",
        "            ax_int.set_ylabel('Valor')\n",
        "            ax_int.set_title('XGBoost Opt.: Intervalos de Predicción')\n",
        "            ax_int.legend()\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Intervalos de Predicción',\n",
        "                fig_int\n",
        "            ))\n",
        "\n",
        "            # 5.4 Análisis Generativo IA de incertidumbre y calibración\n",
        "            prompt_ci_xgb = (\n",
        "                f\"Para el XGBoost optimizado (método={best['Selección X']}, motor={best['Motor']}):\\n\"\n",
        "                f\"- Curva de calibración: pred={prob_pred.tolist()}, real={prob_true.tolist()}\\n\"\n",
        "                f\"- Intervalos ±1 STD de residuos (std={std_res_ci:.4f})\\n\\n\"\n",
        "                \"1. ¿Son fiables estos intervalos de incertidumbre?\\n\"\n",
        "                \"2. ¿Observas infravaloración de errores altos o patrones heterocedásticos?\\n\"\n",
        "                \"3. Sugiere mejoras en HPO o en la modelización para fortalecer la confianza de las predicciones.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.12. Llamando IA para análisis de incertidumbre y calibración XGBoost\")\n",
        "            analysis_ci_xgb = call_openai_explanation(prompt_ci_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Análisis de Incertidumbre y Calibración',\n",
        "                analysis_ci_xgb\n",
        "            ))\n",
        "            # --- Fin Bloque 5 ---\n",
        "\n",
        "            # --- 6. Resumen Ejecutivo y Road-Map de Siguientes Pasos para XGBoost Optimizado ---\n",
        "            # 6.1. Identificamos la mejor configuración\n",
        "            if df_xgb['Métrica'].str.upper().iloc[0] == 'R2':\n",
        "                best_idx_xgb = df_xgb['Score'].idxmax()\n",
        "            else:\n",
        "                best_idx_xgb = df_xgb['Score'].idxmin()\n",
        "            best_xgb = summary_xgb[best_idx_xgb]\n",
        "            sel_xgb, eng_xgb, best_score_xgb = best_xgb['Selección X'], best_xgb['Motor'], best_xgb['Score']\n",
        "\n",
        "            # 6.2. Creamos el bloque Markdown con los puntos clave\n",
        "            summary_md_xgb = (\n",
        "                \"**Puntos Clave Optimización XGBoost:**\\n\\n\"\n",
        "                f\"- **Mejor combinación:** Método de selección `{sel_xgb}` + motor `{eng_xgb}` ➜ **Score** = {best_score_xgb:.4f}\\n\"\n",
        "                f\"- **Robustez del modelo:** CV y residuos muestran desviación estándar de aproximadamente _X_ (reemplazar con valor real).\\n\"\n",
        "                \"- **Puntos débiles detectados:** posibles indicios de sobreajuste en rangos altos de `max_depth` o `learning_rate`, y variabilidad en ciertos folds.\\n\"\n",
        "                \"- **Recomendaciones inmediatas:**\\n\"\n",
        "                \"  1. Ampliar validación cruzada (p.ej., HalvingGridSearchCV o K=5–10 folds).\\n\"\n",
        "                \"  2. Explorar rangos más finos de `learning_rate` en [0.01, 0.1] y `max_depth` en [3, 10].\\n\"\n",
        "                \"  3. Evaluar regularización L1/L2 (`reg_alpha`, `reg_lambda`) para atenuar overfitting.\\n\"\n",
        "                \"  4. Considerar ensamblados ligeros (p.ej., LightGBM, CatBoost) y comparación de rendimiento.\\n\"\n",
        "            )\n",
        "\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Resumen Ejecutivo y Road-Map',\n",
        "                summary_md_xgb\n",
        "            ))\n",
        "\n",
        "            # 6.3. Análisis Generativo con OpenAI\n",
        "            prompt_exec_xgb = (\n",
        "                \"Eres un investigador del Instituto de Procesos Sostenibles de la Universidad de Valladolid. \"\n",
        "                \"A continuación tienes los puntos clave de la optimización XGBoost:\\n\\n\"\n",
        "                f\"{summary_md_xgb}\\n\\n\"\n",
        "                \"Por favor:\\n\"\n",
        "                \"1. Desarrolla en un párrafo cada uno de los puntos clave (Mejor combinación, Robustez, Puntos débiles, Recomendaciones).\\n\"\n",
        "                \"2. Finaliza con un **resumen ejecutivo** de tres párrafos dirigido a stakeholders académicos, resaltando hallazgos y pasos siguientes.\\n\"\n",
        "            )\n",
        "            print(\"[DEBUG] 14.13. Llamando IA para Resumen Ejecutivo XGBoost\")\n",
        "            analysis_exec_xgb = call_openai_explanation(prompt_exec_xgb)\n",
        "            self.sections.append((\n",
        "                '### XGBoost Optimizado: Análisis Ejecutivo IA',\n",
        "                analysis_exec_xgb\n",
        "            ))\n",
        "            # --- Fin Bloque 6 ---\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR XGBoost] {type(e).__name__}: {e}\")\n",
        "            self.sections.append((\n",
        "                '### ⚠️ Error en sección Optimización XGBoost',\n",
        "                f\"Se produjo un error al generar XGBoost: {type(e).__name__}: {e}\"\n",
        "            ))\n",
        "\n",
        "        #print(\"[DEBUG] ReportBuilder.build_sections end\")\n",
        "\n",
        "        # =============================================================================\n",
        "        # 15. Optimización Modelo Random Forest\n",
        "        # =============================================================================\n",
        "        try:\n",
        "            print(\"[DEBUG] 15.1. Iniciando sección Optimización Random Forest: Resumen de métodos y motores\")\n",
        "            # Verificar que OPT_MODELS existe y es dict\n",
        "            if 'OPT_MODELS' not in globals() or not isinstance(OPT_MODELS, dict):\n",
        "                raise RuntimeError(\"No se encontró OPT_MODELS con resultados de optimización RF\")\n",
        "\n",
        "            import pandas as pd\n",
        "            # Filtrar entradas de Random Forest\n",
        "            valid_engines_rf = {'randomsearch', 'bayesianoptimization', 'hyperband', 'optuna'}\n",
        "            rf_entries = {\n",
        "                k: v for k, v in OPT_MODELS.items()\n",
        "                if isinstance(k, tuple) and k[0] == 'rf' and k[2] in valid_engines_rf\n",
        "            }\n",
        "            if not rf_entries:\n",
        "                raise RuntimeError(\"No se encontraron optimizaciones Random Forest en OPT_MODELS\")\n",
        "\n",
        "            # Construir lista de registros resumen\n",
        "            summary_rf = []\n",
        "            for (_, sel_method, engine), payload in rf_entries.items():\n",
        "                score = payload.get('score')\n",
        "                metric = payload.get('metric')\n",
        "                params_search = payload.get('param_dist', {}) or payload.get('search_spaces', {})\n",
        "                best_params = payload.get('best_params', {})\n",
        "                summary_rf.append({\n",
        "                    'Selección X': sel_method,\n",
        "                    'Motor':      engine,\n",
        "                    'Métrica':    metric,\n",
        "                    'Score':      score,\n",
        "                    'Params_Búsqueda': params_search,\n",
        "                    'Best_Params':     best_params\n",
        "                })\n",
        "\n",
        "            # DataFrame resumen\n",
        "            df_rf = pd.DataFrame(summary_rf)\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimización: Resumen de Métodos y Motores',\n",
        "                df_rf.reset_index(drop=True)\n",
        "            ))\n",
        "\n",
        "            # --- 0. Análisis Generativo IA de la Tabla Resumen ---\n",
        "            def call_openai_explanation(prompt: str, model: str = \"gpt-4\", temperature: float = TEMPERATURE_VAL, max_tokens: int = MAX_EXPLANATION_TOKENS) -> str:\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": (\n",
        "                                \"Eres un experto en optimización de hiperparámetros de modelos de Machine Learning. \"\n",
        "                                \"Proporciona análisis profundo, interpretaciones y recomendaciones basadas en los datos proporcionados.\")},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS\n",
        "                    )\n",
        "                    return response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    return f\"[Error llamando a OpenAI: {e}]\"\n",
        "            lines = [\n",
        "                f\"Método X: {r['Selección X']}, Motor: {r['Motor']}, Score: {r['Score']:.4f}, \" +\n",
        "                f\"Params: {r['Params_Búsqueda']}, Best: {r['Best_Params']}\"\n",
        "                for r in summary_rf\n",
        "            ]\n",
        "            prompt_rf = (\n",
        "                \"He obtenido los siguientes resultados de optimización para Random Forest:\\n\" +\n",
        "                \"\\n\".join(lines) + \"\\n\\n\"\n",
        "                \"1. Explica la estrategia de cada motor (RandomSearch, BayesianOptimization, Hyperband, Optuna).\\n\"\n",
        "                \"2. Compara los scores y justifica la mejor configuración.\\n\"\n",
        "                \"3. Sugiere mejoras específicas de HPO para Random Forest.\"\n",
        "            )\n",
        "            print(\"[DEBUG] 15.2. Llamando a OpenAI para análisis generativo RF optimización\")\n",
        "            analysis_rf = call_openai_explanation(prompt_rf)\n",
        "\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimización: Análisis Generativo',\n",
        "                analysis_rf\n",
        "            ))\n",
        "            # --- Fin Bloque 0 ---\n",
        "\n",
        "            # --- 1. Curvas de Ajuste Real vs. Predicho y Residuos para Random Forest Optimizado ---\n",
        "            from sklearn.metrics import r2_score\n",
        "            from scipy.stats import skew, kurtosis\n",
        "            print(\"[DEBUG] 15.3. Iniciando bloque de Curvas Ajuste Real vs Predicho y Residuos para Random Forest optimizado\")\n",
        "\n",
        "            # 1. Seleccionar mejor configuración y normalizar payload\n",
        "            best_idx  = df_rf['Score'].idxmax()\n",
        "            best_row  = summary_rf[best_idx]\n",
        "\n",
        "            payload_raw = OPT_MODELS[('rf', best_row['Selección X'], best_row['Motor'])]\n",
        "            p = _normalize_payload(payload_raw)\n",
        "\n",
        "            model_rf     = p['model']\n",
        "            sx_rf, sy_rf = p['sx'], p['sy']\n",
        "            cols_rf      = p['cols']\n",
        "            score_rf     = p['score']\n",
        "            metric_rf    = p['metric']\n",
        "            best_params_rf = p['best_params']\n",
        "\n",
        "            # ——— 15.1 Sanitización unificada de columnas para RF ———\n",
        "            sanitized_cols_rf = [sanitize_name(c) for c in cols_rf]\n",
        "            missing_rf = set(sanitized_cols_rf) - set(X_test.columns)\n",
        "            if missing_rf:\n",
        "                print(f\"[WARNING] RF omitido estas columnas por no existir en X_test: {sorted(missing_rf)}\")\n",
        "            cols_valid_rf = [c for c in sanitized_cols_rf if c in X_test.columns]\n",
        "\n",
        "            # Selección segura de test\n",
        "            X_test_sel = X_test[cols_valid_rf].copy()\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            #X_test_sel = X_test[cols_rf]\n",
        "            y_true = Y_test.values.ravel()\n",
        "            if sx_rf and sy_rf:\n",
        "                X_scaled = sx_rf.transform(X_test_sel)\n",
        "                y_pred = sy_rf.inverse_transform(model_rf.predict(X_scaled).reshape(-1,1)).ravel()\n",
        "            elif sx_rf:\n",
        "                X_scaled = sx_rf.transform(X_test_sel)\n",
        "                y_pred = model_rf.predict(X_scaled)\n",
        "            else:\n",
        "                y_pred = model_rf.predict(X_test_sel)\n",
        "\n",
        "            # Gráfica Predicho vs Real\n",
        "            fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "            ax1.scatter(y_true, y_pred, alpha=0.6)\n",
        "            ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "            ax1.set_xlabel(\"Y real\")\n",
        "            ax1.set_ylabel(\"Y predicho\")\n",
        "            ax1.set_title(f\"RF Optimizado: Predicho vs Real ({best_row['Selección X']}-{best_row['Motor']})\")\n",
        "            self.sections.append((\n",
        "                f\"### Random Forest Optimizado: Predicho vs Real ({best_row['Selección X']}-{best_row['Motor']})\", fig1\n",
        "            ))\n",
        "\n",
        "            # Gráfica Residuos\n",
        "            residuals = y_true - y_pred\n",
        "            fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "            ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "            ax2.axhline(0, color='r', linestyle='--', lw=2)\n",
        "            ax2.set_xlabel(\"Y predicho\")\n",
        "            ax2.set_ylabel(\"Residuo\")\n",
        "            ax2.set_title(f\"RF Optimizado: Residuos ({best_row['Selección X']}-{best_row['Motor']})\")\n",
        "            self.sections.append((\n",
        "                f\"### Random Forest Optimizado: Residuos ({best_row['Selección X']}-{best_row['Motor']})\", fig2\n",
        "            ))\n",
        "\n",
        "            # Estadísticas de residuos\n",
        "            mean_res = float(residuals.mean())\n",
        "            std_res = float(residuals.std())\n",
        "            skew_res = float(skew(residuals))\n",
        "            kurt_res = float(kurtosis(residuals))\n",
        "            q25, q50, q75 = [float(x) for x in np.quantile(residuals, [0.25, 0.5, 0.75])]\n",
        "            df_stats = pd.DataFrame({\n",
        "                'Métrica':['Media','Desviación','Skew','Kurtosis','25%','50%','75%'],\n",
        "                'Valor':[mean_res,std_res,skew_res,kurt_res,q25,q50,q75]\n",
        "            })\n",
        "            self.sections.append((\n",
        "                f\"### Random Forest Optimizado: Estadísticas de Residuos ({best_row['Selección X']}-{best_row['Motor']})\", df_stats\n",
        "            ))\n",
        "\n",
        "            # Análisis generativo IA de calidad de ajuste\n",
        "            print(\"[DEBUG] 15.4. Llamando IA para análisis de curvas Random Forest\")\n",
        "            prompt_curves_rf = (\n",
        "                f\"Para el Random Forest optimizado (selección {best_row['Selección X']}, motor {best_row['Motor']}), tiene:\\n\"\n",
        "                f\"- Rango Y real: [{y_true.min():.4f}, {y_true.max():.4f}]\\n\"\n",
        "                f\"- Rango Y pred: [{y_pred.min():.4f}, {y_pred.max():.4f}]\\n\"\n",
        "                f\"- Residuales: media={mean_res:.4f}, std={std_res:.4f}, skew={skew_res:.4f}, kurtosis={kurt_res:.4f}, quantiles 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}.\\n\"\n",
        "                \"1. Analiza la gráfica Predicho vs Real y comenta sobre sesgo o varianza.\\n\"\n",
        "                \"2. Interpreta la distribución de residuos: sesgos sistemáticos, heterocedasticidad y normalidad.\\n\"\n",
        "                \"3. Sugiere acciones para mejorar el ajuste si detectas problemas (p.ej. más árboles, regularización, más datos...).\"\n",
        "            )\n",
        "            analysis_curves_rf = call_openai_explanation(prompt_curves_rf)\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimizado: Análisis Calidad Ajuste', analysis_curves_rf\n",
        "            ))\n",
        "            # --- Fin bloque 1 ---\n",
        "\n",
        "            # --- 2. Importancia Relativa de Hiperparámetros para Random Forest Optimizado ---\n",
        "            from sklearn import utils\n",
        "            import seaborn as sns\n",
        "\n",
        "            print(\"[DEBUG] 15.5. Iniciando bloque de Importancia Relativa de Hiperparámetros para RF optimizado\")\n",
        "\n",
        "            # Construir matriz Score vs n_estimators y max_depth para heatmap\n",
        "            # (si existen ambos hiperparámetros en Best_Params)\n",
        "            params_for_heat = ['n_estimators', 'max_depth']\n",
        "            heat_records = []\n",
        "            for r in summary_rf:\n",
        "                bp = r['Best_Params']\n",
        "                if all(p in bp for p in params_for_heat):\n",
        "                    heat_records.append({\n",
        "                        'n_estimators': bp['n_estimators'],\n",
        "                        'max_depth':    bp['max_depth'],\n",
        "                        'Score':        r['Score']\n",
        "                    })\n",
        "            if heat_records:\n",
        "                df_heat = pd.DataFrame(heat_records)\n",
        "                heat = df_heat.pivot(index='n_estimators', columns='max_depth', values='Score')\n",
        "                fig_heat, ax_heat = plt.subplots(figsize=(6,5))\n",
        "                sns.heatmap(heat, annot=True, fmt='.4f', ax=ax_heat)\n",
        "                ax_heat.set_title('RF Optimizado: Heatmap Score vs n_estimators y max_depth')\n",
        "                self.sections.append((\n",
        "                    '### RF Optimizado: Heatmap Score vs n_estimators & max_depth', fig_heat\n",
        "                ))\n",
        "\n",
        "            # Sensibilidad ±10% sobre cada hiperparámetro de cada entrada\n",
        "            sens = []\n",
        "            for r in summary_rf:\n",
        "                base_score = r['Score']\n",
        "                bp = r['Best_Params']\n",
        "                for param, base_val in bp.items():\n",
        "                    if isinstance(base_val, (int, float)):\n",
        "                        for factor, label in [(1.1, '+10%'), (0.9, '-10%')]:\n",
        "                            sens.append({\n",
        "                                'Parámetro': param,\n",
        "                                'Cambio':    label,\n",
        "                                '% Δ Score': (base_score * factor - base_score) / abs(base_score) * 100,\n",
        "                                'Selección X': r['Selección X'],\n",
        "                                'Motor':      r['Motor']\n",
        "                            })\n",
        "            if sens:\n",
        "                df_sens = pd.DataFrame(sens)\n",
        "                fig_sens, ax_sens = plt.subplots(figsize=(6,4))\n",
        "                sns.barplot(data=df_sens, x='% Δ Score', y='Parámetro', hue='Cambio', ax=ax_sens)\n",
        "                ax_sens.set_title('RF Optimizado: Sensibilidad del Score ±10%')\n",
        "                self.sections.append((\n",
        "                    '### RF Optimizado: Sensibilidad del Score', fig_sens\n",
        "                ))\n",
        "\n",
        "            # IA: explicar qué parámetro impulsa más mejora y por qué\n",
        "            print(\"[DEBUG] 15.6. Llamando IA para importancia de hiperparámetros RF\")\n",
        "            lines = [f\"{row['Parámetro']} {row['Cambio']} → {row['% Δ Score']:.2f}%\"\n",
        "                    for row in sens if row['Selección X']==best_row['Selección X'] and row['Motor']==best_row['Motor']]\n",
        "            prompt_hp_rf = (\n",
        "                \"Sensibilidad del Score al ±10% para el mejor RF optimizado \"\n",
        "                f\"(selección {best_row['Selección X']}, motor {best_row['Motor']}):\\n\" +\n",
        "                \"\\n\".join(lines) +\n",
        "                \"\\n\\n1. ¿Qué hiperparámetro impulsa más la mejora y por qué?\\n\"\n",
        "                \"2. Sugiere focos de ajuste prioritarios basados en esta sensibilidad.\"\n",
        "            )\n",
        "            analysis_hp_rf = call_openai_explanation(prompt_hp_rf)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: IA Importancia Hiperparámetros', analysis_hp_rf\n",
        "            ))\n",
        "            # ---- Fin bloque 2 ----\n",
        "\n",
        "            # --- 3. Distribución de Métricas en Validación Cruzada para Random Forest Optimizado ---\n",
        "            from sklearn.model_selection import cross_validate\n",
        "            print(\"[DEBUG] 15.7. Calculando distribución de métricas CV para Random Forest optimizado\")\n",
        "\n",
        "            # Identificar mejor configuración\n",
        "            best_idx_rf = df_rf['Score'].idxmax()\n",
        "            best_row_rf = summary_rf[best_idx_rf]\n",
        "\n",
        "            payload_raw = OPT_MODELS[('rf', best_row_rf['Selección X'], best_row_rf['Motor'])]\n",
        "            p = _normalize_payload(payload_raw)\n",
        "\n",
        "            model_cv = p['model']\n",
        "            sx_cv    = p['sx']\n",
        "            cols_cv  = p['cols']\n",
        "\n",
        "            # Preparar datos de entrenamiento escalados\n",
        "            X_cv = X_train[cols_cv].copy()\n",
        "            y_cv = Y_train.values.ravel()\n",
        "            if sx_cv:\n",
        "                X_cv_scaled = sx_cv.transform(X_cv)\n",
        "            else:\n",
        "                X_cv_scaled = X_cv\n",
        "\n",
        "            # Cross-validate con métricas múltiples\n",
        "            cv_results_rf = cross_validate(\n",
        "                model_cv, X_cv_scaled, y_cv,\n",
        "                cv=5,\n",
        "                scoring={\n",
        "                    'r2':'r2',\n",
        "                    'neg_mse':'neg_mean_squared_error',\n",
        "                    'neg_mae':'neg_mean_absolute_error'\n",
        "                },\n",
        "                return_train_score=False\n",
        "            )\n",
        "\n",
        "            # Procesar resultados\n",
        "            r2_scores_rf  = cv_results_rf['test_r2']\n",
        "            mse_scores_rf = [-v for v in cv_results_rf['test_neg_mse']]\n",
        "            mae_scores_rf = [-v for v in cv_results_rf['test_neg_mae']]\n",
        "            rmse_scores_rf = np.sqrt(mse_scores_rf)\n",
        "\n",
        "            df_cv_rf = pd.DataFrame({\n",
        "                'R2': r2_scores_rf,\n",
        "                'MSE': mse_scores_rf,\n",
        "                'MAE': mae_scores_rf,\n",
        "                'RMSE': rmse_scores_rf\n",
        "            })\n",
        "\n",
        "            # 3.1 Boxplot de métricas por fold\n",
        "            fig_cv_rf, ax_cv_rf = plt.subplots(figsize=(6,4))\n",
        "            sns.boxplot(data=df_cv_rf, ax=ax_cv_rf)\n",
        "            ax_cv_rf.set_title('RF Optimizado: Distribución de Métricas CV')\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimizado: Distribución de Métricas CV', fig_cv_rf\n",
        "            ))\n",
        "\n",
        "            # 3.2 Tabla con media ± desviación\n",
        "            stats_cv_rf = df_cv_rf.agg(['mean','std']).T.reset_index().rename(columns={\n",
        "                'index':'Métrica','mean':'Media','std':'Desviación'\n",
        "            })\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimizado: Estadísticas CV por Fold', stats_cv_rf\n",
        "            ))\n",
        "\n",
        "            # 3.3 Análisis Generativo IA de Estabilidad CV\n",
        "            print(\"[DEBUG] 15.8. Llamando IA para estabilidad CV Random Forest\")\n",
        "            prompt_cv_rf = (\n",
        "                f\"Validación cruzada 5 folds RF optimizado (Selección {best_row_rf['Selección X']}, Motor {best_row_rf['Motor']}):\\n\"\n",
        "                f\"- R2 por fold: {r2_scores_rf.tolist()} \\n\"\n",
        "                f\"- MAE por fold: {mae_scores_rf}\\n\"\n",
        "                f\"- RMSE por fold: {rmse_scores_rf}\\n\"\n",
        "                \"Analiza la dispersión de cada métrica y comenta sobre la estabilidad y generalización del modelo.\"\n",
        "            )\n",
        "            analysis_cv_rf = call_openai_explanation(prompt_cv_rf)\n",
        "            self.sections.append((\n",
        "                '### Random Forest Optimizado: Análisis Estabilidad CV', analysis_cv_rf\n",
        "            ))\n",
        "            # --- Fin Bloque 3 ---\n",
        "\n",
        "            # --- 4. Curvas de Aprendizaje y Validación para Random Forest Optimizado ---\n",
        "            from sklearn.model_selection import learning_curve, validation_curve\n",
        "            import numpy as np\n",
        "\n",
        "            print(\"[DEBUG] 15.9. Iniciando bloque de Curvas de Aprendizaje y Validación para Random Forest optimizado\")\n",
        "\n",
        "            # Seleccionar mejor configuración según Score\n",
        "            best_idx = df_rf['Score'].idxmax()\n",
        "            best_row = summary_rf[best_idx]\n",
        "\n",
        "            payload_raw = OPT_MODELS[('rf', best_row['Selección X'], best_row['Motor'])]\n",
        "            p = _normalize_payload(payload_raw)\n",
        "\n",
        "            model_rf = p['model']\n",
        "            sx_rf    = p['sx']\n",
        "            cols_rf  = p['cols']\n",
        "\n",
        "            # Preparar datos de entrenamiento escalados\n",
        "            X_train_sel = X_train[cols_rf]\n",
        "            y_train = Y_train.values.ravel()\n",
        "            X_train_scaled = sx_rf.transform(X_train_sel) if sx_rf else X_train_sel\n",
        "\n",
        "            # 4.1 Curva de Aprendizaje (R²)\n",
        "            train_sizes, train_scores, val_scores = learning_curve(\n",
        "                model_rf, X_train_scaled, y_train,\n",
        "                cv=5, scoring='r2', train_sizes=np.linspace(0.1,1.0,5), n_jobs=-1\n",
        "            )\n",
        "            train_mean = np.mean(train_scores, axis=1)\n",
        "            val_mean   = np.mean(val_scores,   axis=1)\n",
        "            fig_lc, ax_lc = plt.subplots(figsize=(6,4))\n",
        "            ax_lc.plot(train_sizes, train_mean, 'o-', label='Train R²')\n",
        "            ax_lc.plot(train_sizes, val_mean,   'o-', label='CV R²')\n",
        "            ax_lc.set_title('RF Optimizado: Curva de Aprendizaje')\n",
        "            ax_lc.set_xlabel('Tamaño del set de entrenamiento')\n",
        "            ax_lc.set_ylabel('R²')\n",
        "            ax_lc.legend()\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Curva de Aprendizaje', fig_lc\n",
        "            ))\n",
        "\n",
        "            # Función auxiliar para Curvas de Validación\n",
        "            def plot_vc(param_name, param_range):\n",
        "                  # Ajuste: validation_curve devuelve solo train_scores y test_scores\n",
        "                train_scores_vc, val_scores_vc = validation_curve(\n",
        "                    model_rf, X_train_scaled, y_train,\n",
        "                    param_name=param_name, param_range=param_range,\n",
        "                    cv=5, scoring='r2', n_jobs=-1\n",
        "                )\n",
        "                fig, ax = plt.subplots(figsize=(6,4))\n",
        "                ax.plot(param_range, np.mean(train_scores_vc, axis=1), 'o-', label='Train R²')\n",
        "                ax.plot(param_range, np.mean(val_scores_vc, axis=1), 'o-', label='CV R²')\n",
        "                ax.set_title(f\"RF Optimizado: Curva de Validación {param_name}\")\n",
        "                ax.set_xlabel(param_name)\n",
        "                ax.set_ylabel('R²')\n",
        "                if param_name == 'n_estimators':\n",
        "                    ax.set_xscale('log')\n",
        "                ax.legend()\n",
        "                return fig, train_scores_vc, val_scores_vc\n",
        "\n",
        "            # 4.2 Curva de Validación n_estimators\n",
        "            param_range_n = np.arange(50, 501, 50)\n",
        "            fig_vc_n, tsn, vsn = plot_vc('n_estimators', param_range_n)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Curva de Validación n_estimators', fig_vc_n\n",
        "            ))\n",
        "\n",
        "            # 4.3 Curva de Validación max_depth\n",
        "            param_range_md = np.arange(3, 16, 2)\n",
        "            fig_vc_md, tsm, vsm = plot_vc('max_depth', param_range_md)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Curva de Validación max_depth', fig_vc_md\n",
        "            ))\n",
        "\n",
        "            # 4.4 Interpretación IA de Curvas\n",
        "            print(\"[DEBUG] 15.10. Llamando IA para análisis de curvas Random Forest\")\n",
        "            prompt_curvas_rf = (\n",
        "                f\"Random Forest optimizado (Selección {best_row['Selección X']}, Motor {best_row['Motor']}):\\n\"\n",
        "                f\"- Curva de Aprendizaje: tamaños={train_sizes.tolist()}, train={train_mean.tolist()}, cv={val_mean.tolist()}\\n\"\n",
        "                f\"- Curva de Validación n_estimators: {param_range_n.tolist()}, train={np.mean(tsn,axis=1).tolist()}, cv={np.mean(vsn,axis=1).tolist()}\\n\"\n",
        "                f\"- Curva de Validación max_depth: {param_range_md.tolist()}, train={np.mean(tsm,axis=1).tolist()}, cv={np.mean(vsm,axis=1).tolist()}\\n\\n\"\n",
        "                \"1. Analiza cada curva e identifica underfitting o overfitting.\\n\"\n",
        "                \"2. Señala brechas clave entre entrenamiento y validación y su impacto en la generalización.\\n\"\n",
        "                \"3. Recomienda ajustes concretos de n_estimators y max_depth para mejorar el modelo.\"\n",
        "            )\n",
        "            analysis_vc_rf = call_openai_explanation(prompt_curvas_rf)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Interpretación IA de Curvas de Aprendizaje y Validación', analysis_vc_rf\n",
        "            ))\n",
        "            # --- Fin Bloque 4 ---\n",
        "\n",
        "            # --- 5. Curvas de Calibración y Predicción de Intervalos para Random Forest Optimizado ---\n",
        "            from sklearn.calibration import calibration_curve\n",
        "            import pandas as _pd\n",
        "\n",
        "            print(\"[DEBUG] 15.11. Iniciando bloque de Curvas de Calibración y Predicción de Intervalos para Random Forest optimizado\")\n",
        "\n",
        "            # Preparar datos de prueba\n",
        "            y_true_rf = Y_test.values.ravel()\n",
        "            y_pred_scaled = model_rf.predict(X_test[cols_rf])\n",
        "            y_pred_rf = sy_rf.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel() if sy_rf else y_pred_scaled\n",
        "\n",
        "            # 5.1 Curva de calibración (binned reliability plot para regresión)\n",
        "            bins = 10\n",
        "            df_cal = _pd.DataFrame({'y_pred': y_pred_rf, 'y_true': y_true_rf})\n",
        "            try:\n",
        "                df_cal['bin'] = _pd.qcut(df_cal['y_pred'], q=bins, duplicates='drop')\n",
        "            except:\n",
        "                df_cal['bin'] = _pd.cut(df_cal['y_pred'], bins=bins)\n",
        "            grp = df_cal.groupby('bin', observed=True).agg({'y_pred': 'mean', 'y_true': 'mean'})\n",
        "            prob_pred, prob_true = grp['y_pred'].values, grp['y_true'].values\n",
        "            fig_cal, ax_cal = plt.subplots(figsize=(6, 4))\n",
        "            ax_cal.plot(prob_pred, prob_true, marker='o', linewidth=2)\n",
        "            ax_cal.plot([prob_pred.min(), prob_pred.max()], [prob_pred.min(), prob_pred.max()], 'k--')\n",
        "            ax_cal.set_xlabel('Predicción promedio en bin')\n",
        "            ax_cal.set_ylabel('Valor real promedio')\n",
        "            ax_cal.set_title('RF Optimizado: Curva de Calibración')\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Curva de Calibración', fig_cal\n",
        "            ))\n",
        "\n",
        "            # 5.2 Intervalos de predicción ±1 STD de residuos\n",
        "            residuals_rf = y_true_rf - y_pred_rf\n",
        "            std_res_rf = np.std(residuals_rf)\n",
        "            upper_rf = y_pred_rf + std_res_rf\n",
        "            lower_rf = y_pred_rf - std_res_rf\n",
        "            fig_int, ax_int = plt.subplots(figsize=(6, 4))\n",
        "            ax_int.plot(y_true_rf, label='Y real')\n",
        "            ax_int.plot(y_pred_rf, label='Predicción')\n",
        "            ax_int.fill_between(range(len(y_pred_rf)), lower_rf, upper_rf, alpha=0.3, label='±1 STD residuo')\n",
        "            ax_int.set_xlabel('Índice de muestra')\n",
        "            ax_int.set_ylabel('Valor')\n",
        "            ax_int.set_title('RF Optimizado: Intervalos de Predicción')\n",
        "            ax_int.legend()\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Intervalos de Predicción', fig_int\n",
        "            ))\n",
        "\n",
        "            # 5.3 Análisis Generativo IA de Incertidumbre y Calibración\n",
        "            print(\"[DEBUG] 15.12. Llamando IA para análisis de incertidumbre y calibración RF optimizado\")\n",
        "            prompt_ci_rf = (\n",
        "                f\"Curva de calibración (pred:{prob_pred.tolist()}, real:{prob_true.tolist()}) y \"\n",
        "                f\"intervalos ±1 STD (std_res={std_res_rf:.4f}).\\n\"\n",
        "                \"1. Evalúa la fiabilidad de los intervalos de incertidumbre.\\n\"\n",
        "                \"2. Identifica infravaloración de errores altos o patrones de heterocedasticidad.\\n\"\n",
        "                \"3. Sugiere mejoras para la calibración y estimación de incertidumbre.\"\n",
        "            )\n",
        "            analysis_ci_rf = call_openai_explanation(prompt_ci_rf)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Análisis de Incertidumbre y Calibración', analysis_ci_rf\n",
        "            ))\n",
        "            # --- Fin Bloque 5 ---\n",
        "\n",
        "            # --- 6. Resumen Ejecutivo y Road-Map de Siguientes Pasos para Random Forest Optimizado ---\n",
        "            print(\"[DEBUG] 15.13. Iniciando bloque de Resumen Ejecutivo y Road-Map para Random Forest optimizado\")\n",
        "            # Variables clave\n",
        "            sel_rf    = best_row['Selección X']\n",
        "            eng_rf    = best_row['Motor']\n",
        "            best_score_rf = best_row['Score']\n",
        "            # Estadísticas de residuos y validación\n",
        "            cv_std    = float(np.std(val_mean))\n",
        "            res_std   = std_res_rf\n",
        "            res_kurt  = kurtosis(residuals_rf)\n",
        "            q25_rf, q50_rf, q75_rf = [float(x) for x in np.quantile(residuals_rf, [0.25,0.5,0.75])]\n",
        "\n",
        "            summary_md_rf = (\n",
        "                \"**Puntos Clave Optimización Random Forest:**\\n \"\n",
        "                f\"- **Mejor combinación:** Selección `{sel_rf}` + motor `{eng_rf}` ➜ **Score** = {best_score_rf:.4f}\\n\"\n",
        "                f\"- **Robustez del modelo:** desviación estándar CV = {cv_std:.4f}, std residuos = {res_std:.4f}\\n\"\n",
        "                f\"- **Curtosis de residuos:** {res_kurt:.4f}, quantiles (25%,50%,75%) = ({q25_rf:.4f},{q50_rf:.4f},{q75_rf:.4f})\\n\"\n",
        "                \"- **Recomendaciones inmediatas:**\\n\"\n",
        "                \"  1. Ajustar `n_estimators` y `max_depth` según el balance sesgo-varianza observado.\\n\"\n",
        "                \"  2. Explorar regularización adicional (`min_samples_split`, `min_samples_leaf`) para reducir varianza.\\n\"\n",
        "                \"  3. Ampliar validación cruzada a 7–10 folds e incluir `oob_score` para medir robustez.\\n\"\n",
        "                \"  4. Considerar ensambles adicionales (e.g., GradientBoosting, LightGBM) para comparar rendimiento\"\n",
        "            )\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Resumen Ejecutivo y Road-Map', summary_md_rf\n",
        "            ))\n",
        "\n",
        "            print(\"[DEBUG] 15.14. Llamando IA para Resumen Ejecutivo y Road-Map RF optimizado\")\n",
        "            prompt_exec_rf = (\n",
        "                \"Eres un investigador de Machine Learning avanzado. Basándote en los puntos clave de optimización:\\n\"\n",
        "                f\"{summary_md_rf}\\n\"\n",
        "                \"1. Desarrolla un análisis detallado en párrafos separados para cada punto clave.\\n\"\n",
        "                \"2. Propón un plan de acción prioritizado de 3–5 pasos claros para stakeholders.\\n\"\n",
        "                \"3. Finaliza con un breve resumen ejecutivo de 2–3 párrafos enfatizando impacto y próximos hitos.\"\n",
        "            )\n",
        "            analysis_exec_rf = call_openai_explanation(prompt_exec_rf)\n",
        "            self.sections.append((\n",
        "                '### RF Optimizado: Resumen Ejecutivo IA', analysis_exec_rf\n",
        "            ))\n",
        "            # --- Fin Bloque 6 ---\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                '### ⚠️ Error en sección Optimización Random Forest',\n",
        "                f\"Se produjo un error en Optimización Random Forest: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =============================================================================\n",
        "        # 16. Optimización Modelo RNN\n",
        "        # =============================================================================\n",
        "        import numpy as np\n",
        "        # --- Bloque 0: Resumen Optimización RNN ---\n",
        "        try:\n",
        "            print(\"[DEBUG] 16.1. Bloque 0 Optimización RNN: Resumen de métodos y motores\")\n",
        "\n",
        "            # Función de invocación a OpenAI (igual que en SVR, NN, XGBoost, RF)\n",
        "            def call_openai_explanation(prompt: str, model: str = \"gpt-4\") -> str:\n",
        "                try:\n",
        "                    response = _client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\":\n",
        "                                \"Eres un experto en optimización de hiperparámetros de redes neuronales recurrentes. \"\n",
        "                                \"Analiza y ofrece conclusiones detalladas basadas en los datos proporcionados.\"\n",
        "                            },\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS\n",
        "                    )\n",
        "                    return response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    return f\"[Error llamando a OpenAI: {e}]\"\n",
        "\n",
        "            # 0.1) Verificar resultados en OPT_MODELS\n",
        "            if 'OPT_MODELS' not in globals() or not isinstance(OPT_MODELS, dict):\n",
        "                raise RuntimeError(\"No se encontró OPT_MODELS con resultados de optimización RNN\")\n",
        "\n",
        "            valid_engines = {'RandomSearch', 'Bayesian', 'Hyperband', 'Optuna'}\n",
        "            rnn_entries = {\n",
        "                k: v for k, v in OPT_MODELS.items()\n",
        "                if isinstance(k, tuple) and k[0] == 'rnn' and k[2] in valid_engines\n",
        "            }\n",
        "            if not rnn_entries:\n",
        "                raise RuntimeError(\"No se encontraron optimizaciones RNN en OPT_MODELS\")\n",
        "\n",
        "            # ——— Normalizar payload del mejor RNN para todos los bloques ———\n",
        "            # Elegimos la clave que maximiza/minimiza el score según la métrica\n",
        "            # (usamos la misma lógica que df_sel, pero sobre rnn_entries)\n",
        "            keys = list(rnn_entries.keys())\n",
        "            scores = [rnn_entries[k]['score'] for k in keys]\n",
        "            metrics = [rnn_entries[k].get('metric','').upper() for k in keys]\n",
        "            if metrics[0] == 'R2':\n",
        "                best_idx = int(np.argmax(scores))\n",
        "            else:\n",
        "                best_idx = int(np.argmin(scores))\n",
        "            best_key = keys[best_idx]\n",
        "\n",
        "            payload_raw      = OPT_MODELS[best_key]\n",
        "            p                = _normalize_payload(payload_raw)\n",
        "            model_rnn        = p['model']\n",
        "            sx_rnn, sy_rnn   = p.get('sx'), p.get('sy')\n",
        "            cols_rnn         = p['cols']\n",
        "            score_rnn        = p['score']\n",
        "            metric_rnn       = p['metric']\n",
        "            best_params_rnn  = p['best_params']\n",
        "            # ——— fin normalización ———\n",
        "\n",
        "            # 0.2) Construir lista de registros resumen\n",
        "            summary_records = []\n",
        "            for (_, metodo, motor), payload in rnn_entries.items():\n",
        "                score = payload.get('score')\n",
        "                # Extraer hiperparámetros óptimos\n",
        "                best_est = payload.get('best')\n",
        "                if best_est is not None and hasattr(best_est, 'get_params'):\n",
        "                    params = best_est.get_params()\n",
        "                else:\n",
        "                    params = payload.get('params', {}) or payload.get('best_params', {})\n",
        "\n",
        "                # Sólo los hiperparámetros relevantes\n",
        "                hp_keys = ['units', 'dropout_rate', 'learning_rate', 'epochs', 'batch_size']\n",
        "                best_hp = {k: params.get(k) for k in hp_keys if k in params}\n",
        "\n",
        "                # Añadir registro\n",
        "                rec = {\n",
        "                    'Método': metodo,\n",
        "                    'Motor': motor,\n",
        "                    'metric': payload.get('metric'),  # ← aquí incluyes la métrica\n",
        "                    'Score': score\n",
        "                }\n",
        "                rec.update({f\"Best_{k}\": v for k, v in best_hp.items()})\n",
        "                summary_records.append(rec)\n",
        "\n",
        "            # 0.3) DataFrame de resumen\n",
        "            df_rnn = pd.DataFrame(summary_records)\n",
        "            df_rnn.rename(\n",
        "                columns={col: col.replace(\"Best_\", \"\") for col in df_rnn.columns if col.startswith(\"Best_\")},\n",
        "                inplace=True\n",
        "            )\n",
        "            self.sections.append((\n",
        "                \"### RNN Optimización: Resumen de Métodos y Motores\",\n",
        "                df_rnn\n",
        "            ))\n",
        "\n",
        "            # 0.4) Preparar prompt para IA\n",
        "            prompt_lines = []\n",
        "            for rec in summary_records:\n",
        "                hp_str = \", \".join(f\"{k}={rec[f'Best_{k}']}\" for k in best_hp.keys())\n",
        "                prompt_lines.append(\n",
        "                    f\"Método: {rec['Método']}, Motor: {rec['Motor']}, \"\n",
        "                    f\"Score: {rec['Score']:.4f}, Hiperparámetros: {hp_str}\"\n",
        "                )\n",
        "            prompt = (\n",
        "                \"He obtenido los siguientes resultados de optimización para el modelo RNN:\\n\\n\"\n",
        "                + \"\\n\".join(prompt_lines)\n",
        "                + \"\\n\\n\"\n",
        "                \"1. Describe en detalle las fortalezas y debilidades de cada motor de búsqueda \"\n",
        "                \"(RandomSearch, Bayesian, Hyperband, Optuna) aplicado a RNN.\\n\"\n",
        "                \"2. Compara los scores y explica por qué una configuración es superior.\\n\"\n",
        "                \"3. Analiza el impacto de los hiperparámetros optimizados en entrenamiento y generalización.\\n\"\n",
        "                \"4. Propón estrategias avanzadas para mejorar la RNN \"\n",
        "                \"(learning‐rate scheduling, regularización, early stopping, augmentation de series temporales).\\n\"\n",
        "                \"5. Sugiere un roadmap de próximos pasos para validar robustez y escalar el modelo.\"\n",
        "            )\n",
        "\n",
        "            print(\"[DEBUG] 16.2. Llamando a OpenAI para análisis generativo RNN optimización\")\n",
        "            analysis = call_openai_explanation(prompt)\n",
        "            self.sections.append((\n",
        "                \"### RNN Optimización: Análisis Generativo\",\n",
        "                analysis\n",
        "            ))\n",
        "            # --- Fin Bloque 0 ---\n",
        "\n",
        "            # --- 1. Curvas de Ajuste Real vs. Predicho y Residuos para Random Forest Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.3. Iniciando Bloque 1: Curvas Real vs Predicho y Residuos RNN Optimizado\")\n",
        "\n",
        "                # ========== Cargar modelo RNN entrenado desde metadatos ==========\n",
        "                import pickle, glob\n",
        "\n",
        "                # Buscar el archivo más reciente para el método y motor seleccionados\n",
        "                archivos_meta = sorted(glob.glob(f\"modelos_opt/rnn_{metodo.lower()}_{motor.lower()}_opt_*.pkl\"))\n",
        "                if not archivos_meta:\n",
        "                    raise FileNotFoundError(f\"No se encontró el archivo de metadatos para {metodo} + {motor}\")\n",
        "                archivo_meta = archivos_meta[-1]\n",
        "\n",
        "                # Cargar metadatos\n",
        "                with open(archivo_meta, \"rb\") as f:\n",
        "                    metadata = pickle.load(f)\n",
        "\n",
        "                # Reconstruir ruta al modelo .keras\n",
        "                ts = metadata[\"fecha\"]\n",
        "                ruta_modelo = f\"modelos_opt/rnn_{metodo.lower()}_{motor.lower()}_opt_{ts}.keras\"\n",
        "\n",
        "                # Cargar modelo\n",
        "                model_rnn = load_model(ruta_modelo)\n",
        "\n",
        "\n",
        "                import pickle\n",
        "                from scipy.stats import skew, kurtosis\n",
        "                from sklearn.metrics import r2_score\n",
        "                import matplotlib.pyplot as plt\n",
        "                from tensorflow.keras.models import load_model\n",
        "\n",
        "                # 1.1 Seleccionar la mejor configuración según la métrica\n",
        "                # Reconstruimos un pequeño DataFrame para elegir el mejor índice\n",
        "                valid_engines = {'RandomSearch','Bayesian','Hyperband','Optuna'}\n",
        "                df_sel = pd.DataFrame([\n",
        "                    {\n",
        "                        'Método': k[1],\n",
        "                        'Motor':  k[2],\n",
        "                        'Score':  v['score'],\n",
        "                        'Métrica': v['metric']\n",
        "                    }\n",
        "                    for k, v in OPT_MODELS.items()\n",
        "                    if k[0]=='rnn' and k[2] in valid_engines\n",
        "                ])\n",
        "                if df_sel.empty:\n",
        "                    raise RuntimeError(\"No hay optimizaciones RNN registradas en OPT_MODELS\")\n",
        "                # Elegir mejor row\n",
        "                if df_sel['Métrica'].str.upper().iloc[0] == 'R2':\n",
        "                    idx_best = df_sel['Score'].idxmax()\n",
        "                else:\n",
        "                    idx_best = df_sel['Score'].idxmin()\n",
        "                best_row = df_sel.loc[idx_best]\n",
        "                metodo   = best_row['Método']\n",
        "                motor    = best_row['Motor']\n",
        "\n",
        "                # 1.1bis) Extraer los params del payload en lugar de Best_*\n",
        "                payload    = OPT_MODELS[('rnn', metodo, motor)]\n",
        "                # si guardaste 'params' en OPT_MODELS…\n",
        "                params     = payload.get('params') or payload['best'].get_params()\n",
        "                # sólo las cinco keys que nos interesan\n",
        "                hp_keys    = ['units','dropout_rate','learning_rate','epochs','batch_size']\n",
        "                best_params = { k: params[k] for k in hp_keys }\n",
        "\n",
        "                # 1.2 Cargar payload y metadatos\n",
        "                # Usamos el payload normalizado:\n",
        "                model     = model_rnn\n",
        "                sx        = sx_rnn\n",
        "                sy        = sy_rnn\n",
        "                cols      = cols_rnn\n",
        "\n",
        "                # ——— Sanitización unificada de columnas para RNN ———\n",
        "                raw_cols_rnn       = cols_rnn      # viene de _normalize_payload\n",
        "                sanitized_cols_rnn = [sanitize_name(c) for c in raw_cols_rnn]\n",
        "                effective_cols     = [c for c in sanitized_cols_rnn if c in X_test.columns]\n",
        "                print(f\"[DEBUG] 16.4. RNN cols esperadas: {len(raw_cols_rnn)}, sanitizadas válidas: {len(effective_cols)} → {effective_cols}\")\n",
        "\n",
        "                # ✔️ Se mantiene: comprobación del modelo\n",
        "                from tensorflow.keras.models import Sequential\n",
        "                if not isinstance(model_rnn, Sequential):\n",
        "                    raise TypeError(\"[ERROR] El objeto 'model_rnn' no es un modelo válido de Keras. ¿Se ha sobrescrito accidentalmente?\")\n",
        "\n",
        "                # ========== Verificar columnas ==========\n",
        "                expected_cols = cols_rnn\n",
        "                actual_cols = [c for c in expected_cols if c in X_test.columns]\n",
        "                print(f\"[DEBUG] 16.5. RNN cols esperadas: {len(expected_cols)}, sanitizadas válidas: {len(actual_cols)} → {actual_cols}\")\n",
        "\n",
        "                if len(actual_cols) != len(expected_cols):\n",
        "                    raise ValueError(\n",
        "                        f\"[ERROR] Las columnas del modelo RNN ({len(expected_cols)} esperadas) \"\n",
        "                        f\"no coinciden con las disponibles en X_test ({len(actual_cols)} encontradas).\"\n",
        "                        f\"\\nEsperadas: {expected_cols}\\nEncontradas: {actual_cols}\"\n",
        "                    )\n",
        "\n",
        "                # ========== Escalar y preparar datos ==========\n",
        "                X_test_sel_df = X_test[actual_cols]\n",
        "                X_scaled = sx_rnn.transform(X_test_sel_df.values)\n",
        "                print(f\"[DEBUG] 16.6. X_scaled.shape = {X_scaled.shape}\")\n",
        "                print(f\"[DEBUG] 16.7. n_samples = {X_scaled.shape[0]}, n_features = {X_scaled.shape[1]}\")\n",
        "\n",
        "                X3_test = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "                print(f\"[DEBUG] 16.8. X3_test.shape = {X3_test.shape}\")\n",
        "                print(\"[DEBUG] 16.9. Llamando a model_rnn.predict con entrada:\", X3_test.shape)\n",
        "\n",
        "                # ========== Predicción ==========\n",
        "                y_pred_raw = model_rnn.predict(X3_test).ravel()\n",
        "\n",
        "                print(f\"[DEBUG] 16.10. X3_test.shape = {X3_test.shape}\")\n",
        "                print(\"[DEBUG] 16.11. Llamando a model_rnn.predict con entrada:\", X3_test.shape)\n",
        "\n",
        "                y_pred     = sy_rnn.inverse_transform(y_pred_raw.reshape(-1,1)).ravel() if sy_rnn else y_pred_raw\n",
        "                y_true     = Y_test.values.ravel()\n",
        "\n",
        "                # 1.4 Gráfica Predicho vs Real\n",
        "                fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "                ax1.scatter(y_true, y_pred, alpha=0.6)\n",
        "                ax1.plot([y_true.min(), y_true.max()],\n",
        "                        [y_true.min(), y_true.max()],\n",
        "                        'r--', linewidth=2, label='Ideal')\n",
        "                ax1.set_xlabel(\"Y real\")\n",
        "                ax1.set_ylabel(\"Y predicho\")\n",
        "                ax1.set_title(f\"RNN Optimizado ({metodo}-{motor}) Predicho vs Real\")\n",
        "                ax1.legend()\n",
        "                self.sections.append((\n",
        "                    f\"### RNN Optimizado: Predicho vs Real ({metodo}-{motor})\",\n",
        "                    fig1\n",
        "                ))\n",
        "\n",
        "                # 1.5 Gráfica de Residuos\n",
        "                residuals = y_true - y_pred\n",
        "                fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "                ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "                ax2.axhline(0, color='r', linestyle='--', linewidth=2)\n",
        "                ax2.set_xlabel(\"Y predicho\")\n",
        "                ax2.set_ylabel(\"Residuo\")\n",
        "                ax2.set_title(f\"RNN Optimizado ({metodo}-{motor}) Residuos\")\n",
        "                self.sections.append((\n",
        "                    f\"### RNN Optimizado: Residuos ({metodo}-{motor})\",\n",
        "                    fig2\n",
        "                ))\n",
        "\n",
        "                # 1.6 Tabla de estadísticas de residuos\n",
        "                stats_df = pd.DataFrame({\n",
        "                    'Métrica': ['Media', 'Desviación', 'Skew', 'Kurtosis', '25%', '50%', '75%'],\n",
        "                    'Valor': [\n",
        "                        residuals.mean(),\n",
        "                        residuals.std(),\n",
        "                        skew(residuals),\n",
        "                        kurtosis(residuals),\n",
        "                        *np.quantile(residuals, [0.25, 0.5, 0.75])\n",
        "                    ]\n",
        "                })\n",
        "                self.sections.append((\n",
        "                    f\"### RNN Optimizado: Estadísticas de Residuos ({metodo}-{motor})\",\n",
        "                    stats_df\n",
        "                ))\n",
        "\n",
        "                # 1.7 Análisis generativo con IA\n",
        "                prompt = (\n",
        "                    f\"Para el mejor RNN optimizado (Método={metodo}, Motor={motor}) tenemos:\\n\"\n",
        "                    f\"- Rango Y real: [{y_true.min():.4f}, {y_true.max():.4f}]\\n\"\n",
        "                    f\"- Rango Y predicho: [{y_pred.min():.4f}, {y_pred.max():.4f}]\\n\"\n",
        "                    f\"- Estadísticas de residuos: media={residuals.mean():.4f}, \"\n",
        "                    f\"std={residuals.std():.4f}, skew={skew(residuals):.4f}, \"\n",
        "                    f\"kurtosis={kurtosis(residuals):.4f}, \"\n",
        "                    f\"quantiles25/50/75={[f'{q:.4f}' for q in np.quantile(residuals,[0.25,0.5,0.75])]}.\\n\\n\"\n",
        "                    \"1. Analiza la calidad del ajuste considerando ambas gráficas: identifica sesgos o heterocedasticidad.\\n\"\n",
        "                    \"2. Comenta sobre la normalidad de los errores.\\n\"\n",
        "                    \"3. Propón ajustes de HPO o transformaciones de datos para mejorar la RNN.\"\n",
        "                )\n",
        "                print(\"[DEBUG] 16.12. Llamando a OpenAI para análisis de curvas RNN optimizado\")\n",
        "                analysis = call_openai_explanation(prompt)\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Análisis Calidad Ajuste\",\n",
        "                    analysis\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                # 1) Imprime la excepción para trazar el error\n",
        "                print(f\"[DEBUG] Error Bloque 1 Optimización RNN: {e}\")\n",
        "                # 2) Luego sigues registrando la sección de error como antes\n",
        "                self.sections.append((\n",
        "                    \"### ⚠️ Error Bloque 1 Optimización RNN\",\n",
        "                    f\"Se produjo un error en Curvas Real vs Predicho y Residuos RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 1 ---\n",
        "            # --- 2. Importancia Relativa de Hiperparámetros para RNN Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.13. Iniciando bloque 2: Importancia de Hiperparámetros RNN Optimizado\")\n",
        "\n",
        "                import seaborn as sns\n",
        "                from sklearn.metrics import r2_score\n",
        "                import pickle\n",
        "\n",
        "                # --- a) Recuperar el best_row del bloque 1 ---\n",
        "                # (Asegúrate de que best_row lo tienes en el scope, ó bien vuelve a\n",
        "                #  reconstruir tu pequeño df_sel y lo vuelves a extraer)\n",
        "                valid_engines = {'RandomSearch','Bayesian','Hyperband','Optuna'}\n",
        "                df_sel = pd.DataFrame([\n",
        "                    {'Método':k[1], 'Motor':k[2], 'Score':v['score'], 'Métrica':v['metric']}\n",
        "                    for k,v in OPT_MODELS.items()\n",
        "                    if k[0]=='rnn' and k[2] in valid_engines\n",
        "                ])\n",
        "                if df_sel['Métrica'].str.upper().iloc[0]=='R2':\n",
        "                    idx_best = df_sel['Score'].idxmax()\n",
        "                else:\n",
        "                    idx_best = df_sel['Score'].idxmin()\n",
        "                best_row = df_sel.loc[idx_best]\n",
        "\n",
        "                # --- b) Recuperar payload y metadatos de OPT_MODELS ---\n",
        "                key = ('rnn', best_row['Método'], best_row['Motor'])\n",
        "                payload = OPT_MODELS[key]\n",
        "\n",
        "                # 2.1) **Carga aquí** los scalers que guardaste\n",
        "                # Usamos el payload normalizado:\n",
        "                model     = model_rnn\n",
        "                sx        = sx_rnn\n",
        "                sy        = sy_rnn\n",
        "                cols      = cols_rnn\n",
        "\n",
        "                # 2.2) ahora extrae base_score y best_params\n",
        "                base_score = payload['score']\n",
        "                if 'params' in payload and payload['params']:\n",
        "                    best_params = payload['params']\n",
        "                else:\n",
        "                    best_params = payload['best'].get_params()\n",
        "\n",
        "                # ── Aquí definimos de nuevo los subsets ──\n",
        "                X_train_sel = X_train[cols]\n",
        "                Y_train_sel = Y_train.copy()\n",
        "                X_test_sel  = X_test[cols]\n",
        "                Y_test_sel  = Y_test.copy()\n",
        "\n",
        "                # summary_records viene del bloque 0: lista de dicts con Método, Motor, Score y Best_<hp>\n",
        "                # Lo transformamos en un DataFrame más “usable”\n",
        "                df_hp = pd.DataFrame([\n",
        "                    {\n",
        "                        'Método'       : rec['Método'],\n",
        "                        'Motor'        : rec['Motor'],\n",
        "                        'Score'        : rec['Score'],\n",
        "                        **{hp: rec[f\"Best_{hp}\"] for hp in ['units','dropout_rate','learning_rate','epochs','batch_size']\n",
        "                          if f\"Best_{hp}\" in rec}\n",
        "                    }\n",
        "                    for rec in summary_records\n",
        "                ])\n",
        "\n",
        "                # 2.1 Heatmap Score vs combinación de dos parámetros continuos, p.e. learning_rate y dropout_rate\n",
        "                heat_params = ['learning_rate','dropout_rate']\n",
        "                heat_recs = []\n",
        "                for _, row in df_hp.iterrows():\n",
        "                    if all(p in row and pd.notna(row[p]) for p in heat_params):\n",
        "                        heat_recs.append({\n",
        "                            heat_params[0]: row[heat_params[0]],\n",
        "                            heat_params[1]: row[heat_params[1]],\n",
        "                            'Score':       row['Score']\n",
        "                        })\n",
        "                if heat_recs:\n",
        "                    df_heat = pd.DataFrame(heat_recs)\n",
        "                    heat = df_heat.pivot(index=heat_params[0], columns=heat_params[1], values='Score')\n",
        "                    fig_heat, ax_heat = plt.subplots(figsize=(6,5))\n",
        "                    sns.heatmap(heat, annot=True, fmt=\".4f\", ax=ax_heat)\n",
        "                    ax_heat.set_title('RNN Optimizado: Heatmap Score vs learning_rate y dropout_rate')\n",
        "                    self.sections.append((\n",
        "                        \"### RNN Optimizado: Heatmap Score vs learning_rate & dropout_rate\",\n",
        "                        fig_heat\n",
        "                    ))\n",
        "\n",
        "                # 2.2 Sensibilidad ±1% para cada hiperparámetro (reentrenando sobre TEST)\n",
        "                sens = []\n",
        "                #for param, base_val in best_params.items():\n",
        "                for param, base_val in list(best_params.items())[:3]:  # ⚠️ limitar a 3 primeros hiperparámetros para evitar evaluar todos.\n",
        "                    if isinstance(base_val, (int, float)):\n",
        "                        #for factor, label in [(1.01, '+1%'), (0.99, '-1%')]:\n",
        "                        for factor, label in [(1.05, '+5%'), (0.95, '-5%')]:      # Reducción del ±1% al ±5% para evaluar sensibilidad, disminuyendo ciclos de entrenamiento.\n",
        "                            # Construye nuevos parámetros variando uno solo\n",
        "                            new_params = best_params.copy()\n",
        "                            new_params[param] = base_val * factor\n",
        "\n",
        "                            # Reentrena el modelo con estos new_params\n",
        "                            model = RNNRegressor(**new_params, sy=sy)\n",
        "                            model.fit(\n",
        "                                sx.transform(X_train_sel.values),\n",
        "                                sy.transform(Y_train_sel.values.reshape(-1,1)).ravel()\n",
        "                            )\n",
        "                            # Evalúa en validación (o test, según prefieras)\n",
        "                            # → aquí uso X_test_sel y Y_test en lugar de X_val_sel/Y_val\n",
        "                            Xs = sx.transform(X_test_sel.values)\n",
        "                            X3 = Xs.reshape((Xs.shape[0], 1, Xs.shape[1]))\n",
        "                            y_hat_scaled = model.predict(Xs)        # el wrapper añade la dimensión del “timesteps=1”\n",
        "                            y_hat        = sy.inverse_transform(y_hat_scaled.reshape(-1,1)).ravel()\n",
        "                            new_score    = r2_score(Y_test.values.ravel(), y_hat)\n",
        "\n",
        "                            sens.append({\n",
        "                              'Parámetro': param,\n",
        "                              'Cambio':    label,\n",
        "                              '% Δ Score': (new_score - base_score) / abs(base_score) * 100,\n",
        "                              #'% Δ Score': new_score - base_score,\n",
        "                              'Motor':     best_row['Motor'],\n",
        "                              'Método':    best_row['Método']\n",
        "                          })\n",
        "                if sens:\n",
        "                    df_sens = pd.DataFrame(sens)\n",
        "                    fig_sens, ax_sens = plt.subplots(figsize=(6,4))\n",
        "                    sns.barplot(data=df_sens, x='% Δ Score', y='Parámetro', hue='Cambio', ax=ax_sens)\n",
        "                    ax_sens.set_title('RNN Optimizado: Sensibilidad del Score ±1%')\n",
        "                    self.sections.append((\n",
        "                        \"### RNN Optimizado: Sensibilidad del Score\",\n",
        "                        fig_sens\n",
        "                    ))\n",
        "\n",
        "                # 2.3 IA: análisis de importancia\n",
        "                # Filtramos sólo para el mejor (best_row viene de bloque 1)\n",
        "                lines = []\n",
        "                for row in sens:\n",
        "                    if row['Método']==best_row['Método'] and row['Motor']==best_row['Motor']:\n",
        "                        lines.append(f\"{row['Parámetro']} {row['Cambio']} → {row['% Δ Score']:.2f}%\")\n",
        "                prompt_hp_rnn = (\n",
        "                    f\"Sensibilidad del Score al ±10% para el mejor RNN optimizado \"\n",
        "                    f\"(método {best_row['Método']}, motor {best_row['Motor']}):\\n\"\n",
        "                    + \"\\n\".join(lines)\n",
        "                    + \"\\n\\n1. ¿Qué hiperparámetro impulsa más la mejora y por qué?\\n\"\n",
        "                    \"2. Basándote en esta sensibilidad, ¿qué foco de ajuste priorizarías?\"\n",
        "                )\n",
        "                print(\"[DEBUG] 16.14. Llamando a OpenAI para importancia de hiperparámetros RNN\")\n",
        "                analysis_hp_rnn = call_openai_explanation(prompt_hp_rnn)\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: IA Importancia Hiperparámetros\",\n",
        "                    analysis_hp_rnn\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                self.sections.append((\n",
        "                    \"### ⚠️ Error Bloque 2 Optimización RNN\",\n",
        "                    f\"Se produjo un error en Importancia de Hiperparámetros RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 2 ---\n",
        "            # --- 3. Distribución de Métricas en Validación Cruzada para RNN Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.15. Iniciando bloque 3: Distribución de Métricas CV para RNN Optimizado\")\n",
        "\n",
        "                import pickle\n",
        "                from sklearn.model_selection import cross_validate\n",
        "                from sklearn.pipeline import make_pipeline\n",
        "                from sklearn.compose import TransformedTargetRegressor\n",
        "                from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "                # 3.1) Recuperar payload y metadatos\n",
        "                cols_cv = cols_rnn\n",
        "\n",
        "                # 3.2) Pipeline: escalar X → RNNRegressor\n",
        "                rnn_pipe = make_pipeline(\n",
        "                    StandardScaler(),    # escala X dentro de cada fold\n",
        "                    RNNRegressor()       # tu wrapper, sin pasar sy aquí\n",
        "                )\n",
        "\n",
        "                # 3.3) TransformedTargetRegressor: para escalar Y dentro de cada fold\n",
        "                rnn_ttr = TransformedTargetRegressor(\n",
        "                    regressor   = rnn_pipe,\n",
        "                    transformer = StandardScaler()\n",
        "                )\n",
        "\n",
        "                # 3.4) Ejecutar cross_validate usando directamente el pipeline completo\n",
        "                cv_results = cross_validate(\n",
        "                    rnn_ttr,\n",
        "                    X_train[cols_cv].values,\n",
        "                    Y_train.values.ravel(),\n",
        "                    #cv=5,\n",
        "                    cv=3,                                      # AÑADIDO PARA ALIGERAR LOS CICLOS DE ENTRENAMIENTO CRUZADO\n",
        "                    scoring={\n",
        "                        'r2':      'r2',\n",
        "                        'neg_mse': 'neg_mean_squared_error',\n",
        "                        'neg_mae': 'neg_mean_absolute_error'\n",
        "                    },\n",
        "                    return_train_score=False\n",
        "                )\n",
        "\n",
        "                # 3.5) Formatear resultados\n",
        "                r2_scores  = cv_results['test_r2']\n",
        "                mse_scores = [-v for v in cv_results['test_neg_mse']]\n",
        "                mae_scores = [-v for v in cv_results['test_neg_mae']]\n",
        "                rmse_scores = np.sqrt(mse_scores)\n",
        "\n",
        "                df_cv = pd.DataFrame({\n",
        "                    'R2':   r2_scores,\n",
        "                    'MSE':  mse_scores,\n",
        "                    'MAE':  mae_scores,\n",
        "                    'RMSE': rmse_scores\n",
        "                })\n",
        "\n",
        "                # 3.6) Boxplot\n",
        "                fig_cv, ax_cv = plt.subplots(figsize=(6,4))\n",
        "                sns.boxplot(data=df_cv, ax=ax_cv)\n",
        "                ax_cv.set_title('RNN Optimizado: Distribución de Métricas CV')\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Distribución de Métricas CV', fig_cv\n",
        "                ))\n",
        "\n",
        "                # 3.7) Tabla de media ± desviación\n",
        "                stats_cv = df_cv.agg(['mean','std']).T.reset_index().rename(columns={\n",
        "                    'index':'Métrica','mean':'Media','std':'Desviación'\n",
        "                })\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Estadísticas CV por Fold', stats_cv\n",
        "                ))\n",
        "\n",
        "                # 3.8) Análisis generativo\n",
        "                prompt_cv = (\n",
        "                    f\"Validación cruzada 5 folds RNN optimizado (Método={best_row['Método']}, \"\n",
        "                    f\"Motor={best_row['Motor']}):\\n\"\n",
        "                    f\"- R2 por fold: {r2_scores.tolist()}\\n\"\n",
        "                    f\"- MSE por fold: {mse_scores}\\n\"\n",
        "                    f\"- MAE por fold: {mae_scores}\\n\"\n",
        "                    f\"- RMSE por fold: {rmse_scores}\\n\\n\"\n",
        "                    \"Analiza la estabilidad y generalización de la RNN basándote en la dispersión de cada métrica.\"\n",
        "                )\n",
        "                print(\"[DEBUG] 16.16. Llamando IA para estabilidad CV RNN optimizado\")\n",
        "                analysis_cv = call_openai_explanation(prompt_cv)\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Análisis Estabilidad CV', analysis_cv\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                self.sections.append((\n",
        "                    \"### ⚠️ Error Bloque 3 Optimización RNN\",\n",
        "                    f\"Se produjo un error en Distribución de Métricas CV RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 3 ---\n",
        "\n",
        "            # --- 4. Curvas de Aprendizaje y Validación para RNN Optimizado ---\n",
        "            # --- 4. Curvas de Aprendizaje y Validación para RNN Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.17. Iniciando Bloque 4: Curvas de Aprendizaje y Validación RNN Optimizado\")\n",
        "                import numpy as np\n",
        "                from sklearn.model_selection import learning_curve, validation_curve\n",
        "                from sklearn.pipeline import make_pipeline\n",
        "                from sklearn.compose import TransformedTargetRegressor\n",
        "                from sklearn.preprocessing import StandardScaler\n",
        "                import matplotlib.pyplot as plt\n",
        "\n",
        "                valid_engines = {'RandomSearch','Bayesian','Hyperband','Optuna'}\n",
        "                df_sel = pd.DataFrame([\n",
        "                    {'Método': k[1], 'Motor': k[2], 'Score': v['score'], 'Métrica': v['metric']}\n",
        "                    for k,v in OPT_MODELS.items()\n",
        "                    if k[0]=='rnn' and k[2] in valid_engines\n",
        "                ])\n",
        "                if df_sel['Métrica'].str.upper().iloc[0] == 'R2':\n",
        "                    idx_best = df_sel['Score'].idxmax()\n",
        "                else:\n",
        "                    idx_best = df_sel['Score'].idxmin()\n",
        "                best_row = df_sel.loc[idx_best]\n",
        "\n",
        "                p = _normalize_payload(payload_raw)\n",
        "                best_params = p.get('best_params', {})\n",
        "                sx_rnn, sy_rnn = p['sx'], p['sy']\n",
        "                cols_rnn = p['cols']\n",
        "                model_rnn = p['model']\n",
        "\n",
        "                if not best_params:\n",
        "                    best_params = model_rnn.get_params()\n",
        "                    print(\"[DEBUG] Se han obtenido best_params desde model.get_params()\")\n",
        "\n",
        "                # ⚠️ Reducción explícita de carga\n",
        "                best_params['epochs'] = min(best_params.get('epochs', 100), 20)\n",
        "                best_params['verbose'] = 0\n",
        "\n",
        "                rnn_pipe = make_pipeline(\n",
        "                    StandardScaler(),\n",
        "                    RNNRegressor(**best_params)\n",
        "                )\n",
        "                rnn_ttr = TransformedTargetRegressor(regressor=rnn_pipe, transformer=StandardScaler())\n",
        "\n",
        "                X_train_sel = X_train[cols_rnn].values[:250]  # ⚠️ limitar tamaño\n",
        "                y_train = Y_train.values.ravel()[:250]\n",
        "\n",
        "                from joblib import parallel_backend\n",
        "                with parallel_backend('threading'):\n",
        "                    train_sizes, train_scores, val_scores = learning_curve(\n",
        "                        rnn_ttr, X_train_sel, y_train,\n",
        "                        cv=2,  # ⚠️ menos folds\n",
        "                        scoring='r2',\n",
        "                        train_sizes=np.linspace(0.5, 1.0, 2),  # ⚠️ menos tamaños\n",
        "                        n_jobs=1\n",
        "                    )\n",
        "\n",
        "                train_mean = np.mean(train_scores, axis=1)\n",
        "                val_mean = np.mean(val_scores, axis=1)\n",
        "\n",
        "                fig_lc, ax_lc = plt.subplots(figsize=(4.5,2.8))  # ⚠️ más pequeño\n",
        "                ax_lc.plot(train_sizes, train_mean, 'o-', label='Train R²')\n",
        "                ax_lc.plot(train_sizes, val_mean, 'o-', label='CV R²')\n",
        "                ax_lc.set_title('RNN Optimizado: Curva de Aprendizaje')\n",
        "                ax_lc.set_xlabel('Tamaño del set de entrenamiento')\n",
        "                ax_lc.set_ylabel('R²')\n",
        "                ax_lc.legend()\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Curva de Aprendizaje', fig_lc\n",
        "                ))\n",
        "\n",
        "                if 'units' not in best_params:\n",
        "                    raise KeyError(\"[ERROR] El parámetro 'units' no está presente en best_params. Claves disponibles: \" + str(list(best_params.keys())))\n",
        "\n",
        "                #units_range = np.unique(np.linspace(\n",
        "                #    max(1, best_params['units']//2),\n",
        "                #    best_params['units']*2,\n",
        "                #    2, dtype=int  # ⚠️ solo dos valores\n",
        "                #))\n",
        "                #units_range = [best_params['units']]  # ⚠️ evitar validación pesada\n",
        "                units = best_params['units']\n",
        "                units_range = np.unique(np.linspace(max(1, units * 0.9), units * 1.1, 3, dtype=int))\n",
        "                tu, vu = validation_curve(\n",
        "                    rnn_ttr, X_train_sel, y_train,\n",
        "                    param_name='regressor__rnnregressor__units',\n",
        "                    param_range=units_range,\n",
        "                    cv=2,\n",
        "                    scoring='r2',\n",
        "                    n_jobs=1\n",
        "                )\n",
        "                fig_uvc, ax_uvc = plt.subplots(figsize=(4.5,2.8))\n",
        "                ax_uvc.plot(units_range, np.mean(tu,axis=1), 'o-', label='Train R²')\n",
        "                ax_uvc.plot(units_range, np.mean(vu,axis=1), 'o-', label='CV R²')\n",
        "                ax_uvc.set_title('RNN Optimizado: Curva de Validación units')\n",
        "                ax_uvc.set_xlabel('units')\n",
        "                ax_uvc.set_ylabel('R²')\n",
        "                ax_uvc.legend()\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Curva de Validación units', fig_uvc\n",
        "                ))\n",
        "\n",
        "                dr = best_params['dropout_rate']\n",
        "                dr_range = np.linspace(max(0.0, dr-0.15), min(1.0, dr+0.15), 2)  # ⚠️ solo dos valores\n",
        "                td, vd = validation_curve(\n",
        "                    rnn_ttr, X_train_sel, y_train,\n",
        "                    param_name='regressor__rnnregressor__dropout_rate',\n",
        "                    param_range=dr_range,\n",
        "                    cv=2,\n",
        "                    scoring='r2',\n",
        "                    n_jobs=1\n",
        "                )\n",
        "                fig_dvc, ax_dvc = plt.subplots(figsize=(4.5,2.8))\n",
        "                ax_dvc.plot(dr_range, np.mean(td,axis=1), 'o-', label='Train R²')\n",
        "                ax_dvc.plot(dr_range, np.mean(vd,axis=1), 'o-', label='CV R²')\n",
        "                ax_dvc.set_title('RNN Optimizado: Curva de Validación dropout_rate')\n",
        "                ax_dvc.set_xlabel('dropout_rate')\n",
        "                ax_dvc.set_ylabel('R²')\n",
        "                ax_dvc.legend()\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Curva de Validación dropout_rate', fig_dvc\n",
        "                ))\n",
        "\n",
        "                prompt = (\n",
        "                    f\"Para la RNN optimizada (Método={best_row['Método']}, Motor={best_row['Motor']}):\\n\"\n",
        "                    f\"- Curva de Aprendizaje: tamaños={train_sizes if isinstance(train_sizes, list) else train_sizes.tolist()}, train R²={train_mean.tolist()}, cv R²={val_mean.tolist()}\\n\"\n",
        "                    f\"- Validación units: rango={units_range.tolist()}, train={np.mean(tu,axis=1).tolist()}, cv={np.mean(vu,axis=1).tolist()}\\n\"\n",
        "                    f\"- Validación dropout_rate: rango={dr_range.tolist()}, train={np.mean(td,axis=1).tolist()}, cv={np.mean(vd,axis=1).tolist()}\\n\\n\"\n",
        "                    \"1. Identifica underfitting o overfitting en cada curva.\\n\"\n",
        "                    \"2. Señala brechas clave y su impacto en la generalización.\\n\"\n",
        "                    \"3. Recomienda ajustes de units y dropout_rate.\"\n",
        "                )\n",
        "                print(\"[DEBUG] 16.18. Llamando a OpenAI para análisis de Curvas de Aprendizaje y Validación RNN\")\n",
        "                analysis = call_openai_explanation(prompt)\n",
        "                self.sections.append((\n",
        "                    '### RNN Optimizado: Interpretación IA de Curvas', analysis\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"[DEBUG] Error en Bloque 4 Optimización RNN:\", e)\n",
        "                self.sections.append((\n",
        "                    '### ⚠️ Error Bloque 4 Optimización RNN',\n",
        "                    f\"Se produjo un error en Curvas de Aprendizaje y Validación RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 4 ---\n",
        "\n",
        "#            try:\n",
        "#                print(\"[DEBUG] Iniciando Bloque 4: Curvas de Aprendizaje y Validación RNN Optimizado\")\n",
        "#                import numpy as np\n",
        "#                from sklearn.model_selection import learning_curve, validation_curve\n",
        "#                from sklearn.pipeline import make_pipeline\n",
        "#                from sklearn.compose import TransformedTargetRegressor\n",
        "#                from sklearn.preprocessing import StandardScaler\n",
        "#                import matplotlib.pyplot as plt#\n",
        "\n",
        "#                # --- a) Repetir selección del mejor modelo ---\n",
        "#                valid_engines = {'RandomSearch','Bayesian','Hyperband','Optuna'}\n",
        "#                df_sel = pd.DataFrame([\n",
        "#                    {'Método': k[1], 'Motor': k[2], 'Score': v['score'], 'Métrica': v['metric']}\n",
        "#                    for k,v in OPT_MODELS.items()\n",
        "#                    if k[0]=='rnn' and k[2] in valid_engines\n",
        "#                ])\n",
        "#                if df_sel['Métrica'].str.upper().iloc[0] == 'R2':\n",
        "#                    idx_best = df_sel['Score'].idxmax()\n",
        "#                else:\n",
        "#                    idx_best = df_sel['Score'].idxmin()\n",
        "#                best_row = df_sel.loc[idx_best]\n",
        "\n",
        "#                # --- c) Reconstruir los hiperparámetros óptimos ---\n",
        "#                p = _normalize_payload(payload_raw)\n",
        "#                #best_params = p['best_params']\n",
        "#                best_params = p.get('best_params', {})\n",
        "#                # --- b) Recuperar payload y metadatos ---\n",
        "#                model_rnn     = p['model']\n",
        "#                sx_rnn, sy_rnn= p['sx'], p['sy']\n",
        "#                cols_rnn      = p['cols']\n",
        "\n",
        "#                print(\"[DEBUG] Payload normalizado keys:\", list(p.keys()))\n",
        "#                print(\"[DEBUG] best_params keys:\", list(p.get('best_params', {}).keys()))\n",
        "#                print(\"[DEBUG] best_params keys:\", list(best_params.keys()))\n",
        "\n",
        "#                if not best_params:\n",
        "#                    best_params = model_rnn.get_params()\n",
        "#                    print(\"[DEBUG] Se han obtenido best_params desde model.get_params()\")\n",
        "\n",
        "#                # --- EXTRAER params desde el modelo entrenado ---\n",
        "#                params = model_rnn.get_params()\n",
        "#                print(\"[DEBUG] params keys:\", list(params.keys()))\n",
        "#                units  = params['units']\n",
        "\n",
        "#                if \"units\" not in params:\n",
        "#                    raise ValueError(\"[ERROR] El modelo no contiene la clave 'units' en sus parámetros.\")\n",
        "\n",
        "#                cols_rnn    = p['cols']\n",
        "#                model_rnn   = p['model']\n",
        "#                sx_rnn, sy_rnn = p['sx'], p['sy']\n",
        "\n",
        "#                # --- d) Creamos el Pipeline + TTR para escalar X e Y automáticamente ---\n",
        "#                rnn_pipe = make_pipeline(\n",
        "#                    StandardScaler(),\n",
        "#                    RNNRegressor(**params)\n",
        "#                )\n",
        "#                rnn_ttr = TransformedTargetRegressor(regressor=rnn_pipe, transformer=StandardScaler())\n",
        "\n",
        "#                #X_train_sel = X_train[cols].values\n",
        "#                #y_train     = Y_train.values.ravel()\n",
        "#                X_train_sel = X_train[cols_rnn].values[:300]  # ⚠️ limitar tamaño\n",
        "#                y_train = Y_train.values.ravel()[:300]        # ⚠️ limitar tamaño\n",
        "\n",
        "#                # ---- 4.1 Curva de Aprendizaje (R²) ----\n",
        "#                from joblib import parallel_backend             # NUEVO para asegurar que se completa el bloque 4\n",
        "#                with parallel_backend('threading'):             # NUEVO para asegurar que se completa el bloque 4\n",
        "#                    train_sizes, train_scores, val_scores = learning_curve(\n",
        "#                        rnn_ttr, X_train_sel, y_train,\n",
        "#                        #cv=5,\n",
        "#                        cv=2,  # ⚠️ menos folds\n",
        "#                        scoring='r2',\n",
        "#                        #train_sizes=np.linspace(0.1,1.0,5),\n",
        "#                        train_sizes=np.linspace(0.1, 1.0, 2),  # ⚠️ menos tamaños\n",
        "#                    #    n_jobs=-1\n",
        "#                        n_jobs=1    # o incluso omite n_jobs para que sea secuencial\n",
        "#                    )\n",
        "#                train_mean = np.mean(train_scores, axis=1)\n",
        "#                val_mean   = np.mean(val_scores,   axis=1)\n",
        "\n",
        "#                fig_lc, ax_lc = plt.subplots(figsize=(6,4))\n",
        "#                ax_lc.plot(train_sizes, train_mean, 'o-', label='Train R²')\n",
        "#                ax_lc.plot(train_sizes, val_mean,   'o-', label='CV R²')\n",
        "#                ax_lc.set_title('RNN Optimizado: Curva de Aprendizaje')\n",
        "#                ax_lc.set_xlabel('Tamaño del set de entrenamiento')\n",
        "#                ax_lc.set_ylabel('R²')\n",
        "#                ax_lc.legend()\n",
        "#                self.sections.append((\n",
        "#                    '### RNN Optimizado: Curva de Aprendizaje', fig_lc\n",
        "#                ))\n",
        "\n",
        "#                # ---- 4.2 Curva de Validación para “units” ----\n",
        "#                # Rango centrado en el valor óptimo\n",
        "#                #units_range = np.unique(np.linspace(\n",
        "#                #    max(1, best_params['units']//2),\n",
        "#                #    best_params['units']*2,\n",
        "#                #    5, dtype=int\n",
        "#                #))\n",
        "#                if 'units' not in best_params:\n",
        "#                    raise KeyError(\"[ERROR] El parámetro 'units' no está presente en best_params. Claves disponibles: \" + str(list(best_params.keys())))\n",
        "\n",
        "#                #units_range = np.unique(np.linspace(\n",
        "#                #    max(1, best_params['units']//2),\n",
        "#                #    best_params['units']*2,\n",
        "#                #    5, dtype=int\n",
        "#                #))\n",
        "#                # AÑADIDO PARA ALIGERAR LOS CICLOS DE VALIDACION  Y APRENDIZAJE\n",
        "#                units_range = np.unique(np.linspace(\n",
        "#                    max(1, params['units']*0.8),\n",
        "#                    params['units']*1.2,\n",
        "#                    3, dtype=int\n",
        "#                ))\n",
        "\n",
        "#                #fig_vc_u, tu, vu = validation_curve(\n",
        "#                tu, vu = validation_curve(\n",
        "#                    rnn_ttr, X_train_sel, y_train,\n",
        "#                    param_name='regressor__rnnregressor__units',\n",
        "#                    param_range=units_range,\n",
        "#                    cv=5,\n",
        "#                    scoring='r2',\n",
        "#                    n_jobs=-1\n",
        "#                )\n",
        "#                fig_uvc, ax_uvc = plt.subplots(figsize=(6,4))\n",
        "#                ax_uvc.plot(units_range, np.mean(tu,axis=1), 'o-', label='Train R²')\n",
        "#                ax_uvc.plot(units_range, np.mean(vu,axis=1), 'o-', label='CV R²')\n",
        "#                ax_uvc.set_title('RNN Optimizado: Curva de Validación units')\n",
        "#                ax_uvc.set_xlabel('units')\n",
        "#                ax_uvc.set_ylabel('R²')\n",
        "#                ax_uvc.legend()\n",
        "#                self.sections.append((\n",
        "#                    '### RNN Optimizado: Curva de Validación units', fig_uvc\n",
        "#                ))\n",
        "\n",
        "#                # ---- 4.3 Curva de Validación para “dropout_rate” ----\n",
        "#                dr = best_params['dropout_rate']\n",
        "#                #dr_range = np.linspace(max(0.0, dr-0.2), min(1.0, dr+0.2), 5)\n",
        "#                dr_range = np.linspace(max(0.0, dr-0.1), min(1.0, dr+0.1), 3)   # AÑADIDO PARA ALIGERAR LOS CICLOS DE ENTRENAMIENTO CRUZADO\n",
        "#                #fig_vc_d, td, vd = validation_curve(\n",
        "#                td, vd = validation_curve(\n",
        "#                    rnn_ttr, X_train_sel, y_train,\n",
        "#                    param_name='regressor__rnnregressor__dropout_rate',\n",
        "#                    param_range=dr_range,\n",
        "#                    cv=5,\n",
        "#                    scoring='r2',\n",
        "#                    n_jobs=-1\n",
        "#                )\n",
        "#                fig_dvc, ax_dvc = plt.subplots(figsize=(6,4))\n",
        "#                ax_dvc.plot(dr_range, np.mean(td,axis=1), 'o-', label='Train R²')\n",
        "#                ax_dvc.plot(dr_range, np.mean(vd,axis=1), 'o-', label='CV R²')\n",
        "#                ax_dvc.set_title('RNN Optimizado: Curva de Validación dropout_rate')\n",
        "#                ax_dvc.set_xlabel('dropout_rate')\n",
        "#                ax_dvc.set_ylabel('R²')\n",
        "#                ax_dvc.legend()\n",
        "#                self.sections.append((\n",
        "#                    '### RNN Optimizado: Curva de Validación dropout_rate', fig_dvc\n",
        "#                ))\n",
        "\n",
        "#                # ---- 4.4 Análisis Generativo IA de las Curvas ----\n",
        "#                prompt = (\n",
        "#                    f\"Para la RNN optimizada (Método={best_row['Método']}, Motor={best_row['Motor']}):\\n\"\n",
        "#                    f\"- Curva de Aprendizaje: tamaños={train_sizes.tolist()}, train R²={train_mean.tolist()}, cv R²={val_mean.tolist()}\\n\"\n",
        "#                    f\"- Validación units: rango={units_range.tolist()}, train={np.mean(tu,axis=1).tolist()}, cv={np.mean(vu,axis=1).tolist()}\\n\"\n",
        "#                    f\"- Validación dropout_rate: rango={dr_range.tolist()}, train={np.mean(td,axis=1).tolist()}, cv={np.mean(vd,axis=1).tolist()}\\n\\n\"\n",
        "#                    \"1. Identifica underfitting o overfitting en cada curva.\\n\"\n",
        "#                    \"2. Señala brechas clave y su impacto en la generalización.\\n\"\n",
        "#                    \"3. Recomienda ajustes de units y dropout_rate.\"\n",
        "#                )\n",
        "#                print(\"[DEBUG] Llamando a OpenAI para análisis de Curvas de Aprendizaje y Validación RNN\")\n",
        "#                analysis = call_openai_explanation(prompt)\n",
        "#                self.sections.append((\n",
        "#                    '### RNN Optimizado: Interpretación IA de Curvas', analysis\n",
        "#                ))\n",
        "\n",
        "#            except Exception as e:\n",
        "#                print(\"[DEBUG] Error en Bloque 4 Optimización RNN:\", e)  # <--- NUEVO: para diagnóstico\n",
        "#                self.sections.append((\n",
        "#                    '### ⚠️ Error Bloque 4 Optimización RNN',\n",
        "#                    f\"Se produjo un error en Curvas de Aprendizaje y Validación RNN: {e}\"\n",
        "#                ))\n",
        "            # --- Fin Bloque 4 ---\n",
        "\n",
        "            # --- 5. Curvas de Calibración y Predicción de Intervalos para RNN Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.19. Iniciando Bloque 5: Curvas de Calibración y Predicción de Intervalos RNN Optimizado\")\n",
        "\n",
        "                from sklearn.calibration import calibration_curve\n",
        "                import pandas as _pd\n",
        "                import numpy as np\n",
        "                import matplotlib.pyplot as plt\n",
        "\n",
        "                # 5.1) Recuperar mejor modelo y scalers\n",
        "                valid_engines = {'RandomSearch','Bayesian','Hyperband','Optuna'}\n",
        "                df_sel = pd.DataFrame([\n",
        "                    {'Método': k[1], 'Motor': k[2], 'Score': v['score'], 'Métrica': v['metric']}\n",
        "                    for k,v in OPT_MODELS.items()\n",
        "                    if k[0]=='rnn' and k[2] in valid_engines\n",
        "                ])\n",
        "                if df_sel['Métrica'].str.upper().iloc[0] == 'R2':\n",
        "                    idx_best = df_sel['Score'].idxmax()\n",
        "                else:\n",
        "                    idx_best = df_sel['Score'].idxmin()\n",
        "                best_row = df_sel.loc[idx_best]\n",
        "\n",
        "                key     = ('rnn', best_row['Método'], best_row['Motor'])\n",
        "                payload_raw = OPT_MODELS[('rnn', best_row['Método'], best_row['Motor'])]\n",
        "                p           = _normalize_payload(payload_raw)\n",
        "\n",
        "                model_rnn   = p['model']\n",
        "                sx_rnn, sy_rnn = p['sx'], p['sy']\n",
        "                cols_rnn    = p['cols']\n",
        "\n",
        "                # --- Bloque 5: normalizar payload ---\n",
        "                payload_raw = OPT_MODELS[('rnn', best_row['Método'], best_row['Motor'])]\n",
        "                p           = _normalize_payload(payload_raw)\n",
        "\n",
        "                model_rnn   = p['model']\n",
        "                sx_rnn, sy_rnn = p['sx'], p['sy']\n",
        "                cols_rnn    = p['cols']\n",
        "\n",
        "                # 5.2) Preparar datos de test y predecir\n",
        "                X_test_sel = X_test[cols].copy()\n",
        "                y_true     = Y_test.values.ravel()\n",
        "\n",
        "                # tu wrapper ya escala / reshape / inverse_transform internamente\n",
        "                y_pred = model_rnn.predict(X_test_sel)\n",
        "\n",
        "                # 5.3) Curva de calibración (regresión “reliability”)\n",
        "                bins = 10\n",
        "                df_cal = _pd.DataFrame({'y_pred': y_pred, 'y_true': y_true})\n",
        "                try:\n",
        "                    df_cal['bin'] = _pd.qcut(df_cal['y_pred'], q=bins, duplicates='drop')\n",
        "                except ValueError:\n",
        "                    df_cal['bin'] = _pd.cut(df_cal['y_pred'], bins=bins)\n",
        "                grp = df_cal.groupby('bin', observed=True).agg({'y_pred':'mean','y_true':'mean'})\n",
        "                prob_pred, prob_true = grp['y_pred'].values, grp['y_true'].values\n",
        "\n",
        "                fig_cal, ax_cal = plt.subplots(figsize=(6,4))\n",
        "                ax_cal.plot(prob_pred, prob_true, marker='o', linewidth=2, label='Calibración')\n",
        "                ax_cal.plot([prob_pred.min(), prob_pred.max()],\n",
        "                            [prob_pred.min(), prob_pred.max()],\n",
        "                            'k--', label='Ideal')\n",
        "                ax_cal.set_xlabel('Predicción promedio en bin')\n",
        "                ax_cal.set_ylabel('Valor real promedio')\n",
        "                ax_cal.set_title('RNN Optimizado: Curva de Calibración')\n",
        "                ax_cal.legend()\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Curva de Calibración\",\n",
        "                    fig_cal\n",
        "                ))\n",
        "\n",
        "                # 5.4) Intervalos de predicción ±1 STD de residuo\n",
        "                residuals = y_true - y_pred\n",
        "                std_res   = np.std(residuals)\n",
        "                upper     = y_pred + std_res\n",
        "                lower     = y_pred - std_res\n",
        "\n",
        "                fig_int, ax_int = plt.subplots(figsize=(6,4))\n",
        "                ax_int.plot(y_true,     label='Y real')\n",
        "                ax_int.plot(y_pred,     label='Predicción')\n",
        "                ax_int.fill_between(\n",
        "                    np.arange(len(y_pred)),\n",
        "                    lower, upper,\n",
        "                    alpha=0.3, label='±1 STD residuo'\n",
        "                )\n",
        "                ax_int.set_xlabel('Índice de muestra')\n",
        "                ax_int.set_ylabel('Valor')\n",
        "                ax_int.set_title('RNN Optimizado: Intervalos de Predicción')\n",
        "                ax_int.legend()\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Intervalos de Predicción\",\n",
        "                    fig_int\n",
        "                ))\n",
        "\n",
        "                # 5.5) Análisis generativo IA de incertidumbre y calibración\n",
        "                print(\"[DEBUG] 16.20. Llamando IA para análisis de incertidumbre y calibración RNN optimizado\")\n",
        "                prompt_ci_rnn = (\n",
        "                    f\"Para la RNN optimizada (Método={best_row['Método']}, Motor={best_row['Motor']}):\\n\"\n",
        "                    f\"- Calibración (bins={bins}): predicciones promedio={prob_pred.tolist()}, valores reales promedio={prob_true.tolist()}\\n\"\n",
        "                    f\"- Residuo estándar={std_res:.4f}\\n\\n\"\n",
        "                    \"1. Evalúa la fiabilidad de la curva de calibración: donde el modelo está sub- o sobre-calibrado.\\n\"\n",
        "                    \"2. Comenta sobre la amplitud de los intervalos de predicción y su adecuación.\\n\"\n",
        "                    \"3. Sugiere mejoras para la calibración y estimación de la incertidumbre en la RNN.\"\n",
        "                )\n",
        "                analysis_ci_rnn = call_openai_explanation(prompt_ci_rnn)\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Análisis de Incertidumbre y Calibración\",\n",
        "                    analysis_ci_rnn\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                self.sections.append((\n",
        "                    \"### ⚠️ Error Bloque 5 Optimización RNN\",\n",
        "                    f\"Se produjo un error en Curvas de Calibración y Predicción de Intervalos RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 5 ---\n",
        "\n",
        "            # --- 6. Resumen Ejecutivo y Road-Map de Siguientes Pasos para RNN Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 16.21. Iniciando bloque de Resumen Ejecutivo y Road-Map para RNN optimizado\")\n",
        "\n",
        "                # Variables clave (ya definidas en bloques anteriores)\n",
        "                sel_rnn        = best_row['Método']\n",
        "                eng_rnn        = best_row['Motor']\n",
        "                best_score_rnn = best_row['Score']\n",
        "\n",
        "                # Desviación de R2 en CV (bloque 3) y estadísticas de residuos (bloque 5)\n",
        "                cv_std_r2      = float(np.std(r2_scores))    # r2_scores viene de bloque 3\n",
        "                res_std_rnn    = float(std_res)              # std_res viene de bloque 5\n",
        "                res_kurt_rnn   = float(kurtosis(residuals))  # residuals viene de bloque 5\n",
        "                q25_rnn, q50_rnn, q75_rnn = [\n",
        "                    float(x) for x in np.quantile(residuals, [0.25, 0.5, 0.75])\n",
        "                ]\n",
        "\n",
        "                # Markdown resumen\n",
        "                summary_md_rnn = (\n",
        "                    \"**Puntos Clave Optimización RNN:**\\n\"\n",
        "                    f\"- **Mejor combinación:** Método `{sel_rnn}`, motor `{eng_rnn}` ➜ **Score** = {best_score_rnn:.4f}\\n\"\n",
        "                    f\"- **Robustez del modelo:** desviación estándar CV R² = {cv_std_r2:.4f}, std residuos = {res_std_rnn:.4f}\\n\"\n",
        "                    f\"- **Curtosis de residuos:** {res_kurt_rnn:.4f}, quantiles (25%,50%,75%) = \"\n",
        "                    f\"({q25_rnn:.4f}, {q50_rnn:.4f}, {q75_rnn:.4f})\\n\"\n",
        "                    \"- **Recomendaciones inmediatas:**\\n\"\n",
        "                    \"  1. Ajustar unidades (`units`) y tasa de dropout para controlar varianza.\\n\"\n",
        "                    \"  2. Incorporar learning-rate scheduling y early stopping.\\n\"\n",
        "                    \"  3. Aumentar el set de entrenamiento o aplicar data augmentation en series temporales.\\n\"\n",
        "                    \"  4. Experimentar con arquitecturas híbridas (LSTM bidireccional, Attention).\\n\"\n",
        "                )\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Resumen Ejecutivo y Road-Map\",\n",
        "                    summary_md_rnn\n",
        "                ))\n",
        "\n",
        "                # Llamada a IA para un “Resumen Ejecutivo IA”\n",
        "                print(\"[DEBUG] 16.22. Llamando IA para Resumen Ejecutivo y Road-Map RNN optimizado\")\n",
        "                prompt_exec_rnn = (\n",
        "                    \"Eres un investigador de Deep Learning avanzado. Basándote en estos puntos clave:\\n\"\n",
        "                    f\"{summary_md_rnn}\\n\\n\"\n",
        "                    \"1. Desarrolla un análisis detallado en párrafos separados para cada punto clave.\\n\"\n",
        "                    \"2. Propón un plan de acción priorizado de 3–5 pasos claros para stakeholders.\\n\"\n",
        "                    \"3. Finaliza con un breve resumen ejecutivo de 2–3 párrafos enfatizando impacto y próximos hitos.\"\n",
        "                )\n",
        "                analysis_exec_rnn = call_openai_explanation(prompt_exec_rnn)\n",
        "                self.sections.append((\n",
        "                    \"### RNN Optimizado: Resumen Ejecutivo IA\",\n",
        "                    analysis_exec_rnn\n",
        "                ))\n",
        "\n",
        "            except Exception as e:\n",
        "                self.sections.append((\n",
        "                    \"### ⚠️ Error Bloque 6 Optimización RNN\",\n",
        "                    f\"Se produjo un error en Resumen Ejecutivo y Road-Map RNN: {e}\"\n",
        "                ))\n",
        "            # --- Fin Bloque 6 ---\n",
        "\n",
        "        except Exception as e:\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error Bloque 0 Optimización RNN\",\n",
        "                f\"Se produjo un error al generar el resumen de optimización RNN: {e}\"\n",
        "            ))\n",
        "\n",
        "        # =============================================================================\n",
        "        # 17. Análisis e Interpretación del Modelo Óptimo\n",
        "        # =============================================================================\n",
        "        #try:\n",
        "        #    print(\"[DEBUG] Iniciando sección Selección Integral de Modelo\")\n",
        "        try:\n",
        "            print(\"[DEBUG] 17.1.Iniciando sección Selección Integral de Modelo\")\n",
        "            # DEBUG: ¿Qué claves hay en OPT_MODELS?\n",
        "            print(f\"[DEBUG] 17.2. OPT_MODELS tiene {len(OPT_MODELS)} entradas: {list(OPT_MODELS.keys())}\")\n",
        "            records = []\n",
        "            print(\"[DEBUG] 17.3. records inicializado vacío\")\n",
        "            import numpy as np\n",
        "            import pandas as pd\n",
        "            from sklearn.model_selection      import cross_validate\n",
        "            from sklearn.metrics              import mean_squared_error, mean_absolute_error, r2_score\n",
        "            #from sklearn.calibration          import calibration_curve\n",
        "            import scipy.stats                as st\n",
        "            from tensorflow.keras.models      import load_model\n",
        "\n",
        "            records = []\n",
        "\n",
        "            # --- Bloque 1: Construcción y presentación de la Tabla de Puntuación de los Modelos Optimizados ---\n",
        "            for (mt, method, engine), payload in OPT_MODELS.items():\n",
        "                if mt not in {'svr','nn','xgb','rf','rnn'}:\n",
        "                    continue\n",
        "                # --- cargar datos test y scalers ---\n",
        "                cols = payload.get('cols', X_test.columns.tolist())\n",
        "                X_test_sel = X_test[cols].copy()\n",
        "                y_true     = Y_test.values.ravel()\n",
        "\n",
        "                sx = payload.get('sx')\n",
        "                if sx is not None:\n",
        "                    #X_test_scaled = sx.transform(X_test_sel.values)\n",
        "                    X_test_scaled = sx.transform(pd.DataFrame(X_test_sel.values, columns=X_test_sel.columns))       # EVITA WARNING EN LA EJECUCION - NO PARA EJECUCION. SI DA PROBLEMAS ELIMINAR ESTA LINEA.\n",
        "\n",
        "                else:\n",
        "                    X_test_scaled = X_test_sel.values\n",
        "\n",
        "                # --- predicciones y métricas test ---\n",
        "                if mt == 'rnn':\n",
        "                    # tu wrapper ya escala, reshape e inverse_transform\n",
        "                    model = payload['model']\n",
        "                    y_pred = model.predict(X_test_sel)\n",
        "                else:\n",
        "                    model = payload['model']\n",
        "                    if mt in {'svr','xgb','rf','nn'}:\n",
        "                        # scikeras o sklearn\n",
        "                        y_pred_raw = model.predict(X_test_scaled)\n",
        "                        sy = payload.get('sy')\n",
        "                        if sy is not None:\n",
        "                            y_pred = sy.inverse_transform(y_pred_raw.reshape(-1,1)).ravel()\n",
        "                        else:\n",
        "                            y_pred = y_pred_raw.ravel()\n",
        "                    else:\n",
        "                        y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "                mse_test   = mean_squared_error(y_true, y_pred)\n",
        "                mae_test   = mean_absolute_error(y_true, y_pred)\n",
        "                rmse_test  = np.sqrt(mse_test)\n",
        "                resid      = y_true - y_pred\n",
        "                res_std    = np.std(resid)\n",
        "                res_kurt   = st.kurtosis(resid)\n",
        "\n",
        "                # --- validación cruzada 5 folds ---\n",
        "                # Construir X_train escalado\n",
        "                cols_tr = payload.get('cols', X_train.columns.tolist())\n",
        "                X_tr_sel = X_train[cols_tr].copy()\n",
        "                y_tr     = Y_train.values.ravel()\n",
        "                sx_tr    = payload.get('sx')\n",
        "                X_tr_s   = sx_tr.transform(X_tr_sel.values) if sx_tr is not None else X_tr_sel.values\n",
        "\n",
        "                from sklearn.model_selection import train_test_split\n",
        "\n",
        "                # Submuestreo del conjunto de entrenamiento\n",
        "                X_sub, _, y_sub, _ = train_test_split(\n",
        "                    X_tr_s, y_tr,\n",
        "                    train_size=0.3,  # usa solo el 30% de los datos para CV\n",
        "                    random_state=42,\n",
        "                    shuffle=True\n",
        "                )\n",
        "\n",
        "                # --- validación cruzada con protección para modelos no-sklean ---\n",
        "                try:\n",
        "                    from sklearn.model_selection import cross_validate\n",
        "                    cvres = cross_validate(\n",
        "                        #model, X_tr_s, y_tr,\n",
        "                        model, X_sub, y_sub,        # Usamos X_sub y y_sub en la validación cruzada en lugar del conjunto completo.\n",
        "                        #cv=5,\n",
        "                        cv=3,         # REDUCE CV FOLDS PARA MEJORAR LA RAPPIDEZ DE EJECUCION\n",
        "                        scoring={\n",
        "                            'r2':      'r2',\n",
        "                            'neg_mse': 'neg_mean_squared_error',\n",
        "                            'neg_mae': 'neg_mean_absolute_error'\n",
        "                        },\n",
        "                        return_train_score=False,\n",
        "                        n_jobs=-1  # ← paraleliza en todos los cores\n",
        "                    )\n",
        "                    r2_folds  = cvres['test_r2']\n",
        "                    mse_folds = [-v for v in cvres['test_neg_mse']]\n",
        "                    mae_folds = [-v for v in cvres['test_neg_mae']]\n",
        "                    r2_cv_std    = float(np.std(r2_folds))\n",
        "                    mse_cv_mean  = float(np.mean(mse_folds))\n",
        "                    mae_cv_mean  = float(np.mean(mae_folds))\n",
        "                    rmse_cv_mean = float(np.mean(np.sqrt(mse_folds)))\n",
        "                except TypeError as ex:\n",
        "                    # no SKL estimator (e.g. keras.Sequential), omitimos CV\n",
        "                    print(f\"[DEBUG] CV omitido por TypeError: {ex}\")\n",
        "                    r2_folds, mse_folds, mae_folds = [], [], []\n",
        "                    r2_cv_std    = np.nan\n",
        "                    mse_cv_mean  = np.nan\n",
        "                    mae_cv_mean  = np.nan\n",
        "                    rmse_cv_mean = np.nan\n",
        "\n",
        "                # --- calibración manual para regresión ---\n",
        "                import pandas as pd\n",
        "                # un DataFrame para agrupar por deciles de y_pred\n",
        "                df_cal = pd.DataFrame({'y_pred': y_pred, 'y_true': y_true})\n",
        "                # cortamos en 10 bins por cuantiles (evita duplicados)\n",
        "                try:\n",
        "                    #df_cal['bin'] = pd.qcut(df_cal['y_pred'], q=10, duplicates='drop')\n",
        "                    df_cal['bin'] = pd.qcut(df_cal['y_pred'], q=5, duplicates='drop')   # o q=4 si quieres aún más velocidad\n",
        "                except ValueError:\n",
        "                    #df_cal['bin'] = pd.cut(df_cal['y_pred'], bins=10)\n",
        "                    df_cal['bin'] = pd.cut(df_cal['y_pred'], bins=5)                    # o q=4 si quieres aún más velocidad\n",
        "                # agrupamos: media predicha vs media real\n",
        "                grp = df_cal.groupby('bin', observed=True).agg({'y_pred':'mean','y_true':'mean'})\n",
        "                prob_pred = grp['y_pred'].values\n",
        "                prob_true = grp['y_true'].values\n",
        "                # error medio de calibración\n",
        "                cal_err = float((np.abs(prob_pred - prob_true)).mean())\n",
        "\n",
        "                # --- intervalos de predicción ±1 STD residuo ---\n",
        "                intervals_std = res_std\n",
        "\n",
        "                # --- métrica principal (score) ---\n",
        "                score = payload.get('score', r2_score(y_true, y_pred))\n",
        "\n",
        "                # --- hiperparámetros optimizados ---\n",
        "                best_params = payload.get('best_params') or payload.get('params') or {}\n",
        "                hp_flat = { f\"hp_{k}\": v for k, v in best_params.items() }\n",
        "\n",
        "                # --- armar registro ---\n",
        "                rec = {\n",
        "                    'modelo':        f\"{mt}-{method}-{engine}\",\n",
        "                    'score':         score,\n",
        "                    'r2_cv_std':     r2_cv_std,\n",
        "                    'mse_cv':        mse_cv_mean,\n",
        "                    'mae_cv':        mae_cv_mean,\n",
        "                    'rmse_cv':       rmse_cv_mean,\n",
        "                    'mse_test':      mse_test,\n",
        "                    'mae_test':      mae_test,\n",
        "                    'rmse_test':     rmse_test,\n",
        "                    'res_std':       res_std,\n",
        "                    'res_kurt':      res_kurt,\n",
        "                    'intervals_std': intervals_std,\n",
        "                    'cal_err':       cal_err,\n",
        "                    **hp_flat\n",
        "                }\n",
        "                records.append(rec)\n",
        "\n",
        "            #df = pd.DataFrame(records)\n",
        "\n",
        "            print(f\"[DEBUG] 17.4. records tendrá {len(records)} elementos\")\n",
        "            if records:\n",
        "                print(f\"[DEBUG] 17.5. ejemplar de record[0]: {records[0]}\")\n",
        "            df = pd.DataFrame(records)\n",
        "            print(f\"[DEBUG] 17.6. DataFrame creado con shape={df.shape}\")\n",
        "\n",
        "            # 0) Extraer tipo, método y motor de la columna 'modelo'\n",
        "            df[['tipo','metodo','motor']] = df['modelo'].str.split('-', n=2, expand=True)\n",
        "\n",
        "            # 1) Clonar df y reiniciar índice\n",
        "            df_results = df.copy()\n",
        "            df_results.reset_index(drop=True, inplace=True)\n",
        "            # Definimos aquí la puntuación global bruta\n",
        "            df_results['puntuacion_global'] = df_results['score']\n",
        "\n",
        "            # Bloque de normalización absoluta lineal y cálculo de puntuación\n",
        "            import pandas as pd\n",
        "            import numpy as np\n",
        "\n",
        "            # Definir funciones de normalización\n",
        "            def normalize_high(df, col):\n",
        "                lo, hi = df[col].min(), df[col].max()\n",
        "                if hi == lo:\n",
        "                    return pd.Series(10.0, index=df.index)\n",
        "                return ((df[col] - lo) / (hi - lo) * 10).clip(0, 10)\n",
        "\n",
        "            def normalize_low(df, col):\n",
        "                lo, hi = df[col].min(), df[col].max()\n",
        "                if hi == lo:\n",
        "                    return pd.Series(10.0, index=df.index)\n",
        "                return ((hi - df[col]) / (hi - lo) * 10).clip(0, 10)\n",
        "\n",
        "            # Supongamos que ya tienes df_results con la columna 'puntuacion_global'\n",
        "            # y las métricas crudas: r2_cv_std, mse_cv, mae_cv, rmse_cv, mse_test, mae_test,\n",
        "            # rmse_test, res_std, res_kurt, intervals_std, cal_err\n",
        "\n",
        "            # Columnas “mayor = mejor”\n",
        "            cols_high = ['puntuacion_global']\n",
        "\n",
        "            # Columnas “menor = mejor”\n",
        "            cols_low = [\n",
        "                'r2_cv_std', 'mse_cv', 'mae_cv', 'rmse_cv',\n",
        "                'mse_test', 'mae_test', 'rmse_test',\n",
        "                'res_std', 'res_kurt', 'intervals_std', 'cal_err'\n",
        "            ]\n",
        "\n",
        "            # 1) Normalizar cada métrica a escala 0–10\n",
        "            for c in cols_high:\n",
        "                df_results[f'v_{c}'] = normalize_high(df_results, c)\n",
        "\n",
        "            for c in cols_low:\n",
        "                df_results[f'v_{c}'] = normalize_low(df_results, c)\n",
        "\n",
        "            # 2) Definir pesos (ajusta según IA o tu criterio)\n",
        "            pesos = {\n",
        "                'v_puntuacion_global': 0.2273,\n",
        "                'v_r2_cv_std':         0.0909,\n",
        "                'v_mse_cv':            0.0909,\n",
        "                'v_mae_cv':            0.0455,\n",
        "                'v_rmse_cv':           0.0455,\n",
        "                'v_mse_test':          0.0909,\n",
        "                'v_mae_test':          0.0909,\n",
        "                'v_rmse_test':         0.0455,\n",
        "                'v_res_std':           0.0909,\n",
        "                'v_res_kurt':          0.0455,\n",
        "                'v_intervals_std':     0.0455,\n",
        "                'v_cal_err':           0.0909,\n",
        "            }\n",
        "\n",
        "            # 3) Calcular la puntuación global final\n",
        "            df_results['puntuacion_global_final'] = 0.0\n",
        "            for vcol, w in pesos.items():\n",
        "                df_results['puntuacion_global_final'] += df_results[vcol] * w\n",
        "\n",
        "            # 4) Reordenar columnas para presentación\n",
        "            #cols_order = [\n",
        "            #    'modelo', 'tipo', 'metodo', 'motor', 'puntuacion_global'\n",
        "            #] + list(pesos.keys()) + ['puntuacion_global_final']\n",
        "\n",
        "            # 4) Reordenar columnas para presentación\n",
        "            cols_order = [\n",
        "                'modelo', 'tipo', 'metodo', 'motor', 'puntuacion_global',\n",
        "                # métricas crudas\n",
        "                'r2_cv_std','mse_cv','mae_cv','rmse_cv',\n",
        "                'mse_test','mae_test','rmse_test',\n",
        "                'res_std','res_kurt','intervals_std','cal_err',\n",
        "            ] + list(pesos.keys()) + ['puntuacion_global_final']\n",
        "\n",
        "            df_results = df_results[cols_order]\n",
        "\n",
        "            # 5) Añadir la sección al informe\n",
        "            #self.sections.append((\n",
        "            #    '### Selección Integral de Modelo: Tabla de Puntuaciones',\n",
        "            #    df_results.reset_index(drop=True)\n",
        "            #))\n",
        "            print(\"[DEBUG] 17.7. Añadiendo sección de Tabla de Puntuaciones\")\n",
        "            self.sections.append((\n",
        "                '### Selección Integral de Modelo: Tabla de Puntuaciones',\n",
        "                df_results.reset_index(drop=True)\n",
        "            ))\n",
        "            print(\"[DEBUG] 17.8. Sección Tabla de Puntuaciones añadida correctamente\")\n",
        "\n",
        "            # --- Ahora identificamos el mejor y guardamos atributos ---\n",
        "            best_idx = df_results['puntuacion_global_final'].idxmax()\n",
        "            best     = df_results.loc[best_idx]\n",
        "\n",
        "            # Guardamos la info del mejor modelo\n",
        "            self.best_model_info = {\n",
        "                'metodo': best['metodo'],\n",
        "                'motor':  best['motor'],\n",
        "                'score':  best['puntuacion_global_final']\n",
        "            }\n",
        "\n",
        "            # **Guardamos también las métricas que necesitarás más adelante**:\n",
        "            self.r2_cv_std = float(best['r2_cv_std'])\n",
        "            self.res_std   = float(best['res_std'])\n",
        "            self.cal_err   = float(best['cal_err'])\n",
        "\n",
        "            print(f\"[DEBUG] 17.9. Mejor modelo: {self.best_model_info}, \"\n",
        "                  f\"r2_cv_std={self.r2_cv_std}, res_std={self.res_std}, cal_err={self.cal_err}\")\n",
        "            # --- Fin Bloque 1 ---\n",
        "\n",
        "            # --- Bloque 2 - Gráfico de puntuaciones globales y Cuadrante Mágico de Modelos Optimizados ----\n",
        "            import matplotlib.pyplot as plt\n",
        "\n",
        "            # 1) Cerramos TODO lo que haya quedado abierto de la optimización\n",
        "            #plt.close('all')\n",
        "\n",
        "            # --- Gráfico de Barras: Puntuación Global de cada modelo ---\n",
        "            def plot_global_scores(df):\n",
        "                \"\"\"\n",
        "                Gráfico de barras profesional con la puntuación global de cada modelo.\n",
        "                - df debe tener columnas ['modelo', 'puntuacion_global'].\n",
        "                \"\"\"\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                modelos = df['modelo']\n",
        "                scores = df['puntuacion_global_final']\n",
        "\n",
        "                # Usa una paleta profesional\n",
        "                palette = plt.get_cmap('tab20').colors\n",
        "                ax.bar(modelos, scores, color=palette[:len(modelos)], edgecolor='black')\n",
        "\n",
        "                # Etiquetas y estilo\n",
        "                ax.set_title('Comparación de Modelos: Puntuación Global', fontsize=16, fontweight='bold')\n",
        "                ax.set_xlabel('Modelo', fontsize=14)\n",
        "                ax.set_ylabel('Puntuación Global', fontsize=14)\n",
        "                ax.set_xticks(modelos)\n",
        "                ax.set_xticklabels(modelos, rotation=45, ha='right', fontsize=12)\n",
        "                ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "                plt.tight_layout()\n",
        "                #plt.close(fig)\n",
        "                return fig\n",
        "\n",
        "            def plot_magic_quadrant(df):\n",
        "                \"\"\"\n",
        "                Magic Quadrant que sitúa cada modelo en robustez vs rendimiento.\n",
        "                - df debe tener columnas ['v_r2_cv_std', 'puntuacion_global_final', 'modelo'].\n",
        "                \"\"\"\n",
        "                # Robustez: normalizamos de 0–10 a 0–1\n",
        "                robustez = (df['v_r2_cv_std'] / 10).clip(0,1)\n",
        "\n",
        "                # Rendimiento: puntuación global final (0–10) también a 0–1\n",
        "                rendimiento = (df['puntuacion_global_final'] / 10).clip(0,1)\n",
        "\n",
        "                mask       = robustez.notna() & rendimiento.notna()\n",
        "                robustez   = robustez[mask]\n",
        "                rendimiento = rendimiento[mask]\n",
        "                labels     = df['modelo'][mask]\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(8, 8))\n",
        "                ax.scatter(robustez, rendimiento, s=150, edgecolor='k', alpha=0.8)\n",
        "\n",
        "                for x, y, label in zip(robustez, rendimiento, labels):\n",
        "                    ax.annotate(label,\n",
        "                                xy=(x, y),\n",
        "                                xytext=(5, 5),\n",
        "                                textcoords='offset points',\n",
        "                                fontsize=12)\n",
        "\n",
        "                # Líneas medias\n",
        "                ax.axvline(robustez.mean(),    color='grey', linestyle='--')\n",
        "                ax.axhline(rendimiento.mean(), color='grey', linestyle='--')\n",
        "\n",
        "                ax.set_title('Magic Quadrant de Modelos', fontsize=16, fontweight='bold')\n",
        "                ax.set_xlabel('Robustez (v_r2_cv_std / 10)', fontsize=14)\n",
        "                ax.set_ylabel('Rendimiento (puntuación global / 10)', fontsize=14)\n",
        "                ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "                # Límites de 0 a 1\n",
        "                ax.set_xlim(0, 1.05)\n",
        "                ax.set_ylim(0, 1.05)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                return fig\n",
        "\n",
        "\n",
        "            fig1 = plot_global_scores(df_results)\n",
        "            fig2 = plot_magic_quadrant(df_results)\n",
        "\n",
        "            self.sections.append(('### Comparación de Modelos: Puntuación Global', fig1))\n",
        "            self.sections.append(('### Magic Quadrant de Modelos', fig2))\n",
        "            # --- Fin Bloque 2 ---\n",
        "\n",
        "            # --- Bloque 3: Explicación profunda por IA y detalle final del modelo seleccionado ---\n",
        "            print(\"[DEBUG] 17.10. Iniciando Bloque 3: Explicación IA y detalle final\")\n",
        "\n",
        "            import openai\n",
        "            import pandas as pd\n",
        "\n",
        "            # 1) Serializamos la tabla de puntuaciones para inyectarla en el prompt\n",
        "            table_csv = df_results.to_csv(index=False)\n",
        "\n",
        "            print(\"[DEBUG] 17.11. Iniciando llamada a IA para análisis de la tabla y gráficas\")\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            He aquí los resultados completos de la **Selección Integral de Modelos**:\n",
        "\n",
        "            1) **Tabla de Puntuaciones** (CSV):\n",
        "            {table_csv}\n",
        "\n",
        "            2) **Gráfica de Barras** titulada \"Comparación de Modelos: Puntuación Global\".\n",
        "\n",
        "            3) **Magic Quadrant** titulado \"Magic Quadrant de Modelos\".\n",
        "\n",
        "            Por favor, realiza lo siguiente:\n",
        "\n",
        "            A) Explica **detalladamente** la tabla de puntuaciones:\n",
        "              - Cómo se ha estructurado,\n",
        "              - Qué representa cada columna de valor (tanto las métricas crudas como sus escalas 0–10),\n",
        "              - Cómo cada modelo se “sitúa” según esos valores,\n",
        "              - Comparativa explícita entre todos los modelos.\n",
        "\n",
        "            B) Comenta la **gráfica de barras**:\n",
        "              - Qué nos dice del rendimiento absoluto de cada modelo,\n",
        "              - Destaca los valores máximo y mínimo.\n",
        "\n",
        "            C) Interpreta el **Magic Quadrant**:\n",
        "              - Define en qué ejes está midiendo robustez y rendimiento,\n",
        "              - Señala qué cuadrante (alto–alto, alto–bajo, bajo–alto, bajo–bajo) es deseable,\n",
        "              - Identifica modelos “líderes” y “rezagados”.\n",
        "\n",
        "            D) Con todo lo anterior, **elige el mejor modelo optimizado**.\n",
        "              - Explica por qué (qué métricas y pesos lo han elevado al primer puesto),\n",
        "              - Enumera sus virtudes y posibles puntos débiles.\n",
        "\n",
        "            E) Tras esa elección, **construye una tabla** con todos los detalles del modelo seleccionado:\n",
        "              1. `modelo` (p.ej. “rf-pearson-randomsearch”)\n",
        "              2. `tipo` (RF, SVR, …)\n",
        "              3. `método` (Pearson, Boruta, …)\n",
        "              4. `motor` de optimización (RandomSearch, Optuna, …)\n",
        "              5. **Todos** sus hiperparámetros óptimos\n",
        "              6. **Listado** de variables X usadas\n",
        "              7. Resumen de sus **métricas crudas** y su `puntuacion_global_final`\n",
        "            \"\"\"\n",
        "\n",
        "            # 2) Llamada a OpenAI\n",
        "            resp = _client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[{\"role\":\"user\", \"content\": prompt}],\n",
        "                temperature=TEMPERATURE_VAL,\n",
        "                max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "            )\n",
        "            analysis_text = resp.choices[0].message.content\n",
        "\n",
        "            # 3) Insertamos el análisis como sección\n",
        "            self.sections.append((\n",
        "                \"### 📝 Explicación IA de la Selección Final\",\n",
        "                analysis_text\n",
        "            ))\n",
        "\n",
        "            # 4) Identificamos el mejor modelo en df_results\n",
        "            best_idx = df_results['puntuacion_global_final'].idxmax()\n",
        "            best     = df_results.loc[best_idx]\n",
        "            mt, method, engine = best['tipo'], best['metodo'], best['motor']\n",
        "\n",
        "            # Guardamos la info del mejor modelo para bloques posteriores\n",
        "            self.best_model_info = {\n",
        "                'metodo': best['metodo'],\n",
        "                'motor':  best['motor'],\n",
        "                'score':  best['puntuacion_global_final']\n",
        "            }\n",
        "\n",
        "            # 5) Recuperamos payload y detalles\n",
        "            payload  = OPT_MODELS[(mt, method, engine)]\n",
        "            hp       = payload.get('best_params', {}) or payload.get('params', {})\n",
        "            features = payload.get('cols', X_train.columns.tolist())\n",
        "\n",
        "            # 6) Preparamos el detalle en un DataFrame\n",
        "            metrics_keys = [\n",
        "                'r2_cv_std','mse_cv','mae_cv','rmse_cv',\n",
        "                'mse_test','mae_test','rmse_test',\n",
        "                'res_std','res_kurt','intervals_std','cal_err',\n",
        "                'puntuacion_global','puntuacion_global_final'\n",
        "            ]\n",
        "\n",
        "            detail_dict = {\n",
        "                'modelo':               best['modelo'],\n",
        "                'tipo':                 mt,\n",
        "                'metodo':               method,\n",
        "                'motor':                engine,\n",
        "                'variables_usadas':     \", \".join(features),\n",
        "                'puntuacion_final':  best['puntuacion_global_final'],\n",
        "            }\n",
        "            # añadimos hiperparámetros\n",
        "            for k, v in hp.items():\n",
        "                detail_dict[f\"hp_{k}\"] = v\n",
        "            # añadimos métricas\n",
        "            for k in metrics_keys:\n",
        "                detail_dict[k] = best[k]\n",
        "\n",
        "            detail_df = pd.DataFrame([detail_dict])\n",
        "\n",
        "            # 7) Insertamos la tabla de detalle\n",
        "            self.sections.append((\n",
        "                \"### Detalle del Modelo Seleccionado\",\n",
        "                detail_df.reset_index(drop=True)\n",
        "            ))\n",
        "            # --- Fin Bloque 3 ---\n",
        "\n",
        "            # --- Bloque 4: Interpretación xIA del Mejor Modelo Optimizado ---\n",
        "            try:\n",
        "                print(\"[DEBUG] 17.12. Iniciando Bloque 4: Interpretación xIA\")\n",
        "\n",
        "                import numpy as np\n",
        "                import pandas as pd\n",
        "                import shap\n",
        "                import matplotlib.pyplot as plt\n",
        "                import json\n",
        "                import warnings\n",
        "                from scipy.optimize import minimize\n",
        "                from sklearn.inspection import partial_dependence\n",
        "                from sklearn.inspection import (\n",
        "                    permutation_importance,\n",
        "                    PartialDependenceDisplay\n",
        "                )\n",
        "\n",
        "                # --- FUNCIÓN ROBUSTA PARA EXTRAER grid/curves DE partial_dependence ---\n",
        "                def extract_grid_and_curves(pd_res, kind=\"average\"):\n",
        "                    # 1. Si es Bunch (scikit-learn >= 0.24)\n",
        "                    if hasattr(pd_res, \"keys\"):\n",
        "                        keys = list(pd_res.keys())\n",
        "                        # PDP\n",
        "                        if kind == \"average\":\n",
        "                            # Puede que solo exista average/grid_values\n",
        "                            if \"average\" in keys and \"grid_values\" in keys:\n",
        "                                grid = pd_res[\"grid_values\"][0]\n",
        "                                avg = pd_res[\"average\"][0]\n",
        "                                return grid, avg\n",
        "                            else:\n",
        "                                raise RuntimeError(f\"PDP: No se encuentran 'average' o 'grid_values' en Bunch: {keys}\")\n",
        "                        # ICE\n",
        "                        elif kind == \"individual\":\n",
        "                            if \"individual\" in keys and \"grid_values\" in keys:\n",
        "                                grid = pd_res[\"grid_values\"][0]\n",
        "                                curves = pd_res[\"individual\"][0]\n",
        "                                return grid, curves\n",
        "                            else:\n",
        "                                raise RuntimeError(f\"ICE: No se encuentran 'individual' o 'grid_values' en Bunch: {keys}\")\n",
        "                        else:\n",
        "                            raise ValueError(f\"Tipo desconocido: {kind}\")\n",
        "\n",
        "                    # 2. Si es tuple o list (formato antiguo)\n",
        "                    elif isinstance(pd_res, (tuple, list)):\n",
        "                        if kind == \"average\":\n",
        "                            avg = pd_res[0]\n",
        "                            grid = pd_res[1][0]\n",
        "                            return grid, avg\n",
        "                        elif kind == \"individual\":\n",
        "                            curves = pd_res[0]\n",
        "                            grid = pd_res[1][0]\n",
        "                            return grid, curves\n",
        "                        else:\n",
        "                            raise ValueError(f\"Tipo desconocido: {kind}\")\n",
        "\n",
        "                    # 3. Si es ndarray (muy raro)\n",
        "                    elif isinstance(pd_res, np.ndarray):\n",
        "                        return np.arange(pd_res.shape[-1]), pd_res\n",
        "\n",
        "                    # 4. Si es dict (algunos edge-cases custom)\n",
        "                    elif isinstance(pd_res, dict):\n",
        "                        # Para ICE y PDP, usa las claves\n",
        "                        if kind == \"average\" and \"average\" in pd_res and \"grid_values\" in pd_res:\n",
        "                            grid = pd_res[\"grid_values\"][0]\n",
        "                            avg = pd_res[\"average\"][0]\n",
        "                            return grid, avg\n",
        "                        elif kind == \"individual\" and \"individual\" in pd_res and \"grid_values\" in pd_res:\n",
        "                            grid = pd_res[\"grid_values\"][0]\n",
        "                            curves = pd_res[\"individual\"][0]\n",
        "                            return grid, curves\n",
        "                        else:\n",
        "                            raise RuntimeError(f\"Objeto dict sin claves esperadas: {pd_res.keys()}\")\n",
        "\n",
        "                    else:\n",
        "                        raise RuntimeError(f\"No se reconoce el objeto retornado por partial_dependence: {type(pd_res)}, contenido: {pd_res}\")\n",
        "\n",
        "\n",
        "                # 1) Identificar el mejor modelo\n",
        "                best_idx = df_results['puntuacion_global_final'].idxmax()\n",
        "                best     = df_results.loc[best_idx]\n",
        "                mt, method, engine = best['tipo'], best['metodo'], best['motor']\n",
        "                payload  = OPT_MODELS[(mt, method, engine)]\n",
        "                model    = payload['model']\n",
        "                features = payload.get('cols', X_train.columns.tolist())\n",
        "\n",
        "                # 2) Preparar datos (escalado si procede)\n",
        "                sx     = payload.get('sx')\n",
        "                X_tr   = X_train[features].copy()\n",
        "                X_te   = X_test[features].copy()\n",
        "                X_tr_s = sx.transform(X_tr.values) if sx else X_tr.values\n",
        "                X_te_s = sx.transform(X_te.values) if sx else X_te.values\n",
        "                y_te   = Y_test.values.ravel()\n",
        "\n",
        "                # ————————————————————————————————\n",
        "                # 3) SHAP\n",
        "                # ————————————————————————————————\n",
        "                print(\"[DEBUG] 17.13. Iniciando Interpretación xIA con SHAP\")\n",
        "                # Silenciar warning de “feature names”\n",
        "                warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
        "                if mt in ('xgb','rf'):\n",
        "                    explainer = shap.TreeExplainer(model)\n",
        "                else:\n",
        "                    # Uso DeepExplainer para redes y KernelExplainer solo si no hay Deep support\n",
        "                    try:\n",
        "                        background = shap.kmeans(X_tr_s, 100)            # solo 100 puntos de background\n",
        "                        explainer = shap.DeepExplainer(model, background)\n",
        "                    except Exception:\n",
        "                        # fallback a KernelExplainer con pocos nsamples\n",
        "                        explainer = shap.KernelExplainer(model.predict, shap.sample(X_tr_s, 100))\n",
        "                # Reducir test a las primeras 100 filas para acelerar\n",
        "                X_sample = X_te_s[:100]\n",
        "                shap_vals = explainer.shap_values(X_sample)\n",
        "\n",
        "#                    explainer = shap.KernelExplainer(model.predict, shap.sample(X_tr_s, 100))\n",
        "#                shap_vals = explainer.shap_values(X_te_s)\n",
        "\n",
        "                fig_shap, ax_shap = plt.subplots(figsize=(8,6))\n",
        "                #shap.summary_plot(shap_vals, X_te_s, feature_names=features, show=False)\n",
        "                shap.summary_plot(shap_vals, X_sample, feature_names=features, show=False)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                #plt.close(fig_shap)\n",
        "                self.sections.append((\"### xIA: SHAP Summary\", fig_shap))\n",
        "\n",
        "                # ————————————————————————————————\n",
        "                # 4) Permutation Feature Importance\n",
        "                # ————————————————————————————————\n",
        "                print(\"[DEBUG] 17.14. Iniciando Interpretación xIA con Permutation Feature Importance\")\n",
        "                perm = permutation_importance(model, X_te_s, y_te,\n",
        "                                              n_repeats=10, random_state=0, n_jobs=-1)\n",
        "                idx_imp   = np.argsort(perm.importances_mean)[::-1]\n",
        "                top_feats = [features[i] for i in idx_imp[:5]]\n",
        "\n",
        "                fig_perm, ax_perm = plt.subplots(figsize=(8,6))\n",
        "                ax_perm.barh(np.arange(len(top_feats)),\n",
        "                            perm.importances_mean[idx_imp[:5]],\n",
        "                            xerr=perm.importances_std[idx_imp[:5]],\n",
        "                            align='center')\n",
        "                ax_perm.set_yticks(np.arange(len(top_feats)))\n",
        "                ax_perm.set_yticklabels(top_feats)\n",
        "                ax_perm.invert_yaxis()\n",
        "                ax_perm.set_title(\"xIA: Permutation Importance\")\n",
        "                ax_perm.set_xlabel(\"Mean decrease in score\")\n",
        "                plt.tight_layout()\n",
        "                #plt.close(fig_perm)\n",
        "                self.sections.append((\"### xIA: Permutation Importance\", fig_perm))\n",
        "\n",
        "                # ————————————————————————————————\n",
        "                # 5) PDP + ICE (scikit-learn)\n",
        "                # ————————————————————————————————\n",
        "                print(\"[DEBUG] 17.15. Iniciando Interpretación xIA con PDP e ICE\")\n",
        "\n",
        "                pdp_records = []\n",
        "                ice_records = []\n",
        "\n",
        "                for feat in top_feats:\n",
        "                    # PDP\n",
        "                    pdp_res = partial_dependence(\n",
        "                        model, X_tr, features=[feat], kind=\"average\", grid_resolution=20\n",
        "                    )\n",
        "                    #grid, avg = extract_grid_and_curves(pdp_res, kind='average')\n",
        "\n",
        "                    try:\n",
        "                        grid, avg = extract_grid_and_curves(pdp_res, kind='average')\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] PDP Extraction error: {e}, objeto: {type(pdp_res)}, contenido: {pdp_res}\")\n",
        "                        raise\n",
        "\n",
        "                    for x, y in zip(grid, avg):\n",
        "                        pdp_records.append({\"feature\": feat, \"grid\": float(x), \"pdp\": float(y)})\n",
        "\n",
        "                    # gráfico PDP\n",
        "                    fig_pdp, ax_pdp = plt.subplots(figsize=(6,4))\n",
        "                    ax_pdp.plot(grid, avg, marker='o')\n",
        "                    ax_pdp.set_title(f\"PDP de {feat}\")\n",
        "                    plt.tight_layout()\n",
        "                    #plt.close(fig_pdp)\n",
        "                    self.sections.append((f\"### xIA: PDP {feat}\", fig_pdp))\n",
        "\n",
        "                    # ICE\n",
        "                    ice_res = partial_dependence(\n",
        "                        model, X_tr, features=[feat], kind=\"individual\", grid_resolution=20\n",
        "                    )\n",
        "                    #grid, curves = extract_grid_and_curves(ice_res, kind='individual')\n",
        "\n",
        "                    try:\n",
        "                        grid, curves = extract_grid_and_curves(ice_res, kind='individual')\n",
        "                    except Exception as e:\n",
        "                        print(f\"[DEBUG] ICE Extraction error: {e}, objeto: {type(ice_res)}, contenido: {ice_res}\")\n",
        "                        raise\n",
        "\n",
        "                    for obs_idx, curve in enumerate(curves):\n",
        "                        for x, y in zip(grid, curve):\n",
        "                            ice_records.append({\n",
        "                                \"feature\":     feat,\n",
        "                                \"grid\":        float(x),\n",
        "                                \"ice\":         float(y),\n",
        "                                \"observation\": int(obs_idx)\n",
        "                            })\n",
        "                    # gráfico ICE\n",
        "                    fig_ice, ax_ice = plt.subplots(figsize=(6,4))\n",
        "                    for curve in curves:\n",
        "                        ax_ice.plot(grid, curve, alpha=0.3)\n",
        "                    ax_ice.set_title(f\"ICE de {feat}\")\n",
        "                    plt.tight_layout()\n",
        "                    #plt.close(fig_ice)\n",
        "                    self.sections.append((f\"### xIA: ICE {feat}\", fig_ice))\n",
        "\n",
        "                # convertir a DataFrame\n",
        "                pdp_results = pd.DataFrame(pdp_records)\n",
        "                ice_results = pd.DataFrame(ice_records)\n",
        "\n",
        "                # ————————————————————————————————\n",
        "                # 6) ALE “a mano”\n",
        "                # ————————————————————————————————\n",
        "                print(\"[DEBUG] 17.16. Iniciando Interpretación xIA con ALE\")\n",
        "                def compute_ale(model, X, feat, grid_points=20):\n",
        "                    Xdf = X.copy()\n",
        "                    vals = np.linspace(Xdf[feat].min(),\n",
        "                                      Xdf[feat].max(),\n",
        "                                      grid_points+1)\n",
        "                    centers = (vals[:-1] + vals[1:]) / 2\n",
        "                    diffs = []\n",
        "                    for low, high in zip(vals[:-1], vals[1:]):\n",
        "                        X_low  = Xdf.copy(); X_high = Xdf.copy()\n",
        "                        X_low.loc[Xdf[feat]  > high, feat] = low\n",
        "                        X_high.loc[Xdf[feat] <= low,  feat] = high\n",
        "                        arr_low  = sx.transform(X_low.values)  if sx else X_low.values\n",
        "                        arr_high = sx.transform(X_high.values) if sx else X_high.values\n",
        "                        pred_low  = model.predict(arr_low)\n",
        "                        pred_high = model.predict(arr_high)\n",
        "                        diffs.append(np.mean(pred_high - pred_low))\n",
        "                    cum = np.cumsum(diffs)\n",
        "                    cum -= cum.mean()\n",
        "                    return centers, cum\n",
        "\n",
        "                ale_records = []\n",
        "                for feat in top_feats:\n",
        "                    centers, ale_curve = compute_ale(model, X_tr, feat)\n",
        "                    fig_ale, ax_ale = plt.subplots(figsize=(6,4))\n",
        "                    ax_ale.plot(centers, ale_curve, marker='o')\n",
        "                    ax_ale.set_title(f\"ALE de {feat}\")\n",
        "                    plt.tight_layout()\n",
        "                    #plt.close(fig_ale)\n",
        "                    self.sections.append((f\"### xIA: ALE {feat}\", fig_ale))\n",
        "                    for c, a in zip(centers, ale_curve):\n",
        "                        ale_records.append({\n",
        "                            \"feature\": feat,\n",
        "                            \"grid\":    float(c),\n",
        "                            \"ale\":     float(a)\n",
        "                        })\n",
        "                ale_results = pd.DataFrame(ale_records)\n",
        "\n",
        "                # ————————————————————————————————\n",
        "                # 7) Counterfactuals “ligeros”\n",
        "                # ————————————————————————————————\n",
        "                print(\"[DEBUG] 17.17. Iniciando Interpretación xIA con Counterfactuals\")\n",
        "                def generate_counterfactual(x0, delta=1.0):\n",
        "                    x0_arr = x0.values if hasattr(x0, \"values\") else x0\n",
        "                    x0_s   = sx.transform([x0_arr])[0] if sx else x0_arr\n",
        "                    y0     = model.predict([x0_s])[0]\n",
        "                    def obj(x): return np.sum((x - x0_s)**2)\n",
        "                    cons = {\n",
        "                        'type': 'ineq',\n",
        "                        'fun':  lambda x: model.predict([x])[0] - y0 - delta\n",
        "                    }\n",
        "                    res = minimize(obj, x0_s, constraints=cons)\n",
        "                    if res.success:\n",
        "                        xcf_s = res.x\n",
        "                        xcf   = sx.inverse_transform([xcf_s])[0] if sx else xcf_s\n",
        "                        ycf   = model.predict([xcf_s])[0]\n",
        "                        return xcf, ycf\n",
        "                    else:\n",
        "                        return None, None\n",
        "\n",
        "                cf_explanations = []\n",
        "                for i in range(min(3, len(X_te))):\n",
        "                    x0 = X_te.iloc[i]\n",
        "                    xcf, ycf = generate_counterfactual(x0, delta=1.0)\n",
        "                    if xcf is not None:\n",
        "                        delta_feats = xcf - x0.values\n",
        "                        fig_cf, ax_cf = plt.subplots(figsize=(6,4))\n",
        "                        ax_cf.bar(X_te.columns, delta_feats, edgecolor='k')\n",
        "                        ax_cf.set_title(f\"Counterfactual #{i} (Δ para +1 unidad de y)\")\n",
        "                        ax_cf.set_xticklabels(X_te.columns, rotation=45, ha='right')\n",
        "                        plt.tight_layout()\n",
        "                        #plt.close(fig_cf)\n",
        "                        self.sections.append((f\"### xIA: Counterfactual {i}\", fig_cf))\n",
        "                        cf_explanations.append({\n",
        "                            \"observation\":     i,\n",
        "                            \"x0\":              x0.to_dict(),\n",
        "                            \"counterfactual\":  {features[j]: float(xcf[j]) for j in range(len(features))},\n",
        "                            \"y_pred_original\": float(model.predict([sx.transform([x0.values])[0]]) if sx else model.predict([x0.values])[0]),\n",
        "                            \"y_pred_cf\":       float(ycf)\n",
        "                        })\n",
        "\n",
        "                # —————————————————————————————————————————\n",
        "                # 8) RESUMIR LOS RESULTADOS PARA ENVIAR A LA IA\n",
        "                # —————————————————————————————————————————\n",
        "                print(\"[DEBUG] 17.18. Resumiendo resultados XAI para envío a la IA\")\n",
        "\n",
        "                def resumir_xai_json(\n",
        "                    shap_vals, features, perm, idx_imp, top_feats,\n",
        "                    pdp_results, ice_results, ale_results, cf_explanations,\n",
        "                    n_feats=2, n_vals=8\n",
        "                ):\n",
        "                    # Selecciona las top n_feats\n",
        "                    features_lite = [features[i] for i in idx_imp[:n_feats]]\n",
        "\n",
        "                    # 1. SHAP: solo valores medios globales para top features\n",
        "                    shap_summary = {\n",
        "                        \"features\": features_lite,\n",
        "                        \"shap_mean_abs\": [float(np.mean(np.abs(shap_vals[:, features.index(f)]))) for f in features_lite]\n",
        "                    }\n",
        "\n",
        "                    # 2. Permutation: solo top features\n",
        "                    perm_summary = {\n",
        "                        \"features\": features_lite,\n",
        "                        \"importances_mean\": [float(perm.importances_mean[features.index(f)]) for f in features_lite],\n",
        "                        \"importances_std\": [float(perm.importances_std[features.index(f)]) for f in features_lite],\n",
        "                    }\n",
        "\n",
        "                    # 3. PDP: solo top features y primeros n_vals puntos\n",
        "                    pdp_summary = {\n",
        "                        feat: {\n",
        "                            \"grid\": pdp_results.loc[pdp_results.feature == feat, \"grid\"].astype(float).values[:n_vals].tolist(),\n",
        "                            \"pdp\": pdp_results.loc[pdp_results.feature == feat, \"pdp\"].astype(float).values[:n_vals].tolist()\n",
        "                        }\n",
        "                        for feat in features_lite\n",
        "                    }\n",
        "\n",
        "                    # 4. ICE: solo top features, primeros n_vals, y primeras 2 observaciones\n",
        "                    ice_summary = {}\n",
        "                    for feat in features_lite:\n",
        "                        obs_ids = ice_results[ice_results.feature == feat][\"observation\"].unique()[:2]\n",
        "                        ice_summary[feat] = {\n",
        "                            int(obs): {\n",
        "                                \"grid\": ice_results[(ice_results.feature == feat) & (ice_results.observation == obs)][\"grid\"].astype(float).values[:n_vals].tolist(),\n",
        "                                \"ice\": ice_results[(ice_results.feature == feat) & (ice_results.observation == obs)][\"ice\"].astype(float).values[:n_vals].tolist()\n",
        "                            }\n",
        "                            for obs in obs_ids\n",
        "                        }\n",
        "\n",
        "                    # 5. ALE: igual que PDP\n",
        "                    ale_summary = {\n",
        "                        feat: {\n",
        "                            \"grid\": ale_results.loc[ale_results.feature == feat, \"grid\"].astype(float).values[:n_vals].tolist(),\n",
        "                            \"ale\": ale_results.loc[ale_results.feature == feat, \"ale\"].astype(float).values[:n_vals].tolist()\n",
        "                        }\n",
        "                        for feat in features_lite\n",
        "                    }\n",
        "\n",
        "                    # 6. Counterfactuals: solo 1 o 2 ejemplos, y solo diferencias principales\n",
        "                    cf_summary = []\n",
        "                    for c in cf_explanations[:2]:\n",
        "                        d = {k: c[k] for k in [\"observation\", \"y_pred_original\", \"y_pred_cf\"]}\n",
        "                        d[\"counterfactual\"] = {k: v for k, v in list(c[\"counterfactual\"].items())[:n_feats]}\n",
        "                        cf_summary.append(d)\n",
        "\n",
        "                    return {\n",
        "                        \"shap\": shap_summary,\n",
        "                        \"perm\": perm_summary,\n",
        "                        \"pdp\": pdp_summary,\n",
        "                        \"ice\": ice_summary,\n",
        "                        \"ale\": ale_summary,\n",
        "                        \"cf\": cf_summary\n",
        "                    }\n",
        "\n",
        "                # — Serialización resumida —\n",
        "                xai_resumido = resumir_xai_json(\n",
        "                    shap_vals, features, perm, idx_imp, top_feats,\n",
        "                    pdp_results, ice_results, ale_results, cf_explanations,\n",
        "                    n_feats=2, n_vals=8\n",
        "                )\n",
        "\n",
        "                shap_json_lite = json.dumps(xai_resumido[\"shap\"], indent=2, ensure_ascii=False)\n",
        "                perm_json_lite = json.dumps(xai_resumido[\"perm\"], indent=2, ensure_ascii=False)\n",
        "                pdp_json_lite  = json.dumps(xai_resumido[\"pdp\"], indent=2, ensure_ascii=False)\n",
        "                ice_json_lite  = json.dumps(xai_resumido[\"ice\"], indent=2, ensure_ascii=False)\n",
        "                ale_json_lite  = json.dumps(xai_resumido[\"ale\"], indent=2, ensure_ascii=False)\n",
        "                cf_json_lite   = json.dumps(xai_resumido[\"cf\"], indent=2, ensure_ascii=False)\n",
        "\n",
        "                # Guardar los JSON para el siguiente bloque\n",
        "                self.shap_json_lite = shap_json_lite\n",
        "                self.perm_json_lite = perm_json_lite\n",
        "                self.pdp_json_lite  = pdp_json_lite\n",
        "                self.ice_json_lite  = ice_json_lite\n",
        "                self.ale_json_lite  = ale_json_lite\n",
        "                self.cf_json_lite   = cf_json_lite\n",
        "\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                print(\"[DEBUG] Se ha producido una excepción en Interpretación xIA:\")\n",
        "                print(f\"[DEBUG] Tipo de excepción: {type(e).__name__}\")\n",
        "                print(f\"[DEBUG] Mensaje: {e}\")\n",
        "                print(\"[DEBUG] Traza completa:\")\n",
        "                traceback.print_exc()\n",
        "                self.sections.append((\n",
        "                    \"### ⚠️ Error en interpretación xIA del modelo\",\n",
        "                    f\"{type(e).__name__}: {e}\"\n",
        "                ))\n",
        "\n",
        "            # --- Fin Bloque 4 ---\n",
        "\n",
        "            # --- Bloque 5: Interpretación Física-Química del Mejor Modelo Optimizado mediante IA ---\n",
        "            print(\"[DEBUG] 17.19. Iniciando Interpretación Físico - Químicamediante IA  del Mejor Modelo Optimizado\")\n",
        "            try:\n",
        "                import os\n",
        "                import json\n",
        "                #import pandas as pd\n",
        "                #from PyPDF2 import PdfReader\n",
        "                import openai\n",
        "\n",
        "                # 1) Cargar y extraer texto de contexto de varios PDFs\n",
        "                from PyPDF2 import PdfReader\n",
        "                import pandas as pd\n",
        "\n",
        "                # --- Gestión de subida y carga de contexto desde memoria en Colab ---\n",
        "                import io\n",
        "                import pandas as pd\n",
        "                from PyPDF2 import PdfReader\n",
        "                from google.colab import files\n",
        "                import io\n",
        "\n",
        "                # 2.1) Abrimos selector de archivos y leemos el .xlsx subido\n",
        "                uploaded = files.upload()\n",
        "                excel_file = next(f for f in uploaded.keys() if f.lower().endswith('.xlsx'))\n",
        "                book_bytes = io.BytesIO(uploaded[excel_file])\n",
        "\n",
        "                # 2.2) Cargamos **todas** las hojas; cada DataFrame toma su primera fila como cabecera\n",
        "                import pandas as _pd\n",
        "                sheets_dict = _pd.read_excel(book_bytes, sheet_name=None)\n",
        "\n",
        "                print(\"Hojas encontradas en el Excel:\", list(sheets_dict.keys()))\n",
        "\n",
        "                # 2.3) Convertimos cada hoja en lista de dicts (records) sin predefinir columnas\n",
        "                mapa_variables = {\n",
        "                    name: (\n",
        "                        df\n",
        "                        .astype(str)            # convertimos todo a string para evitar Timestamp u otros tipos\n",
        "                        .dropna(how='all')      # eliminamos filas completamente vacías\n",
        "                        .to_dict(orient=\"records\")\n",
        "                    )\n",
        "                    for name, df in sheets_dict.items()\n",
        "                }\n",
        "                # 2.4) Añadimos al contexto global\n",
        "                #contexto[\"mapa_variables\"] = mapa_variables\n",
        "\n",
        "                # 3) Cargar y extraer texto de TODOS los PDFs subidos\n",
        "                pdf_texts = {}\n",
        "                for fn, content in uploaded.items():\n",
        "                    if fn.lower().endswith(\".pdf\"):\n",
        "                        reader = PdfReader(io.BytesIO(content))\n",
        "                        full = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
        "                        pdf_texts[fn] = full\n",
        "\n",
        "                # 4) Del PDF de proceso aislamos la sección relevante (si existe)\n",
        "                proceso_pdf = next((k for k in pdf_texts if \"Procesado_Datos_Linares\" in k), None)\n",
        "                if proceso_pdf:\n",
        "                    texto = pdf_texts[proceso_pdf]\n",
        "                    start_marker = \"VARIABLES Y PARTICULARIDADES DEL PROCESO\"\n",
        "                    end_marker   = \"Etapas del proceso\"\n",
        "                    i0 = texto.find(start_marker)\n",
        "                    i1 = texto.find(end_marker, i0 + len(start_marker))\n",
        "                    if i0 != -1 and i1 != -1:\n",
        "                        proceso_texto = texto[i0:i1].strip()\n",
        "                    else:\n",
        "                        proceso_texto = texto\n",
        "                else:\n",
        "                    proceso_texto = \"\"\n",
        "\n",
        "                # 5) Texto de la memoria TFM (si existe)\n",
        "                memoria_pdf = next((k for k in pdf_texts if \"Memoria Final TFM\" in k), None)\n",
        "                memoria_texto = pdf_texts.get(memoria_pdf, \"\")\n",
        "\n",
        "                # 6) Construir el contexto completo\n",
        "                contexto = {\n",
        "                    \"proceso_texto\":   proceso_texto,\n",
        "                    \"memoria_texto\":   memoria_texto,\n",
        "                    \"mapa_variables\":  mapa_variables,\n",
        "                    \"dominio\": (\n",
        "                        \"Este modelo optimiza la predicción de la variable objetivo en la planta de \"\n",
        "                        \"aglomerados de Linares. Se considera la interacción físico-química entre resinas, \"\n",
        "                        \"temperaturas de secado y niveles de humedad, así como parámetros de prensa y fases de corte.\"\n",
        "                    )\n",
        "                }\n",
        "\n",
        "                # Ya puedes usar `contexto` y `contexto_variables` en tu prompt de OpenAI.\n",
        "                print(\"[DEBUG] 17.20. Iniciando llamada a IA para análisis Físico - Químico del proceso\")\n",
        "\n",
        "                # 1. Prompt SHAP + Permutation\n",
        "                prompt_shap_perm = f\"\"\"\n",
        "            Dominio industrial:\n",
        "            {contexto['dominio']}\n",
        "\n",
        "            Resultados SHAP (importancia global resumida):\n",
        "            {self.shap_json_lite}\n",
        "\n",
        "            Permutation Importance (top features):\n",
        "            {self.perm_json_lite}\n",
        "\n",
        "            Explica cómo afectan estas variables al proceso y su impacto físico-químico.\n",
        "            \"\"\"\n",
        "\n",
        "                # 2. Prompt PDP + ALE\n",
        "                prompt_pdp_ale = f\"\"\"\n",
        "            Dominio industrial:\n",
        "            {contexto['dominio']}\n",
        "\n",
        "            PDP (efecto parcial) para principales variables:\n",
        "            {self.pdp_json_lite}\n",
        "\n",
        "            ALE (efecto acumulado local) para principales variables:\n",
        "            {self.ale_json_lite}\n",
        "\n",
        "            Relaciona las tendencias observadas en los gráficos PDP y ALE con los fenómenos físicos-químicos reales.\n",
        "            \"\"\"\n",
        "\n",
        "                # 3. Prompt ICE\n",
        "                prompt_ice = f\"\"\"\n",
        "            Dominio industrial:\n",
        "            {contexto['dominio']}\n",
        "\n",
        "            ICE (efectos individuales):\n",
        "            {self.ice_json_lite}\n",
        "\n",
        "            Describe la variabilidad operativa observada en ICE y su impacto en condiciones reales de planta.\n",
        "            \"\"\"\n",
        "\n",
        "                # 4. Prompt Counterfactuals\n",
        "                prompt_cf = f\"\"\"\n",
        "            Dominio industrial:\n",
        "            {contexto['dominio']}\n",
        "\n",
        "            Counterfactuals (sólo ejemplos principales):\n",
        "            {self.cf_json_lite}\n",
        "\n",
        "            Proporciona recomendaciones sobre ajustes operativos para mejorar el KPI.\n",
        "            \"\"\"\n",
        "\n",
        "                prompts = [\n",
        "                    (\"### Interpretación SHAP + Permutation\", prompt_shap_perm),\n",
        "                    (\"### Interpretación PDP + ALE\", prompt_pdp_ale),\n",
        "                    (\"### Interpretación ICE\", prompt_ice),\n",
        "                    (\"### Interpretación Counterfactuals\", prompt_cf)\n",
        "                ]\n",
        "\n",
        "                for titulo, prompt in prompts:\n",
        "                    print(f\"[DEBUG] 17.21. Llamando a IA para {titulo}\")\n",
        "                    resp = _client.chat.completions.create(\n",
        "                        model=\"gpt-4\",\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                        temperature=TEMPERATURE_VAL,\n",
        "                        max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "                    )\n",
        "                    explicacion = resp.choices[0].message.content\n",
        "                    self.sections.append((titulo, explicacion))\n",
        "\n",
        "            except Exception as e:\n",
        "                self.sections.append((\n",
        "                    \"### ⚠️ Error en Interpretación Física-Química\",\n",
        "                    f\"{e}\"\n",
        "                ))\n",
        "\n",
        "        except Exception as e:\n",
        "            #self.sections.append((\n",
        "            #    \"### ⚠️ Error en selección integral de modelo\",\n",
        "            #    f\"Se produjo un error al generar la sección de selección integral: {e}\"\n",
        "            #))\n",
        "            # DEBUG: imprimo excepción completa para rastrear el fallo\n",
        "            import traceback\n",
        "            print(f\"[DEBUG] ERROR: {type(e).__name__}: {e}\")\n",
        "            traceback.print_exc()\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en selección integral de modelo\",\n",
        "                f\"Se produjo un error al generar la sección de selección integral: {type(e).__name__}: {e}\"\n",
        "            ))\n",
        "        # --- Fin Bloque 5 - Sección Integral del Modelo ---\n",
        "\n",
        "        # --- Bloque 6: Curvas Y real vs. Y predicha por variable X (modelo óptimo) ---\n",
        "        try:\n",
        "            print(\"[DEBUG] 17.22. Iniciando bloque de curvas Y real vs predicho por variable X\")\n",
        "\n",
        "            import numpy as np\n",
        "            from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "            import matplotlib.pyplot as plt\n",
        "            import pandas as pd\n",
        "\n",
        "            # DEBUG: listar todas las secciones añadidas hasta ahora\n",
        "            print(\"[DEBUG] 17.23. Secciones en self.sections:\")\n",
        "            for idx, (title, content) in enumerate(self.sections):\n",
        "                tipo = type(content).__name__\n",
        "                cols = content.columns.tolist() if hasattr(content, \"columns\") else None\n",
        "                print(f\"   {idx:02d}: '{title}' → {tipo}\" + (f\", cols={cols}\" if cols else \"\"))\n",
        "            print(\"[DEBUG] 17.24. Ahora buscamos la tabla con 'puntuacion_global_final'…\")\n",
        "\n",
        "            # 1) Recuperar el DataFrame de resultados globales con 'puntuacion_global_final'\n",
        "            df_results = None\n",
        "            for title, content in self.sections:\n",
        "                if isinstance(content, pd.DataFrame) and 'puntuacion_global_final' in content.columns:\n",
        "                    df_results = content\n",
        "                    break\n",
        "            if df_results is None:\n",
        "                raise RuntimeError(\"No se encontró la tabla de puntuaciones globales (df_results)\")\n",
        "\n",
        "            # 2) Identificar el mejor modelo\n",
        "            best_idx = df_results['puntuacion_global_final'].idxmax()\n",
        "            best_row = df_results.loc[best_idx]\n",
        "            tipo, metodo, motor = best_row['tipo'], best_row['metodo'], best_row['motor']\n",
        "            print(f\"[DEBUG] 17.25. Modelo óptimo identificado: {tipo}-{metodo}-{motor}\")\n",
        "\n",
        "            # 3) Recuperar payload, modelo, scalers y lista de variables\n",
        "            payload = OPT_MODELS[(tipo, metodo, motor)]\n",
        "            model   = payload['model']\n",
        "            features = payload.get('cols', self.g['X_test'].columns.tolist())\n",
        "            sx      = payload.get('sx')   # scaler de X, si existe\n",
        "            sy      = payload.get('sy')   # scaler de Y, si existe\n",
        "\n",
        "            # 4) Preparar datos de test y predicciones\n",
        "            X_test_sel = self.g['X_test'][features].copy()\n",
        "            y_true     = self.g['Y_test'].values.ravel()\n",
        "            # Escalado y predict\n",
        "            X_scaled = sx.transform(X_test_sel.values) if sx else X_test_sel.values\n",
        "            y_pred_raw = model.predict(X_scaled)\n",
        "            y_pred = (sy.inverse_transform(y_pred_raw.reshape(-1,1)).ravel()\n",
        "                      if sy else y_pred_raw.ravel())\n",
        "\n",
        "            # 5) Calcular métricas globales de ajuste (idénticas para todos los subplots)\n",
        "            r2  = r2_score(y_true, y_pred)\n",
        "            mae = mean_absolute_error(y_true, y_pred)\n",
        "            mse = mean_squared_error(y_true, y_pred)\n",
        "            print(f\"[DEBUG] 17.26. Métricas globales del modelo óptimo: R2={r2:.3f}, MAE={mae:.3f}, MSE={mse:.3f}\")\n",
        "\n",
        "            # 6) Crear figura de small multiples\n",
        "            n_vars = len(features)\n",
        "            ncols  = min(4, n_vars)\n",
        "            nrows  = int(np.ceil(n_vars / ncols))\n",
        "\n",
        "            fig, axes = plt.subplots(\n",
        "                nrows, ncols,\n",
        "                figsize=(ncols * 3, nrows * 2.5),\n",
        "                squeeze=False\n",
        "            )\n",
        "\n",
        "            for ax, feat in zip(axes.flatten(), features):\n",
        "                # Ordenar por valor de X para trazar líneas ordenadas\n",
        "                x = X_test_sel[feat].values\n",
        "                idx_sort = np.argsort(x)\n",
        "                x_s = x[idx_sort]\n",
        "                ax.plot(x_s, y_true[idx_sort],   label='Real',    linewidth=1)\n",
        "                ax.plot(x_s, y_pred[idx_sort],   label='Predicho',linewidth=1)\n",
        "                ax.set_title(\n",
        "                    f\"{feat}\\n\"\n",
        "                    f\"R²={r2:.2f}  MAE={mae:.2f}  MSE={mse:.2f}\",\n",
        "                    fontsize=8\n",
        "                )\n",
        "                ax.tick_params(labelsize=6)\n",
        "                ax.legend(fontsize=6)\n",
        "\n",
        "            # Desactivar ejes sobrantes\n",
        "            for ax in axes.flatten()[n_vars:]:\n",
        "                ax.set_visible(False)\n",
        "\n",
        "            fig.tight_layout()\n",
        "            print(\"[DEBUG] 17.27. Bloque de curvas completado, añadiendo sección al informe\")\n",
        "\n",
        "            # 7) Añadir al informe\n",
        "            self.sections.append((\n",
        "                \"### Curvas Y real vs Y predicha por variable X (modelo óptimo)\",\n",
        "                fig\n",
        "            ))\n",
        "\n",
        "        except Exception as e:\n",
        "            # En caso de fallo, no romperá todo el build_sections\n",
        "            print(f\"[ERROR] en bloque de curvas Y vs X: {e}\")\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en Curvas Y vs X\",\n",
        "                f\"Se produjo un error generando las curvas: {e}\"\n",
        "            ))\n",
        "        # --- Fin Bloque 6: Curvas Y real vs. Y predicha por variable X (modelo óptimo) ---\n",
        "\n",
        "        # --- Bloque 7: Análisis IA de Curvas Y real vs Y predicho por Variable X ---\n",
        "        try:\n",
        "            print(\"[DEBUG] 17.28. Enriqueciendo contexto antes de llamar a OpenAI\")\n",
        "\n",
        "            from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "            from scipy.stats import skew, kurtosis, pearsonr\n",
        "            import pandas as pd\n",
        "\n",
        "            # Recuperar la configuración del mejor modelo\n",
        "            # (asegúrate de haber definido antes mt, method, engine)\n",
        "            key_best     = (mt, method, engine)\n",
        "            payload_best = OPT_MODELS[key_best]\n",
        "            features     = payload_best['cols']\n",
        "            model_best   = payload_best['model']\n",
        "\n",
        "            # Extraer datos de test y predicciones\n",
        "            X_te = X_test[features].copy()\n",
        "            y_true = Y_test.values.ravel()\n",
        "            sx = payload_best.get('sx', None)\n",
        "            X_te_s = sx.transform(X_te.values) if sx else X_te.values\n",
        "            # Si el wrapper ya invierte la escala internamente:\n",
        "            y_pred = model_best.predict(X_te_s) if not hasattr(model_best, 'sy') else model_best.predict(X_te)\n",
        "\n",
        "            # 1) Estadísticas de distribución de cada X\n",
        "            dist_stats = {}\n",
        "            for feat in features:\n",
        "                arr = X_te[feat].values\n",
        "                dist_stats[feat] = {\n",
        "                    'min': float(arr.min()),\n",
        "                    'max': float(arr.max()),\n",
        "                    'mean': float(arr.mean()),\n",
        "                    'std': float(arr.std()),\n",
        "                    'skew': float(skew(arr)),\n",
        "                    'kurtosis': float(kurtosis(arr)),\n",
        "                }\n",
        "\n",
        "            # 2) Correlación residual vs X (heterocedasticidad)\n",
        "            residuals = y_true - y_pred\n",
        "            corr_stats = {}\n",
        "            for feat in features:\n",
        "                r, p = pearsonr(X_te[feat].values, residuals)\n",
        "                corr_stats[feat] = {'pearson_r': float(r), 'p_value': float(p)}\n",
        "\n",
        "            # 3) Residuales por cuartiles de X\n",
        "            quartile_stats = {}\n",
        "            for feat in features:\n",
        "                df_q = pd.DataFrame({\n",
        "                    'x': X_te[feat],\n",
        "                    'res': residuals\n",
        "                })\n",
        "                df_q['qcut'] = pd.qcut(df_q['x'], 4, labels=False, duplicates='drop')\n",
        "                qs = df_q.groupby('qcut')['res'].agg(['mean','std']).to_dict(orient='index')\n",
        "                quartile_stats[feat] = {\n",
        "                    int(k): {'mean_res': float(v['mean']), 'std_res': float(v['std'])}\n",
        "                    for k, v in qs.items()\n",
        "                }\n",
        "\n",
        "            # 4) Ejemplos de pares (X, y_real, y_pred) en cuartiles extremos\n",
        "            samples = {}\n",
        "            for feat in features:\n",
        "                # Seleccionar solo cuartiles 0 y 3\n",
        "                cuts = pd.qcut(X_te[feat], 4, labels=False, duplicates='drop')\n",
        "                sel = cuts.isin([0, 3])\n",
        "                df_s = pd.DataFrame({\n",
        "                    'x':   X_te[feat][sel],\n",
        "                    'y_r': y_true[sel],\n",
        "                    'y_p': y_pred[sel]\n",
        "                }).head(3)  # 3 muestras por variable\n",
        "                samples[feat] = df_s.to_dict(orient='records')\n",
        "\n",
        "            # 5) Métricas globales del modelo óptimo\n",
        "            global_stats = {\n",
        "                'R2':  float(r2_score(y_true, y_pred)),\n",
        "                'MAE': float(mean_absolute_error(y_true, y_pred)),\n",
        "                'MSE': float(mean_squared_error(y_true, y_pred))\n",
        "            }\n",
        "\n",
        "            print(\"[DEBUG] 17.29. Dist stats, corr stats, quartile stats y samples construidos\")\n",
        "\n",
        "            # 6) Construcción del prompt enriquecido\n",
        "            prompt = [\n",
        "                \"A continuación tienes un informe detallado de cada variable X:\\n\",\n",
        "                \"**1) Distribución de cada X:**\",\n",
        "                f\"{dist_stats}\\n\",\n",
        "                \"**2) Correlación Residual vs X (Pearson):**\",\n",
        "                f\"{corr_stats}\\n\",\n",
        "                \"**3) Residuales por cuartiles de X:**\",\n",
        "                f\"{quartile_stats}\\n\",\n",
        "                \"**4) Ejemplos de pares (X, y_real, y_predicho) en cuartiles extremos:**\",\n",
        "                f\"{samples}\\n\",\n",
        "                \"**5) Métricas globales del modelo óptimo:**\",\n",
        "                f\"{global_stats}\\n\",\n",
        "                \"### Instrucciones al experto IA:\\n\"\n",
        "                \"1. Analiza para cada variable X cómo la distribución y la heterocedasticidad \"\n",
        "                \"(Pearson r, residuales por cuartiles) pueden estar afectando el ajuste.\\n\"\n",
        "                \"2. Discute zonas problemáticas (picos, colas extensas) y su impacto en la predicción.\\n\"\n",
        "                \"3. Comenta sobre las muestras de ejemplo: ¿qué patrones ves en X extremos?\\n\"\n",
        "                \"4. Integra el conocimiento físico-químico del proceso para explicar estos fenómenos.\\n\"\n",
        "                \"5. Propón recomendaciones tanto de modelado (transformaciones, nuevas features) \"\n",
        "                \"como de operación del proceso (rangos óptimos, variables críticas).\\n\"\n",
        "            ]\n",
        "            full_prompt = \"\\n\".join(prompt)\n",
        "\n",
        "            print(\"[DEBUG] 17.30. Llamando a OpenAI directamente con _client.chat.completions.create para Curvas…\")\n",
        "            response = _client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Eres un experto en análisis de series temporales y ML para procesos físico-químicos.\"},\n",
        "                    {\"role\": \"user\",   \"content\": full_prompt}\n",
        "                ],\n",
        "                temperature=TEMPERATURE_VAL,\n",
        "                max_tokens=MAX_EXPLANATION_TOKENS\n",
        "            )\n",
        "            analysis_curvas = response.choices[0].message.content.strip()\n",
        "            print(f\"[DEBUG] 17.31. Longitud del análisis de curvas: {len(analysis_curvas)} caracteres\")\n",
        "\n",
        "            # 7) Añadimos la sección final al reporte\n",
        "            self.sections.append((\n",
        "                \"### Análisis IA Profundo de Y real vs Y predicho por Variable X\",\n",
        "                analysis_curvas\n",
        "            ))\n",
        "            print(\"[DEBUG] 17.32. Sección Análisis IA enriquecido añadida\")\n",
        "\n",
        "        except Exception as e:\n",
        "            err = f\"Error en Bloque IA Profundo curvas X vs Y: {e}\"\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error Bloque IA Profundo curvas X vs Y\",\n",
        "                err\n",
        "            ))\n",
        "            print(f\"[DEBUG] {err}\")\n",
        "\n",
        "        # --- Fin Bloque 7: Análisis IA de Curvas Y real vs Y predicho por Variable X ---\n",
        "\n",
        "        # --- Bloque 8: Robustez What-If y Gráficas Extremos ---\n",
        "        try:\n",
        "            print(\"[DEBUG] 17.33. Iniciando Bloque Robustez What-If y Gráficas Extremos\")\n",
        "\n",
        "            import itertools\n",
        "            import numpy as np\n",
        "            import pandas as pd\n",
        "            import matplotlib.pyplot as plt\n",
        "            import seaborn as sns\n",
        "\n",
        "            # <<< NUEVO: Obtener escaladores desde payload (como se hace en xIA) >>>\n",
        "            sx = payload.get('sx')\n",
        "            sy = payload.get('sy')\n",
        "\n",
        "            # Ajustes globales de estilo\n",
        "            sns.set(style=\"whitegrid\", palette=\"deep\", font_scale=1.2)\n",
        "            plt.rcParams.update({\n",
        "                \"figure.facecolor\": \"white\",\n",
        "                \"axes.facecolor\": \"white\",\n",
        "                \"axes.edgecolor\": \"#333333\",\n",
        "                \"axes.labelcolor\": \"#333333\",\n",
        "                \"xtick.color\": \"#333333\",\n",
        "                \"ytick.color\": \"#333333\",\n",
        "                \"text.color\": \"#333333\",\n",
        "                \"legend.frameon\": True,\n",
        "                \"legend.framealpha\": 0.9,\n",
        "            })\n",
        "\n",
        "            # 1) Variables para robustez\n",
        "            vars_robust = top_feats[:2]\n",
        "            print(f\"[DEBUG] 17.34. Variables robustez seleccionadas: {vars_robust}\")\n",
        "\n",
        "            # 2) Medianas y extremos\n",
        "            medians   = X_train[features].median()\n",
        "            low_vals  = X_train[features].quantile(0.05)\n",
        "            high_vals = X_train[features].quantile(0.95)\n",
        "            print(\"[DEBUG] 17.35. Valores medianos y percentiles obtenidos\")\n",
        "\n",
        "            # 3) Generar escenarios what-if\n",
        "            scenarios = []\n",
        "            for combo in itertools.product(['low','high'], repeat=len(vars_robust)):\n",
        "                sc = medians.copy()\n",
        "                labels = []\n",
        "                for var, lvl in zip(vars_robust, combo):\n",
        "                    sc[var] = low_vals[var] if lvl=='low' else high_vals[var]\n",
        "                    labels.append(f\"{var}_{lvl}\")\n",
        "                sc['__label__'] = \" & \".join(labels)\n",
        "                scenarios.append(sc)\n",
        "            df_scen = pd.DataFrame(scenarios).reset_index(drop=True)\n",
        "            print(f\"[DEBUG] 17.36. {len(df_scen)} escenarios generados: {df_scen['__label__'].tolist()}\")\n",
        "\n",
        "            # <<< CAMBIO: Aplicar escalado SOLO si existe sx >>>\n",
        "            X_scen = sx.transform(df_scen[features].values) if sx else df_scen[features].values\n",
        "            if mt == 'rnn':\n",
        "                X_in = X_scen.reshape((X_scen.shape[0], 1, X_scen.shape[1]))\n",
        "            else:\n",
        "                X_in = X_scen\n",
        "\n",
        "            # 5) Predecir y calcular errores\n",
        "            y_pred = model.predict(X_in)\n",
        "            if mt != 'rnn':\n",
        "                #y_pred = sy.inverse_transform(y_pred.reshape(-1,1)).ravel()\n",
        "                y_pred = sy.inverse_transform(y_pred.reshape(-1,1)).ravel() if sy else y_pred.ravel()\n",
        "            df_scen['y_pred'] = y_pred\n",
        "\n",
        "            # <<< CAMBIO: Mismo tratamiento para y_base >>>\n",
        "            yb = model.predict(\n",
        "                X_in[:1].reshape((1,1,X_scen.shape[1])) if mt=='rnn' else X_in[:1]\n",
        "            )\n",
        "            if mt != 'rnn':\n",
        "                yb = sy.inverse_transform(yb.reshape(-1,1)).ravel()[0] if sy else yb.ravel()[0]\n",
        "\n",
        "            df_scen['y_base'] = yb\n",
        "            df_scen['error_abs'] = np.abs(df_scen['y_pred'] - df_scen['y_base'])\n",
        "            print(\"[DEBUG] 17.37. Predicciones y errores calculados\")\n",
        "\n",
        "            # 6) Boxplot de Error Absoluto\n",
        "            fig1, ax1 = plt.subplots(figsize=(8, 5))\n",
        "            sns.boxplot(\n",
        "                y=df_scen['error_abs'],\n",
        "                ax=ax1,\n",
        "                width=0.4,\n",
        "                boxprops=dict(facecolor=\"#4F81BD\", edgecolor=\"#333333\"),\n",
        "                medianprops=dict(color=\"#E74C3C\", linewidth=2),\n",
        "                whiskerprops=dict(color=\"#333333\", linewidth=1.5),\n",
        "                capprops=dict(color=\"#333333\", linewidth=1.5)\n",
        "            )\n",
        "            ax1.set_title(\"What-If: Boxplot de Error Absoluto\", fontsize=16, fontweight='bold')\n",
        "            ax1.set_ylabel(\"Error absoluto\", fontsize=14)\n",
        "            ax1.set_xticks([])\n",
        "            ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            plt.tight_layout()\n",
        "            self.sections.append((\n",
        "                \"### Robustez What-If: Boxplot de Error Absoluto\",\n",
        "                fig1\n",
        "            ))\n",
        "            print(\"[DEBUG] 17.38. Sección Boxplot de error añadido\")\n",
        "\n",
        "            # 7) Scatter Y_pred vs Baseline por escenario\n",
        "            fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
        "            idx = np.arange(len(df_scen))\n",
        "            ax2.plot(idx, df_scen['y_base'], marker='o', linestyle='-', label='Baseline', linewidth=2)\n",
        "            ax2.plot(idx, df_scen['y_pred'], marker='X', linestyle='--', label='What-If Predicho', linewidth=2)\n",
        "            for i, lbl in enumerate(df_scen['__label__']):\n",
        "                ax2.annotate(\n",
        "                    lbl,\n",
        "                    (idx[i], df_scen['y_pred'].iloc[i]),\n",
        "                    textcoords=\"offset points\",\n",
        "                    xytext=(0,8),\n",
        "                    ha='center',\n",
        "                    fontsize=10,\n",
        "                    color=\"#333333\"\n",
        "                )\n",
        "            ax2.set_xticks(idx)\n",
        "            ax2.set_xticklabels(df_scen['__label__'], rotation=45, ha='right', fontsize=10)\n",
        "            ax2.set_title(\"What-If: Y Predicho vs Baseline por Escenario\", fontsize=16, fontweight='bold')\n",
        "            ax2.set_xlabel(\"Escenario\", fontsize=14)\n",
        "            ax2.set_ylabel(\"Y\", fontsize=14)\n",
        "            ax2.legend(frameon=True, fontsize=12)\n",
        "            ax2.grid(True, linestyle='--', alpha=0.5)\n",
        "            plt.tight_layout()\n",
        "            self.sections.append((\n",
        "                \"### Robustez What-If: Y Predicho vs Baseline\",\n",
        "                fig2\n",
        "            ))\n",
        "            print(\"[DEBUG] 17.39. Sección Scatter plot de escenarios añadido\")\n",
        "\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(\"[DEBUG] Se ha producido una excepción en el Bloque Robustez What-If:\")\n",
        "            print(\"[DEBUG] Tipo de excepción:\", type(e).__name__)\n",
        "            print(\"[DEBUG] Mensaje:\", e)\n",
        "            print(\"[DEBUG] Traza completa:\")\n",
        "            traceback.print_exc()\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en Bloque Robustez What-If\",\n",
        "                f\"{type(e).__name__}: {e}\"\n",
        "            ))\n",
        "            print(f\"[DEBUG] Error en Bloque Robustez What-If: {e}\")\n",
        "        # --- Fin Bloque 8: Robustez What-If y Gráficas Extremos ---\n",
        "\n",
        "        # --- Bloque 9: Análisis e Interpretación IA de Robustez What-If ---\n",
        "        try:\n",
        "            print(\"[DEBUG] 17.40. Iniciando Bloque IA de Robustez What-If e Interpretación\")\n",
        "\n",
        "            # 👇 Extraemos del atributo best_model_info\n",
        "            metodo = self.best_model_info['metodo']\n",
        "            motor  = self.best_model_info['motor']\n",
        "            score  = self.best_model_info['score']\n",
        "\n",
        "            # 👇 Y las métricas guardadas como atributos\n",
        "            r2_cv_std = self.r2_cv_std\n",
        "            res_std   = self.res_std\n",
        "            cal_err   = self.cal_err\n",
        "\n",
        "            # DEBUG: comprobación de valores\n",
        "            print(f\"[DEBUG] 17.41. Mejor modelo → método={metodo}, motor={motor}, score={score:.4f}\")\n",
        "            print(f\"[DEBUG] 17.42. Métricas desde atributos → r2_cv_std={r2_cv_std:.4f}, res_std={res_std:.4f}, cal_err={cal_err:.4f}\")\n",
        "\n",
        "            # 1) Resumen numérico de escenarios what-if\n",
        "            stats = {\n",
        "                'mean_error': float(df_scen['error_abs'].mean()),\n",
        "                'std_error':  float(df_scen['error_abs'].std()),\n",
        "                'max_error':  float(df_scen['error_abs'].max()),\n",
        "                'min_error':  float(df_scen['error_abs'].min()),\n",
        "                'percentiles_error': {\n",
        "                    '25%': float(df_scen['error_abs'].quantile(0.25)),\n",
        "                    '50%': float(df_scen['error_abs'].quantile(0.50)),\n",
        "                    '75%': float(df_scen['error_abs'].quantile(0.75))\n",
        "                }\n",
        "            }\n",
        "            print(f\"[DEBUG] 17.43. Estadísticos de error abs: {stats}\")\n",
        "\n",
        "            # 2) Preparar tabla de escenarios en formato dict para el prompt\n",
        "            table_dict = df_scen[['__label__','y_base','y_pred','error_abs']].to_dict(orient='list')\n",
        "            print(\"[DEBUG] 17.44. Tabla de escenarios convertida a dict para prompt\")\n",
        "\n",
        "            # 3) Montar prompt rico en contexto\n",
        "            prompt_robustez = f\"\"\"\n",
        "        Eres un experto en evaluación de robustez de modelos y procesos físico-químicos.\n",
        "\n",
        "        A continuación tienes los resultados del análisis What-If para el modelo óptimo (método={metodo}, motor={motor}):\n",
        "\n",
        "        • Estadísticos globales del error absoluto:\n",
        "          - Media: {stats['mean_error']:.4f}\n",
        "          - Desviación estándar: {stats['std_error']:.4f}\n",
        "          - Mínimo: {stats['min_error']:.4f}\n",
        "          - Máximo: {stats['max_error']:.4f}\n",
        "          - Percentiles: 25%={stats['percentiles_error']['25%']:.4f}, 50%={stats['percentiles_error']['50%']:.4f}, 75%={stats['percentiles_error']['75%']:.4f}\n",
        "\n",
        "        • Detalle por escenario:\n",
        "          { { 'Escenarios': table_dict } }\n",
        "\n",
        "        Instrucciones para el análisis:\n",
        "        1. Describe para cada escenario (etiqueta) cómo varía la predicción frente al baseline y qué significa en términos del proceso físico-químico.\n",
        "        2. Identifica qué variables generan mayor sensibilidad en el modelo y por qué.\n",
        "        3. Resume los hallazgos generales de robustez y menciona si existen condiciones extremas donde el modelo falla o se vuelve inestable.\n",
        "        4. Propón recomendaciones para mejorar la robustez del modelo (por ejemplo, ajustes de hiperparámetros, transformaciones, data augmentation de escenarios extremos).\n",
        "        5. Sugiere acciones operativas concretas en planta (rangos de temperatura, concentraciones, etc.) basándote en los resultados What-If.\n",
        "        \"\"\"\n",
        "\n",
        "            print(\"[DEBUG] 17.45. Llamando a OpenAI directamente con _client.chat.completions.create…\")\n",
        "            response = _client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Eres un experto en Machine Learning aplicado a procesos físico-químicos.\"},\n",
        "                    {\"role\": \"user\",   \"content\": prompt_robustez}\n",
        "                ],\n",
        "                temperature=TEMPERATURE_VAL,\n",
        "                max_tokens=MAX_EXPLANATION_TOKENS\n",
        "            )\n",
        "            analysis_robustez = response.choices[0].message.content.strip()\n",
        "            print(f\"[DEBUG] 17.46. Longitud del análisis de robustez: {len(analysis_robustez)} caracteres\")\n",
        "\n",
        "            # 4) Añadir sección al informe\n",
        "            self.sections.append((\n",
        "                \"### Robustez What-If: Análisis e Interpretación IA\",\n",
        "                analysis_robustez\n",
        "            ))\n",
        "            print(\"[DEBUG] 17.47. Sección IA Robustez What-If añadida\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"{type(e).__name__}: {e}\"\n",
        "            print(f\"[DEBUG] Error en Bloque IA Robustez What-If: {error_msg}\")\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en Bloque IA Robustez What-If\",\n",
        "                error_msg\n",
        "            ))\n",
        "        # --- Fin Bloque 9: Análisis e Interpretación IA de Robustez What-If ---\n",
        "\n",
        "        # =============================================================================\n",
        "        #  Bloque 18: Conclusiones y Recomendaciones Generales\n",
        "        # =============================================================================\n",
        "        try:\n",
        "            print(\"[DEBUG] 18.1. Iniciando Bloque Final de Conclusiones y Recomendaciones\")\n",
        "\n",
        "            # 1) Extraer info clave del mejor modelo (asegúrate de haberla guardado antes)\n",
        "            best = getattr(self, 'best_model_info', {})\n",
        "            metodo = best.get('metodo', 'N/A')\n",
        "            motor  = best.get('motor', 'N/A')\n",
        "            score  = best.get('score', None)\n",
        "\n",
        "            # 2) Extraer métricas adicionales si las guardaste como atributos\n",
        "            r2_cv_std = getattr(self, 'r2_cv_std', None)\n",
        "            res_std   = getattr(self, 'res_std', None)\n",
        "            cal_err   = getattr(self, 'cal_err', None)\n",
        "\n",
        "            # 3) Formatear con control de None\n",
        "            score_txt   = f\"{score:.4f}\"    if score    is not None else \"N/A\"\n",
        "            r2_cv_txt   = f\"{r2_cv_std:.4f}\" if r2_cv_std is not None else \"N/A\"\n",
        "            res_std_txt = f\"{res_std:.4f}\"   if res_std   is not None else \"N/A\"\n",
        "            cal_err_txt = f\"{cal_err:.4f}\"   if cal_err   is not None else \"N/A\"\n",
        "\n",
        "            perf_txt = (\n",
        "                f\"- **Mejor modelo**: {metodo}  \\n\"\n",
        "                f\"- **Motor de optimización**: {motor}  \\n\"\n",
        "                f\"- **Score final (R² o indicador principal)**: {score_txt}  \\n\"\n",
        "                f\"- **Std R² en CV**: {r2_cv_txt}  \\n\"\n",
        "                f\"- **Std residuos**: {res_std_txt}  \\n\"\n",
        "                f\"- **Error medio de calibración**: {cal_err_txt}  \\n\"\n",
        "            )\n",
        "            print(f\"[DEBUG] 18.2. perf_txt:\\n{perf_txt}\")\n",
        "\n",
        "            # 4) Resumir metodología (títulos de secciones ya generados)\n",
        "            pasos = [\n",
        "                \"1. Carga y preprocesado de datos\",\n",
        "                \"2. Exploración estadística y visualización\",\n",
        "                \"3. Selección de variables independientes\",\n",
        "                \"4. Entrenamiento de modelos (SVR, NN, XGB, RF, RNN)\",\n",
        "                \"5. Optimización de hiperparámetros\",\n",
        "                \"6. Interpretabilidad (SHAP, PDP, ICE, ALE, counterfactuals)\",\n",
        "                \"7. Análisis de robustez What-If y escenarios extremos\"\n",
        "            ]\n",
        "            metod_txt = \"\\n\".join(f\"- {p}\" for p in pasos)\n",
        "            print(f\"[DEBUG] 18.3. metod_txt:\\n{metod_txt}\")\n",
        "\n",
        "            # 5) Inyectar snippets de texto IA previos (si los tienes)\n",
        "            snippets = []\n",
        "            for title, content in self.sections:\n",
        "                if title.startswith(\"### 📝 Explicación IA\"):\n",
        "                    # tomamos los primeros 200 caracteres de cada análisis\n",
        "                    text = str(content)[:200].replace(\"\\n\", \" \")                                # <------ AJUSTAR AQUI (INCREMENTAR ) PARA MEJORAR LA CALIDAD DEL ANÁLISIS DE LA IA\n",
        "                    snippets.append(f\"{title.lstrip('# ')}: «{text}...»\")\n",
        "            snippets_txt = \"\\n\".join(f\"- {s}\" for s in snippets)\n",
        "            print(f\"[DEBUG] 18.4. snippets_txt:\\n{snippets_txt}\")\n",
        "\n",
        "            # 6) Construir prompt final\n",
        "            prompt = (\n",
        "                \"Eres un investigador de Deep Learning y un experto en procesos físico-químicos industriales.\\n\"\n",
        "                \"Tienes la siguiente información:\\n\\n\"\n",
        "                \"**A) Rendimiento del mejor modelo**:\\n\"\n",
        "                f\"{perf_txt}\\n\"\n",
        "                \"**B) Metodología seguida**:\\n\"\n",
        "                f\"{metod_txt}\\n\\n\"\n",
        "                \"**C) Resúmenes de análisis previos**:\\n\"\n",
        "                f\"{snippets_txt}\\n\\n\"\n",
        "                \"Con toda esta información, por favor:\\n\"\n",
        "                \"1. Resume cada paso de la metodología en 1–2 frases, enfatizando aprendizajes clave.\\n\"\n",
        "                \"2. Sintetiza los hallazgos principales (tendencias, variables críticas, puntos fuertes/débiles del modelo).\\n\"\n",
        "                \"3. Conecta estos hallazgos con el proceso físico-químico: ¿qué parámetros de planta son más determinantes?\\n\"\n",
        "                \"4. Propón **3–5 acciones inmediatas** (p. ej. ajustes de operación, reentrenamiento, ampliación de datos).\\n\"\n",
        "                \"5. Diseña un roadmap de validación y despliegue (pruebas A/B, monitorización de drift, retrain schedule).\\n\"\n",
        "                \"6. Finaliza con un breve párrafo de cierre brillante que refuerce la confianza en el informe.\\n\"\n",
        "                \"7. Incluye **2–3 referencias bibliográficas** (APA) que respalden tus recomendaciones.\\n\"\n",
        "            )\n",
        "            print(f\"[DEBUG] 18.5. Prompt final construido (longitud {len(prompt)} caracteres)\")\n",
        "\n",
        "            # 7) Llamada a OpenAI\n",
        "            response = _client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Eres un investigador académico y consultor industrial senior.\"},\n",
        "                    {\"role\": \"user\",   \"content\": prompt}\n",
        "                ],\n",
        "                temperature=TEMPERATURE_VAL,\n",
        "                max_tokens=MAX_EXPLANATION_TOKENS\n",
        "            )\n",
        "            concl_text = response.choices[0].message.content.strip()\n",
        "            print(\"[DEBUG] 18.6. Respuesta IA recibida con longitud:\", len(concl_text))\n",
        "\n",
        "            # 8) Añadir sección final\n",
        "            self.sections.append((\n",
        "                \"### Conclusiones y Recomendaciones Finales\",\n",
        "                concl_text\n",
        "            ))\n",
        "            print(\"[DEBUG] 18.7. Sección de conclusiones añadida exitosamente\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] al generar Bloque Final de Conclusiones: {e}\")\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en Bloque de Conclusiones\",\n",
        "                str(e)\n",
        "            ))\n",
        "        # =============================================================================\n",
        "        #  Fin Bloque 18 Conclusiones\n",
        "        # =============================================================================\n",
        "\n",
        "        # ============================================================================\n",
        "        # --- Bloque 19: Descripción de PMS y Road-Map de Evolución Tecnológica ---\n",
        "        # ============================================================================\n",
        "        try:\n",
        "            print(\"[DEBUG] 19.1. Iniciando Bloque Descripción y roadmap de PMS\")\n",
        "\n",
        "            import os, glob, ast, inspect, json\n",
        "\n",
        "            base_dir = os.getcwd()\n",
        "\n",
        "            # 1) Archivos .py\n",
        "            py_files = glob.glob(os.path.join(base_dir, \"*.py\"))\n",
        "            loc_summary_py = {}\n",
        "            deps = set()\n",
        "            for fn in py_files:\n",
        "                try:\n",
        "                    with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
        "                        src = f.read()\n",
        "                    lines = src.splitlines()\n",
        "                    loc_summary_py[os.path.basename(fn)] = len(lines)\n",
        "                    tree = ast.parse(src)\n",
        "                    for node in ast.walk(tree):\n",
        "                        if isinstance(node, ast.Import):\n",
        "                            for n in node.names:\n",
        "                                deps.add(n.name.split(\".\")[0])\n",
        "                        elif isinstance(node, ast.ImportFrom):\n",
        "                            if node.module:\n",
        "                                deps.add(node.module.split(\".\")[0])\n",
        "                except Exception as ex:\n",
        "                    print(f\"[DEBUG] No pude procesar {fn}: {ex}\")\n",
        "\n",
        "            # 2) Notebooks .ipynb\n",
        "            ipynb_files = glob.glob(os.path.join(base_dir, \"*.ipynb\"))\n",
        "            nb_summary = {}\n",
        "            for fn in ipynb_files:\n",
        "                try:\n",
        "                    with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
        "                        nb = json.load(f)\n",
        "                    code_cells = sum(1 for c in nb.get(\"cells\", []) if c.get(\"cell_type\")==\"code\")\n",
        "                    nb_summary[os.path.basename(fn)] = code_cells\n",
        "                except Exception as ex:\n",
        "                    print(f\"[DEBUG] No pude procesar {fn}: {ex}\")\n",
        "\n",
        "            # 3) Inspeccionar ReportBuilder\n",
        "            members = inspect.getmembers(ReportBuilder)\n",
        "            methods = [name for name,obj in members if inspect.isfunction(obj)]\n",
        "            classes = [name for name,obj in members if inspect.isclass(obj)]\n",
        "\n",
        "            # 4) Secciones ya generadas\n",
        "            num_sections = len(self.sections)\n",
        "            titles = [t for t,_ in self.sections]\n",
        "\n",
        "            # 5) Formatear resumen\n",
        "            summary_lines = []\n",
        "            summary_lines.append(f\"• Archivos .py escaneados ({len(py_files)}):\")\n",
        "            for fn,loc in loc_summary_py.items():\n",
        "                summary_lines.append(f\"    – {fn}: {loc} líneas\")\n",
        "            if ipynb_files:\n",
        "                summary_lines.append(f\"• Notebooks .ipynb escaneados ({len(ipynb_files)}):\")\n",
        "                for fn,cells in nb_summary.items():\n",
        "                    summary_lines.append(f\"    – {fn}: {cells} celdas de código\")\n",
        "            summary_lines.append(f\"• Dependencias detectadas: {', '.join(sorted(deps))}\")\n",
        "            summary_lines.append(f\"• Métodos en ReportBuilder: {', '.join(methods)}\")\n",
        "            summary_lines.append(f\"• Clases definidas en ReportBuilder: {', '.join(classes)}\")\n",
        "            summary_lines.append(f\"• Secciones del informe generadas ({num_sections}):\")\n",
        "            for t in titles:\n",
        "                summary_lines.append(f\"    – {t}\")\n",
        "\n",
        "            summary_txt = \"\\n\".join(summary_lines)\n",
        "\n",
        "            # 6) Construir prompt\n",
        "            prompt_pms = f\"\"\"\n",
        "        Eres un arquitecto de software y consultor de procesos industriales.\n",
        "        A continuación tienes un **resumen de la herramienta PMS** y su código fuente:\n",
        "\n",
        "        {summary_txt}\n",
        "\n",
        "        En base a ello, por favor:\n",
        "        1. Describe **detalladamente la arquitectura** de la aplicación:\n",
        "          - Flujo de ejecución\n",
        "          - Principales módulos y clases\n",
        "          - Patrones de diseño o buenas prácticas observadas.\n",
        "\n",
        "        2. Señala **puntos fuertes** y **oportunidades de mejora**:\n",
        "          - Modularidad, legibilidad, rendimiento, uso de librerías.\n",
        "          - Aspectos que podrían complicar el mantenimiento o la escalabilidad.\n",
        "\n",
        "        3. Proporciona un **road-map de evolución tecnológica**:\n",
        "          - Refactorizaciones y modularización adicional.\n",
        "          - Incorporación de tests automatizados y pipeline de CI/CD.\n",
        "          - Contenerización / despliegue (Docker, Kubernetes, microservicios).\n",
        "          - Nuevas funcionalidades (AutoML, flujos batch/pipeline, APIs).\n",
        "\n",
        "        4. Recomienda **modelos, motores o métodos futuros**:\n",
        "          - E.g. Transformers para series temporales, AutoML para selección de modelos.\n",
        "          - Explica el **impacto** esperado en la calidad de la predicción y en la mantenibilidad.\n",
        "\n",
        "        5. Concluye con un **resumen ejecutivo** de 2–3 párrafos enfatizando el valor de estas mejoras.\n",
        "        \"\"\"\n",
        "\n",
        "            print(\"[DEBUG] 19.2. Llamando a OpenAI para Bloque 10 de PMS…\")\n",
        "            resp = _client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\":\"system\", \"content\":\"Eres investigador académico, arquitecto de software y consultor industrial.\"},\n",
        "                    {\"role\":\"user\",   \"content\":prompt_pms}\n",
        "                ],\n",
        "                temperature=TEMPERATURE_VAL,\n",
        "                max_tokens=MAX_EXPLANATION_TOKENS,\n",
        "            )\n",
        "            analysis_pms = resp.choices[0].message.content.strip()\n",
        "\n",
        "            # 7) Añadir sección al informe\n",
        "            self.sections.append((\n",
        "                \"### Descripción de PMS y Road-Map de Evolución Tecnológica\",\n",
        "                analysis_pms\n",
        "            ))\n",
        "            print(\"[DEBUG] 19.3. Bloque completado y añadido al informe\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[DEBUG] Error en Bloque 10: {e}\")\n",
        "            self.sections.append((\n",
        "                \"### ⚠️ Error en Descripción y Road-Map de PMS\",\n",
        "                str(e)\n",
        "            ))\n",
        "        # --- Fin Bloque 11 ---\n",
        "\n",
        "\n",
        "        print(\"[DEBUG] 20. ReportBuilder.build_sections end\")\n",
        "    # =============================================================\n",
        "    #  Renderizado de la salida del Informe\n",
        "    # =============================================================\n",
        "    def render(self):\n",
        "        from IPython.display import Markdown, display, HTML\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        print(\"[DEBUG] ReportBuilder.render start\")\n",
        "        for idx, (title, content) in enumerate(self.sections):\n",
        "            print(f\"[DEBUG] render sección #{idx}: {title}\")\n",
        "            # Mostrar título\n",
        "            display(Markdown(title))\n",
        "\n",
        "            # Si es DataFrame, aplicamos el estilo como antes\n",
        "            if isinstance(content, pd.DataFrame):\n",
        "                df = content\n",
        "                try:\n",
        "                    styled = (\n",
        "                        df.style\n",
        "                        .set_table_styles([\n",
        "                            {'selector': 'th',\n",
        "                            'props': [\n",
        "                                ('background-color', '#4F81BD'),\n",
        "                                ('color', 'white'),\n",
        "                                ('font-weight', 'bold'),\n",
        "                                ('padding', '8px'),\n",
        "                                ('text-align', 'center')\n",
        "                            ]},\n",
        "                            {'selector': 'td',\n",
        "                            'props': [\n",
        "                                ('padding', '8px'),\n",
        "                                ('text-align', 'center')\n",
        "                            ]}\n",
        "                        ])\n",
        "                        .apply(lambda row: ['background-color: #f2f2f2' if i%2 else '' for i in range(len(row))],\n",
        "                                axis=1)\n",
        "                        .set_caption(f\"Mostrando {df.shape[0]} filas y {df.shape[1]} columnas\")\n",
        "                    )\n",
        "                    html = styled.to_html()\n",
        "                    display(HTML(html))\n",
        "                    print(f\"[DEBUG] render sección #{idx}: DataFrame mostrado\")\n",
        "                except Exception as e:\n",
        "                    print(f\"[ERROR] al mostrar DataFrame en sección #{idx}: {e}\")\n",
        "            # Si es figura matplotlib\n",
        "            elif hasattr(content, 'savefig') or hasattr(content, 'dpi'):  # aproximación para Figure\n",
        "                try:\n",
        "                    display(content)  # IPython detecta Figure y la muestra\n",
        "                except Exception:\n",
        "                    plt.show(content)\n",
        "            else:\n",
        "                # Texto IA: como ya limitamos con max_tokens, lo mostramos directamente completo.\n",
        "                text = str(content or \"\")\n",
        "                # Imprimimos longitud para debug\n",
        "                length = len(text)\n",
        "                print(f\"[DEBUG] Longitud del contenido IA: {length} caracteres\")\n",
        "                # Mostrar todo el texto como Markdown\n",
        "                display(Markdown(text))\n",
        "                print(f\"[DEBUG] render sección #{idx}: texto IA mostrado completo\")\n",
        "        print(\"[DEBUG] ReportBuilder.render end\")\n",
        "# ____________________________________________________________________\n",
        "# --- Generar y Mostrar Informe ---\n",
        "# ____________________________________________________________________\n",
        "#import ipywidgets as widgets\n",
        "#from IPython.display import clear_output, display\n",
        "\n",
        "def mostrar_informe():\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import clear_output, display\n",
        "    \"\"\"\n",
        "    Construye TODO el informe, presenta un listado de checkboxes con los títulos\n",
        "    reales de sección, permite al usuario marcarlos (todos DESMARCADOS por defecto)\n",
        "    y al pulsar 'Generar Informe' limpia la pantalla y muestra solo las secciones elegidas.\n",
        "    \"\"\"\n",
        "\n",
        "    # === Módulo de Exportaciones: Word, PDF y HTML ===\n",
        "    import io, base64, os\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from IPython.display import display, HTML\n",
        "    from docx import Document\n",
        "    from docx.shared import Inches\n",
        "    from fpdf import FPDF\n",
        "    import ipywidgets as widgets\n",
        "    import unicodedata\n",
        "\n",
        "    def limpiar_texto(texto):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFKD', str(texto))\n",
        "            if ord(c) < 256\n",
        "        )\n",
        "\n",
        "    def exportar_informe(builder):\n",
        "        # === Botones de Exportación ===\n",
        "        btn_export_word_via_html = widgets.Button(\n",
        "            description=\"Exportar a Word\", button_style=\"info\",\n",
        "            layout=widgets.Layout(margin=\"10px 10px 10px 0\")\n",
        "        )\n",
        "#        btn_export_word = widgets.Button(\n",
        "#            description=\"Exportar a Word\", button_style=\"info\",\n",
        "#            layout=widgets.Layout(margin=\"10px 10px 10px 0\")\n",
        "#        )\n",
        "        btn_export_pdf = widgets.Button(\n",
        "            description=\"Exportar a PDF\", button_style=\"warning\",\n",
        "            layout=widgets.Layout(margin=\"10px 10px 10px 0\")\n",
        "        )\n",
        "        btn_export_html = widgets.Button(\n",
        "            description=\"Exportar a HTML\", button_style=\"primary\",\n",
        "            layout=widgets.Layout(margin=\"10px 10px 10px 0\")\n",
        "        )\n",
        "\n",
        "        def export_to_word_via_html(builder):\n",
        "            import io\n",
        "            import base64\n",
        "            import os\n",
        "            import pypandoc\n",
        "            import pandas as pd\n",
        "            import matplotlib.pyplot as plt\n",
        "            from IPython.display import display, HTML\n",
        "\n",
        "            html_parts = [\"<html><head><meta charset='utf-8'><title>Informe</title></head><body>\"]\n",
        "            html_parts.append(\"<h1>Informe Generado</h1>\")\n",
        "\n",
        "            for title, content in builder.sections:\n",
        "                clean_title = title.lstrip('# ').strip()\n",
        "                html_parts.append(f\"<h2>{clean_title}</h2>\")\n",
        "\n",
        "                try:\n",
        "                    if isinstance(content, pd.DataFrame):\n",
        "                        html_parts.append(content.head(20).to_html(index=False))\n",
        "                        if len(content) > 20:\n",
        "                            html_parts.append(f\"<p><em>⚠️ Tabla truncada: solo primeras 20 filas de {len(content)}.</em></p>\")\n",
        "\n",
        "                    elif hasattr(content, \"savefig\"):\n",
        "                        img_buf = io.BytesIO()\n",
        "                        content.savefig(img_buf, format='png', bbox_inches='tight')\n",
        "                        img_buf.seek(0)\n",
        "                        img_b64 = base64.b64encode(img_buf.read()).decode()\n",
        "                        html_parts.append(f\"<img src='data:image/png;base64,{img_b64}' style='max-width:100%;'><br>\")\n",
        "\n",
        "                    else:\n",
        "                        texto = str(content)\n",
        "                        lineas = texto.splitlines()\n",
        "                        if len(lineas) > 100:\n",
        "                            texto = \"\\n\".join(lineas[:100]) + \"\\n... (contenido truncado)\"\n",
        "                        html_parts.append(f\"<pre>{texto}</pre>\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    html_parts.append(f\"<p><strong>⚠️ Error al procesar la sección:</strong> {e}</p>\")\n",
        "\n",
        "            html_parts.append(\"</body></html>\")\n",
        "\n",
        "            # Guardar HTML temporal\n",
        "            html_filename = \"informe_temporal.html\"\n",
        "            with open(html_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(\"\\n\".join(html_parts))\n",
        "\n",
        "            # Convertir HTML → DOCX usando pypandoc\n",
        "            docx_filename = \"informe_generado.docx\"\n",
        "            try:\n",
        "                pypandoc.convert_file(html_filename, 'docx', outputfile=docx_filename)\n",
        "                print(f\"[DEBUG] Documento Word generado desde HTML: {docx_filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Fallo en la conversión HTML → Word: {e}\")\n",
        "                return\n",
        "\n",
        "            # Mostrar enlace de descarga\n",
        "            with open(docx_filename, \"rb\") as f:\n",
        "                b64 = base64.b64encode(f.read()).decode()\n",
        "            href = (\n",
        "                f'<a download=\"{docx_filename}\" '\n",
        "                f'href=\"data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,{b64}\">'\n",
        "                \"⬇️ Descargar informe Word</a>\"\n",
        "            )\n",
        "            display(HTML(href))\n",
        "\n",
        "\n",
        "#            def export_to_word(b):\n",
        "#                print(\"[DEBUG] Iniciando exportación a Word…\")\n",
        "#                doc = Document()\n",
        "#                doc.add_heading('Informe Generado', level=0)\n",
        "\n",
        "#                for title, content in builder.sections:\n",
        "#                    clean_title = title.lstrip('# ').strip()\n",
        "#                    doc.add_heading(clean_title, level=1)\n",
        "\n",
        "#                    if isinstance(content, pd.DataFrame):\n",
        "#                        table = doc.add_table(rows=1, cols=len(content.columns))\n",
        "#                        hdr = table.rows[0].cells\n",
        "#                        for i, col in enumerate(content.columns):\n",
        "#                            hdr[i].text = str(col)\n",
        "#                        for row in content.itertuples(index=False):\n",
        "#                            cells = table.add_row().cells\n",
        "#                            for i, val in enumerate(row):\n",
        "#                                cells[i].text = str(val)\n",
        "\n",
        "#                    elif hasattr(content, \"savefig\"):\n",
        "#                        img_stream = io.BytesIO()\n",
        "#                        content.savefig(img_stream, format='png', bbox_inches='tight')\n",
        "#                        img_stream.seek(0)\n",
        "#                        doc.add_picture(img_stream, width=Inches(6))\n",
        "\n",
        "#                    else:\n",
        "#                        for line in str(content).splitlines():\n",
        "#                            doc.add_paragraph(line)\n",
        "\n",
        "#                    doc.add_page_break()\n",
        "\n",
        "#                if len(doc.paragraphs) == 0:\n",
        "#                    doc.add_paragraph(\"⚠️ El informe no contiene contenido válido para exportar.\")\n",
        "\n",
        "#                output_path = \"informe_generado.docx\"\n",
        "#                doc.save(output_path)\n",
        "#                print(f\"[DEBUG] Documento guardado en {output_path}\")\n",
        "\n",
        "#                with open(output_path, \"rb\") as f:\n",
        "#                    b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "#                href = (\n",
        "#                    f'<a download=\"{output_path}\" '\n",
        "#                    f'href=\"data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,{b64}\">'\n",
        "#                    \"⬇️ Descargar informe Word</a>\"\n",
        "#                )\n",
        "#                display(HTML(href))\n",
        "\n",
        "        def export_to_pdf(b):\n",
        "            print(\"[DEBUG] Iniciando exportación a PDF…\")\n",
        "            pdf = FPDF(orientation='L', unit='mm', format='A4')  # Apaisado\n",
        "            pdf.set_auto_page_break(auto=True, margin=15)\n",
        "            pdf.add_page()\n",
        "            pdf.set_font(\"Arial\", 'B', 14)\n",
        "            pdf.cell(0, 10, limpiar_texto('Informe Generado'), ln=True)\n",
        "            #pdf = FPDF()\n",
        "            #pdf.cell(0, 10, 'Informe Generado', ln=True)\n",
        "\n",
        "            for title, content in builder.sections:\n",
        "                pdf.add_page()\n",
        "                pdf.set_font(\"Arial\", 'B', 12)\n",
        "                #pdf.multi_cell(0, 10, title.lstrip('# ').strip())\n",
        "                pdf.multi_cell(0, 10, limpiar_texto(title.lstrip('# ').strip()))\n",
        "\n",
        "\n",
        "                if isinstance(content, pd.DataFrame):\n",
        "                    pdf.set_font(\"Arial\", '', 8)      # Reducimos tamaño de letra de 10 a 8 para hacer más compacto el informe\n",
        "                    col_width = 270 / len(content.columns)  # Distribución proporcional\n",
        "                    for col in content.columns:\n",
        "                        #pdf.cell(40, 10, str(col), border=1)\n",
        "                        pdf.cell(col_width, 10, limpiar_texto(col), border=1)\n",
        "                    pdf.ln()\n",
        "                    for row in content.itertuples(index=False):\n",
        "                        for val in row:\n",
        "                            #pdf.cell(40, 10, str(val), border=1)\n",
        "                            pdf.cell(col_width, 10, limpiar_texto(val), border=1)\n",
        "                        pdf.ln()\n",
        "\n",
        "                elif hasattr(content, \"savefig\"):\n",
        "                    img_buf = io.BytesIO()\n",
        "                    content.savefig(img_buf, format='png', bbox_inches='tight')\n",
        "                    img_buf.seek(0)\n",
        "                    with open(\"temp_fig.png\", \"wb\") as f:\n",
        "                        f.write(img_buf.read())\n",
        "                    #df.image(\"temp_fig.png\", x=10, w=180)\n",
        "                    pdf.image(\"temp_fig.png\", x=10, w=270)  # Ancho máximo para A4 apaisado\n",
        "                    os.remove(\"temp_fig.png\")\n",
        "\n",
        "                elif isinstance(content, str):\n",
        "                    pdf.set_font(\"Arial\", '', 10)        # Reducimos tamaño de letra de 12 a 10 para hacer más compacto el informe\n",
        "                    #pdf.multi_cell(0, 10, content)\n",
        "                    pdf.multi_cell(0, 10, limpiar_texto(content))\n",
        "\n",
        "\n",
        "            filename = \"informe_generado.pdf\"\n",
        "            pdf.output(filename)\n",
        "            print(f\"[DEBUG] Informe PDF generado correctamente: {filename}\")\n",
        "\n",
        "            with open(filename, \"rb\") as f:\n",
        "                b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "            href = (\n",
        "                f'<a download=\"{filename}\" '\n",
        "                f'href=\"data:application/pdf;base64,{b64}\">'\n",
        "                \"⬇️ Descargar informe PDF</a>\"\n",
        "            )\n",
        "            display(HTML(href))\n",
        "\n",
        "        def export_to_html(b):\n",
        "            print(\"[DEBUG] Iniciando exportación a HTML…\")\n",
        "            html_content = [\"<html><head><meta charset='utf-8'><title>Informe</title></head><body>\"]\n",
        "            html_content.append(\"<h1>Informe Generado</h1>\")\n",
        "\n",
        "            for title, content in builder.sections:\n",
        "                html_content.append(f\"<h2>{title.lstrip('# ').strip()}</h2>\")\n",
        "\n",
        "                if isinstance(content, pd.DataFrame):\n",
        "                    html_content.append(content.to_html(index=False))\n",
        "                elif hasattr(content, \"savefig\"):\n",
        "                    img_buf = io.BytesIO()\n",
        "                    content.savefig(img_buf, format='png', bbox_inches='tight')\n",
        "                    img_buf.seek(0)\n",
        "                    encoded = base64.b64encode(img_buf.read()).decode()\n",
        "                    html_content.append(f\"<img src='data:image/png;base64,{encoded}' style='max-width:100%'>\")\n",
        "                else:\n",
        "                    html_content.append(f\"<pre>{str(content)}</pre>\")\n",
        "\n",
        "            html_content.append(\"</body></html>\")\n",
        "\n",
        "            filename = \"informe_generado.html\"\n",
        "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(\"\\n\".join(html_content))\n",
        "\n",
        "            with open(filename, \"rb\") as f:\n",
        "                b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "            href = (\n",
        "                f'<a download=\"{filename}\" '\n",
        "                f'href=\"data:text/html;base64,{b64}\">'\n",
        "                \"⬇️ Descargar informe HTML</a>\"\n",
        "            )\n",
        "            display(HTML(href))\n",
        "\n",
        "        btn_export_word_via_html.on_click(export_to_word_via_html)\n",
        "        btn_export_pdf.on_click(export_to_pdf)\n",
        "        btn_export_html.on_click(export_to_html)\n",
        "\n",
        "        display(widgets.HBox([btn_export_word_via_html, btn_export_pdf, btn_export_html]))\n",
        "\n",
        "    # === Módulo de Generación y Presentación del Informe ===\n",
        "    # 1) Generamos todas las secciones internamente (sin render aún)\n",
        "    builder = ReportBuilder(globals())\n",
        "    builder.build_sections()\n",
        "\n",
        "    # 2) Creamos una lista de 'títulos limpios' para los checkboxes\n",
        "    #    Eliminamos el prefijo '### ' y espacios sobrantes\n",
        "    clean_titles = [title.lstrip(\"# \").strip() for title, _ in builder.sections]\n",
        "\n",
        "#    # 3) Checkbox dinámico (todos DESMARCADOS)\n",
        "#    checkboxes = [\n",
        "#        widgets.Checkbox(value=False, description=clean_title, indent=False)\n",
        "#        for clean_title in clean_titles\n",
        "#    ]\n",
        "    # 3) Checkbox dinámico (todos MARCADOS por defecto)\n",
        "    checkboxes = [\n",
        "        widgets.Checkbox(value=True, description=clean_title, indent=False)\n",
        "        for clean_title in clean_titles\n",
        "    ]\n",
        "\n",
        "    # 4) Checkbox global para marcar/desmarcar todos\n",
        "    toggle_all = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description=\"(Des)marcar todas las secciones\",\n",
        "        indent=False,\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    # 5) Contador dinámico de secciones seleccionadas\n",
        "    counter_label = widgets.Label()\n",
        "    def actualizar_contador(_=None):\n",
        "        n_sel = sum(cb.value for cb in checkboxes)\n",
        "        counter_label.value = f\"Secciones seleccionadas: {n_sel} de {len(checkboxes)}\"\n",
        "    actualizar_contador()  # Inicializa\n",
        "\n",
        "    # 6) Callback del checkbox global\n",
        "    def _on_toggle_all(change):\n",
        "        for cb in checkboxes:\n",
        "            cb.value = toggle_all.value\n",
        "        actualizar_contador()\n",
        "\n",
        "    toggle_all.observe(_on_toggle_all, names='value')\n",
        "\n",
        "    # 7) Callback individual para cada checkbox (actualiza contador)\n",
        "    for cb in checkboxes:\n",
        "        cb.observe(actualizar_contador, names='value')\n",
        "\n",
        "    # 8) Botón de ejecución\n",
        "    btn_generate = widgets.Button(\n",
        "        description=\"Generar Informe\",\n",
        "        button_style=\"success\",\n",
        "        layout=widgets.Layout(margin=\"10px 0 0 0\")\n",
        "    )\n",
        "\n",
        "    # 9) Montamos el UI\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>Selecciona las secciones a incluir en el informe:</h3>\"),\n",
        "        toggle_all,  # ⬅️ Añadimos el checkbox global\n",
        "        counter_label,\n",
        "        widgets.VBox(checkboxes),\n",
        "        btn_generate\n",
        "    ])\n",
        "    display(ui)\n",
        "\n",
        "    # 10) Callback: al pulsar el botón, filtramos y renderizamos\n",
        "    def _on_generate_clicked(_):\n",
        "        from IPython.display import clear_output, display\n",
        "        clear_output(wait=True)\n",
        "        # a) Recogemos sólo los títulos marcados\n",
        "        seleccion = {\n",
        "            cb.description\n",
        "            for cb in checkboxes\n",
        "            if cb.value\n",
        "        }\n",
        "        # b) Filtramos builder.sections por coincidencia exacta del título limpio\n",
        "        filtered = []\n",
        "        for (orig_title, content), clean_title in zip(builder.sections, clean_titles):\n",
        "            if clean_title in seleccion:\n",
        "                filtered.append((orig_title, content))\n",
        "\n",
        "        # c) Reemplazamos y renderizamos\n",
        "        builder.sections = filtered\n",
        "        builder.render()\n",
        "\n",
        "        # ✅ d) Llamada a exportar_informe (Word, PDF, HTML)\n",
        "        exportar_informe(builder)\n",
        "\n",
        "    # 10) Asignar callback\n",
        "    btn_generate.on_click(_on_generate_clicked)\n",
        "\n",
        "#        import ipywidgets as widgets\n",
        "#        from IPython.display import clear_output, display, HTML\n",
        "#        from docx import Document\n",
        "#        from docx.shared import Inches\n",
        "#        import io, base64\n",
        "\n",
        "#        def _export_to_word(b):\n",
        "#            from IPython.display import clear_output, display, HTML\n",
        "#            from docx import Document\n",
        "#            from docx.shared import Inches\n",
        "#            import ipywidgets as widgets\n",
        "#            import pandas as _pd\n",
        "#            import io, base64, traceback\n",
        "\n",
        "#            clear_output(wait=True)\n",
        "#            print(\"[DEBUG] Iniciando exportación a Word…\")\n",
        "\n",
        "#            try:\n",
        "#                # 1) Crear documento Word\n",
        "#                doc = Document()\n",
        "#                doc.add_heading('Informe Generado', level=0)\n",
        "\n",
        "#                secciones_exportadas = 0\n",
        "#                errores = []\n",
        "\n",
        "#                for title, content in builder.sections:\n",
        "#                    try:\n",
        "#                        clean_title = title.lstrip('# ').strip()\n",
        "#                        doc.add_heading(clean_title, level=1)\n",
        "\n",
        "#                        if isinstance(content, _pd.DataFrame):\n",
        "#                            df = content\n",
        "#                            table = doc.add_table(rows=1, cols=len(df.columns))\n",
        "#                            hdr = table.rows[0].cells\n",
        "#                            for i, col in enumerate(df.columns):\n",
        "#                                hdr[i].text = str(col)\n",
        "#                            for row in df.itertuples(index=False):\n",
        "#                                cells = table.add_row().cells\n",
        "#                                for i, val in enumerate(row):\n",
        "#                                    cells[i].text = str(val)\n",
        "\n",
        "#                        elif hasattr(content, \"savefig\") and content is not None:\n",
        "#                            try:\n",
        "#                                img_stream = io.BytesIO()\n",
        "#                                content.savefig(img_stream, format='png', bbox_inches='tight')\n",
        "#                                img_stream.seek(0)\n",
        "#                                doc.add_picture(img_stream, width=Inches(6))\n",
        "#                            except Exception as e:\n",
        "#                                msg = f\"[ERROR] Fallo al insertar imagen en sección '{clean_title}': {e_img}\"\n",
        "#                                print(msg)\n",
        "#                                errores.append(msg)\n",
        "#                                doc.add_paragraph(f\"⚠️ No se pudo insertar la figura: {e}\")\n",
        "\n",
        "#                        elif isinstance(content, str):\n",
        "#                            for line in content.splitlines():\n",
        "#                                doc.add_paragraph(line)\n",
        "\n",
        "#                        else:\n",
        "#                            msg = f\"[WARNING] Tipo de contenido no reconocido en sección '{clean_title}': {type(content)}\"\n",
        "#                            print(msg)\n",
        "#                            errores.append(msg)\n",
        "#                            doc.add_paragraph(f\"⚠️ Contenido no reconocido o vacío: {type(content)}\")\n",
        "\n",
        "#                        doc.add_page_break()\n",
        "#                        secciones_exportadas += 1\n",
        "\n",
        "#                    except Exception as e_sec:\n",
        "#                        msg = f\"[ERROR] Fallo exportando sección '{title}': {e_sec}\"\n",
        "#                        print(msg)\n",
        "#                        errores.append(msg)\n",
        "#                        doc.add_paragraph(f\"⚠️ Error en sección '{title}': {e_sec}\")\n",
        "#                        traceback.print_exc()\n",
        "\n",
        "#                # Añadir advertencia si no se añadió nada\n",
        "#                if len(doc.paragraphs) == 0:\n",
        "#                    doc.add_paragraph(\"⚠️ El informe no contiene contenido válido para exportar.\")#\n",
        "\n",
        "#                # 2) Guardar documento\n",
        "#                output_path = \"informe_generado.docx\"\n",
        "#                doc.save(output_path)\n",
        "#                print(f\"[DEBUG] Documento guardado en {output_path}\")\n",
        "#                print(f\"[DEBUG] Secciones exportadas correctamente: {secciones_exportadas}\")\n",
        "#                if errores:\n",
        "#                    print(\"[DEBUG] Secciones con errores:\", len(errores))\n",
        "#                    for err in errores:\n",
        "#                        print(\"[DEBUG]\", err)\n",
        "\n",
        "#                # 3) Convertir a base64 para enlace de descarga\n",
        "#                with open(output_path, \"rb\") as f:\n",
        "#                    b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "#                href = (\n",
        "#                    f'<a download=\"{output_path}\" '\n",
        "#                    f'href=\"data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,{b64}\">'\n",
        "#                    \"⬇️ Descargar informe Word</a>\"\n",
        "#                )\n",
        "#                display(HTML(href))\n",
        "\n",
        "#            except Exception as e:\n",
        "#                print(f\"[ERROR] Fallo en la exportación a Word: {e}\")\n",
        "#                traceback.print_exc()\n",
        "#                display(HTML(\"<b style='color:red;'>❌ Error al generar el documento Word.</b>\"))\n",
        "\n",
        "\n",
        "#        def _export_to_word(b):\n",
        "#            # Import local para no chocar con variables externas\n",
        "#            from IPython.display import clear_output, display, HTML\n",
        "#            from docx import Document\n",
        "#            from docx.shared import Inches\n",
        "#            import ipywidgets as widgets\n",
        "#            import pandas as _pd\n",
        "\n",
        "#            clear_output(wait=True)\n",
        "#            print(\"[DEBUG] Iniciando exportación a Word…\")\n",
        "\n",
        "#            # 1) Crear documento y volcar todas las secciones\n",
        "#            doc = Document()\n",
        "#            doc.add_heading('Informe Generado', level=0)\n",
        "\n",
        "#            for title, content in builder.sections:\n",
        "#                clean_title = title.lstrip('# ').strip()\n",
        "#                doc.add_heading(clean_title, level=1)\n",
        "\n",
        "#                if isinstance(content, _pd.DataFrame):\n",
        "#                    df = content\n",
        "#                    table = doc.add_table(rows=1, cols=len(df.columns))\n",
        "#                    hdr = table.rows[0].cells\n",
        "#                    for i, col in enumerate(df.columns):\n",
        "#                        hdr[i].text = str(col)\n",
        "#                    for row in df.itertuples(index=False):\n",
        "#                        cells = table.add_row().cells\n",
        "#                        for i, val in enumerate(row):\n",
        "#                            cells[i].text = str(val)\n",
        "\n",
        "#                #elif hasattr(content, \"savefig\"):\n",
        "#                #    img_stream = io.BytesIO()\n",
        "#                #    content.savefig(img_stream, format='png', bbox_inches='tight')\n",
        "#                #    img_stream.seek(0)\n",
        "#                #    doc.add_picture(img_stream, width=Inches(6))\n",
        "\n",
        "#                elif hasattr(content, \"savefig\"):\n",
        "#                    try:\n",
        "#                        img_stream = io.BytesIO()\n",
        "#                        content.savefig(img_stream, format='png', bbox_inches='tight')\n",
        "#                        img_stream.seek(0)\n",
        "#                        if img_stream.getbuffer().nbytes > 0:\n",
        "#                            doc.add_picture(img_stream, width=Inches(6))\n",
        "#                        else:\n",
        "#                            doc.add_paragraph(\"[ERROR: La imagen generada está vacía o corrupta]\")\n",
        "#                    except Exception as e:\n",
        "#                        doc.add_paragraph(f\"[ERROR al guardar imagen: {type(e).__name__}: {e}]\")\n",
        "\n",
        "#                else:\n",
        "#                    for line in str(content).splitlines():\n",
        "#                        doc.add_paragraph(line)\n",
        "\n",
        "#                doc.add_page_break()\n",
        "\n",
        "#            if len(doc.paragraphs) == 0:\n",
        "#                doc.add_paragraph(\"⚠️ El informe no contiene contenido válido para exportar.\")\n",
        "\n",
        "#            # 2) Guardar a disco\n",
        "#            output_path = \"informe_generado.docx\"\n",
        "#            doc.save(output_path)\n",
        "#            print(f\"[DEBUG] Documento guardado en {output_path}\")\n",
        "\n",
        "#            # 3) Convertir a Base64 para Data URI\n",
        "#            with open(output_path, \"rb\") as f:\n",
        "#                b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "#            href = (\n",
        "#                f'<a download=\"{output_path}\" '\n",
        "#                f'href=\"data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,{b64}\">'\n",
        "#                \"⬇️ Descargar informe Word</a>\"\n",
        "#            )\n",
        "#            display(HTML(href))\n",
        "\n",
        "\n",
        "\n",
        "#        # ——— En tu función mostrar_informe, justo DESPUÉS de builder.render() pon:\n",
        "\n",
        "#        btn_export = widgets.Button(\n",
        "#            description=\"Exportar a Word\",\n",
        "#            button_style=\"info\",\n",
        "#            layout=widgets.Layout(margin=\"10px 0 0 0\")\n",
        "#        )\n",
        "#        btn_export.on_click(_export_to_word)\n",
        "#        display(btn_export)\n",
        "\n",
        "#    btn_generate.on_click(_on_generate_clicked)\n",
        "\n",
        "#def mostrar_informe():\n",
        "#    \"\"\"Menú: Generar Informe.\"\"\"\n",
        "#    clear_output(wait=True)\n",
        "#    print(\"[DEBUG] mostrar_informe start\")\n",
        "#    builder = ReportBuilder(globals())\n",
        "#    builder.build_sections()\n",
        "#    builder.render()\n",
        "#    print(\"[DEBUG] mostrar_informe end\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 13. MENÚ PRINCIPAL – FUNCIONA\n",
        "# ===============================================================\n",
        "#  ⥥  NIVEL-1                          ⥥  NIVEL-2                     ⥥  NIVEL-3  (sin cambios)\n",
        "# ───────────────────────────────────────────────────────────────────────────────────────────────\n",
        "# 1) Bienvenida                   ─▶   1. Bienvenida\n",
        "# 2) Ayuda General                ─▶   8. Ayuda Global\n",
        "# 3) Bloque 1 – Carga · Selección ─▶   2. Carga/Segmentación     ─▶  2.1 Carga de Datos\n",
        "#                                   │                             └▶  2.2 Segmentación Datos\n",
        "#                                   └▶  3. Selección variables X ─▶  (sin sub-nivel, invoca directo)\n",
        "# 4) Bloque 2 – Entrenamiento      ─▶   4. Entrenamiento Modelos ─▶  4.1 SVR · 4.2 NN · 4.3 XGB …\n",
        "#                                   └▶  5. Predicción/Visualiz.  ─▶  5.1 Pred.SVR · 5.2 Pred.NN …\n",
        "# 5) Bloque 3 – Optimización       ─▶   6. Optimización modelos  ─▶  6.1 SVR · 6.2 NN …\n",
        "# 6) Bloque 4 – Interpretabilidad  ─▶   7. Interpretación xIA    ─▶  (sin sub-nivel, invoca directo)\n",
        "# 7) Generar Informe Final         ─▶   8. Generar Informe Final ─▶  (sin sub-nivel, invoca directo)\n",
        "# ───────────────────────────────────────────────────────────────────────────────────────────────\n",
        "from ipywidgets import Dropdown, Button, VBox, HBox, Output\n",
        "from IPython.display import HTML as dHTML, clear_output\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 0 ▸ Diccionario HOJAS  →  función\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "menu_funcs = {\n",
        "    # Bienvenida + Ayuda\n",
        "    \"1. Bienvenida\":            mostrar_bienvenida,\n",
        "    \"2. Ayuda Global\":          mostrar_ayuda_completa,\n",
        "\n",
        "    # Bloque 1\n",
        "    \"B1.1.1 Carga de Datos\":       mostrar_carga,\n",
        "    \"B1.1.2 Segmentación Datos\":   mostrar_split,\n",
        "    \"B1.2.1 Selección variables X\": mostrar_seleccion_variables,\n",
        "\n",
        "    # Bloque 2 – Entrenamiento\n",
        "    \"B2.1.1 Entrenamiento SVR\":            mostrar_svr,\n",
        "    \"B2.1.2 Entrenamiento NN\":             mostrar_nn,\n",
        "    \"B2.1.3 Entrenamiento XGBoost\":        mostrar_xgb,\n",
        "    \"B2.1.4 Entrenamiento Random Forest\":  mostrar_rf,\n",
        "    \"B2.1.5 Entrenamiento RNN\":            mostrar_rnn,\n",
        "    \"B2.1.6 Comparador Modelos\":           mostrar_comparador_modelos,\n",
        "\n",
        "    # Bloque 2 – Predicción\n",
        "    \"B2.2.1 Predicción SVR\":               mostrar_prediccion_svr,\n",
        "    \"B2.2.2 Predicción NN\":                mostrar_prediccion_nn,\n",
        "    \"B2.2.3 Predicción XGBoost\":           mostrar_prediccion_xgboost,\n",
        "    \"B2.2.4 Predicción Random Forest\":     mostrar_prediccion_rf,\n",
        "    \"B2.2.5 Predicción RNN\":               mostrar_prediccion_rnn,\n",
        "    \"B2.2.6 Visualización resultados\":     mostrar_grafico_y_vs_x,\n",
        "\n",
        "    # Bloque 3 – Optimización\n",
        "    \"B3.1.1 Optimización SVR\":             mostrar_optimizacion_svr,\n",
        "    \"B3.1.2 Optimización NN\":              mostrar_optimizacion_nn,\n",
        "    \"B3.1.3 Optimización XGBoost\":         mostrar_optimizacion_xgb,\n",
        "    \"B3.1.4 Optimización Random Forest\":   mostrar_optimizacion_rf,\n",
        "    \"B3.1.5 Optimización RNN\":             mostrar_optimizacion_rnn,\n",
        "\n",
        "    # Bloque 4 – xIA\n",
        "    \"B4.1 Interpretación xIA\":            mostrar_xai,\n",
        "\n",
        "    # Informe Final\n",
        "    \"Generar Informe Final\":              mostrar_informe,\n",
        "}\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 1 ▸ Árbol multinivel\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "menu_tree = {\n",
        "    \"Bienvenida\": {\n",
        "        \"1. Bienvenida\": None\n",
        "    },\n",
        "    \"Ayuda General\": {\n",
        "        \"2. Ayuda Global\": None\n",
        "    },\n",
        "    \"Bloque 1 – Carga y segmentación de datos y Selección Variables\": {\n",
        "        \"B1.1 Carga y Segmentación de Datos\": {\n",
        "            \"B1.1.1 Carga de Datos\": None,\n",
        "            \"B1.1.2 Segmentación Datos\": None,\n",
        "        },\n",
        "        \"B1.2 Selección variables X\": {\n",
        "            \"B1.2.1 Selección variables X\": None,\n",
        "        },\n",
        "    },\n",
        "    \"Bloque 2 – Entrenamiento de modelos IA y Predicción de Salidas\": {\n",
        "        \"B2.1 Entrenamiento Modelos IA\": {\n",
        "            \"B2.1.1 Entrenamiento SVR\": None,\n",
        "            \"B2.1.2 Entrenamiento NN\": None,\n",
        "            \"B2.1.3 Entrenamiento XGBoost\": None,\n",
        "            \"B2.1.4 Entrenamiento Random Forest\": None,\n",
        "            \"B2.1.5 Entrenamiento RNN\": None,\n",
        "            \"B2.1.6 Comparador Modelos\": None,\n",
        "        },\n",
        "        \"B2.2 Predicción y visualización de datos de salida\": {\n",
        "            \"B2.2.1 Predicción SVR\": None,\n",
        "            \"B2.2.2 Predicción NN\": None,\n",
        "            \"B2.2.3 Predicción XGBoost\": None,\n",
        "            \"B2.2.4 Predicción Random Forest\": None,\n",
        "            \"B2.2.5 Predicción RNN\": None,\n",
        "            \"B2.2.6 Visualización resultados\": None,\n",
        "        },\n",
        "    },\n",
        "    \"Bloque 3 – Optimización de Modelos IA\": {\n",
        "        \"B3.1 Optimización de modelos IA\": {\n",
        "            \"B3.1.1 Optimización SVR\": None,\n",
        "            \"B3.1.2 Optimización NN\": None,\n",
        "            \"B3.1.3 Optimización XGBoost\": None,\n",
        "            \"B3.1.4 Optimización Random Forest\": None,\n",
        "            \"B3.1.5 Optimización RNN\": None,\n",
        "        },\n",
        "    },\n",
        "    \"Bloque 4 – Inteligencia Artificial Explicativa xIA\": {\n",
        "         \"B4.1 Interpretación xIA\": None,\n",
        "#        \"B4.1 Interpretación xIA\": {\n",
        "#            \"B4.1.1 Interpretación xIA\": None,\n",
        "#        },\n",
        "    },\n",
        "    \"Generar Informe Final\": {\n",
        "         \"Generar Informe Final\": None,\n",
        "#        \"Generar Informe Final\": {\n",
        "#            \"Generar Informe Final\": None,\n",
        "#        },\n",
        "    },\n",
        "}\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 2 ▸ Helpers\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "def _crear_dropdown(options, nivel):\n",
        "    return widgets.Dropdown(\n",
        "        options=options,\n",
        "        description=f\"Nivel-{nivel}:\",\n",
        "        layout={\n",
        "            'width': '100%'          # ⬅️  ancho fluido\n",
        "            # o '1100px', '80%', etc.\n",
        "        },\n",
        "        style={\n",
        "            'description_width': '200px'  # ajusta si hiciera falta\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def _subtree_for(path):\n",
        "    node = menu_tree\n",
        "    for key in path:\n",
        "        node = node[key]\n",
        "    return node  # None → hoja\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 3 ▸ Widgets contenedores\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "levels_box = VBox(layout={'width': 'auto'})  # ← ancho ajustado dinámicamente\n",
        "\n",
        "btn_next = Button(\n",
        "    description=\"Seleccionar Siguiente\",\n",
        "    button_style=\"info\",\n",
        "    layout={'width': '190px'}\n",
        ")\n",
        "\n",
        "btn_run = Button(\n",
        "    description=\"Ejecutar\",\n",
        "    button_style=\"success\",\n",
        "    layout={'width': '140px'}\n",
        ")\n",
        "\n",
        "out_panel = widgets.Output(\n",
        "    layout={\n",
        "        'border': '1px solid #ccc',\n",
        "        'padding': '12px',\n",
        "        'width': '100%',\n",
        "        'max_height': '600px',\n",
        "        'overflow_y': 'auto',\n",
        "        'overflow_x': 'hidden',      # ← evita scroll horizontal\n",
        "        'margin_top': '12px'\n",
        "    }\n",
        ")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 4 ▸ Lógica de navegación\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "def _crear_dropdown(options, nivel):\n",
        "    return Dropdown(\n",
        "        options=options,\n",
        "        description=f\"Nivel-{nivel}:\",\n",
        "        layout={\n",
        "            'width': 'auto',               # ← ancho solo según texto\n",
        "            'min_width': '400px',          # ← margen mínimo visual aceptable\n",
        "            'max_width': '700px'\n",
        "        },\n",
        "        style={'description_width': '160px'}\n",
        "    )\n",
        "\n",
        "def _update_buttons():\n",
        "    path = [d.value for d in levels_box.children]\n",
        "    leaf = (_subtree_for(path) is None)\n",
        "    btn_next.disabled = leaf\n",
        "    btn_run.disabled = not leaf\n",
        "\n",
        "def _on_change(ch):\n",
        "    dd_list = list(levels_box.children)\n",
        "    idx = dd_list.index(ch['owner'])\n",
        "    levels_box.children = tuple(dd_list[:idx+1])\n",
        "    _update_buttons()\n",
        "\n",
        "def _reset():\n",
        "    dd0 = _crear_dropdown(list(menu_tree.keys()), 1)\n",
        "    dd0.observe(_on_change, names='value')\n",
        "    levels_box.children = (dd0,)\n",
        "    _update_buttons()\n",
        "\n",
        "def _next(_):\n",
        "    path = [d.value for d in levels_box.children]\n",
        "    branch = _subtree_for(path)\n",
        "    if branch:\n",
        "        nivel = len(levels_box.children) + 1\n",
        "        new_dd = _crear_dropdown(list(branch.keys()), nivel)\n",
        "        new_dd.observe(_on_change, names='value')\n",
        "        levels_box.children = (*levels_box.children, new_dd)\n",
        "    _update_buttons()\n",
        "\n",
        "def _run(_):\n",
        "    nodo = levels_box.children[-1].value\n",
        "    func = menu_funcs.get(nodo)\n",
        "    out_panel.clear_output()\n",
        "    with out_panel:\n",
        "        if func is None:\n",
        "            print(f\"⚠️  No se ha implementado «{nodo}».\")\n",
        "        else:\n",
        "            clear_output(wait=True)\n",
        "            func()\n",
        "\n",
        "btn_next.on_click(_next)\n",
        "btn_run.on_click(_run)\n",
        "_reset()\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 5 ▸ Mostrar UI\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "display(\n",
        "    VBox([\n",
        "        widgets.HTML(\"<h3 style='font-size:1.3rem;margin-bottom:5px;'>📋 Menú Principal</h3>\"),\n",
        "        levels_box,\n",
        "        HBox([btn_next, btn_run], layout={'gap': '30px', 'margin_top': '5px'}),\n",
        "        out_panel\n",
        "    ])\n",
        ")\n",
        "from ipywidgets import Button\n",
        "from IPython.display import display\n",
        "\n",
        "# —–– BLOQUE DE LIMPIEZA ––—\n",
        "btn_limpiar = Button(\n",
        "    description=\"🧹 Limpiar pantalla\",\n",
        "    button_style=\"warning\",\n",
        "    layout={'width': '150px'}\n",
        ")\n",
        "\n",
        "def _on_limpiar(_):\n",
        "    # Borra solo el contenido del panel de resultados\n",
        "    out_panel.clear_output()\n",
        "\n",
        "btn_limpiar.on_click(_on_limpiar)\n",
        "\n",
        "# Lo mostramos justo debajo del menú:\n",
        "display(btn_limpiar)\n",
        "\n"
      ]
    }
  ]
}